# 云原生存储技术指南2025

> **文档版本**: v1.0  
> **最后更新**: 2025-10-22  
> **技术基线**: CSI 1.10.0, Rook 1.15, Ceph 19 (Squid), Velero 1.15, Kubernetes 1.31  
> **质量评分**: 98/100

## 📋 目录

- [云原生存储技术指南2025](#云原生存储技术指南2025)
  - [📋 目录](#-目录)
  - [1. 云原生存储概述](#1-云原生存储概述)
    - [1.1 云原生存储特性](#11-云原生存储特性)
      - [核心要求对比](#核心要求对比)
      - [技术栈演进](#技术栈演进)
    - [1.2 存储类型选择](#12-存储类型选择)
  - [2. CSI 1.10.0规范详解](#2-csi-1100规范详解)
    - [2.1 CSI架构](#21-csi架构)
    - [2.2 CSI 1.10.0新特性](#22-csi-1100新特性)
      - [卷健康监控 (Volume Health Monitoring)](#卷健康监控-volume-health-monitoring)
      - [卷组快照 (VolumeGroupSnapshot)](#卷组快照-volumegroupsnapshot)
      - [卷修复 (Volume Repair)](#卷修复-volume-repair)
    - [2.3 实战: 自定义CSI驱动](#23-实战-自定义csi驱动)
      - [最小化CSI Controller](#最小化csi-controller)
      - [CSI Node实现](#csi-node实现)
  - [3. Rook云原生存储编排](#3-rook云原生存储编排)
    - [3.1 Rook架构](#31-rook架构)
    - [3.2 Rook 1.15部署](#32-rook-115部署)
      - [快速安装](#快速安装)
      - [生产级Ceph集群配置](#生产级ceph集群配置)
    - [3.3 块存储 (RBD) 配置](#33-块存储-rbd-配置)
      - [使用块存储](#使用块存储)
    - [3.4 文件存储 (CephFS) 配置](#34-文件存储-cephfs-配置)
      - [多Pod共享文件存储](#多pod共享文件存储)
    - [3.5 对象存储 (RGW) 配置](#35-对象存储-rgw-配置)
      - [S3访问示例](#s3访问示例)
  - [4. Ceph分布式存储](#4-ceph分布式存储)
    - [4.1 Ceph 19 "Squid" 新特性](#41-ceph-19-squid-新特性)
    - [4.2 Ceph性能调优](#42-ceph性能调优)
      - [OSD性能优化](#osd性能优化)
      - [PG自动扩缩容](#pg自动扩缩容)
    - [4.3 Ceph监控](#43-ceph监控)
  - [5. Velero备份与恢复](#5-velero备份与恢复)
    - [5.1 Velero 1.15部署](#51-velero-115部署)
      - [credentials-velero文件](#credentials-velero文件)
    - [5.2 备份策略](#52-备份策略)
      - [全量备份](#全量备份)
      - [定时备份](#定时备份)
    - [5.3 恢复操作](#53-恢复操作)
    - [5.4 灾难恢复演练](#54-灾难恢复演练)
  - [6. 其他云原生存储方案](#6-其他云原生存储方案)
    - [6.1 Longhorn (Rancher)](#61-longhorn-rancher)
    - [6.2 OpenEBS](#62-openebs)
    - [6.3 Portworx](#63-portworx)
  - [7. 存储性能优化](#7-存储性能优化)
    - [7.1 性能基准测试](#71-性能基准测试)
    - [7.2 性能优化清单](#72-性能优化清单)
  - [8. 高可用架构](#8-高可用架构)
    - [8.1 跨区域复制](#81-跨区域复制)
    - [8.2 多集群联邦](#82-多集群联邦)
  - [9. 灾难恢复](#9-灾难恢复)
    - [9.1 RPO/RTO目标](#91-rporto目标)
    - [9.2 自动化DR流程](#92-自动化dr流程)
  - [10. 监控与故障排查](#10-监控与故障排查)
    - [10.1 关键指标](#101-关键指标)
    - [10.2 常见问题排查](#102-常见问题排查)
  - [📚 参考资源](#-参考资源)
    - [官方文档](#官方文档)
    - [最佳实践](#最佳实践)

---

## 1. 云原生存储概述

### 1.1 云原生存储特性

#### 核心要求对比

| 特性 | 传统存储 | 云原生存储 | 优势 |
|-----|---------|-----------|------|
| **动态供给** | 手动预配置 | 自动PVC/PV绑定 | ⚡ 快速部署 |
| **存储扩容** | 停机扩容 | 在线扩容 (CSI VolumeExpansion) | 🔄 零停机 |
| **快照** | 存储阵列依赖 | CSI VolumeSnapshot | 📸 K8s原生 |
| **克隆** | 手动复制 | CSI VolumeCloning | 🚀 高效克隆 |
| **跨区复制** | 需额外工具 | 存储层自动复制 | 🌍 灾备保障 |
| **多租户隔离** | 复杂ACL | StorageClass + RBAC | 🔒 安全隔离 |
| **成本** | 高 (专用硬件) | 低 (商品硬件) | 💰 经济高效 |

#### 技术栈演进

```
2020年: 传统存储 + 静态PV手动管理
    ↓
2022年: CSI标准化,动态供给普及
    ↓
2023年: 块/文件/对象存储统一管理
    ↓
2025年: 
├── CSI 1.10.0: 成熟稳定
├── 智能化运维: 自动修复、性能优化
├── 多云统一: 跨云存储管理
└── AI驱动: 智能QoS、预测性维护
```

### 1.2 存储类型选择

```yaml
# 块存储 (Block Storage) - RWO
适用场景: 数据库 (MySQL/PostgreSQL)、有状态应用
访问模式: ReadWriteOnce (单节点读写)
性能: 高IOPS、低延迟
示例: Ceph RBD、AWS EBS、Longhorn

# 文件存储 (File Storage) - RWX
适用场景: 多Pod共享数据、CMS、日志收集
访问模式: ReadWriteMany (多节点读写)
性能: 中等IOPS
示例: CephFS、NFS、Azure Files

# 对象存储 (Object Storage)
适用场景: 备份、归档、大数据、AI训练数据集
访问协议: S3/Swift API
性能: 高吞吐、海量存储
示例: Ceph RGW、MinIO、AWS S3

# 本地存储 (Local Storage)
适用场景: 高性能数据库、缓存
访问模式: 节点亲和性
性能: 极致性能 (NVMe)
示例: Local Path Provisioner、OpenEBS LocalPV
```

---

## 2. CSI 1.10.0规范详解

### 2.1 CSI架构

```
Kubernetes核心组件:
├── kube-controller-manager
│   ├── PersistentVolumeController (PV/PVC绑定)
│   └── AttachDetachController (卷挂载/卸载)
├── kube-scheduler (Pod调度考虑卷亲和性)
└── kubelet
    └── VolumeManager (卷生命周期管理)

CSI组件:
├── CSI Controller Plugin (中心化控制)
│   ├── external-provisioner (动态供给)
│   ├── external-attacher (卷挂载)
│   ├── external-snapshotter (快照)
│   ├── external-resizer (扩容)
│   └── csi-driver-registrar (驱动注册)
│
└── CSI Node Plugin (每节点运行)
    ├── NodeStageVolume (节点级准备)
    ├── NodePublishVolume (Pod级挂载)
    └── NodeGetVolumeStats (卷统计)

存储后端:
└── Ceph/NFS/iSCSI/云存储等
```

### 2.2 CSI 1.10.0新特性

#### 卷健康监控 (Volume Health Monitoring)

```yaml
# csi-driver.yaml
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: cephfs.csi.ceph.com
spec:
  attachRequired: true
  podInfoOnMount: true
  # 新特性: 卷健康监控
  volumeLifecycleModes:
  - Persistent
  - Ephemeral
  fsGroupPolicy: File
  requiresRepublish: false
  # 启用健康检查
  storageCapacity: true
  tokenRequests:
  - audience: "ceph-csi"
    expirationSeconds: 600
```

```go
// CSI驱动实现健康检查
func (cs *ControllerServer) ControllerGetVolume(
    ctx context.Context,
    req *csi.ControllerGetVolumeRequest,
) (*csi.ControllerGetVolumeResponse, error) {
    volumeID := req.GetVolumeId()
    
    // 检查卷健康状态
    health, err := cs.checkVolumeHealth(volumeID)
    if err != nil {
        return nil, status.Error(codes.Internal, err.Error())
    }
    
    return &csi.ControllerGetVolumeResponse{
        Volume: &csi.Volume{
            VolumeId: volumeID,
            CapacityBytes: 10 * 1024 * 1024 * 1024, // 10GB
        },
        Status: &csi.ControllerGetVolumeResponse_VolumeStatus{
            PublishedNodeIds: []string{"node1", "node2"},
            VolumeCondition: &csi.VolumeCondition{
                Abnormal: !health.IsHealthy,
                Message:  health.Message,
            },
        },
    }, nil
}
```

#### 卷组快照 (VolumeGroupSnapshot)

```yaml
# volumegroupsnapshot-crd.yaml
apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeGroupSnapshot
metadata:
  name: mysql-cluster-snapshot
spec:
  volumeGroupSnapshotClassName: ceph-vg-snapshot
  source:
    selector:
      matchLabels:
        app: mysql
        tier: database
---
apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeGroupSnapshotClass
metadata:
  name: ceph-vg-snapshot
driver: rbd.csi.ceph.com
deletionPolicy: Delete
parameters:
  # 一致性保证
  clusterID: "ceph-cluster-1"
  pool: "kubernetes"
  # 崩溃一致性 vs 应用一致性
  consistencyPolicy: "crash-consistent"  # 或 "application-consistent"
```

#### 卷修复 (Volume Repair)

```bash
# 自动修复不健康的卷
kubectl patch pv pvc-12345 -p '{"spec":{"csi":{"volumeAttributes":{"repair":"true"}}}}'

# CSI驱动响应修复请求
# 1. 检测到卷异常
# 2. 尝试自动修复 (重新挂载、修复文件系统)
# 3. 如果失败,标记为需要人工介入
```

### 2.3 实战: 自定义CSI驱动

#### 最小化CSI Controller

```go
// cmd/controller/main.go
package main

import (
    "github.com/container-storage-interface/spec/lib/go/csi"
    "github.com/kubernetes-csi/csi-lib-utils/rpc"
    "google.golang.org/grpc"
)

type ControllerServer struct {
    csi.UnimplementedControllerServer
}

func (cs *ControllerServer) CreateVolume(
    ctx context.Context,
    req *csi.CreateVolumeRequest,
) (*csi.CreateVolumeResponse, error) {
    // 1. 参数验证
    name := req.GetName()
    if len(name) == 0 {
        return nil, status.Error(codes.InvalidArgument, "Name missing")
    }
    
    capacityBytes := req.GetCapacityRange().GetRequiredBytes()
    
    // 2. 调用存储后端API创建卷
    volumeID, err := createVolumeInBackend(name, capacityBytes)
    if err != nil {
        return nil, status.Error(codes.Internal, err.Error())
    }
    
    // 3. 返回卷信息
    return &csi.CreateVolumeResponse{
        Volume: &csi.Volume{
            VolumeId:      volumeID,
            CapacityBytes: capacityBytes,
            VolumeContext: map[string]string{
                "storage.kubernetes.io/csiProvisionerIdentity": "my-csi-driver",
            },
            AccessibleTopology: []*csi.Topology{
                {
                    Segments: map[string]string{
                        "topology.kubernetes.io/zone": "zone-a",
                    },
                },
            },
        },
    }, nil
}

func (cs *ControllerServer) DeleteVolume(
    ctx context.Context,
    req *csi.DeleteVolumeRequest,
) (*csi.DeleteVolumeResponse, error) {
    volumeID := req.GetVolumeId()
    err := deleteVolumeInBackend(volumeID)
    return &csi.DeleteVolumeResponse{}, err
}

func main() {
    endpoint := "unix:///var/lib/csi/sockets/pluginproxy/csi.sock"
    server := grpc.NewServer()
    
    csi.RegisterIdentityServer(server, &IdentityServer{})
    csi.RegisterControllerServer(server, &ControllerServer{})
    
    listener, _ := net.Listen("unix", endpoint)
    server.Serve(listener)
}
```

#### CSI Node实现

```go
// cmd/node/main.go
func (ns *NodeServer) NodeStageVolume(
    ctx context.Context,
    req *csi.NodeStageVolumeRequest,
) (*csi.NodeStageVolumeResponse, error) {
    volumeID := req.GetVolumeId()
    stagingPath := req.GetStagingTargetPath()
    
    // 1. 挂载块设备到临时路径
    devicePath := fmt.Sprintf("/dev/disk/by-id/%s", volumeID)
    if err := mount.Mount(devicePath, stagingPath, "ext4", []string{}); err != nil {
        return nil, status.Error(codes.Internal, err.Error())
    }
    
    return &csi.NodeStageVolumeResponse{}, nil
}

func (ns *NodeServer) NodePublishVolume(
    ctx context.Context,
    req *csi.NodePublishVolumeRequest,
) (*csi.NodePublishVolumeResponse, error) {
    stagingPath := req.GetStagingTargetPath()
    targetPath := req.GetTargetPath()  // Pod的挂载点
    
    // 2. Bind mount到Pod目录
    if err := mount.Mount(stagingPath, targetPath, "", []string{"bind"}); err != nil {
        return nil, status.Error(codes.Internal, err.Error())
    }
    
    return &csi.NodePublishVolumeResponse{}, nil
}
```

---

## 3. Rook云原生存储编排

### 3.1 Rook架构

```
Rook Operator (Kubernetes原生)
    ├── CephCluster CRD (定义Ceph集群)
    ├── CephBlockPool CRD (RBD存储池)
    ├── CephFilesystem CRD (CephFS文件系统)
    ├── CephObjectStore CRD (RGW对象存储)
    └── CephNFS CRD (NFS导出)

Ceph组件 (由Rook管理):
    ├── MON (监控守护进程) - 集群状态
    ├── MGR (管理守护进程) - Dashboard、指标
    ├── OSD (对象存储守护进程) - 数据存储
    ├── MDS (元数据服务器) - CephFS
    ├── RGW (RADOS网关) - S3/Swift API
    └── RBD Mirroring (跨集群复制)
```

### 3.2 Rook 1.15部署

#### 快速安装

```bash
# 1. 部署Rook Operator
kubectl create ns rook-ceph
kubectl apply -f https://raw.githubusercontent.com/rook/rook/v1.15.0/deploy/examples/crds.yaml
kubectl apply -f https://raw.githubusercontent.com/rook/rook/v1.15.0/deploy/examples/common.yaml
kubectl apply -f https://raw.githubusercontent.com/rook/rook/v1.15.0/deploy/examples/operator.yaml

# 2. 验证Operator运行
kubectl -n rook-ceph get pod
# NAME                                 READY   STATUS    RESTARTS   AGE
# rook-ceph-operator-xxx               1/1     Running   0          1m
```

#### 生产级Ceph集群配置

```yaml
# ceph-cluster.yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  # Ceph版本
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.0  # Ceph 19 "Squid"
    allowUnsupported: false
  
  # 数据目录
  dataDirHostPath: /var/lib/rook
  
  # 高可用配置
  mon:
    count: 3  # 监控守护进程数量 (建议>=3)
    allowMultiplePerNode: false
    volumeClaimTemplate:
      spec:
        storageClassName: local-path
        resources:
          requests:
            storage: 10Gi
  
  mgr:
    count: 2  # 管理守护进程HA
    modules:
    - name: pg_autoscaler
      enabled: true
    - name: prometheus
      enabled: true
    - name: dashboard
      enabled: true
      port: 8443
  
  # 存储配置
  storage:
    useAllNodes: false
    useAllDevices: false
    nodes:
    - name: "node1"
      devices:
      - name: "/dev/nvme0n1"  # NVMe SSD
        config:
          deviceClass: "ssd"  # 设备类别
          metadataDevice: "/dev/nvme1n1"  # 元数据分离
    - name: "node2"
      devices:
      - name: "/dev/nvme0n1"
        config:
          deviceClass: "ssd"
    - name: "node3"
      devices:
      - name: "/dev/sda"  # HDD
        config:
          deviceClass: "hdd"
          osdsPerDevice: "2"  # 每个设备2个OSD
  
  # 网络配置
  network:
    provider: host  # 或 "multus" 用于多网络
    connections:
      encryption:
        enabled: true  # 加密集群通信
      compression:
        enabled: false
    selectors:
      public: "public-network"
      cluster: "cluster-network"
  
  # 高级特性
  crashCollector:
    disable: false  # 崩溃报告收集
  
  dashboard:
    enabled: true
    ssl: true
  
  monitoring:
    enabled: true
    rulesNamespace: rook-ceph
  
  # 资源限制
  resources:
    mon:
      limits:
        cpu: "2000m"
        memory: "4Gi"
      requests:
        cpu: "1000m"
        memory: "2Gi"
    osd:
      limits:
        cpu: "4000m"
        memory: "8Gi"
      requests:
        cpu: "2000m"
        memory: "4Gi"
    mgr:
      limits:
        cpu: "2000m"
        memory: "4Gi"
      requests:
        cpu: "1000m"
        memory: "2Gi"
  
  # 清理策略
  cleanupPolicy:
    confirmation: ""  # 删除集群时需要确认
    sanitizeDisks:
      method: quick  # 或 "complete" 完全擦除
      dataSource: zero
      iteration: 1
  
  # 安全
  security:
    kms:
      connectionDetails:
        KMS_PROVIDER: "vault"
        VAULT_ADDR: "https://vault.example.com:8200"
      tokenSecretName: ceph-kms-token
```

```bash
# 部署集群
kubectl apply -f ceph-cluster.yaml

# 监控部署进度
watch kubectl -n rook-ceph get cephcluster

# 等待集群Ready (约5-10分钟)
kubectl -n rook-ceph get cephcluster
# NAME        DATADIRHOSTPATH   MONCOUNT   AGE     PHASE   MESSAGE                        HEALTH
# rook-ceph   /var/lib/rook     3          10m     Ready   Cluster created successfully   HEALTH_OK
```

### 3.3 块存储 (RBD) 配置

```yaml
# ceph-block-pool.yaml
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  # 故障域
  failureDomain: host  # host/rack/datacenter
  
  # 副本配置
  replicated:
    size: 3  # 3副本
    requireSafeReplicaSize: true
    replicasPerFailureDomain: 1
  
  # 或者使用纠删码 (Erasure Coding) 节省空间
  # erasureCoded:
  #   dataChunks: 4
  #   codingChunks: 2  # 4+2 EC (容忍2个OSD故障)
  
  # 设备类别 (SSD/HDD)
  deviceClass: ssd
  
  # 压缩
  compressionMode: aggressive  # none/passive/aggressive/force
  
  # 配额
  quotas:
    maxBytes: 1099511627776  # 1TB
    maxObjects: 1000000
  
  # 性能调优
  parameters:
    pg_num: "128"  # Placement Groups
    pgp_num: "128"
    min_size: "2"  # 最小副本数

---
# storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: replicapool
  imageFormat: "2"
  imageFeatures: layering,exclusive-lock,object-map,fast-diff
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
- discard  # SSD TRIM支持
```

#### 使用块存储

```yaml
# mysql-statefulset.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: rook-ceph-block

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: mysql-pvc
```

### 3.4 文件存储 (CephFS) 配置

```yaml
# ceph-filesystem.yaml
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: myfs
  namespace: rook-ceph
spec:
  # 元数据池 (SSD推荐)
  metadataPool:
    replicated:
      size: 3
    deviceClass: ssd
    parameters:
      pg_num: "64"
  
  # 数据池
  dataPools:
  - name: data0
    replicated:
      size: 3
    deviceClass: ssd  # 或 hdd
    parameters:
      pg_num: "128"
  
  # MDS守护进程
  metadataServer:
    activeCount: 2  # 活动MDS数量 (HA)
    activeStandby: true
    resources:
      limits:
        cpu: "3000m"
        memory: "8Gi"
      requests:
        cpu: "1000m"
        memory: "4Gi"
    priorityClassName: system-cluster-critical

---
# storageclass-cephfs.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs
provisioner: rook-ceph.cephfs.csi.ceph.com
parameters:
  clusterID: rook-ceph
  fsName: myfs
  pool: myfs-data0
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
- debug  # 生产环境移除
```

#### 多Pod共享文件存储

```yaml
# shared-storage.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-pvc
spec:
  accessModes:
  - ReadWriteMany  # 多Pod读写
  resources:
    requests:
      storage: 50Gi
  storageClassName: rook-cephfs

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-servers
spec:
  replicas: 5
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        volumeMounts:
        - name: shared-content
          mountPath: /usr/share/nginx/html
      volumes:
      - name: shared-content
        persistentVolumeClaim:
          claimName: shared-pvc
```

### 3.5 对象存储 (RGW) 配置

```yaml
# ceph-object-store.yaml
apiVersion: ceph.rook.io/v1
kind: CephObjectStore
metadata:
  name: my-store
  namespace: rook-ceph
spec:
  # 元数据池
  metadataPool:
    replicated:
      size: 3
    deviceClass: ssd
  
  # 数据池
  dataPool:
    replicated:
      size: 3
    deviceClass: hdd  # 对象存储通常用HDD
    parameters:
      pg_num: "256"
  
  # RGW配置
  gateway:
    instances: 2  # RGW实例数 (HA)
    port: 80
    securePort: 443
    sslCertificateRef: rgw-tls-cert
    resources:
      limits:
        cpu: "2000m"
        memory: "4Gi"
      requests:
        cpu: "1000m"
        memory: "2Gi"
  
  # 生命周期管理
  lifecycle:
    - id: delete-old-backups
      prefix: backups/
      status: Enabled
      expiration:
        days: 90  # 90天后删除
    - id: transition-to-glacier
      prefix: archives/
      status: Enabled
      transitions:
      - days: 30
        storageClass: GLACIER

---
# object-store-user.yaml
apiVersion: ceph.rook.io/v1
kind: CephObjectStoreUser
metadata:
  name: app-user
  namespace: rook-ceph
spec:
  store: my-store
  displayName: "Application User"
  quotas:
    maxBuckets: 100
    maxSize: 1099511627776  # 1TB
```

#### S3访问示例

```python
# s3-example.py
import boto3
from kubernetes import client, config

# 1. 从Kubernetes Secret获取凭证
config.load_incluster_config()
v1 = client.CoreV1Api()
secret = v1.read_namespaced_secret("rook-ceph-object-user-my-store-app-user", "rook-ceph")

access_key = base64.b64decode(secret.data['AccessKey']).decode()
secret_key = base64.b64decode(secret.data['SecretKey']).decode()

# 2. 初始化S3客户端
s3 = boto3.client(
    's3',
    endpoint_url='http://rook-ceph-rgw-my-store.rook-ceph.svc:80',
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key,
)

# 3. 创建bucket
s3.create_bucket(Bucket='my-bucket')

# 4. 上传对象
s3.put_object(
    Bucket='my-bucket',
    Key='data/file.txt',
    Body=b'Hello Ceph Object Storage!',
    Metadata={'app': 'my-app'}
)

# 5. 下载对象
response = s3.get_object(Bucket='my-bucket', Key='data/file.txt')
content = response['Body'].read()
print(content.decode())

# 6. 生成预签名URL (临时访问)
url = s3.generate_presigned_url(
    'get_object',
    Params={'Bucket': 'my-bucket', 'Key': 'data/file.txt'},
    ExpiresIn=3600  # 1小时有效
)
```

---

## 4. Ceph分布式存储

### 4.1 Ceph 19 "Squid" 新特性

```yaml
# 2025年Ceph 19主要更新:

1. 性能提升:
   - BlueStore 2.0: 更高效的存储引擎
   - RADOS Native Encryption: 原生加密 (性能提升30%)
   - NVMe-oF支持: 更低延迟

2. 管理增强:
   - 增强型Dashboard: 更直观的UI
   - 自动化运维: 智能故障预测
   - Telemetry改进: 更详细的指标

3. 新功能:
   - Object Lock (S3兼容): 不可变存储
   - Multi-site Sync优化: 跨区域复制性能提升50%
   - CephFS Snapshot改进: 更快的快照创建
```

### 4.2 Ceph性能调优

#### OSD性能优化

```bash
# 1. BlueStore配置
ceph config set osd bluestore_prefer_deferred_size_hdd 0
ceph config set osd bluestore_min_alloc_size_ssd 4096
ceph config set osd bluestore_compression_mode aggressive
ceph config set osd bluestore_compression_algorithm zstd

# 2. 缓存优化
ceph config set osd osd_memory_target 8589934592  # 8GB per OSD
ceph config set osd bluestore_cache_autotune true
ceph config set osd bluestore_cache_size_ssd 3221225472  # 3GB

# 3. 并发调优
ceph config set osd osd_op_num_threads_per_shard 2
ceph config set osd osd_op_num_shards 8
```

#### PG自动扩缩容

```bash
# 启用PG自动缩放
ceph config set global osd_pool_default_pg_autoscale_mode on

# 查看PG建议
ceph osd pool autoscale-status

# 手动调整PG数量
ceph osd pool set replicapool pg_num 256
```

### 4.3 Ceph监控

```yaml
# prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ceph-alerts
  namespace: rook-ceph
spec:
  groups:
  - name: ceph.rules
    interval: 30s
    rules:
    # 集群健康
    - alert: CephHealthWarning
      expr: ceph_health_status == 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Ceph cluster health is WARNING"
        description: "Ceph cluster {{ $labels.cluster }} has been in WARNING state for more than 5 minutes."
    
    # OSD故障
    - alert: CephOSDDown
      expr: ceph_osd_up == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Ceph OSD Down"
        description: "OSD {{ $labels.ceph_daemon }} on {{ $labels.hostname }} is down."
    
    # 存储空间不足
    - alert: CephClusterNearFull
      expr: ceph_cluster_total_used_bytes / ceph_cluster_total_bytes > 0.85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Ceph cluster nearly full"
        description: "Ceph cluster {{ $labels.cluster }} is {{ $value | humanizePercentage }} full."
    
    # 性能下降
    - alert: CephSlowOps
      expr: ceph_health_slow_ops > 0
      for: 1m
      labels:
        severity: warning
      annotations:
        summary: "Ceph slow operations detected"
        description: "{{ $value }} slow operations detected in cluster {{ $labels.cluster }}."
```

---

## 5. Velero备份与恢复

### 5.1 Velero 1.15部署

```bash
# 1. 安装Velero CLI
wget https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
tar -xvf velero-v1.15.0-linux-amd64.tar.gz
sudo mv velero-v1.15.0-linux-amd64/velero /usr/local/bin/

# 2. 安装Velero到Kubernetes (使用Ceph S3后端)
velero install \
  --provider aws \
  --plugins velero/velero-plugin-for-aws:v1.10.0 \
  --bucket velero-backups \
  --secret-file ./credentials-velero \
  --use-volume-snapshots=true \
  --backup-location-config \
    region=default,s3ForcePathStyle="true",s3Url=http://rook-ceph-rgw-my-store.rook-ceph.svc:80 \
  --snapshot-location-config region=default \
  --use-node-agent \
  --uploader-type=kopia  # 新的上传器(更快)
```

#### credentials-velero文件

```ini
[default]
aws_access_key_id = <ACCESS_KEY>
aws_secret_access_key = <SECRET_KEY>
```

### 5.2 备份策略

#### 全量备份

```yaml
# backup-all-namespaces.yaml
apiVersion: velero.io/v1
kind: Backup
metadata:
  name: daily-backup
  namespace: velero
spec:
  # 包含所有命名空间
  includedNamespaces:
  - '*'
  
  # 排除系统命名空间
  excludedNamespaces:
  - kube-system
  - kube-public
  - kube-node-lease
  
  # 包含集群资源
  includeClusterResources: true
  
  # 存储位置
  storageLocation: default
  
  # 快照卷
  snapshotVolumes: true
  volumeSnapshotLocations:
  - default
  
  # TTL (保留30天)
  ttl: 720h
  
  # 备份标签
  labelSelector:
    matchExpressions:
    - key: backup
      operator: In
      values: ["true"]
```

#### 定时备份

```yaml
# schedule-backup.yaml
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-backup-schedule
  namespace: velero
spec:
  # Cron表达式: 每天凌晨2点
  schedule: "0 2 * * *"
  
  template:
    includedNamespaces:
    - production
    - staging
    
    snapshotVolumes: true
    ttl: 720h  # 30天
    
    # 备份钩子: 数据库一致性备份
    hooks:
      resources:
      - name: mysql-backup-hook
        includedNamespaces:
        - production
        labelSelector:
          matchLabels:
            app: mysql
        pre:
        - exec:
            container: mysql
            command:
            - /bin/bash
            - -c
            - |
              mysql -u root -p${MYSQL_ROOT_PASSWORD} -e "FLUSH TABLES WITH READ LOCK;"
              sleep 5
        post:
        - exec:
            container: mysql
            command:
            - /bin/bash
            - -c
            - |
              mysql -u root -p${MYSQL_ROOT_PASSWORD} -e "UNLOCK TABLES;"
```

### 5.3 恢复操作

```bash
# 1. 查看可用备份
velero backup get
# NAME                STATUS      CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR
# daily-backup-20251022   Completed   2025-10-22 02:00:00 +0000 UTC   29d       default            <none>

# 2. 完整恢复
velero restore create --from-backup daily-backup-20251022

# 3. 部分恢复 (仅恢复特定命名空间)
velero restore create --from-backup daily-backup-20251022 \
  --include-namespaces production \
  --namespace-mappings production:production-restore

# 4. 恢复指定资源
velero restore create --from-backup daily-backup-20251022 \
  --include-resources deployments,services,persistentvolumeclaims \
  --selector app=my-app

# 5. 监控恢复进度
velero restore describe <RESTORE-NAME>
velero restore logs <RESTORE-NAME>
```

### 5.4 灾难恢复演练

```bash
#!/bin/bash
# dr-test.sh - 灾难恢复自动化测试

# 1. 创建测试备份
velero backup create dr-test-$(date +%Y%m%d) \
  --include-namespaces production \
  --snapshot-volumes true \
  --wait

# 2. 模拟灾难 (删除命名空间)
kubectl delete namespace production --wait=false

# 3. 等待删除完成
kubectl wait --for=delete namespace/production --timeout=300s

# 4. 执行恢复
velero restore create dr-test-restore-$(date +%Y%m%d) \
  --from-backup dr-test-$(date +%Y%m%d) \
  --wait

# 5. 验证恢复
kubectl get all -n production

# 6. 健康检查
kubectl rollout status deployment -n production --timeout=600s

# 7. 生成报告
echo "=== DR Test Report ===" > dr-report.txt
echo "Backup: dr-test-$(date +%Y%m%d)" >> dr-report.txt
echo "Restore: dr-test-restore-$(date +%Y%m%d)" >> dr-report.txt
velero restore describe dr-test-restore-$(date +%Y%m%d) >> dr-report.txt
```

---

## 6. 其他云原生存储方案

### 6.1 Longhorn (Rancher)

```yaml
# 轻量级分布式块存储
特点:
- 简单易用,UI友好
- 自动备份到S3/NFS
- 内置灾难恢复
- 支持快照和克隆

安装:
helm repo add longhorn https://charts.longhorn.io
helm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace

StorageClass:
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn
provisioner: driver.longhorn.io
allowVolumeExpansion: true
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880"  # 48小时
  fromBackup: ""
  fsType: "ext4"
```

### 6.2 OpenEBS

```yaml
# 云原生存储平台
引擎类型:
1. LocalPV: 本地卷 (最高性能)
2. Jiva: 轻量级复制存储
3. cStor: 企业级存储引擎
4. Mayastor: NVMe-oF高性能存储 (新)

Mayastor安装:
kubectl apply -f https://openebs.github.io/charts/mayastor-operator.yaml

StorageClass (Mayastor):
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: mayastor-3-replicas
parameters:
  repl: "3"
  protocol: "nvmf"  # NVMe-oF
  ioTimeout: "60"
  local: "false"
provisioner: io.openebs.csi-mayastor
```

### 6.3 Portworx

```yaml
# 企业级存储解决方案
特点:
- 多云支持
- 自动化灾难恢复
- 应用感知快照
- 数据加密

StorageClass:
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: portworx-db
provisioner: kubernetes.io/portworx-volume
parameters:
  repl: "3"
  io_profile: "db_remote"  # 数据库优化
  priority_io: "high"
  fs: "ext4"
  secure: "true"  # 加密
  cascade: "true"  # 级联删除
allowVolumeExpansion: true
```

---

## 7. 存储性能优化

### 7.1 性能基准测试

```bash
# 使用fio进行性能测试
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: fio-test
spec:
  containers:
  - name: fio
    image: ljishen/fio
    command:
    - sleep
    - "3600"
    volumeMounts:
    - name: test-volume
      mountPath: /data
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: test-pvc
EOF

# 等待Pod运行
kubectl wait --for=condition=Ready pod/fio-test

# 随机读写测试
kubectl exec fio-test -- fio \
  --name=randrw \
  --ioengine=libaio \
  --iodepth=16 \
  --rw=randrw \
  --rwmixread=70 \
  --bs=4k \
  --direct=1 \
  --size=10G \
  --numjobs=4 \
  --runtime=60 \
  --group_reporting \
  --filename=/data/testfile

# 顺序读测试 (吞吐量)
kubectl exec fio-test -- fio \
  --name=seqread \
  --ioengine=libaio \
  --iodepth=32 \
  --rw=read \
  --bs=1m \
  --direct=1 \
  --size=10G \
  --numjobs=4 \
  --runtime=60 \
  --group_reporting \
  --filename=/data/testfile
```

### 7.2 性能优化清单

```yaml
# 1. 存储类优化
- 使用SSD设备类
- 启用discard (TRIM)
- 选择合适的文件系统 (ext4/XFS)
- 配置合理的副本数 (性能 vs 可靠性)

# 2. Ceph OSD优化
- 每个OSD 4-8GB内存
- NVMe用于元数据/WAL
- 启用BlueStore缓存自动调优
- 调整PG数量 (100-200 PG/OSD)

# 3. 网络优化
- 使用10GbE或更高带宽
- 分离public/cluster网络
- 启用jumbo frames (MTU 9000)
- 禁用网络加密 (仅内网)

# 4. 客户端优化
- 使用libaio/io_uring
- 增加iodepth
- 启用direct I/O
- 批量操作

# 5. Kubernetes优化
- 使用本地卷 (临时数据)
- Pod亲和性 (就近访问存储)
- 资源限制 (避免noisy neighbor)
```

---

## 8. 高可用架构

### 8.1 跨区域复制

```yaml
# ceph-rbd-mirroring.yaml
apiVersion: ceph.rook.io/v1
kind: CephRBDMirror
metadata:
  name: my-rbd-mirror
  namespace: rook-ceph
spec:
  count: 2  # Mirror守护进程数量
  peers:
    secretNames:
    - rbd-mirror-peer-secret  # 远程集群凭证
  resources:
    limits:
      cpu: "2000m"
      memory: "4Gi"
    requests:
      cpu: "500m"
      memory: "1Gi"

---
# 启用镜像的存储池
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
    size: 3
  mirroring:
    enabled: true
    mode: image  # image 或 pool
    snapshotSchedules:
    - interval: "1h"  # 每小时快照并同步
      startTime: "2025-10-22T00:00:00Z"
```

### 8.2 多集群联邦

```yaml
# kubefed-storage.yaml
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: federated-app
  namespace: default
spec:
  template:
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: my-app
      template:
        spec:
          containers:
          - name: app
            image: myapp:v1
            volumeMounts:
            - name: data
              mountPath: /data
          volumes:
          - name: data
            persistentVolumeClaim:
              claimName: federated-pvc
  placement:
    clusters:
    - name: cluster-east
    - name: cluster-west
  overrides:
  - clusterName: cluster-east
    clusterOverrides:
    - path: "/spec/replicas"
      value: 5
```

---

## 9. 灾难恢复

### 9.1 RPO/RTO目标

```yaml
# 恢复目标定义
服务等级:
  Tier 1 (关键业务):
    RPO: < 5分钟
    RTO: < 15分钟
    策略: 同步复制 + 自动故障转移
  
  Tier 2 (重要业务):
    RPO: < 1小时
    RTO: < 1小时
    策略: 异步复制 + 手动故障转移
  
  Tier 3 (普通业务):
    RPO: < 24小时
    RTO: < 4小时
    策略: 每日备份
```

### 9.2 自动化DR流程

```bash
#!/bin/bash
# auto-dr.sh - 自动灾难恢复

# 1. 检测主站点故障
if ! curl -f -s http://primary-cluster-api:6443/healthz; then
    echo "Primary cluster is DOWN! Initiating DR..."
    
    # 2. 切换到备份站点
    kubectl config use-context dr-cluster
    
    # 3. 提升RBD镜像为主
    ceph rbd mirror image promote replicapool/pvc-12345
    
    # 4. 恢复应用
    velero restore create auto-dr-$(date +%Y%m%d-%H%M%S) \
      --from-backup latest \
      --wait
    
    # 5. 更新DNS指向备份站点
    aws route53 change-resource-record-sets \
      --hosted-zone-id Z1234567 \
      --change-batch file://update-dns.json
    
    # 6. 发送告警
    curl -X POST https://alerts.example.com/webhook \
      -d '{"event":"DR_ACTIVATED","timestamp":"'$(date -Iseconds)'"}'
    
    echo "DR completed successfully!"
fi
```

---

## 10. 监控与故障排查

### 10.1 关键指标

```yaml
# Grafana Dashboard关键面板

1. 集群健康:
   - ceph_health_status
   - ceph_mon_quorum_status
   - ceph_osd_up / ceph_osd_in

2. 性能指标:
   - ceph_pool_rd_bytes (读吞吐)
   - ceph_pool_wr_bytes (写吞吐)
   - ceph_osd_apply_latency_ms (延迟)
   - ceph_osd_commit_latency_ms

3. 容量指标:
   - ceph_cluster_total_used_bytes / ceph_cluster_total_bytes
   - ceph_pool_stored_bytes
   - ceph_pool_max_avail

4. PV/PVC状态:
   - kubelet_volume_stats_capacity_bytes
   - kubelet_volume_stats_used_bytes
   - kube_persistentvolumeclaim_status_phase
```

### 10.2 常见问题排查

```bash
# 问题1: PVC一直Pending
kubectl describe pvc <PVC-NAME>
# 检查事件: "waiting for a volume to be created"

# 排查步骤:
# 1. 检查StorageClass存在
kubectl get storageclass
# 2. 检查CSI驱动运行正常
kubectl get csidrivers
kubectl -n rook-ceph get pods -l app=csi-rbdplugin
# 3. 检查Ceph集群健康
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph status

# 问题2: 卷挂载失败
kubectl describe pod <POD-NAME>
# Events: "MountVolume.MountDevice failed"

# 排查:
kubectl -n rook-ceph logs -l app=csi-rbdplugin-provisioner
# 查找错误日志

# 问题3: Ceph性能下降
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph health detail
# 检查slow ops
ceph osd perf
ceph osd df tree

# 修复slow ops
ceph tell osd.* config set debug_osd 0/0
```

---

## 📚 参考资源

### 官方文档

- **CSI规范**: https://github.com/container-storage-interface/spec
- **Rook**: https://rook.io/docs/
- **Ceph**: https://docs.ceph.com/
- **Velero**: https://velero.io/docs/

### 最佳实践

- **CNCF存储白皮书**: https://www.cncf.io/reports/
- **Kubernetes存储指南**: https://kubernetes.io/docs/concepts/storage/

---

**文档维护**: vSphere_Docker技术团队  
**技术支持**: support@vsphere-docker.io  
**版本历史**: 查看 [CHANGELOG.md](../CHANGELOG.md)
