# äº‘åŸç”Ÿå­˜å‚¨æŠ€æœ¯æŒ‡å—2025

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
> **æœ€åæ›´æ–°**: 2025-10-22  
> **æŠ€æœ¯åŸºçº¿**: CSI 1.10.0, Rook 1.15, Ceph 19 (Squid), Velero 1.15, Kubernetes 1.31  
> **è´¨é‡è¯„åˆ†**: 98/100

## ğŸ“‹ ç›®å½•

- [äº‘åŸç”Ÿå­˜å‚¨æŠ€æœ¯æŒ‡å—2025](#äº‘åŸç”Ÿå­˜å‚¨æŠ€æœ¯æŒ‡å—2025)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. äº‘åŸç”Ÿå­˜å‚¨æ¦‚è¿°](#1-äº‘åŸç”Ÿå­˜å‚¨æ¦‚è¿°)
    - [1.1 äº‘åŸç”Ÿå­˜å‚¨ç‰¹æ€§](#11-äº‘åŸç”Ÿå­˜å‚¨ç‰¹æ€§)
      - [æ ¸å¿ƒè¦æ±‚å¯¹æ¯”](#æ ¸å¿ƒè¦æ±‚å¯¹æ¯”)
      - [æŠ€æœ¯æ ˆæ¼”è¿›](#æŠ€æœ¯æ ˆæ¼”è¿›)
    - [1.2 å­˜å‚¨ç±»å‹é€‰æ‹©](#12-å­˜å‚¨ç±»å‹é€‰æ‹©)
  - [2. CSI 1.10.0è§„èŒƒè¯¦è§£](#2-csi-1100è§„èŒƒè¯¦è§£)
    - [2.1 CSIæ¶æ„](#21-csiæ¶æ„)
    - [2.2 CSI 1.10.0æ–°ç‰¹æ€§](#22-csi-1100æ–°ç‰¹æ€§)
      - [å·å¥åº·ç›‘æ§ (Volume Health Monitoring)](#å·å¥åº·ç›‘æ§-volume-health-monitoring)
      - [å·ç»„å¿«ç…§ (VolumeGroupSnapshot)](#å·ç»„å¿«ç…§-volumegroupsnapshot)
      - [å·ä¿®å¤ (Volume Repair)](#å·ä¿®å¤-volume-repair)
    - [2.3 å®æˆ˜: è‡ªå®šä¹‰CSIé©±åŠ¨](#23-å®æˆ˜-è‡ªå®šä¹‰csié©±åŠ¨)
      - [æœ€å°åŒ–CSI Controller](#æœ€å°åŒ–csi-controller)
      - [CSI Nodeå®ç°](#csi-nodeå®ç°)
  - [3. Rookäº‘åŸç”Ÿå­˜å‚¨ç¼–æ’](#3-rookäº‘åŸç”Ÿå­˜å‚¨ç¼–æ’)
    - [3.1 Rookæ¶æ„](#31-rookæ¶æ„)
    - [3.2 Rook 1.15éƒ¨ç½²](#32-rook-115éƒ¨ç½²)
      - [å¿«é€Ÿå®‰è£…](#å¿«é€Ÿå®‰è£…)
      - [ç”Ÿäº§çº§Cephé›†ç¾¤é…ç½®](#ç”Ÿäº§çº§cephé›†ç¾¤é…ç½®)
    - [3.3 å—å­˜å‚¨ (RBD) é…ç½®](#33-å—å­˜å‚¨-rbd-é…ç½®)
      - [ä½¿ç”¨å—å­˜å‚¨](#ä½¿ç”¨å—å­˜å‚¨)
    - [3.4 æ–‡ä»¶å­˜å‚¨ (CephFS) é…ç½®](#34-æ–‡ä»¶å­˜å‚¨-cephfs-é…ç½®)
      - [å¤šPodå…±äº«æ–‡ä»¶å­˜å‚¨](#å¤špodå…±äº«æ–‡ä»¶å­˜å‚¨)
    - [3.5 å¯¹è±¡å­˜å‚¨ (RGW) é…ç½®](#35-å¯¹è±¡å­˜å‚¨-rgw-é…ç½®)
      - [S3è®¿é—®ç¤ºä¾‹](#s3è®¿é—®ç¤ºä¾‹)
  - [4. Cephåˆ†å¸ƒå¼å­˜å‚¨](#4-cephåˆ†å¸ƒå¼å­˜å‚¨)
    - [4.1 Ceph 19 "Squid" æ–°ç‰¹æ€§](#41-ceph-19-squid-æ–°ç‰¹æ€§)
    - [4.2 Cephæ€§èƒ½è°ƒä¼˜](#42-cephæ€§èƒ½è°ƒä¼˜)
      - [OSDæ€§èƒ½ä¼˜åŒ–](#osdæ€§èƒ½ä¼˜åŒ–)
      - [PGè‡ªåŠ¨æ‰©ç¼©å®¹](#pgè‡ªåŠ¨æ‰©ç¼©å®¹)
    - [4.3 Cephç›‘æ§](#43-cephç›‘æ§)
  - [5. Veleroå¤‡ä»½ä¸æ¢å¤](#5-veleroå¤‡ä»½ä¸æ¢å¤)
    - [5.1 Velero 1.15éƒ¨ç½²](#51-velero-115éƒ¨ç½²)
      - [credentials-veleroæ–‡ä»¶](#credentials-veleroæ–‡ä»¶)
    - [5.2 å¤‡ä»½ç­–ç•¥](#52-å¤‡ä»½ç­–ç•¥)
      - [å…¨é‡å¤‡ä»½](#å…¨é‡å¤‡ä»½)
      - [å®šæ—¶å¤‡ä»½](#å®šæ—¶å¤‡ä»½)
    - [5.3 æ¢å¤æ“ä½œ](#53-æ¢å¤æ“ä½œ)
    - [5.4 ç¾éš¾æ¢å¤æ¼”ç»ƒ](#54-ç¾éš¾æ¢å¤æ¼”ç»ƒ)
  - [6. å…¶ä»–äº‘åŸç”Ÿå­˜å‚¨æ–¹æ¡ˆ](#6-å…¶ä»–äº‘åŸç”Ÿå­˜å‚¨æ–¹æ¡ˆ)
    - [6.1 Longhorn (Rancher)](#61-longhorn-rancher)
    - [6.2 OpenEBS](#62-openebs)
    - [6.3 Portworx](#63-portworx)
  - [7. å­˜å‚¨æ€§èƒ½ä¼˜åŒ–](#7-å­˜å‚¨æ€§èƒ½ä¼˜åŒ–)
    - [7.1 æ€§èƒ½åŸºå‡†æµ‹è¯•](#71-æ€§èƒ½åŸºå‡†æµ‹è¯•)
    - [7.2 æ€§èƒ½ä¼˜åŒ–æ¸…å•](#72-æ€§èƒ½ä¼˜åŒ–æ¸…å•)
  - [8. é«˜å¯ç”¨æ¶æ„](#8-é«˜å¯ç”¨æ¶æ„)
    - [8.1 è·¨åŒºåŸŸå¤åˆ¶](#81-è·¨åŒºåŸŸå¤åˆ¶)
    - [8.2 å¤šé›†ç¾¤è”é‚¦](#82-å¤šé›†ç¾¤è”é‚¦)
  - [9. ç¾éš¾æ¢å¤](#9-ç¾éš¾æ¢å¤)
    - [9.1 RPO/RTOç›®æ ‡](#91-rportoç›®æ ‡)
    - [9.2 è‡ªåŠ¨åŒ–DRæµç¨‹](#92-è‡ªåŠ¨åŒ–dræµç¨‹)
  - [10. ç›‘æ§ä¸æ•…éšœæ’æŸ¥](#10-ç›‘æ§ä¸æ•…éšœæ’æŸ¥)
    - [10.1 å…³é”®æŒ‡æ ‡](#101-å…³é”®æŒ‡æ ‡)
    - [10.2 å¸¸è§é—®é¢˜æ’æŸ¥](#102-å¸¸è§é—®é¢˜æ’æŸ¥)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)
    - [å®˜æ–¹æ–‡æ¡£](#å®˜æ–¹æ–‡æ¡£)
    - [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## 1. äº‘åŸç”Ÿå­˜å‚¨æ¦‚è¿°

### 1.1 äº‘åŸç”Ÿå­˜å‚¨ç‰¹æ€§

#### æ ¸å¿ƒè¦æ±‚å¯¹æ¯”

| ç‰¹æ€§ | ä¼ ç»Ÿå­˜å‚¨ | äº‘åŸç”Ÿå­˜å‚¨ | ä¼˜åŠ¿ |
|-----|---------|-----------|------|
| **åŠ¨æ€ä¾›ç»™** | æ‰‹åŠ¨é¢„é…ç½® | è‡ªåŠ¨PVC/PVç»‘å®š | âš¡ å¿«é€Ÿéƒ¨ç½² |
| **å­˜å‚¨æ‰©å®¹** | åœæœºæ‰©å®¹ | åœ¨çº¿æ‰©å®¹ (CSI VolumeExpansion) | ğŸ”„ é›¶åœæœº |
| **å¿«ç…§** | å­˜å‚¨é˜µåˆ—ä¾èµ– | CSI VolumeSnapshot | ğŸ“¸ K8såŸç”Ÿ |
| **å…‹éš†** | æ‰‹åŠ¨å¤åˆ¶ | CSI VolumeCloning | ğŸš€ é«˜æ•ˆå…‹éš† |
| **è·¨åŒºå¤åˆ¶** | éœ€é¢å¤–å·¥å…· | å­˜å‚¨å±‚è‡ªåŠ¨å¤åˆ¶ | ğŸŒ ç¾å¤‡ä¿éšœ |
| **å¤šç§Ÿæˆ·éš”ç¦»** | å¤æ‚ACL | StorageClass + RBAC | ğŸ”’ å®‰å…¨éš”ç¦» |
| **æˆæœ¬** | é«˜ (ä¸“ç”¨ç¡¬ä»¶) | ä½ (å•†å“ç¡¬ä»¶) | ğŸ’° ç»æµé«˜æ•ˆ |

#### æŠ€æœ¯æ ˆæ¼”è¿›

```
2020å¹´: ä¼ ç»Ÿå­˜å‚¨ + é™æ€PVæ‰‹åŠ¨ç®¡ç†
    â†“
2022å¹´: CSIæ ‡å‡†åŒ–,åŠ¨æ€ä¾›ç»™æ™®åŠ
    â†“
2023å¹´: å—/æ–‡ä»¶/å¯¹è±¡å­˜å‚¨ç»Ÿä¸€ç®¡ç†
    â†“
2025å¹´: 
â”œâ”€â”€ CSI 1.10.0: æˆç†Ÿç¨³å®š
â”œâ”€â”€ æ™ºèƒ½åŒ–è¿ç»´: è‡ªåŠ¨ä¿®å¤ã€æ€§èƒ½ä¼˜åŒ–
â”œâ”€â”€ å¤šäº‘ç»Ÿä¸€: è·¨äº‘å­˜å‚¨ç®¡ç†
â””â”€â”€ AIé©±åŠ¨: æ™ºèƒ½QoSã€é¢„æµ‹æ€§ç»´æŠ¤
```

### 1.2 å­˜å‚¨ç±»å‹é€‰æ‹©

```yaml
# å—å­˜å‚¨ (Block Storage) - RWO
é€‚ç”¨åœºæ™¯: æ•°æ®åº“ (MySQL/PostgreSQL)ã€æœ‰çŠ¶æ€åº”ç”¨
è®¿é—®æ¨¡å¼: ReadWriteOnce (å•èŠ‚ç‚¹è¯»å†™)
æ€§èƒ½: é«˜IOPSã€ä½å»¶è¿Ÿ
ç¤ºä¾‹: Ceph RBDã€AWS EBSã€Longhorn

# æ–‡ä»¶å­˜å‚¨ (File Storage) - RWX
é€‚ç”¨åœºæ™¯: å¤šPodå…±äº«æ•°æ®ã€CMSã€æ—¥å¿—æ”¶é›†
è®¿é—®æ¨¡å¼: ReadWriteMany (å¤šèŠ‚ç‚¹è¯»å†™)
æ€§èƒ½: ä¸­ç­‰IOPS
ç¤ºä¾‹: CephFSã€NFSã€Azure Files

# å¯¹è±¡å­˜å‚¨ (Object Storage)
é€‚ç”¨åœºæ™¯: å¤‡ä»½ã€å½’æ¡£ã€å¤§æ•°æ®ã€AIè®­ç»ƒæ•°æ®é›†
è®¿é—®åè®®: S3/Swift API
æ€§èƒ½: é«˜ååã€æµ·é‡å­˜å‚¨
ç¤ºä¾‹: Ceph RGWã€MinIOã€AWS S3

# æœ¬åœ°å­˜å‚¨ (Local Storage)
é€‚ç”¨åœºæ™¯: é«˜æ€§èƒ½æ•°æ®åº“ã€ç¼“å­˜
è®¿é—®æ¨¡å¼: èŠ‚ç‚¹äº²å’Œæ€§
æ€§èƒ½: æè‡´æ€§èƒ½ (NVMe)
ç¤ºä¾‹: Local Path Provisionerã€OpenEBS LocalPV
```

---

## 2. CSI 1.10.0è§„èŒƒè¯¦è§£

### 2.1 CSIæ¶æ„

```
Kubernetesæ ¸å¿ƒç»„ä»¶:
â”œâ”€â”€ kube-controller-manager
â”‚   â”œâ”€â”€ PersistentVolumeController (PV/PVCç»‘å®š)
â”‚   â””â”€â”€ AttachDetachController (å·æŒ‚è½½/å¸è½½)
â”œâ”€â”€ kube-scheduler (Podè°ƒåº¦è€ƒè™‘å·äº²å’Œæ€§)
â””â”€â”€ kubelet
    â””â”€â”€ VolumeManager (å·ç”Ÿå‘½å‘¨æœŸç®¡ç†)

CSIç»„ä»¶:
â”œâ”€â”€ CSI Controller Plugin (ä¸­å¿ƒåŒ–æ§åˆ¶)
â”‚   â”œâ”€â”€ external-provisioner (åŠ¨æ€ä¾›ç»™)
â”‚   â”œâ”€â”€ external-attacher (å·æŒ‚è½½)
â”‚   â”œâ”€â”€ external-snapshotter (å¿«ç…§)
â”‚   â”œâ”€â”€ external-resizer (æ‰©å®¹)
â”‚   â””â”€â”€ csi-driver-registrar (é©±åŠ¨æ³¨å†Œ)
â”‚
â””â”€â”€ CSI Node Plugin (æ¯èŠ‚ç‚¹è¿è¡Œ)
    â”œâ”€â”€ NodeStageVolume (èŠ‚ç‚¹çº§å‡†å¤‡)
    â”œâ”€â”€ NodePublishVolume (Podçº§æŒ‚è½½)
    â””â”€â”€ NodeGetVolumeStats (å·ç»Ÿè®¡)

å­˜å‚¨åç«¯:
â””â”€â”€ Ceph/NFS/iSCSI/äº‘å­˜å‚¨ç­‰
```

### 2.2 CSI 1.10.0æ–°ç‰¹æ€§

#### å·å¥åº·ç›‘æ§ (Volume Health Monitoring)

```yaml
# csi-driver.yaml
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: cephfs.csi.ceph.com
spec:
  attachRequired: true
  podInfoOnMount: true
  # æ–°ç‰¹æ€§: å·å¥åº·ç›‘æ§
  volumeLifecycleModes:
  - Persistent
  - Ephemeral
  fsGroupPolicy: File
  requiresRepublish: false
  # å¯ç”¨å¥åº·æ£€æŸ¥
  storageCapacity: true
  tokenRequests:
  - audience: "ceph-csi"
    expirationSeconds: 600
```

```go
// CSIé©±åŠ¨å®ç°å¥åº·æ£€æŸ¥
func (cs *ControllerServer) ControllerGetVolume(
    ctx context.Context,
    req *csi.ControllerGetVolumeRequest,
) (*csi.ControllerGetVolumeResponse, error) {
    volumeID := req.GetVolumeId()
    
    // æ£€æŸ¥å·å¥åº·çŠ¶æ€
    health, err := cs.checkVolumeHealth(volumeID)
    if err != nil {
        return nil, status.Error(codes.Internal, err.Error())
    }
    
    return &csi.ControllerGetVolumeResponse{
        Volume: &csi.Volume{
            VolumeId: volumeID,
            CapacityBytes: 10 * 1024 * 1024 * 1024, // 10GB
        },
        Status: &csi.ControllerGetVolumeResponse_VolumeStatus{
            PublishedNodeIds: []string{"node1", "node2"},
            VolumeCondition: &csi.VolumeCondition{
                Abnormal: !health.IsHealthy,
                Message:  health.Message,
            },
        },
    }, nil
}
```

#### å·ç»„å¿«ç…§ (VolumeGroupSnapshot)

```yaml
# volumegroupsnapshot-crd.yaml
apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeGroupSnapshot
metadata:
  name: mysql-cluster-snapshot
spec:
  volumeGroupSnapshotClassName: ceph-vg-snapshot
  source:
    selector:
      matchLabels:
        app: mysql
        tier: database
---
apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeGroupSnapshotClass
metadata:
  name: ceph-vg-snapshot
driver: rbd.csi.ceph.com
deletionPolicy: Delete
parameters:
  # ä¸€è‡´æ€§ä¿è¯
  clusterID: "ceph-cluster-1"
  pool: "kubernetes"
  # å´©æºƒä¸€è‡´æ€§ vs åº”ç”¨ä¸€è‡´æ€§
  consistencyPolicy: "crash-consistent"  # æˆ– "application-consistent"
```

#### å·ä¿®å¤ (Volume Repair)

```bash
# è‡ªåŠ¨ä¿®å¤ä¸å¥åº·çš„å·
kubectl patch pv pvc-12345 -p '{"spec":{"csi":{"volumeAttributes":{"repair":"true"}}}}'

# CSIé©±åŠ¨å“åº”ä¿®å¤è¯·æ±‚
# 1. æ£€æµ‹åˆ°å·å¼‚å¸¸
# 2. å°è¯•è‡ªåŠ¨ä¿®å¤ (é‡æ–°æŒ‚è½½ã€ä¿®å¤æ–‡ä»¶ç³»ç»Ÿ)
# 3. å¦‚æœå¤±è´¥,æ ‡è®°ä¸ºéœ€è¦äººå·¥ä»‹å…¥
```

### 2.3 å®æˆ˜: è‡ªå®šä¹‰CSIé©±åŠ¨

#### æœ€å°åŒ–CSI Controller

```go
// cmd/controller/main.go
package main

import (
    "github.com/container-storage-interface/spec/lib/go/csi"
    "github.com/kubernetes-csi/csi-lib-utils/rpc"
    "google.golang.org/grpc"
)

type ControllerServer struct {
    csi.UnimplementedControllerServer
}

func (cs *ControllerServer) CreateVolume(
    ctx context.Context,
    req *csi.CreateVolumeRequest,
) (*csi.CreateVolumeResponse, error) {
    // 1. å‚æ•°éªŒè¯
    name := req.GetName()
    if len(name) == 0 {
        return nil, status.Error(codes.InvalidArgument, "Name missing")
    }
    
    capacityBytes := req.GetCapacityRange().GetRequiredBytes()
    
    // 2. è°ƒç”¨å­˜å‚¨åç«¯APIåˆ›å»ºå·
    volumeID, err := createVolumeInBackend(name, capacityBytes)
    if err != nil {
        return nil, status.Error(codes.Internal, err.Error())
    }
    
    // 3. è¿”å›å·ä¿¡æ¯
    return &csi.CreateVolumeResponse{
        Volume: &csi.Volume{
            VolumeId:      volumeID,
            CapacityBytes: capacityBytes,
            VolumeContext: map[string]string{
                "storage.kubernetes.io/csiProvisionerIdentity": "my-csi-driver",
            },
            AccessibleTopology: []*csi.Topology{
                {
                    Segments: map[string]string{
                        "topology.kubernetes.io/zone": "zone-a",
                    },
                },
            },
        },
    }, nil
}

func (cs *ControllerServer) DeleteVolume(
    ctx context.Context,
    req *csi.DeleteVolumeRequest,
) (*csi.DeleteVolumeResponse, error) {
    volumeID := req.GetVolumeId()
    err := deleteVolumeInBackend(volumeID)
    return &csi.DeleteVolumeResponse{}, err
}

func main() {
    endpoint := "unix:///var/lib/csi/sockets/pluginproxy/csi.sock"
    server := grpc.NewServer()
    
    csi.RegisterIdentityServer(server, &IdentityServer{})
    csi.RegisterControllerServer(server, &ControllerServer{})
    
    listener, _ := net.Listen("unix", endpoint)
    server.Serve(listener)
}
```

#### CSI Nodeå®ç°

```go
// cmd/node/main.go
func (ns *NodeServer) NodeStageVolume(
    ctx context.Context,
    req *csi.NodeStageVolumeRequest,
) (*csi.NodeStageVolumeResponse, error) {
    volumeID := req.GetVolumeId()
    stagingPath := req.GetStagingTargetPath()
    
    // 1. æŒ‚è½½å—è®¾å¤‡åˆ°ä¸´æ—¶è·¯å¾„
    devicePath := fmt.Sprintf("/dev/disk/by-id/%s", volumeID)
    if err := mount.Mount(devicePath, stagingPath, "ext4", []string{}); err != nil {
        return nil, status.Error(codes.Internal, err.Error())
    }
    
    return &csi.NodeStageVolumeResponse{}, nil
}

func (ns *NodeServer) NodePublishVolume(
    ctx context.Context,
    req *csi.NodePublishVolumeRequest,
) (*csi.NodePublishVolumeResponse, error) {
    stagingPath := req.GetStagingTargetPath()
    targetPath := req.GetTargetPath()  // Podçš„æŒ‚è½½ç‚¹
    
    // 2. Bind mountåˆ°Podç›®å½•
    if err := mount.Mount(stagingPath, targetPath, "", []string{"bind"}); err != nil {
        return nil, status.Error(codes.Internal, err.Error())
    }
    
    return &csi.NodePublishVolumeResponse{}, nil
}
```

---

## 3. Rookäº‘åŸç”Ÿå­˜å‚¨ç¼–æ’

### 3.1 Rookæ¶æ„

```
Rook Operator (KubernetesåŸç”Ÿ)
    â”œâ”€â”€ CephCluster CRD (å®šä¹‰Cephé›†ç¾¤)
    â”œâ”€â”€ CephBlockPool CRD (RBDå­˜å‚¨æ± )
    â”œâ”€â”€ CephFilesystem CRD (CephFSæ–‡ä»¶ç³»ç»Ÿ)
    â”œâ”€â”€ CephObjectStore CRD (RGWå¯¹è±¡å­˜å‚¨)
    â””â”€â”€ CephNFS CRD (NFSå¯¼å‡º)

Cephç»„ä»¶ (ç”±Rookç®¡ç†):
    â”œâ”€â”€ MON (ç›‘æ§å®ˆæŠ¤è¿›ç¨‹) - é›†ç¾¤çŠ¶æ€
    â”œâ”€â”€ MGR (ç®¡ç†å®ˆæŠ¤è¿›ç¨‹) - Dashboardã€æŒ‡æ ‡
    â”œâ”€â”€ OSD (å¯¹è±¡å­˜å‚¨å®ˆæŠ¤è¿›ç¨‹) - æ•°æ®å­˜å‚¨
    â”œâ”€â”€ MDS (å…ƒæ•°æ®æœåŠ¡å™¨) - CephFS
    â”œâ”€â”€ RGW (RADOSç½‘å…³) - S3/Swift API
    â””â”€â”€ RBD Mirroring (è·¨é›†ç¾¤å¤åˆ¶)
```

### 3.2 Rook 1.15éƒ¨ç½²

#### å¿«é€Ÿå®‰è£…

```bash
# 1. éƒ¨ç½²Rook Operator
kubectl create ns rook-ceph
kubectl apply -f https://raw.githubusercontent.com/rook/rook/v1.15.0/deploy/examples/crds.yaml
kubectl apply -f https://raw.githubusercontent.com/rook/rook/v1.15.0/deploy/examples/common.yaml
kubectl apply -f https://raw.githubusercontent.com/rook/rook/v1.15.0/deploy/examples/operator.yaml

# 2. éªŒè¯Operatorè¿è¡Œ
kubectl -n rook-ceph get pod
# NAME                                 READY   STATUS    RESTARTS   AGE
# rook-ceph-operator-xxx               1/1     Running   0          1m
```

#### ç”Ÿäº§çº§Cephé›†ç¾¤é…ç½®

```yaml
# ceph-cluster.yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  # Cephç‰ˆæœ¬
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.0  # Ceph 19 "Squid"
    allowUnsupported: false
  
  # æ•°æ®ç›®å½•
  dataDirHostPath: /var/lib/rook
  
  # é«˜å¯ç”¨é…ç½®
  mon:
    count: 3  # ç›‘æ§å®ˆæŠ¤è¿›ç¨‹æ•°é‡ (å»ºè®®>=3)
    allowMultiplePerNode: false
    volumeClaimTemplate:
      spec:
        storageClassName: local-path
        resources:
          requests:
            storage: 10Gi
  
  mgr:
    count: 2  # ç®¡ç†å®ˆæŠ¤è¿›ç¨‹HA
    modules:
    - name: pg_autoscaler
      enabled: true
    - name: prometheus
      enabled: true
    - name: dashboard
      enabled: true
      port: 8443
  
  # å­˜å‚¨é…ç½®
  storage:
    useAllNodes: false
    useAllDevices: false
    nodes:
    - name: "node1"
      devices:
      - name: "/dev/nvme0n1"  # NVMe SSD
        config:
          deviceClass: "ssd"  # è®¾å¤‡ç±»åˆ«
          metadataDevice: "/dev/nvme1n1"  # å…ƒæ•°æ®åˆ†ç¦»
    - name: "node2"
      devices:
      - name: "/dev/nvme0n1"
        config:
          deviceClass: "ssd"
    - name: "node3"
      devices:
      - name: "/dev/sda"  # HDD
        config:
          deviceClass: "hdd"
          osdsPerDevice: "2"  # æ¯ä¸ªè®¾å¤‡2ä¸ªOSD
  
  # ç½‘ç»œé…ç½®
  network:
    provider: host  # æˆ– "multus" ç”¨äºå¤šç½‘ç»œ
    connections:
      encryption:
        enabled: true  # åŠ å¯†é›†ç¾¤é€šä¿¡
      compression:
        enabled: false
    selectors:
      public: "public-network"
      cluster: "cluster-network"
  
  # é«˜çº§ç‰¹æ€§
  crashCollector:
    disable: false  # å´©æºƒæŠ¥å‘Šæ”¶é›†
  
  dashboard:
    enabled: true
    ssl: true
  
  monitoring:
    enabled: true
    rulesNamespace: rook-ceph
  
  # èµ„æºé™åˆ¶
  resources:
    mon:
      limits:
        cpu: "2000m"
        memory: "4Gi"
      requests:
        cpu: "1000m"
        memory: "2Gi"
    osd:
      limits:
        cpu: "4000m"
        memory: "8Gi"
      requests:
        cpu: "2000m"
        memory: "4Gi"
    mgr:
      limits:
        cpu: "2000m"
        memory: "4Gi"
      requests:
        cpu: "1000m"
        memory: "2Gi"
  
  # æ¸…ç†ç­–ç•¥
  cleanupPolicy:
    confirmation: ""  # åˆ é™¤é›†ç¾¤æ—¶éœ€è¦ç¡®è®¤
    sanitizeDisks:
      method: quick  # æˆ– "complete" å®Œå…¨æ“¦é™¤
      dataSource: zero
      iteration: 1
  
  # å®‰å…¨
  security:
    kms:
      connectionDetails:
        KMS_PROVIDER: "vault"
        VAULT_ADDR: "https://vault.example.com:8200"
      tokenSecretName: ceph-kms-token
```

```bash
# éƒ¨ç½²é›†ç¾¤
kubectl apply -f ceph-cluster.yaml

# ç›‘æ§éƒ¨ç½²è¿›åº¦
watch kubectl -n rook-ceph get cephcluster

# ç­‰å¾…é›†ç¾¤Ready (çº¦5-10åˆ†é’Ÿ)
kubectl -n rook-ceph get cephcluster
# NAME        DATADIRHOSTPATH   MONCOUNT   AGE     PHASE   MESSAGE                        HEALTH
# rook-ceph   /var/lib/rook     3          10m     Ready   Cluster created successfully   HEALTH_OK
```

### 3.3 å—å­˜å‚¨ (RBD) é…ç½®

```yaml
# ceph-block-pool.yaml
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  # æ•…éšœåŸŸ
  failureDomain: host  # host/rack/datacenter
  
  # å‰¯æœ¬é…ç½®
  replicated:
    size: 3  # 3å‰¯æœ¬
    requireSafeReplicaSize: true
    replicasPerFailureDomain: 1
  
  # æˆ–è€…ä½¿ç”¨çº åˆ ç  (Erasure Coding) èŠ‚çœç©ºé—´
  # erasureCoded:
  #   dataChunks: 4
  #   codingChunks: 2  # 4+2 EC (å®¹å¿2ä¸ªOSDæ•…éšœ)
  
  # è®¾å¤‡ç±»åˆ« (SSD/HDD)
  deviceClass: ssd
  
  # å‹ç¼©
  compressionMode: aggressive  # none/passive/aggressive/force
  
  # é…é¢
  quotas:
    maxBytes: 1099511627776  # 1TB
    maxObjects: 1000000
  
  # æ€§èƒ½è°ƒä¼˜
  parameters:
    pg_num: "128"  # Placement Groups
    pgp_num: "128"
    min_size: "2"  # æœ€å°å‰¯æœ¬æ•°

---
# storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: replicapool
  imageFormat: "2"
  imageFeatures: layering,exclusive-lock,object-map,fast-diff
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
- discard  # SSD TRIMæ”¯æŒ
```

#### ä½¿ç”¨å—å­˜å‚¨

```yaml
# mysql-statefulset.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: rook-ceph-block

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: mysql-pvc
```

### 3.4 æ–‡ä»¶å­˜å‚¨ (CephFS) é…ç½®

```yaml
# ceph-filesystem.yaml
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: myfs
  namespace: rook-ceph
spec:
  # å…ƒæ•°æ®æ±  (SSDæ¨è)
  metadataPool:
    replicated:
      size: 3
    deviceClass: ssd
    parameters:
      pg_num: "64"
  
  # æ•°æ®æ± 
  dataPools:
  - name: data0
    replicated:
      size: 3
    deviceClass: ssd  # æˆ– hdd
    parameters:
      pg_num: "128"
  
  # MDSå®ˆæŠ¤è¿›ç¨‹
  metadataServer:
    activeCount: 2  # æ´»åŠ¨MDSæ•°é‡ (HA)
    activeStandby: true
    resources:
      limits:
        cpu: "3000m"
        memory: "8Gi"
      requests:
        cpu: "1000m"
        memory: "4Gi"
    priorityClassName: system-cluster-critical

---
# storageclass-cephfs.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs
provisioner: rook-ceph.cephfs.csi.ceph.com
parameters:
  clusterID: rook-ceph
  fsName: myfs
  pool: myfs-data0
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
- debug  # ç”Ÿäº§ç¯å¢ƒç§»é™¤
```

#### å¤šPodå…±äº«æ–‡ä»¶å­˜å‚¨

```yaml
# shared-storage.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-pvc
spec:
  accessModes:
  - ReadWriteMany  # å¤šPodè¯»å†™
  resources:
    requests:
      storage: 50Gi
  storageClassName: rook-cephfs

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-servers
spec:
  replicas: 5
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        volumeMounts:
        - name: shared-content
          mountPath: /usr/share/nginx/html
      volumes:
      - name: shared-content
        persistentVolumeClaim:
          claimName: shared-pvc
```

### 3.5 å¯¹è±¡å­˜å‚¨ (RGW) é…ç½®

```yaml
# ceph-object-store.yaml
apiVersion: ceph.rook.io/v1
kind: CephObjectStore
metadata:
  name: my-store
  namespace: rook-ceph
spec:
  # å…ƒæ•°æ®æ± 
  metadataPool:
    replicated:
      size: 3
    deviceClass: ssd
  
  # æ•°æ®æ± 
  dataPool:
    replicated:
      size: 3
    deviceClass: hdd  # å¯¹è±¡å­˜å‚¨é€šå¸¸ç”¨HDD
    parameters:
      pg_num: "256"
  
  # RGWé…ç½®
  gateway:
    instances: 2  # RGWå®ä¾‹æ•° (HA)
    port: 80
    securePort: 443
    sslCertificateRef: rgw-tls-cert
    resources:
      limits:
        cpu: "2000m"
        memory: "4Gi"
      requests:
        cpu: "1000m"
        memory: "2Gi"
  
  # ç”Ÿå‘½å‘¨æœŸç®¡ç†
  lifecycle:
    - id: delete-old-backups
      prefix: backups/
      status: Enabled
      expiration:
        days: 90  # 90å¤©ååˆ é™¤
    - id: transition-to-glacier
      prefix: archives/
      status: Enabled
      transitions:
      - days: 30
        storageClass: GLACIER

---
# object-store-user.yaml
apiVersion: ceph.rook.io/v1
kind: CephObjectStoreUser
metadata:
  name: app-user
  namespace: rook-ceph
spec:
  store: my-store
  displayName: "Application User"
  quotas:
    maxBuckets: 100
    maxSize: 1099511627776  # 1TB
```

#### S3è®¿é—®ç¤ºä¾‹

```python
# s3-example.py
import boto3
from kubernetes import client, config

# 1. ä»Kubernetes Secretè·å–å‡­è¯
config.load_incluster_config()
v1 = client.CoreV1Api()
secret = v1.read_namespaced_secret("rook-ceph-object-user-my-store-app-user", "rook-ceph")

access_key = base64.b64decode(secret.data['AccessKey']).decode()
secret_key = base64.b64decode(secret.data['SecretKey']).decode()

# 2. åˆå§‹åŒ–S3å®¢æˆ·ç«¯
s3 = boto3.client(
    's3',
    endpoint_url='http://rook-ceph-rgw-my-store.rook-ceph.svc:80',
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key,
)

# 3. åˆ›å»ºbucket
s3.create_bucket(Bucket='my-bucket')

# 4. ä¸Šä¼ å¯¹è±¡
s3.put_object(
    Bucket='my-bucket',
    Key='data/file.txt',
    Body=b'Hello Ceph Object Storage!',
    Metadata={'app': 'my-app'}
)

# 5. ä¸‹è½½å¯¹è±¡
response = s3.get_object(Bucket='my-bucket', Key='data/file.txt')
content = response['Body'].read()
print(content.decode())

# 6. ç”Ÿæˆé¢„ç­¾åURL (ä¸´æ—¶è®¿é—®)
url = s3.generate_presigned_url(
    'get_object',
    Params={'Bucket': 'my-bucket', 'Key': 'data/file.txt'},
    ExpiresIn=3600  # 1å°æ—¶æœ‰æ•ˆ
)
```

---

## 4. Cephåˆ†å¸ƒå¼å­˜å‚¨

### 4.1 Ceph 19 "Squid" æ–°ç‰¹æ€§

```yaml
# 2025å¹´Ceph 19ä¸»è¦æ›´æ–°:

1. æ€§èƒ½æå‡:
   - BlueStore 2.0: æ›´é«˜æ•ˆçš„å­˜å‚¨å¼•æ“
   - RADOS Native Encryption: åŸç”ŸåŠ å¯† (æ€§èƒ½æå‡30%)
   - NVMe-oFæ”¯æŒ: æ›´ä½å»¶è¿Ÿ

2. ç®¡ç†å¢å¼º:
   - å¢å¼ºå‹Dashboard: æ›´ç›´è§‚çš„UI
   - è‡ªåŠ¨åŒ–è¿ç»´: æ™ºèƒ½æ•…éšœé¢„æµ‹
   - Telemetryæ”¹è¿›: æ›´è¯¦ç»†çš„æŒ‡æ ‡

3. æ–°åŠŸèƒ½:
   - Object Lock (S3å…¼å®¹): ä¸å¯å˜å­˜å‚¨
   - Multi-site Syncä¼˜åŒ–: è·¨åŒºåŸŸå¤åˆ¶æ€§èƒ½æå‡50%
   - CephFS Snapshotæ”¹è¿›: æ›´å¿«çš„å¿«ç…§åˆ›å»º
```

### 4.2 Cephæ€§èƒ½è°ƒä¼˜

#### OSDæ€§èƒ½ä¼˜åŒ–

```bash
# 1. BlueStoreé…ç½®
ceph config set osd bluestore_prefer_deferred_size_hdd 0
ceph config set osd bluestore_min_alloc_size_ssd 4096
ceph config set osd bluestore_compression_mode aggressive
ceph config set osd bluestore_compression_algorithm zstd

# 2. ç¼“å­˜ä¼˜åŒ–
ceph config set osd osd_memory_target 8589934592  # 8GB per OSD
ceph config set osd bluestore_cache_autotune true
ceph config set osd bluestore_cache_size_ssd 3221225472  # 3GB

# 3. å¹¶å‘è°ƒä¼˜
ceph config set osd osd_op_num_threads_per_shard 2
ceph config set osd osd_op_num_shards 8
```

#### PGè‡ªåŠ¨æ‰©ç¼©å®¹

```bash
# å¯ç”¨PGè‡ªåŠ¨ç¼©æ”¾
ceph config set global osd_pool_default_pg_autoscale_mode on

# æŸ¥çœ‹PGå»ºè®®
ceph osd pool autoscale-status

# æ‰‹åŠ¨è°ƒæ•´PGæ•°é‡
ceph osd pool set replicapool pg_num 256
```

### 4.3 Cephç›‘æ§

```yaml
# prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ceph-alerts
  namespace: rook-ceph
spec:
  groups:
  - name: ceph.rules
    interval: 30s
    rules:
    # é›†ç¾¤å¥åº·
    - alert: CephHealthWarning
      expr: ceph_health_status == 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Ceph cluster health is WARNING"
        description: "Ceph cluster {{ $labels.cluster }} has been in WARNING state for more than 5 minutes."
    
    # OSDæ•…éšœ
    - alert: CephOSDDown
      expr: ceph_osd_up == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Ceph OSD Down"
        description: "OSD {{ $labels.ceph_daemon }} on {{ $labels.hostname }} is down."
    
    # å­˜å‚¨ç©ºé—´ä¸è¶³
    - alert: CephClusterNearFull
      expr: ceph_cluster_total_used_bytes / ceph_cluster_total_bytes > 0.85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Ceph cluster nearly full"
        description: "Ceph cluster {{ $labels.cluster }} is {{ $value | humanizePercentage }} full."
    
    # æ€§èƒ½ä¸‹é™
    - alert: CephSlowOps
      expr: ceph_health_slow_ops > 0
      for: 1m
      labels:
        severity: warning
      annotations:
        summary: "Ceph slow operations detected"
        description: "{{ $value }} slow operations detected in cluster {{ $labels.cluster }}."
```

---

## 5. Veleroå¤‡ä»½ä¸æ¢å¤

### 5.1 Velero 1.15éƒ¨ç½²

```bash
# 1. å®‰è£…Velero CLI
wget https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz
tar -xvf velero-v1.15.0-linux-amd64.tar.gz
sudo mv velero-v1.15.0-linux-amd64/velero /usr/local/bin/

# 2. å®‰è£…Veleroåˆ°Kubernetes (ä½¿ç”¨Ceph S3åç«¯)
velero install \
  --provider aws \
  --plugins velero/velero-plugin-for-aws:v1.10.0 \
  --bucket velero-backups \
  --secret-file ./credentials-velero \
  --use-volume-snapshots=true \
  --backup-location-config \
    region=default,s3ForcePathStyle="true",s3Url=http://rook-ceph-rgw-my-store.rook-ceph.svc:80 \
  --snapshot-location-config region=default \
  --use-node-agent \
  --uploader-type=kopia  # æ–°çš„ä¸Šä¼ å™¨(æ›´å¿«)
```

#### credentials-veleroæ–‡ä»¶

```ini
[default]
aws_access_key_id = <ACCESS_KEY>
aws_secret_access_key = <SECRET_KEY>
```

### 5.2 å¤‡ä»½ç­–ç•¥

#### å…¨é‡å¤‡ä»½

```yaml
# backup-all-namespaces.yaml
apiVersion: velero.io/v1
kind: Backup
metadata:
  name: daily-backup
  namespace: velero
spec:
  # åŒ…å«æ‰€æœ‰å‘½åç©ºé—´
  includedNamespaces:
  - '*'
  
  # æ’é™¤ç³»ç»Ÿå‘½åç©ºé—´
  excludedNamespaces:
  - kube-system
  - kube-public
  - kube-node-lease
  
  # åŒ…å«é›†ç¾¤èµ„æº
  includeClusterResources: true
  
  # å­˜å‚¨ä½ç½®
  storageLocation: default
  
  # å¿«ç…§å·
  snapshotVolumes: true
  volumeSnapshotLocations:
  - default
  
  # TTL (ä¿ç•™30å¤©)
  ttl: 720h
  
  # å¤‡ä»½æ ‡ç­¾
  labelSelector:
    matchExpressions:
    - key: backup
      operator: In
      values: ["true"]
```

#### å®šæ—¶å¤‡ä»½

```yaml
# schedule-backup.yaml
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-backup-schedule
  namespace: velero
spec:
  # Cronè¡¨è¾¾å¼: æ¯å¤©å‡Œæ™¨2ç‚¹
  schedule: "0 2 * * *"
  
  template:
    includedNamespaces:
    - production
    - staging
    
    snapshotVolumes: true
    ttl: 720h  # 30å¤©
    
    # å¤‡ä»½é’©å­: æ•°æ®åº“ä¸€è‡´æ€§å¤‡ä»½
    hooks:
      resources:
      - name: mysql-backup-hook
        includedNamespaces:
        - production
        labelSelector:
          matchLabels:
            app: mysql
        pre:
        - exec:
            container: mysql
            command:
            - /bin/bash
            - -c
            - |
              mysql -u root -p${MYSQL_ROOT_PASSWORD} -e "FLUSH TABLES WITH READ LOCK;"
              sleep 5
        post:
        - exec:
            container: mysql
            command:
            - /bin/bash
            - -c
            - |
              mysql -u root -p${MYSQL_ROOT_PASSWORD} -e "UNLOCK TABLES;"
```

### 5.3 æ¢å¤æ“ä½œ

```bash
# 1. æŸ¥çœ‹å¯ç”¨å¤‡ä»½
velero backup get
# NAME                STATUS      CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR
# daily-backup-20251022   Completed   2025-10-22 02:00:00 +0000 UTC   29d       default            <none>

# 2. å®Œæ•´æ¢å¤
velero restore create --from-backup daily-backup-20251022

# 3. éƒ¨åˆ†æ¢å¤ (ä»…æ¢å¤ç‰¹å®šå‘½åç©ºé—´)
velero restore create --from-backup daily-backup-20251022 \
  --include-namespaces production \
  --namespace-mappings production:production-restore

# 4. æ¢å¤æŒ‡å®šèµ„æº
velero restore create --from-backup daily-backup-20251022 \
  --include-resources deployments,services,persistentvolumeclaims \
  --selector app=my-app

# 5. ç›‘æ§æ¢å¤è¿›åº¦
velero restore describe <RESTORE-NAME>
velero restore logs <RESTORE-NAME>
```

### 5.4 ç¾éš¾æ¢å¤æ¼”ç»ƒ

```bash
#!/bin/bash
# dr-test.sh - ç¾éš¾æ¢å¤è‡ªåŠ¨åŒ–æµ‹è¯•

# 1. åˆ›å»ºæµ‹è¯•å¤‡ä»½
velero backup create dr-test-$(date +%Y%m%d) \
  --include-namespaces production \
  --snapshot-volumes true \
  --wait

# 2. æ¨¡æ‹Ÿç¾éš¾ (åˆ é™¤å‘½åç©ºé—´)
kubectl delete namespace production --wait=false

# 3. ç­‰å¾…åˆ é™¤å®Œæˆ
kubectl wait --for=delete namespace/production --timeout=300s

# 4. æ‰§è¡Œæ¢å¤
velero restore create dr-test-restore-$(date +%Y%m%d) \
  --from-backup dr-test-$(date +%Y%m%d) \
  --wait

# 5. éªŒè¯æ¢å¤
kubectl get all -n production

# 6. å¥åº·æ£€æŸ¥
kubectl rollout status deployment -n production --timeout=600s

# 7. ç”ŸæˆæŠ¥å‘Š
echo "=== DR Test Report ===" > dr-report.txt
echo "Backup: dr-test-$(date +%Y%m%d)" >> dr-report.txt
echo "Restore: dr-test-restore-$(date +%Y%m%d)" >> dr-report.txt
velero restore describe dr-test-restore-$(date +%Y%m%d) >> dr-report.txt
```

---

## 6. å…¶ä»–äº‘åŸç”Ÿå­˜å‚¨æ–¹æ¡ˆ

### 6.1 Longhorn (Rancher)

```yaml
# è½»é‡çº§åˆ†å¸ƒå¼å—å­˜å‚¨
ç‰¹ç‚¹:
- ç®€å•æ˜“ç”¨,UIå‹å¥½
- è‡ªåŠ¨å¤‡ä»½åˆ°S3/NFS
- å†…ç½®ç¾éš¾æ¢å¤
- æ”¯æŒå¿«ç…§å’Œå…‹éš†

å®‰è£…:
helm repo add longhorn https://charts.longhorn.io
helm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace

StorageClass:
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn
provisioner: driver.longhorn.io
allowVolumeExpansion: true
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880"  # 48å°æ—¶
  fromBackup: ""
  fsType: "ext4"
```

### 6.2 OpenEBS

```yaml
# äº‘åŸç”Ÿå­˜å‚¨å¹³å°
å¼•æ“ç±»å‹:
1. LocalPV: æœ¬åœ°å· (æœ€é«˜æ€§èƒ½)
2. Jiva: è½»é‡çº§å¤åˆ¶å­˜å‚¨
3. cStor: ä¼ä¸šçº§å­˜å‚¨å¼•æ“
4. Mayastor: NVMe-oFé«˜æ€§èƒ½å­˜å‚¨ (æ–°)

Mayastorå®‰è£…:
kubectl apply -f https://openebs.github.io/charts/mayastor-operator.yaml

StorageClass (Mayastor):
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: mayastor-3-replicas
parameters:
  repl: "3"
  protocol: "nvmf"  # NVMe-oF
  ioTimeout: "60"
  local: "false"
provisioner: io.openebs.csi-mayastor
```

### 6.3 Portworx

```yaml
# ä¼ä¸šçº§å­˜å‚¨è§£å†³æ–¹æ¡ˆ
ç‰¹ç‚¹:
- å¤šäº‘æ”¯æŒ
- è‡ªåŠ¨åŒ–ç¾éš¾æ¢å¤
- åº”ç”¨æ„ŸçŸ¥å¿«ç…§
- æ•°æ®åŠ å¯†

StorageClass:
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: portworx-db
provisioner: kubernetes.io/portworx-volume
parameters:
  repl: "3"
  io_profile: "db_remote"  # æ•°æ®åº“ä¼˜åŒ–
  priority_io: "high"
  fs: "ext4"
  secure: "true"  # åŠ å¯†
  cascade: "true"  # çº§è”åˆ é™¤
allowVolumeExpansion: true
```

---

## 7. å­˜å‚¨æ€§èƒ½ä¼˜åŒ–

### 7.1 æ€§èƒ½åŸºå‡†æµ‹è¯•

```bash
# ä½¿ç”¨fioè¿›è¡Œæ€§èƒ½æµ‹è¯•
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: fio-test
spec:
  containers:
  - name: fio
    image: ljishen/fio
    command:
    - sleep
    - "3600"
    volumeMounts:
    - name: test-volume
      mountPath: /data
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: test-pvc
EOF

# ç­‰å¾…Podè¿è¡Œ
kubectl wait --for=condition=Ready pod/fio-test

# éšæœºè¯»å†™æµ‹è¯•
kubectl exec fio-test -- fio \
  --name=randrw \
  --ioengine=libaio \
  --iodepth=16 \
  --rw=randrw \
  --rwmixread=70 \
  --bs=4k \
  --direct=1 \
  --size=10G \
  --numjobs=4 \
  --runtime=60 \
  --group_reporting \
  --filename=/data/testfile

# é¡ºåºè¯»æµ‹è¯• (ååé‡)
kubectl exec fio-test -- fio \
  --name=seqread \
  --ioengine=libaio \
  --iodepth=32 \
  --rw=read \
  --bs=1m \
  --direct=1 \
  --size=10G \
  --numjobs=4 \
  --runtime=60 \
  --group_reporting \
  --filename=/data/testfile
```

### 7.2 æ€§èƒ½ä¼˜åŒ–æ¸…å•

```yaml
# 1. å­˜å‚¨ç±»ä¼˜åŒ–
- ä½¿ç”¨SSDè®¾å¤‡ç±»
- å¯ç”¨discard (TRIM)
- é€‰æ‹©åˆé€‚çš„æ–‡ä»¶ç³»ç»Ÿ (ext4/XFS)
- é…ç½®åˆç†çš„å‰¯æœ¬æ•° (æ€§èƒ½ vs å¯é æ€§)

# 2. Ceph OSDä¼˜åŒ–
- æ¯ä¸ªOSD 4-8GBå†…å­˜
- NVMeç”¨äºå…ƒæ•°æ®/WAL
- å¯ç”¨BlueStoreç¼“å­˜è‡ªåŠ¨è°ƒä¼˜
- è°ƒæ•´PGæ•°é‡ (100-200 PG/OSD)

# 3. ç½‘ç»œä¼˜åŒ–
- ä½¿ç”¨10GbEæˆ–æ›´é«˜å¸¦å®½
- åˆ†ç¦»public/clusterç½‘ç»œ
- å¯ç”¨jumbo frames (MTU 9000)
- ç¦ç”¨ç½‘ç»œåŠ å¯† (ä»…å†…ç½‘)

# 4. å®¢æˆ·ç«¯ä¼˜åŒ–
- ä½¿ç”¨libaio/io_uring
- å¢åŠ iodepth
- å¯ç”¨direct I/O
- æ‰¹é‡æ“ä½œ

# 5. Kubernetesä¼˜åŒ–
- ä½¿ç”¨æœ¬åœ°å· (ä¸´æ—¶æ•°æ®)
- Podäº²å’Œæ€§ (å°±è¿‘è®¿é—®å­˜å‚¨)
- èµ„æºé™åˆ¶ (é¿å…noisy neighbor)
```

---

## 8. é«˜å¯ç”¨æ¶æ„

### 8.1 è·¨åŒºåŸŸå¤åˆ¶

```yaml
# ceph-rbd-mirroring.yaml
apiVersion: ceph.rook.io/v1
kind: CephRBDMirror
metadata:
  name: my-rbd-mirror
  namespace: rook-ceph
spec:
  count: 2  # Mirrorå®ˆæŠ¤è¿›ç¨‹æ•°é‡
  peers:
    secretNames:
    - rbd-mirror-peer-secret  # è¿œç¨‹é›†ç¾¤å‡­è¯
  resources:
    limits:
      cpu: "2000m"
      memory: "4Gi"
    requests:
      cpu: "500m"
      memory: "1Gi"

---
# å¯ç”¨é•œåƒçš„å­˜å‚¨æ± 
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
    size: 3
  mirroring:
    enabled: true
    mode: image  # image æˆ– pool
    snapshotSchedules:
    - interval: "1h"  # æ¯å°æ—¶å¿«ç…§å¹¶åŒæ­¥
      startTime: "2025-10-22T00:00:00Z"
```

### 8.2 å¤šé›†ç¾¤è”é‚¦

```yaml
# kubefed-storage.yaml
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: federated-app
  namespace: default
spec:
  template:
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: my-app
      template:
        spec:
          containers:
          - name: app
            image: myapp:v1
            volumeMounts:
            - name: data
              mountPath: /data
          volumes:
          - name: data
            persistentVolumeClaim:
              claimName: federated-pvc
  placement:
    clusters:
    - name: cluster-east
    - name: cluster-west
  overrides:
  - clusterName: cluster-east
    clusterOverrides:
    - path: "/spec/replicas"
      value: 5
```

---

## 9. ç¾éš¾æ¢å¤

### 9.1 RPO/RTOç›®æ ‡

```yaml
# æ¢å¤ç›®æ ‡å®šä¹‰
æœåŠ¡ç­‰çº§:
  Tier 1 (å…³é”®ä¸šåŠ¡):
    RPO: < 5åˆ†é’Ÿ
    RTO: < 15åˆ†é’Ÿ
    ç­–ç•¥: åŒæ­¥å¤åˆ¶ + è‡ªåŠ¨æ•…éšœè½¬ç§»
  
  Tier 2 (é‡è¦ä¸šåŠ¡):
    RPO: < 1å°æ—¶
    RTO: < 1å°æ—¶
    ç­–ç•¥: å¼‚æ­¥å¤åˆ¶ + æ‰‹åŠ¨æ•…éšœè½¬ç§»
  
  Tier 3 (æ™®é€šä¸šåŠ¡):
    RPO: < 24å°æ—¶
    RTO: < 4å°æ—¶
    ç­–ç•¥: æ¯æ—¥å¤‡ä»½
```

### 9.2 è‡ªåŠ¨åŒ–DRæµç¨‹

```bash
#!/bin/bash
# auto-dr.sh - è‡ªåŠ¨ç¾éš¾æ¢å¤

# 1. æ£€æµ‹ä¸»ç«™ç‚¹æ•…éšœ
if ! curl -f -s http://primary-cluster-api:6443/healthz; then
    echo "Primary cluster is DOWN! Initiating DR..."
    
    # 2. åˆ‡æ¢åˆ°å¤‡ä»½ç«™ç‚¹
    kubectl config use-context dr-cluster
    
    # 3. æå‡RBDé•œåƒä¸ºä¸»
    ceph rbd mirror image promote replicapool/pvc-12345
    
    # 4. æ¢å¤åº”ç”¨
    velero restore create auto-dr-$(date +%Y%m%d-%H%M%S) \
      --from-backup latest \
      --wait
    
    # 5. æ›´æ–°DNSæŒ‡å‘å¤‡ä»½ç«™ç‚¹
    aws route53 change-resource-record-sets \
      --hosted-zone-id Z1234567 \
      --change-batch file://update-dns.json
    
    # 6. å‘é€å‘Šè­¦
    curl -X POST https://alerts.example.com/webhook \
      -d '{"event":"DR_ACTIVATED","timestamp":"'$(date -Iseconds)'"}'
    
    echo "DR completed successfully!"
fi
```

---

## 10. ç›‘æ§ä¸æ•…éšœæ’æŸ¥

### 10.1 å…³é”®æŒ‡æ ‡

```yaml
# Grafana Dashboardå…³é”®é¢æ¿

1. é›†ç¾¤å¥åº·:
   - ceph_health_status
   - ceph_mon_quorum_status
   - ceph_osd_up / ceph_osd_in

2. æ€§èƒ½æŒ‡æ ‡:
   - ceph_pool_rd_bytes (è¯»åå)
   - ceph_pool_wr_bytes (å†™åå)
   - ceph_osd_apply_latency_ms (å»¶è¿Ÿ)
   - ceph_osd_commit_latency_ms

3. å®¹é‡æŒ‡æ ‡:
   - ceph_cluster_total_used_bytes / ceph_cluster_total_bytes
   - ceph_pool_stored_bytes
   - ceph_pool_max_avail

4. PV/PVCçŠ¶æ€:
   - kubelet_volume_stats_capacity_bytes
   - kubelet_volume_stats_used_bytes
   - kube_persistentvolumeclaim_status_phase
```

### 10.2 å¸¸è§é—®é¢˜æ’æŸ¥

```bash
# é—®é¢˜1: PVCä¸€ç›´Pending
kubectl describe pvc <PVC-NAME>
# æ£€æŸ¥äº‹ä»¶: "waiting for a volume to be created"

# æ’æŸ¥æ­¥éª¤:
# 1. æ£€æŸ¥StorageClasså­˜åœ¨
kubectl get storageclass
# 2. æ£€æŸ¥CSIé©±åŠ¨è¿è¡Œæ­£å¸¸
kubectl get csidrivers
kubectl -n rook-ceph get pods -l app=csi-rbdplugin
# 3. æ£€æŸ¥Cephé›†ç¾¤å¥åº·
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph status

# é—®é¢˜2: å·æŒ‚è½½å¤±è´¥
kubectl describe pod <POD-NAME>
# Events: "MountVolume.MountDevice failed"

# æ’æŸ¥:
kubectl -n rook-ceph logs -l app=csi-rbdplugin-provisioner
# æŸ¥æ‰¾é”™è¯¯æ—¥å¿—

# é—®é¢˜3: Cephæ€§èƒ½ä¸‹é™
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph health detail
# æ£€æŸ¥slow ops
ceph osd perf
ceph osd df tree

# ä¿®å¤slow ops
ceph tell osd.* config set debug_osd 0/0
```

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£

- **CSIè§„èŒƒ**: https://github.com/container-storage-interface/spec
- **Rook**: https://rook.io/docs/
- **Ceph**: https://docs.ceph.com/
- **Velero**: https://velero.io/docs/

### æœ€ä½³å®è·µ

- **CNCFå­˜å‚¨ç™½çš®ä¹¦**: https://www.cncf.io/reports/
- **Kuberneteså­˜å‚¨æŒ‡å—**: https://kubernetes.io/docs/concepts/storage/

---

**æ–‡æ¡£ç»´æŠ¤**: vSphere_DockeræŠ€æœ¯å›¢é˜Ÿ  
**æŠ€æœ¯æ”¯æŒ**: support@vsphere-docker.io  
**ç‰ˆæœ¬å†å²**: æŸ¥çœ‹ [CHANGELOG.md](../CHANGELOG.md)
