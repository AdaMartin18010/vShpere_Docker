# 06 - å­˜å‚¨æ€§èƒ½ä¼˜åŒ–

**ä½œè€…**: äº‘åŸç”Ÿå­˜å‚¨ä¸“å®¶å›¢é˜Ÿ
**åˆ›å»ºæ—¥æœŸ**: 2025-10-19
**æœ€åæ›´æ–°**: 2025-10-19
**ç‰ˆæœ¬**: v1.0

---

## ğŸ“‹ æœ¬ç« å¯¼èˆª

- [06 - å­˜å‚¨æ€§èƒ½ä¼˜åŒ–](#06---å­˜å‚¨æ€§èƒ½ä¼˜åŒ–)
  - [ğŸ“‹ æœ¬ç« å¯¼èˆª](#-æœ¬ç« å¯¼èˆª)
  - [1. æ€§èƒ½æŒ‡æ ‡](#1-æ€§èƒ½æŒ‡æ ‡)
    - [1.1 æ ¸å¿ƒæŒ‡æ ‡](#11-æ ¸å¿ƒæŒ‡æ ‡)
    - [1.2 æŒ‡æ ‡é‡‡é›†](#12-æŒ‡æ ‡é‡‡é›†)
    - [1.3 æ€§èƒ½åŸºå‡†](#13-æ€§èƒ½åŸºå‡†)
  - [2. æ€§èƒ½æµ‹è¯•](#2-æ€§èƒ½æµ‹è¯•)
    - [2.1 FIOæµ‹è¯•](#21-fioæµ‹è¯•)
    - [2.2 ddæµ‹è¯•](#22-ddæµ‹è¯•)
    - [2.3 sysbenchæµ‹è¯•](#23-sysbenchæµ‹è¯•)
    - [2.4 Kuberneteså­˜å‚¨æ€§èƒ½æµ‹è¯•](#24-kuberneteså­˜å‚¨æ€§èƒ½æµ‹è¯•)
  - [3. Cephæ€§èƒ½ä¼˜åŒ–](#3-cephæ€§èƒ½ä¼˜åŒ–)
    - [3.1 OSDä¼˜åŒ–](#31-osdä¼˜åŒ–)
    - [3.2 ç½‘ç»œä¼˜åŒ–](#32-ç½‘ç»œä¼˜åŒ–)
    - [3.3 å®¢æˆ·ç«¯ä¼˜åŒ–](#33-å®¢æˆ·ç«¯ä¼˜åŒ–)
  - [4. æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–](#4-æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–)
    - [4.1 æ–‡ä»¶ç³»ç»Ÿé€‰æ‹©](#41-æ–‡ä»¶ç³»ç»Ÿé€‰æ‹©)
    - [4.2 æŒ‚è½½é€‰é¡¹](#42-æŒ‚è½½é€‰é¡¹)
  - [5. å†…æ ¸å‚æ•°ä¼˜åŒ–](#5-å†…æ ¸å‚æ•°ä¼˜åŒ–)
    - [5.1 I/Oè°ƒåº¦å™¨](#51-ioè°ƒåº¦å™¨)
    - [5.2 å†…æ ¸å‚æ•°](#52-å†…æ ¸å‚æ•°)
  - [6. å®¹é‡è§„åˆ’](#6-å®¹é‡è§„åˆ’)
    - [6.1 å®¹é‡é¢„æµ‹](#61-å®¹é‡é¢„æµ‹)
    - [6.2 æ‰©å®¹ç­–ç•¥](#62-æ‰©å®¹ç­–ç•¥)
    - [6.3 æˆæœ¬ä¼˜åŒ–](#63-æˆæœ¬ä¼˜åŒ–)
  - [7. æ€§èƒ½ç›‘æ§](#7-æ€§èƒ½ç›‘æ§)
    - [7.1 Prometheusç›‘æ§](#71-prometheusç›‘æ§)
    - [7.2 Grafanaä»ªè¡¨ç›˜](#72-grafanaä»ªè¡¨ç›˜)
    - [7.3 å‘Šè­¦è§„åˆ™](#73-å‘Šè­¦è§„åˆ™)
  - [8. æ€»ç»“](#8-æ€»ç»“)
  - [ç›¸å…³æ–‡æ¡£](#ç›¸å…³æ–‡æ¡£)
    - [æœ¬æ¨¡å—ç›¸å…³](#æœ¬æ¨¡å—ç›¸å…³)
    - [å…¶ä»–æ¨¡å—ç›¸å…³](#å…¶ä»–æ¨¡å—ç›¸å…³)

---

## 1. æ€§èƒ½æŒ‡æ ‡

### 1.1 æ ¸å¿ƒæŒ‡æ ‡

**å­˜å‚¨æ€§èƒ½å››å¤§æŒ‡æ ‡**:

```yaml
1. IOPS (Input/Output Operations Per Second):
   å®šä¹‰:
     - æ¯ç§’è¯»å†™æ“ä½œæ¬¡æ•°
     - è¡¡é‡éšæœºè¯»å†™æ€§èƒ½

   å…¸å‹å€¼:
     - HDD: 100-200 IOPS
     - SATA SSD: 10,000-100,000 IOPS
     - NVMe SSD: 100,000-1,000,000+ IOPS
     - Ceph (3-way replica): 20,000-50,000 IOPS

   å½±å“å› ç´ :
     - å­˜å‚¨ä»‹è´¨ (HDD/SSD/NVMe)
     - å—å¤§å° (4K/8K/16K)
     - è¯»å†™æ¯”ä¾‹ (Read/Write)
     - éšæœº/é¡ºåºè®¿é—®
     - é˜Ÿåˆ—æ·±åº¦ (QD)

2. Throughput (ååé‡):
   å®šä¹‰:
     - æ¯ç§’ä¼ è¾“çš„æ•°æ®é‡ (MB/s, GB/s)
     - è¡¡é‡é¡ºåºè¯»å†™æ€§èƒ½

   å…¸å‹å€¼:
     - HDD (7200RPM): 100-200 MB/s
     - SATA SSD: 500-600 MB/s
     - NVMe SSD: 3,000-7,000 MB/s
     - Ceph (3-way replica): 500-2,000 MB/s
     - 10GbEç½‘ç»œ: ~1,200 MB/s
     - 25GbEç½‘ç»œ: ~3,000 MB/s

   å½±å“å› ç´ :
     - å­˜å‚¨ä»‹è´¨
     - å—å¤§å°
     - ç½‘ç»œå¸¦å®½
     - CPUæ€§èƒ½
     - å†…å­˜

3. Latency (å»¶è¿Ÿ):
   å®šä¹‰:
     - å•æ¬¡I/Oæ“ä½œçš„å“åº”æ—¶é—´
     - å•ä½: æ¯«ç§’ (ms)ã€å¾®ç§’ (Î¼s)

   å…¸å‹å€¼:
     - HDD: 5-10 ms
     - SATA SSD: 0.1-1 ms
     - NVMe SSD: 0.01-0.1 ms (10-100 Î¼s)
     - Ceph (local): 1-5 ms
     - Ceph (remote): 5-20 ms

   å½±å“å› ç´ :
     - å­˜å‚¨ä»‹è´¨
     - ç½‘ç»œå»¶è¿Ÿ
     - é˜Ÿåˆ—æ·±åº¦
     - CPUè°ƒåº¦
     - ç³»ç»Ÿè´Ÿè½½

   P99å»¶è¿Ÿ:
     - 99%çš„è¯·æ±‚åœ¨æ­¤å»¶è¿Ÿå†…å®Œæˆ
     - æ¯”å¹³å‡å»¶è¿Ÿæ›´é‡è¦ (tail latency)

4. Capacity (å®¹é‡):
   å®šä¹‰:
     - å¯ç”¨å­˜å‚¨ç©ºé—´
     - å®é™…å¯ç”¨ vs åŸå§‹å®¹é‡

   è®¡ç®—:
     - Ceph (3-way replica): å®é™…å®¹é‡ = åŸå§‹å®¹é‡ / 3
     - Ceph (Erasure Coding 8+2): å®é™…å®¹é‡ = åŸå§‹å®¹é‡ * 0.8

   å®¹é‡è§„åˆ’:
     - é¢„ç•™20-30%ç©ºé—²ç©ºé—´
     - è€ƒè™‘å¢é•¿ç‡
     - æ‰©å®¹æ—¶é—´çª—å£
```

---

### 1.2 æŒ‡æ ‡é‡‡é›†

**Linuxå·¥å…·**:

```bash
# 1. iostat - I/Oç»Ÿè®¡
iostat -x 1 10
# å…³é”®åˆ—:
# - %util: è®¾å¤‡åˆ©ç”¨ç‡ (100%è¡¨ç¤ºé¥±å’Œ)
# - r/s, w/s: æ¯ç§’è¯»å†™æ¬¡æ•° (IOPS)
# - rMB/s, wMB/s: æ¯ç§’è¯»å†™ååé‡
# - await: å¹³å‡ç­‰å¾…æ—¶é—´ (å»¶è¿Ÿ)
# - svctm: å¹³å‡æœåŠ¡æ—¶é—´

# 2. sar - ç³»ç»Ÿæ´»åŠ¨æŠ¥å‘Š
sar -d 1 10
# ç£ç›˜I/Oç»Ÿè®¡

# 3. iotop - I/Oè¿›ç¨‹ç›‘æ§
iotop -o
# æ˜¾ç¤ºI/Oå ç”¨Topè¿›ç¨‹

# 4. blktrace - å—è®¾å¤‡è·Ÿè¸ª
blktrace -d /dev/sda -o trace
blkparse -i trace

# 5. dstat - ç»¼åˆç›‘æ§
dstat -cdngy
# CPUã€ç£ç›˜ã€ç½‘ç»œã€å†…å­˜ç»Ÿè®¡
```

---

### 1.3 æ€§èƒ½åŸºå‡†

**å­˜å‚¨æ€§èƒ½åˆ†çº§**:

```yaml
é«˜æ€§èƒ½ (High Performance):
  IOPS: 100,000+
  ååé‡: 3,000+ MB/s
  å»¶è¿Ÿ: < 1 ms (P99)
  åº”ç”¨: æ•°æ®åº“ã€å®æ—¶åˆ†æ
  æˆæœ¬: é«˜
  å­˜å‚¨: NVMe SSD, æœ¬åœ°SSD

ä¸­ç­‰æ€§èƒ½ (Medium Performance):
  IOPS: 10,000-100,000
  ååé‡: 500-3,000 MB/s
  å»¶è¿Ÿ: 1-5 ms (P99)
  åº”ç”¨: Webåº”ç”¨ã€ç¼“å­˜
  æˆæœ¬: ä¸­
  å­˜å‚¨: SATA SSD, Ceph RBD

ä¸€èˆ¬æ€§èƒ½ (Standard Performance):
  IOPS: 1,000-10,000
  ååé‡: 100-500 MB/s
  å»¶è¿Ÿ: 5-20 ms (P99)
  åº”ç”¨: æ—¥å¿—ã€å½’æ¡£
  æˆæœ¬: ä½
  å­˜å‚¨: HDD, Ceph (HDD)

å½’æ¡£ (Archive):
  IOPS: < 1,000
  ååé‡: < 100 MB/s
  å»¶è¿Ÿ: 20+ ms
  åº”ç”¨: å†·æ•°æ®
  æˆæœ¬: æä½
  å­˜å‚¨: å¯¹è±¡å­˜å‚¨, ç£å¸¦
```

---

## 2. æ€§èƒ½æµ‹è¯•

### 2.1 FIOæµ‹è¯•

**FIO (Flexible I/O Tester)** æ˜¯æœ€å¼ºå¤§çš„å­˜å‚¨æ€§èƒ½æµ‹è¯•å·¥å…·ã€‚

**å®‰è£…FIO**:

```bash
# Ubuntu/Debian
apt-get install -y fio

# CentOS/RHEL
yum install -y fio
```

**éšæœºè¯»æµ‹è¯•**:

```bash
# éšæœºè¯» (4Kå—, QD=32)
fio --name=randread --ioengine=libaio --iodepth=32 --rw=randread \
    --bs=4k --direct=1 --size=1G --numjobs=4 --runtime=60 \
    --group_reporting --filename=/mnt/test-file

# å‚æ•°è¯´æ˜:
# --ioengine=libaio: ä½¿ç”¨Linuxå¼‚æ­¥I/O
# --iodepth=32: é˜Ÿåˆ—æ·±åº¦32
# --rw=randread: éšæœºè¯»
# --bs=4k: å—å¤§å°4KB
# --direct=1: ç»•è¿‡ç¼“å­˜ (Direct I/O)
# --size=1G: æµ‹è¯•æ–‡ä»¶å¤§å°
# --numjobs=4: 4ä¸ªå¹¶å‘ä»»åŠ¡
# --runtime=60: è¿è¡Œ60ç§’
```

**éšæœºå†™æµ‹è¯•**:

```bash
# éšæœºå†™ (4Kå—, QD=32)
fio --name=randwrite --ioengine=libaio --iodepth=32 --rw=randwrite \
    --bs=4k --direct=1 --size=1G --numjobs=4 --runtime=60 \
    --group_reporting --filename=/mnt/test-file
```

**é¡ºåºè¯»æµ‹è¯•**:

```bash
# é¡ºåºè¯» (1Må—, QD=16)
fio --name=seqread --ioengine=libaio --iodepth=16 --rw=read \
    --bs=1m --direct=1 --size=4G --numjobs=1 --runtime=60 \
    --group_reporting --filename=/mnt/test-file
```

**é¡ºåºå†™æµ‹è¯•**:

```bash
# é¡ºåºå†™ (1Må—, QD=16)
fio --name=seqwrite --ioengine=libaio --iodepth=16 --rw=write \
    --bs=1m --direct=1 --size=4G --numjobs=1 --runtime=60 \
    --group_reporting --filename=/mnt/test-file
```

**æ··åˆè¯»å†™æµ‹è¯•**:

```bash
# 70%è¯» + 30%å†™
fio --name=randrw --ioengine=libaio --iodepth=32 --rw=randrw \
    --rwmixread=70 --bs=4k --direct=1 --size=1G --numjobs=4 \
    --runtime=60 --group_reporting --filename=/mnt/test-file
```

**FIOé…ç½®æ–‡ä»¶**:

```ini
# fio.conf
[global]
ioengine=libaio
direct=1
size=1G
runtime=60
group_reporting

[randread-4k]
rw=randread
bs=4k
iodepth=32
numjobs=4
filename=/mnt/test-file-1

[randwrite-4k]
rw=randwrite
bs=4k
iodepth=32
numjobs=4
filename=/mnt/test-file-2

[seqread-1m]
rw=read
bs=1m
iodepth=16
numjobs=1
filename=/mnt/test-file-3

[seqwrite-1m]
rw=write
bs=1m
iodepth=16
numjobs=1
filename=/mnt/test-file-4

# è¿è¡Œ:
# fio fio.conf
```

**è§£æFIOè¾“å‡º**:

```bash
# ç¤ºä¾‹è¾“å‡º:
randread: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=32
...
  read: IOPS=12.5k, BW=48.8MiB/s (51.2MB/s)(2930MiB/60001msec)
    slat (usec): min=2, max=1234, avg=12.34, stdev=23.45
    clat (usec): min=123, max=12345, avg=2048.56, stdev=345.67
     lat (usec): min=234, max=13456, avg=2060.90, stdev=347.89
    clat percentiles (usec):
     |  1.00th=[  300],  5.00th=[  400], 10.00th=[  500], 20.00th=[  700],
     | 30.00th=[ 1000], 40.00th=[ 1500], 50.00th=[ 1900], 60.00th=[ 2200],
     | 70.00th=[ 2600], 80.00th=[ 3000], 90.00th=[ 3500], 95.00th=[ 4000],
     | 99.00th=[ 5000], 99.50th=[ 5500], 99.90th=[ 7000], 99.95th=[ 8000],
     | 99.99th=[10000]

# å…³é”®æŒ‡æ ‡:
# - IOPS=12.5k: æ¯ç§’12,500æ¬¡è¯»æ“ä½œ
# - BW=48.8MiB/s: ååé‡48.8 MB/s
# - avg=2048.56: å¹³å‡å»¶è¿Ÿ2.05 ms
# - 99.00th=[5000]: P99å»¶è¿Ÿ5 ms
```

---

### 2.2 ddæµ‹è¯•

**dd (Data Duplicator)** æ˜¯ç®€å•çš„I/Oæµ‹è¯•å·¥å…·ã€‚

**é¡ºåºå†™æµ‹è¯•**:

```bash
# å†™å…¥1GBæ•°æ®
dd if=/dev/zero of=/mnt/test-file bs=1M count=1024 oflag=direct
# 1024+0 records in
# 1024+0 records out
# 1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.12345 s, 505 MB/s
```

**é¡ºåºè¯»æµ‹è¯•**:

```bash
# è¯»å–1GBæ•°æ®
dd if=/mnt/test-file of=/dev/null bs=1M count=1024 iflag=direct
```

**éšæœºè¯»æµ‹è¯•** (ä½¿ç”¨dd + seek):

```bash
# éšæœºè¯» (ä¸æ¨èï¼ŒFIOæ›´å¥½)
for i in {1..1000}; do
  dd if=/mnt/test-file of=/dev/null bs=4k count=1 skip=$((RANDOM % 262144)) iflag=direct 2>&1 | grep copied
done
```

---

### 2.3 sysbenchæµ‹è¯•

**sysbench** æ”¯æŒæ–‡ä»¶I/Oå’Œæ•°æ®åº“æ€§èƒ½æµ‹è¯•ã€‚

**å®‰è£…sysbench**:

```bash
# Ubuntu/Debian
apt-get install -y sysbench

# CentOS/RHEL
yum install -y sysbench
```

**æ–‡ä»¶I/Oæµ‹è¯•**:

```bash
# 1. å‡†å¤‡æµ‹è¯•æ–‡ä»¶ (16ä¸ª, æ¯ä¸ª1GB)
sysbench fileio --file-total-size=16G prepare

# 2. éšæœºè¯»å†™æµ‹è¯•
sysbench fileio --file-total-size=16G --file-test-mode=rndrw \
  --time=60 --max-requests=0 --threads=16 run

# 3. é¡ºåºè¯»æµ‹è¯•
sysbench fileio --file-total-size=16G --file-test-mode=seqrd \
  --time=60 --max-requests=0 --threads=4 run

# 4. é¡ºåºå†™æµ‹è¯•
sysbench fileio --file-total-size=16G --file-test-mode=seqwr \
  --time=60 --max-requests=0 --threads=4 run

# 5. æ¸…ç†
sysbench fileio --file-total-size=16G cleanup
```

---

### 2.4 Kuberneteså­˜å‚¨æ€§èƒ½æµ‹è¯•

**dbench** - Kuberneteså­˜å‚¨æ€§èƒ½æµ‹è¯•å·¥å…·:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: dbench
spec:
  template:
    spec:
      containers:
      - name: dbench
        image: sotoaster/dbench:latest
        imagePullPolicy: Always
        env:
        - name: DBENCH_MOUNTPOINT
          value: /data
        volumeMounts:
        - name: dbench-pv
          mountPath: /data
      restartPolicy: Never
      volumes:
      - name: dbench-pv
        persistentVolumeClaim:
          claimName: dbench-pvc
  backoffLimit: 4

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dbench-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: rook-ceph-block

# è¿è¡Œ:
# kubectl apply -f dbench.yaml
# kubectl logs job/dbench
```

**kubestr** - Kuberneteså­˜å‚¨æµ‹è¯•å·¥å…·:

```bash
# å®‰è£…kubestr
curl -LO https://github.com/kastenhq/kubestr/releases/download/v0.4.37/kubestr-v0.4.37-linux-amd64.tar.gz
tar -xvzf kubestr-v0.4.37-linux-amd64.tar.gz
chmod +x kubestr
mv kubestr /usr/local/bin/

# FIOæµ‹è¯•
kubestr fio -s rook-ceph-block -z 10Gi

# ç¤ºä¾‹è¾“å‡º:
# PVC created kubestr-fio-pvc-xxxxx
# Pod created kubestr-fio-pod-xxxxx
# Running FIO test (random-read-write) on StorageClass (rook-ceph-block)...
#
# FIO test results:
#   READ IOPS:    12345
#   WRITE IOPS:   10234
#   READ BW:      48.2 MB/s
#   WRITE BW:     40.1 MB/s
#   READ LATENCY: 2.5 ms
#   WRITE LATENCY: 3.1 ms
```

---

## 3. Cephæ€§èƒ½ä¼˜åŒ–

### 3.1 OSDä¼˜åŒ–

**Ceph OSDå‚æ•°è°ƒä¼˜**:

```yaml
# ceph.conf
[osd]
# 1. Journal/WALä¼˜åŒ– (BlueStore)
bluestore_block_db_size = 21474836480  # 20GB DB (SSD)
bluestore_block_wal_size = 1073741824  # 1GB WAL (SSD)
bluestore_min_alloc_size_hdd = 65536   # 64KB (HDD)
bluestore_min_alloc_size_ssd = 16384   # 16KB (SSD)

# 2. BlueStoreç¼“å­˜
bluestore_cache_size_hdd = 1073741824  # 1GB (HDD)
bluestore_cache_size_ssd = 3221225472  # 3GB (SSD)

# 3. OSDçº¿ç¨‹
osd_op_threads = 8                     # OSDæ“ä½œçº¿ç¨‹ (å»ºè®®: CPUæ ¸æ•°)
osd_op_num_threads_per_shard = 2      # æ¯åˆ†ç‰‡çº¿ç¨‹æ•°
osd_op_num_shards = 32                 # åˆ†ç‰‡æ•°

# 4. å®¢æˆ·ç«¯æ¶ˆæ¯é˜Ÿåˆ—
osd_client_message_size_cap = 524288000  # 500MB

# 5. Recovery (æ¢å¤)
osd_max_backfills = 1                  # æœ€å¤§åŒæ—¶æ¢å¤æ•°
osd_recovery_max_active = 3            # æœ€å¤§æ´»è·ƒæ¢å¤
osd_recovery_op_priority = 3           # æ¢å¤ä¼˜å…ˆçº§ (ä½)
osd_recovery_sleep_hdd = 0.1           # HDDæ¢å¤å»¶è¿Ÿ
osd_recovery_sleep_ssd = 0             # SSDæ— å»¶è¿Ÿ

# 6. Scrub (æ ¡éªŒ)
osd_scrub_begin_hour = 2               # å‡Œæ™¨2ç‚¹å¼€å§‹
osd_scrub_end_hour = 6                 # æ—©ä¸Š6ç‚¹ç»“æŸ
osd_scrub_during_recovery = false      # æ¢å¤æ—¶ä¸æ ¡éªŒ
osd_scrub_load_threshold = 0.5         # è´Ÿè½½é˜ˆå€¼
```

**åº”ç”¨é…ç½®**:

```bash
# æ–¹å¼1: ceph.conf (é‡å¯OSD)
# ç¼–è¾‘ceph.confåé‡å¯OSD Pod

# æ–¹å¼2: åœ¨çº¿è°ƒæ•´ (ä¸´æ—¶)
ceph tell osd.* injectargs '--osd-max-backfills 1'
ceph tell osd.* injectargs '--osd-recovery-max-active 3'

# æ–¹å¼3: é…ç½®æ•°æ®åº“ (æ°¸ä¹…)
ceph config set osd osd_max_backfills 1
ceph config set osd osd_recovery_max_active 3
```

---

### 3.2 ç½‘ç»œä¼˜åŒ–

**Cephç½‘ç»œé…ç½®**:

```yaml
ç½‘ç»œè¦æ±‚:
  å‰ç«¯ç½‘ç»œ (client):
    - 10GbE+ (æ¨è25GbE)
    - ä½å»¶è¿Ÿ (< 1ms)
    - ç”¨äºå®¢æˆ·ç«¯è®¿é—®

  åç«¯ç½‘ç»œ (cluster):
    - 10GbE+ (æ¨è25GbEæˆ–40GbE)
    - é«˜å¸¦å®½
    - ç”¨äºOSDå¤åˆ¶

ä¼˜åŒ–:
  1. ä¸“ç”¨ç½‘ç»œ:
     - å‰åç«¯åˆ†ç¦»
     - VLANéš”ç¦»
     - é¿å…ä¸å…¶ä»–æµé‡ç«äº‰

  2. MTU (Jumbo Frame):
     - è®¾ç½®MTU=9000
     - å‡å°‘TCPåˆ†ç‰‡
     - æå‡ååé‡

  3. TCPå‚æ•°:
     - tcp_wmem/tcp_rmem
     - tcp_congestion_control=bbr
     - net.core.rmem_max/wmem_max
```

**é…ç½®MTU=9000**:

```bash
# 1. æ£€æŸ¥å½“å‰MTU
ip link show | grep mtu

# 2. è®¾ç½®MTU (ä¸´æ—¶)
ip link set eth0 mtu 9000
ip link set eth1 mtu 9000

# 3. æ°¸ä¹…é…ç½® (/etc/network/interfaces)
auto eth0
iface eth0 inet static
  address 10.0.1.10
  netmask 255.255.255.0
  mtu 9000

# 4. éªŒè¯
ping -M do -s 8972 10.0.1.11
# 8972 + 28 (IP+ICMP header) = 9000
```

**TCPè°ƒä¼˜**:

```bash
# /etc/sysctl.conf
net.core.rmem_max = 536870912         # 512MBæ¥æ”¶ç¼“å†²
net.core.wmem_max = 536870912         # 512MBå‘é€ç¼“å†²
net.ipv4.tcp_rmem = 4096 87380 536870912
net.ipv4.tcp_wmem = 4096 65536 536870912
net.ipv4.tcp_congestion_control = bbr  # BBRæ‹¥å¡æ§åˆ¶
net.core.netdev_max_backlog = 5000
net.ipv4.tcp_max_syn_backlog = 8192

# åº”ç”¨
sysctl -p
```

---

### 3.3 å®¢æˆ·ç«¯ä¼˜åŒ–

**RBDå®¢æˆ·ç«¯ä¼˜åŒ–**:

```yaml
# StorageClasså‚æ•°
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block-optimized
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: replicapool
  imageFormat: "2"
  imageFeatures: layering,exclusive-lock,object-map,fast-diff
  csi.storage.k8s.io/fstype: ext4

  # RBDæ€§èƒ½å‚æ•°
  mounter: rbd-nbd  # æˆ– rbd-kernel
  mapOptions: "krbd:rxbounce"

reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
- discard                # TRIMæ”¯æŒ
- noatime                # ä¸æ›´æ–°è®¿é—®æ—¶é—´
- nodiratime             # ç›®å½•ä¸æ›´æ–°è®¿é—®æ—¶é—´
```

**CephFSå®¢æˆ·ç«¯ä¼˜åŒ–**:

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs-optimized
provisioner: rook-ceph.cephfs.csi.ceph.com
parameters:
  clusterID: rook-ceph
  fsName: myfs
  pool: myfs-data0

  # CephFSæ€§èƒ½å‚æ•°
  mounter: kernel  # æˆ– fuse

reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
- noatime
- nodiratime
- readdir_max_entries=1000
```

---

## 4. æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–

### 4.1 æ–‡ä»¶ç³»ç»Ÿé€‰æ‹©

**æ–‡ä»¶ç³»ç»Ÿå¯¹æ¯”**:

```yaml
ext4:
  ä¼˜åŠ¿:
    âœ… æˆç†Ÿç¨³å®š
    âœ… å…¼å®¹æ€§å¥½
    âœ… æ€§èƒ½å‡è¡¡

  åŠ£åŠ¿:
    âŒ ä¸æ”¯æŒå†™æ—¶å¤åˆ¶ (COW)
    âŒ æ‰©å±•æ€§ä¸€èˆ¬

  é€‚ç”¨:
    - é€šç”¨åœºæ™¯
    - æ•°æ®åº“

  æ¨èé…ç½®:
    mkfs.ext4 -E lazy_itable_init=0,lazy_journal_init=0 /dev/sda1

xfs:
  ä¼˜åŠ¿:
    âœ… é«˜æ€§èƒ½
    âœ… å¤§æ–‡ä»¶æ”¯æŒå¥½
    âœ… å¹¶è¡ŒI/O
    âœ… åœ¨çº¿æ‰©å®¹

  åŠ£åŠ¿:
    âŒ ä¸æ”¯æŒåœ¨çº¿ç¼©å®¹
    âŒ å…ƒæ•°æ®æ¢å¤æ…¢

  é€‚ç”¨:
    - å¤§æ–‡ä»¶
    - é«˜å¹¶å‘
    - Ceph OSD (BlueStoreåœ¨è£¸ç›˜ä¸Šè¿è¡Œï¼Œä¸éœ€è¦æ–‡ä»¶ç³»ç»Ÿ)

  æ¨èé…ç½®:
    mkfs.xfs -f -i size=2048 /dev/sda1

btrfs:
  ä¼˜åŠ¿:
    âœ… å†™æ—¶å¤åˆ¶ (COW)
    âœ… å¿«ç…§
    âœ… æ•°æ®æ ¡éªŒ
    âœ… é€æ˜å‹ç¼©

  åŠ£åŠ¿:
    âŒ æ€§èƒ½å¼€é”€
    âŒ ç¨³å®šæ€§ (ç›¸å¯¹)

  é€‚ç”¨:
    - å¿«ç…§éœ€æ±‚
    - æ•°æ®æ ¡éªŒ

f2fs (Flash-Friendly File System):
  ä¼˜åŠ¿:
    âœ… ä¸ºSSDä¼˜åŒ–
    âœ… å‡å°‘å†™æ”¾å¤§

  é€‚ç”¨:
    - SSD/NVMe
    - Android

é€‰æ‹©å»ºè®®:
  é€šç”¨: ext4
  é«˜æ€§èƒ½: xfs
  å¿«ç…§: btrfs
  SSD: f2fs
```

---

### 4.2 æŒ‚è½½é€‰é¡¹

**ext4æŒ‚è½½é€‰é¡¹**:

```bash
mount -t ext4 -o noatime,nodiratime,discard,commit=60 /dev/sda1 /mnt

# é€‰é¡¹è¯´æ˜:
# - noatime: ä¸æ›´æ–°è®¿é—®æ—¶é—´ (å‡å°‘å†™å…¥)
# - nodiratime: ç›®å½•ä¸æ›´æ–°è®¿é—®æ—¶é—´
# - discard: TRIMæ”¯æŒ (SSD)
# - commit=60: 60ç§’æäº¤ä¸€æ¬¡ (é»˜è®¤5ç§’)
```

**/etc/fstabé…ç½®**:

```bash
# /etc/fstab
/dev/sda1  /mnt  ext4  noatime,nodiratime,discard,commit=60  0 2
```

**xfsæŒ‚è½½é€‰é¡¹**:

```bash
mount -t xfs -o noatime,nodiratime,discard,logbsize=256k /dev/sda1 /mnt

# é€‰é¡¹è¯´æ˜:
# - logbsize=256k: æ—¥å¿—ç¼“å†²åŒº256KB (é»˜è®¤32KB)
```

---

## 5. å†…æ ¸å‚æ•°ä¼˜åŒ–

### 5.1 I/Oè°ƒåº¦å™¨

**Linux I/Oè°ƒåº¦å™¨**:

```yaml
è°ƒåº¦å™¨ç±»å‹:

1. noop (No Operation):
   ç‰¹ç‚¹:
     - æœ€ç®€å•
     - FIFOé˜Ÿåˆ—
     - æ— åˆå¹¶ã€æ— æ’åº

   é€‚ç”¨:
     - SSD/NVMe (è‡ªå¸¦è°ƒåº¦)
     - RAIDæ§åˆ¶å™¨

   æ€§èƒ½:
     - å»¶è¿Ÿä½
     - IOPSé«˜

2. deadline:
   ç‰¹ç‚¹:
     - ä¿è¯æœ€å¤§å»¶è¿Ÿ
     - è¯»ä¼˜å…ˆ

   é€‚ç”¨:
     - æ•°æ®åº“
     - å®æ—¶åº”ç”¨

   æ€§èƒ½:
     - å»¶è¿Ÿå¯é¢„æµ‹
     - è¯»æ€§èƒ½å¥½

3. cfq (Completely Fair Queuing):
   ç‰¹ç‚¹:
     - å…¬å¹³è°ƒåº¦
     - æŒ‰è¿›ç¨‹åˆ†é…å¸¦å®½

   é€‚ç”¨:
     - é€šç”¨æ¡Œé¢
     - å¤šä»»åŠ¡

   æ€§èƒ½:
     - å…¬å¹³æ€§å¥½
     - ååé‡ä¸€èˆ¬

4. mq-deadline (Multi-Queue Deadline):
   ç‰¹ç‚¹:
     - å¤šé˜Ÿåˆ—
     - NVMeä¼˜åŒ–

   é€‚ç”¨:
     - NVMe SSD
     - é«˜IOPS

   æ€§èƒ½:
     - é«˜å¹¶å‘
     - ä½å»¶è¿Ÿ

5. kyber:
   ç‰¹ç‚¹:
     - åŠ¨æ€è°ƒæ•´
     - å»¶è¿Ÿä¼˜å…ˆ

   é€‚ç”¨:
     - NVMe SSD
     - å»¶è¿Ÿæ•æ„Ÿ

   æ€§èƒ½:
     - P99å»¶è¿Ÿä½
```

**æŸ¥çœ‹å’Œè®¾ç½®I/Oè°ƒåº¦å™¨**:

```bash
# æŸ¥çœ‹å½“å‰è°ƒåº¦å™¨
cat /sys/block/sda/queue/scheduler
# [mq-deadline] kyber bfq none

# ä¸´æ—¶è®¾ç½®
echo deadline > /sys/block/sda/queue/scheduler

# æ°¸ä¹…è®¾ç½® (GRUB)
# /etc/default/grub
GRUB_CMDLINE_LINUX="elevator=deadline"
# æ›´æ–°GRUB:
update-grub

# æˆ–ä½¿ç”¨udevè§„åˆ™
# /etc/udev/rules.d/60-scheduler.rules
ACTION=="add|change", KERNEL=="sd[a-z]", ATTR{queue/scheduler}="deadline"
ACTION=="add|change", KERNEL=="nvme[0-9]n[0-9]", ATTR{queue/scheduler}="none"
```

**æ¨èé…ç½®**:

```yaml
å­˜å‚¨ç±»å‹æ¨è:
  HDD: deadlineæˆ–cfq
  SATA SSD: deadlineæˆ–noop
  NVMe SSD: noneæˆ–mq-deadlineæˆ–kyber
  Ceph OSD (HDD): deadline
  Ceph OSD (SSD): noneæˆ–mq-deadline
```

---

### 5.2 å†…æ ¸å‚æ•°

**å­˜å‚¨ç›¸å…³å†…æ ¸å‚æ•°**:

```bash
# /etc/sysctl.conf

# 1. è™šæ‹Ÿå†…å­˜
vm.swappiness = 10                    # é™ä½swapä½¿ç”¨ (é»˜è®¤60)
vm.dirty_ratio = 15                   # è„é¡µæ¯”ä¾‹15% (é»˜è®¤20%)
vm.dirty_background_ratio = 5         # åå°å†™å›5% (é»˜è®¤10%)
vm.vfs_cache_pressure = 50            # ç¼“å­˜å‹åŠ› (é»˜è®¤100)

# 2. I/O
vm.min_free_kbytes = 1048576          # æœ€å°ç©ºé—²å†…å­˜1GB
kernel.pid_max = 4194303              # æœ€å¤§PIDæ•°

# 3. æ–‡ä»¶ç³»ç»Ÿ
fs.file-max = 6553600                 # æœ€å¤§æ‰“å¼€æ–‡ä»¶æ•°
fs.aio-max-nr = 1048576               # æœ€å¤§å¼‚æ­¥I/Oè¯·æ±‚

# 4. ç½‘ç»œ (å·²åœ¨3.2èŠ‚)
...

# åº”ç”¨
sysctl -p
```

**éªŒè¯**:

```bash
# æŸ¥çœ‹å½“å‰å€¼
sysctl vm.swappiness
sysctl vm.dirty_ratio

# æŸ¥çœ‹æ‰€æœ‰
sysctl -a | grep vm.
```

---

## 6. å®¹é‡è§„åˆ’

### 6.1 å®¹é‡é¢„æµ‹

**å®¹é‡å¢é•¿æ¨¡å‹**:

```yaml
çº¿æ€§å¢é•¿æ¨¡å‹:
  å…¬å¼: å®¹é‡(t) = åˆå§‹å®¹é‡ + å¢é•¿ç‡ * t

  ç¤ºä¾‹:
    åˆå§‹å®¹é‡: 100TB
    æœˆå¢é•¿ç‡: 5TB/æœˆ
    6ä¸ªæœˆå: 100 + 5 * 6 = 130TB

æŒ‡æ•°å¢é•¿æ¨¡å‹:
  å…¬å¼: å®¹é‡(t) = åˆå§‹å®¹é‡ * (1 + å¢é•¿ç‡)^t

  ç¤ºä¾‹:
    åˆå§‹å®¹é‡: 100TB
    æœˆå¢é•¿ç‡: 5% (0.05)
    6ä¸ªæœˆå: 100 * (1.05)^6 â‰ˆ 134TB

å®é™…å¢é•¿ (æ··åˆ):
  - åº”ç”¨æ•°æ®: çº¿æ€§å¢é•¿
  - æ—¥å¿—/ç›‘æ§: çº¿æ€§å¢é•¿
  - å¤‡ä»½/å¿«ç…§: æŒ‡æ•°å¢é•¿ (å¦‚æœä¿ç•™å†å²)
  - AI/MLæ•°æ®: æŒ‡æ•°å¢é•¿
```

**å®¹é‡ç›‘æ§**:

```bash
# Cephå®¹é‡
ceph df
# RAW STORAGE:
#     CLASS     SIZE        AVAIL       USED        RAW USED     %RAW USED
#     hdd       100 TiB     80 TiB      20 TiB      20 TiB           20.00
#     ssd        20 TiB     16 TiB       4 TiB       4 TiB           20.00
#     TOTAL     120 TiB     96 TiB      24 TiB      24 TiB           20.00
#
# POOLS:
#     POOL              ID     STORED      OBJECTS     USED        %USED     MAX AVAIL
#     replicapool        1     6.5 TiB     1.7M        19.5 TiB    24.38     25.5 TiB
#     myfs-data0         2       1 TiB     256k         3 TiB      3.75     25.5 TiB

# Kubernetes PVCä½¿ç”¨
kubectl get pvc -A -o custom-columns=\
NAMESPACE:.metadata.namespace,\
NAME:.metadata.name,\
CAPACITY:.status.capacity.storage,\
STORAGECLASS:.spec.storageClassName

# èšåˆç»Ÿè®¡
kubectl get pvc -A -o json | jq -r '
  .items[] | select(.status.phase == "Bound") |
  .status.capacity.storage' |
  awk '{sum+=$1} END {print sum}'
```

---

### 6.2 æ‰©å®¹ç­–ç•¥

**Cephæ‰©å®¹**:

```yaml
æ‰©å®¹æ–¹å¼:
  1. æ·»åŠ OSD (æ¨ªå‘æ‰©å±•):
     - å¢åŠ å­˜å‚¨å®¹é‡
     - æå‡IOPSå’Œååé‡
     - Cephè‡ªåŠ¨Rebalance
     - æ¨è: æ¯æ¬¡æ·»åŠ 3-6ä¸ªOSD

  2. æ›¿æ¢OSD (çºµå‘æ‰©å±•):
     - æ›´å¤§å®¹é‡ç£ç›˜
     - é€ä¸ªæ›¿æ¢
     - Rebalanceå½±å“å°

  3. æ·»åŠ èŠ‚ç‚¹:
     - æ¨ªå‘æ‰©å±•
     - æå‡ç½‘ç»œå¸¦å®½
     - æå‡å†—ä½™

æ‰©å®¹æµç¨‹:
  1. è§„åˆ’:
     - å®¹é‡éœ€æ±‚
     - æ€§èƒ½éœ€æ±‚
     - é¢„ç®—

  2. æ·»åŠ OSD:
     - æ·»åŠ ç£ç›˜åˆ°èŠ‚ç‚¹
     - Rookè‡ªåŠ¨å‘ç°
     - æˆ–æ‰‹åŠ¨æ·»åŠ 

  3. Rebalance:
     - Cephè‡ªåŠ¨æ•°æ®è¿ç§»
     - ç›‘æ§Rebalanceè¿›åº¦
     - æ§åˆ¶Rebalanceé€Ÿåº¦

  4. éªŒè¯:
     - æ£€æŸ¥OSDçŠ¶æ€
     - æ£€æŸ¥PGåˆ†å¸ƒ
     - æ€§èƒ½æµ‹è¯•

Rebalanceæ§åˆ¶:
  # é™ä½Rebalanceå½±å“ (ç™½å¤©)
  ceph tell osd.* injectargs '--osd-max-backfills 1'
  ceph tell osd.* injectargs '--osd-recovery-max-active 1'
  ceph tell osd.* injectargs '--osd-recovery-sleep-hdd 0.5'

  # åŠ é€ŸRebalance (å¤œé—´)
  ceph tell osd.* injectargs '--osd-max-backfills 4'
  ceph tell osd.* injectargs '--osd-recovery-max-active 6'
  ceph tell osd.* injectargs '--osd-recovery-sleep-hdd 0'
```

---

### 6.3 æˆæœ¬ä¼˜åŒ–

**å­˜å‚¨æˆæœ¬ä¼˜åŒ–ç­–ç•¥**:

```yaml
1. åˆ†å±‚å­˜å‚¨ (Tiering):
   çƒ­æ•°æ® (Hot):
     - SSD/NVMe
     - é«˜IOPS
     - é«˜æˆæœ¬
     - ç¤ºä¾‹: æ•°æ®åº“

   æ¸©æ•°æ® (Warm):
     - SATA SSD
     - ä¸­ç­‰IOPS
     - ä¸­ç­‰æˆæœ¬
     - ç¤ºä¾‹: åº”ç”¨æ–‡ä»¶

   å†·æ•°æ® (Cold):
     - HDD
     - ä½IOPS
     - ä½æˆæœ¬
     - ç¤ºä¾‹: æ—¥å¿—ã€å½’æ¡£

   å†°æ•°æ® (Frozen):
     - å¯¹è±¡å­˜å‚¨
     - æä½IOPS
     - æä½æˆæœ¬
     - ç¤ºä¾‹: å¤‡ä»½

2. å‹ç¼©:
   - Ceph BlueStoreå‹ç¼©
   - èŠ‚çœ30-70%ç©ºé—´
   - è½»å¾®æ€§èƒ½æŸå¤±

   é…ç½®:
     ceph osd pool set replicapool compression_mode aggressive
     ceph osd pool set replicapool compression_algorithm snappy

3. Erasure Coding:
   - æ›¿ä»£3å‰¯æœ¬
   - èŠ‚çœ50%+ç©ºé—´
   - é€‚ç”¨äºå†·æ•°æ®

   ç¤ºä¾‹:
     3å‰¯æœ¬: å®é™…å®¹é‡ = åŸå§‹ / 3 (33%åˆ©ç”¨ç‡)
     EC 8+2: å®é™…å®¹é‡ = åŸå§‹ * 0.8 (80%åˆ©ç”¨ç‡)

   é…ç½®:
     ceph osd pool create ec-pool 128 128 erasure
     ceph osd pool set ec-pool allow_ec_overwrites true

4. ç”Ÿå‘½å‘¨æœŸç®¡ç†:
   - è‡ªåŠ¨è¿ç§»æ•°æ® (Hot -> Warm -> Cold)
   - è‡ªåŠ¨åˆ é™¤è¿‡æœŸæ•°æ®
   - ç¤ºä¾‹: æ—¥å¿—ä¿ç•™30å¤©

5. é‡å¤æ•°æ®åˆ é™¤ (Deduplication):
   - é€‚ç”¨äºå¤‡ä»½
   - èŠ‚çœç©ºé—´
   - æ€§èƒ½å¼€é”€

æˆæœ¬æ¨¡å‹:
  SSDæˆæœ¬: ~$100/TB
  HDDæˆæœ¬: ~$20/TB
  å¯¹è±¡å­˜å‚¨: ~$5/TB (äº‘)

  ç¤ºä¾‹ (100TB):
    å…¨SSD: 100TB * $100 = $10,000
    å…¨HDD: 100TB * $20 = $2,000
    åˆ†å±‚ (20TB SSD + 80TB HDD):
      20 * $100 + 80 * $20 = $3,600
```

---

## 7. æ€§èƒ½ç›‘æ§

### 7.1 Prometheusç›‘æ§

**Ceph Exporter**:

```yaml
# Rookè‡ªåŠ¨éƒ¨ç½²Ceph Exporter
# æŸ¥çœ‹PrometheusæŒ‡æ ‡:
kubectl -n rook-ceph port-forward svc/rook-ceph-mgr 9283:9283
curl http://localhost:9283/metrics

# å…³é”®æŒ‡æ ‡:
ceph_health_status                        # é›†ç¾¤å¥åº·çŠ¶æ€
ceph_osd_up                               # OSDå¯åŠ¨æ•°
ceph_osd_in                               # OSDåŠ å…¥æ•°
ceph_pool_stored_bytes                    # Poolå­˜å‚¨å­—èŠ‚æ•°
ceph_pool_objects                         # Poolå¯¹è±¡æ•°
ceph_osd_apply_latency_ms                 # OSDåº”ç”¨å»¶è¿Ÿ
ceph_osd_commit_latency_ms                # OSDæäº¤å»¶è¿Ÿ
ceph_pool_rd_bytes                        # Poolè¯»å­—èŠ‚æ•°
ceph_pool_wr_bytes                        # Poolå†™å­—èŠ‚æ•°
```

**Prometheusé…ç½®**:

```yaml
# prometheus.yaml
scrape_configs:
- job_name: 'ceph'
  static_configs:
  - targets: ['rook-ceph-mgr.rook-ceph:9283']
```

---

### 7.2 Grafanaä»ªè¡¨ç›˜

**å¯¼å…¥Cephä»ªè¡¨ç›˜**:

```bash
# Grafanaä»ªè¡¨ç›˜ID:
# - 7056: Ceph Cluster
# - 5336: Ceph OSD
# - 5342: Ceph Pools

# å¯¼å…¥æ­¥éª¤:
# 1. Grafana -> Dashboards -> Import
# 2. è¾“å…¥ID: 7056
# 3. é€‰æ‹©Prometheusæ•°æ®æº
# 4. Import
```

**è‡ªå®šä¹‰ä»ªè¡¨ç›˜**:

```json
{
  "dashboard": {
    "title": "Ceph Performance",
    "panels": [
      {
        "title": "IOPS",
        "targets": [
          {
            "expr": "rate(ceph_pool_rd[5m]) + rate(ceph_pool_wr[5m])"
          }
        ]
      },
      {
        "title": "Throughput",
        "targets": [
          {
            "expr": "rate(ceph_pool_rd_bytes[5m]) + rate(ceph_pool_wr_bytes[5m])"
          }
        ]
      },
      {
        "title": "Latency (P99)",
        "targets": [
          {
            "expr": "histogram_quantile(0.99, rate(ceph_osd_op_latency_bucket[5m]))"
          }
        ]
      }
    ]
  }
}
```

---

### 7.3 å‘Šè­¦è§„åˆ™

**Prometheuså‘Šè­¦**:

```yaml
# ceph-alerts.yaml
groups:
- name: ceph
  interval: 30s
  rules:

  # é›†ç¾¤å¥åº·
  - alert: CephClusterWarning
    expr: ceph_health_status == 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Ceph cluster is in warning state"
      description: "Ceph cluster {{ $labels.instance }} has been in HEALTH_WARN for more than 5 minutes."

  - alert: CephClusterError
    expr: ceph_health_status == 2
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Ceph cluster is in error state"
      description: "Ceph cluster {{ $labels.instance }} is in HEALTH_ERR."

  # OSD
  - alert: CephOSDDown
    expr: ceph_osd_up == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Ceph OSD is down"
      description: "OSD {{ $labels.ceph_daemon }} on {{ $labels.hostname }} is down."

  # å®¹é‡
  - alert: CephPoolNearFull
    expr: ceph_pool_percent_used > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Ceph pool is near full"
      description: "Pool {{ $labels.name }} is {{ $value }}% full."

  # æ€§èƒ½
  - alert: CephHighLatency
    expr: ceph_osd_apply_latency_ms > 100
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Ceph high latency"
      description: "OSD {{ $labels.ceph_daemon }} latency is {{ $value }}ms."

  # IOPS
  - alert: CephLowIOPS
    expr: rate(ceph_pool_rd[5m]) + rate(ceph_pool_wr[5m]) < 100
    for: 15m
    labels:
      severity: info
    annotations:
      summary: "Ceph low IOPS"
      description: "Pool {{ $labels.name }} IOPS is {{ $value }}."
```

---

## 8. æ€»ç»“

```yaml
æ ¸å¿ƒçŸ¥è¯†:
  âœ… æ€§èƒ½æŒ‡æ ‡ (IOPS/ååé‡/å»¶è¿Ÿ/å®¹é‡)
  âœ… æ€§èƒ½æµ‹è¯• (FIO/dd/sysbench/kubestr)
  âœ… Cephä¼˜åŒ– (OSD/ç½‘ç»œ/å®¢æˆ·ç«¯)
  âœ… æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ– (ext4/xfs)
  âœ… å†…æ ¸ä¼˜åŒ– (I/Oè°ƒåº¦å™¨/sysctl)
  âœ… å®¹é‡è§„åˆ’ (é¢„æµ‹/æ‰©å®¹/æˆæœ¬)
  âœ… æ€§èƒ½ç›‘æ§ (Prometheus/Grafana/å‘Šè­¦)

ä»£ç ç¤ºä¾‹:
  - FIOæµ‹è¯•é…ç½®
  - Cephè°ƒä¼˜å‚æ•°
  - å†…æ ¸å‚æ•°
  - Prometheusç›‘æ§
  - Grafanaä»ªè¡¨ç›˜
  - 40+å‘½ä»¤å’Œé…ç½®

å®æˆ˜æŠ€èƒ½:
  - æ€§èƒ½åŸºå‡†æµ‹è¯•
  - Cephç”Ÿäº§è°ƒä¼˜
  - å®¹é‡è§„åˆ’
  - æ€§èƒ½ç›‘æ§å‘Šè­¦
```

---

**å®Œæˆæ—¥æœŸ**: 2025-10-19
**ç‰ˆæœ¬**: v1.0
**ä½œè€…**: äº‘åŸç”Ÿå­˜å‚¨ä¸“å®¶å›¢é˜Ÿ

**Tags**: `#Performance` `#Optimization` `#Ceph` `#FIO` `#Monitoring` `#CloudNativeStorage`

---

## ç›¸å…³æ–‡æ¡£

### æœ¬æ¨¡å—ç›¸å…³

- [äº‘åŸç”Ÿå­˜å‚¨æ¦‚è¿°ä¸æ¶æ„](./01_äº‘åŸç”Ÿå­˜å‚¨æ¦‚è¿°ä¸æ¶æ„.md) - äº‘åŸç”Ÿå­˜å‚¨æ¦‚è¿°ä¸æ¶æ„
- [Kuberneteså­˜å‚¨åŸºç¡€](./02_Kuberneteså­˜å‚¨åŸºç¡€.md) - Kuberneteså­˜å‚¨åŸºç¡€
- [Rook Cephæ·±åº¦è§£æ](./03_Rook_Cephæ·±åº¦è§£æ.md) - Rook Cephæ·±åº¦è§£æ
- [Veleroå¤‡ä»½æ¢å¤](./04_Veleroå¤‡ä»½æ¢å¤.md) - Veleroå¤‡ä»½æ¢å¤
- [CSIé©±åŠ¨è¯¦è§£](./05_CSIé©±åŠ¨è¯¦è§£.md) - CSIé©±åŠ¨è¯¦è§£
- [å¤šäº‘å­˜å‚¨](./07_å¤šäº‘å­˜å‚¨.md) - å¤šäº‘å­˜å‚¨
- [å­˜å‚¨å®‰å…¨](./08_å­˜å‚¨å®‰å…¨.md) - å­˜å‚¨å®‰å…¨
- [å®æˆ˜æ¡ˆä¾‹](./09_å®æˆ˜æ¡ˆä¾‹.md) - å®æˆ˜æ¡ˆä¾‹
- [æœ€ä½³å®è·µ](./10_æœ€ä½³å®è·µ.md) - æœ€ä½³å®è·µ
- [README.md](./README.md) - æœ¬æ¨¡å—å¯¼èˆª

### å…¶ä»–æ¨¡å—ç›¸å…³

- [å®¹å™¨æ€§èƒ½è°ƒä¼˜](../06_å®¹å™¨ç›‘æ§ä¸è¿ç»´/03_å®¹å™¨æ€§èƒ½è°ƒä¼˜.md) - å®¹å™¨æ€§èƒ½è°ƒä¼˜
- [å®¹å™¨ç›‘æ§æŠ€æœ¯](../06_å®¹å™¨ç›‘æ§ä¸è¿ç»´/01_å®¹å™¨ç›‘æ§æŠ€æœ¯.md) - å®¹å™¨ç›‘æ§æŠ€æœ¯
- [KubernetesæŠ€æœ¯è¯¦è§£](../03_KubernetesæŠ€æœ¯è¯¦è§£/README.md) - KubernetesæŠ€æœ¯ä½“ç³»
- [eBPFæ€§èƒ½ä¼˜åŒ–](../16_eBPFæŠ€æœ¯è¯¦è§£/06_eBPFæ€§èƒ½ä¼˜åŒ–.md) - eBPFæ€§èƒ½ä¼˜åŒ–

---

**æœ€åæ›´æ–°**: 2025å¹´11æœˆ11æ—¥
**ç»´æŠ¤çŠ¶æ€**: æŒç»­æ›´æ–°
