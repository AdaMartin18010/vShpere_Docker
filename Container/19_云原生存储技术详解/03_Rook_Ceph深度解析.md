# 03 - Rook/Cephæ·±åº¦è§£æ

**ä½œè€…**: äº‘åŸç”Ÿå­˜å‚¨ä¸“å®¶å›¢é˜Ÿ
**åˆ›å»ºæ—¥æœŸ**: 2025-10-19
**æœ€åæ›´æ–°**: 2025-10-19
**ç‰ˆæœ¬**: v1.0

---

## ğŸ“‹ æœ¬ç« å¯¼èˆª

- [03 - Rook/Cephæ·±åº¦è§£æ](#03---rookcephæ·±åº¦è§£æ)
  - [ğŸ“‹ æœ¬ç« å¯¼èˆª](#-æœ¬ç« å¯¼èˆª)
  - [1. Cephæ¶æ„ä¸åŸç†](#1-cephæ¶æ„ä¸åŸç†)
    - [1.1 Cephæ ¸å¿ƒç»„ä»¶](#11-cephæ ¸å¿ƒç»„ä»¶)
    - [1.2 CRUSHç®—æ³•](#12-crushç®—æ³•)
    - [1.3 æ•°æ®åˆ†å¸ƒä¸å‰¯æœ¬](#13-æ•°æ®åˆ†å¸ƒä¸å‰¯æœ¬)
    - [1.4 Poolç®¡ç†](#14-poolç®¡ç†)
  - [2. Rook Operator](#2-rook-operator)
    - [2.1 Rookæ¶æ„](#21-rookæ¶æ„)
    - [2.2 CRDè¯¦è§£](#22-crdè¯¦è§£)
    - [2.3 Operatoréƒ¨ç½²](#23-operatoréƒ¨ç½²)
    - [2.4 é›†ç¾¤ç®¡ç†](#24-é›†ç¾¤ç®¡ç†)
  - [3. å—å­˜å‚¨ (RBD)](#3-å—å­˜å‚¨-rbd)
    - [3.1 RBDæ¦‚è¿°ä¸StorageClass](#31-rbdæ¦‚è¿°ä¸storageclass)
  - [4. æ–‡ä»¶å­˜å‚¨ (CephFS)](#4-æ–‡ä»¶å­˜å‚¨-cephfs)
    - [4.1 CephFSéƒ¨ç½²](#41-cephfséƒ¨ç½²)
  - [5. å¯¹è±¡å­˜å‚¨ (RGW)](#5-å¯¹è±¡å­˜å‚¨-rgw)
    - [5.1 RGWéƒ¨ç½²](#51-rgwéƒ¨ç½²)
  - [6. ç›‘æ§ä¸å‘Šè­¦](#6-ç›‘æ§ä¸å‘Šè­¦)
    - [6.1 Prometheusé›†æˆ](#61-prometheusé›†æˆ)
  - [7. ç”Ÿäº§çº§è°ƒä¼˜](#7-ç”Ÿäº§çº§è°ƒä¼˜)
    - [7.1 OSDè°ƒä¼˜](#71-osdè°ƒä¼˜)
  - [8. æ€»ç»“](#8-æ€»ç»“)
    - [8.1 æœ¬ç« è¦ç‚¹](#81-æœ¬ç« è¦ç‚¹)
  - [ç›¸å…³æ–‡æ¡£](#ç›¸å…³æ–‡æ¡£)
    - [æœ¬æ¨¡å—ç›¸å…³](#æœ¬æ¨¡å—ç›¸å…³)
    - [å…¶ä»–æ¨¡å—ç›¸å…³](#å…¶ä»–æ¨¡å—ç›¸å…³)

---

## 1. Cephæ¶æ„ä¸åŸç†

### 1.1 Cephæ ¸å¿ƒç»„ä»¶

**Cephæ¶æ„æ¦‚è§ˆ**:

```text
Cephåˆ†å¸ƒå¼å­˜å‚¨æ¶æ„:

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         Client Applications         â”‚
                    â”‚  (RBD, CephFS, RGW, librados)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                         â”‚                         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚   RBD   â”‚              â”‚ CephFS  â”‚              â”‚   RGW   â”‚
    â”‚  å—å­˜å‚¨  â”‚              â”‚ æ–‡ä»¶å­˜å‚¨ â”‚              â”‚ å¯¹è±¡å­˜å‚¨ â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚                         â”‚                         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         RADOS               â”‚
                    â”‚  (Reliable Autonomic        â”‚
                    â”‚   Distributed Object Store) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                         â”‚                         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚   MON   â”‚              â”‚   OSD   â”‚              â”‚   MGR   â”‚
    â”‚  ç›‘æ§    â”‚              â”‚  å­˜å‚¨   â”‚              â”‚  ç®¡ç†   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒç»„ä»¶è¯¦è§£**:

```yaml
1. MON (Monitor):
   åŠŸèƒ½:
     - ç»´æŠ¤é›†ç¾¤çŠ¶æ€ (cluster map)
     - ç®¡ç†è®¤è¯ (cephx)
     - æä¾›é›†ç¾¤ä¿¡æ¯
     - ä»²è£æœåŠ¡ (Paxos)

   éƒ¨ç½²è¦æ±‚:
     - æ¨èå¥‡æ•°ä¸ª (3/5/7)
     - æœ€å°‘3ä¸ªä¿è¯é«˜å¯ç”¨
     - ä½å»¶è¿Ÿç½‘ç»œ
     - ç‹¬ç«‹ç£ç›˜ (SSDæ¨è)

   æ ¸å¿ƒMap:
     - Monitor Map: MONæ‹“æ‰‘
     - OSD Map: OSDçŠ¶æ€
     - PG Map: PGåˆ†å¸ƒ
     - CRUSH Map: æ•°æ®æ”¾ç½®è§„åˆ™
     - MDS Map: MDSçŠ¶æ€ (CephFS)

2. OSD (Object Storage Daemon):
   åŠŸèƒ½:
     - å­˜å‚¨æ•°æ® (å¯¹è±¡)
     - æ•°æ®å¤åˆ¶
     - æ•°æ®æ¢å¤
     - æ•°æ®å†å¹³è¡¡
     - å¿ƒè·³æ£€æŸ¥

   éƒ¨ç½²è¦æ±‚:
     - æ¯å—ç£ç›˜ä¸€ä¸ªOSD
     - æœ€å°‘3ä¸ªOSD (3å‰¯æœ¬)
     - SSD/NVMeæ¨è
     - 10Gb+ç½‘ç»œ

   å­˜å‚¨å¼•æ“:
     - BlueStore (æ¨è): ç›´æ¥ç®¡ç†è£¸è®¾å¤‡
       - RocksDBå…ƒæ•°æ®
       - æ— éœ€æ–‡ä»¶ç³»ç»Ÿ
       - æ›´é«˜æ€§èƒ½
     - FileStore (åºŸå¼ƒ): åŸºäºæ–‡ä»¶ç³»ç»Ÿ
       - XFSæ¨è
       - åŒå†™å¼€é”€

3. MGR (Manager):
   åŠŸèƒ½:
     - é›†ç¾¤ç›‘æ§
     - æŒ‡æ ‡æ”¶é›†
     - ç®¡ç†é¢æ¿ (Dashboard)
     - REST API
     - æ’ä»¶æ¡†æ¶

   éƒ¨ç½²è¦æ±‚:
     - è‡³å°‘1ä¸ª (æ¨è2ä¸ª)
     - è‡ªåŠ¨failover
     - ä½èµ„æºæ¶ˆè€—

   å¸¸ç”¨æ¨¡å—:
     - dashboard: Web UI
     - prometheus: ç›‘æ§å¯¼å‡º
     - restful: REST API
     - balancer: æ•°æ®å‡è¡¡
     - alerts: å‘Šè­¦

4. MDS (Metadata Server) - CephFSä¸“ç”¨:
   åŠŸèƒ½:
     - ç®¡ç†æ–‡ä»¶å…ƒæ•°æ®
     - ç›®å½•æ ‘
     - æ–‡ä»¶å±æ€§
     - è®¿é—®æ§åˆ¶

   éƒ¨ç½²è¦æ±‚:
     - è‡³å°‘1ä¸ªactive
     - å¯é…ç½®standby
     - å¤§å†…å­˜éœ€æ±‚ (64GB+)
     - é«˜æ€§èƒ½CPU

   å·¥ä½œæ¨¡å¼:
     - Active: æœåŠ¡å…ƒæ•°æ®è¯·æ±‚
     - Standby: çƒ­å¤‡
     - Standby-replay: å®æ—¶replayæ—¥å¿—

5. RGW (RADOS Gateway) - å¯¹è±¡å­˜å‚¨ä¸“ç”¨:
   åŠŸèƒ½:
     - S3 APIå…¼å®¹
     - Swift APIå…¼å®¹
     - å¤šç§Ÿæˆ·
     - Multi-siteå¤åˆ¶

   éƒ¨ç½²è¦æ±‚:
     - æ— çŠ¶æ€ï¼Œå¯æ°´å¹³æ‰©å±•
     - è´Ÿè½½å‡è¡¡ (HAProxy/Nginx)
     - SSL/TLSæ”¯æŒ

   ç‰¹æ€§:
     - Bucketç®¡ç†
     - IAMæƒé™
     - ç”Ÿå‘½å‘¨æœŸç­–ç•¥
     - ç‰ˆæœ¬æ§åˆ¶
     - è·¨åŸŸèµ„æºå…±äº« (CORS)
```

**ç»„ä»¶é€šä¿¡**:

```yaml
é€šä¿¡åè®®:
  MON <-> MON: Paxos (ä»²è£)
  Client <-> MON: librados (è·å–cluster map)
  Client <-> OSD: librados (æ•°æ®I/O)
  OSD <-> OSD: æ•°æ®å¤åˆ¶, å¿ƒè·³
  MGR <-> MON: è·å–é›†ç¾¤çŠ¶æ€
  MDS <-> OSD: å…ƒæ•°æ®å­˜å‚¨

ç«¯å£:
  MON: 6789 (v1), 3300 (v2)
  OSD: 6800-7300
  MGR: 8443 (Dashboard), 9283 (Prometheus)
  MDS: 6800+
  RGW: 7480 (HTTP), 7481 (HTTPS)

ç½‘ç»œè¦æ±‚:
  å…¬å…±ç½‘ç»œ (public network):
    - Clientè®¿é—®
    - MONé€šä¿¡
    - 10Gb+æ¨è

  é›†ç¾¤ç½‘ç»œ (cluster network):
    - OSDé—´å¤åˆ¶
    - æ•°æ®æ¢å¤
    - 10Gb+å¿…é¡»
    - ä¸å…¬å…±ç½‘ç»œåˆ†ç¦» (æ¨è)
```

---

### 1.2 CRUSHç®—æ³•

**CRUSH (Controlled Replication Under Scalable Hashing)** æ˜¯Cephçš„æ ¸å¿ƒæ•°æ®åˆ†å¸ƒç®—æ³•ã€‚

**CRUSHç‰¹ç‚¹**:

```yaml
æ ¸å¿ƒç‰¹æ€§:
  âœ… ä¼ªéšæœºåˆ†å¸ƒ
  âœ… ç¡®å®šæ€§ (è¾“å…¥ç›¸åŒè¾“å‡ºç›¸åŒ)
  âœ… å¯æ§å‰¯æœ¬æ”¾ç½®
  âœ… æ•…éšœåŸŸéš”ç¦»
  âœ… åŠ¨æ€å†å¹³è¡¡

ä¼˜åŠ¿:
  âœ… æ— ä¸­å¿ƒå…ƒæ•°æ®
  âœ… é«˜æ€§èƒ½ (O(log n))
  âœ… å¯æ‰©å±• (PBçº§)
  âœ… æ•…éšœè‡ªæ„ˆ
  âœ… æ”¯æŒå¤æ‚æ‹“æ‰‘
```

**CRUSH Mapå±‚æ¬¡ç»“æ„**:

```text
CRUSH Mapç¤ºä¾‹:

root default
â”œâ”€â”€ datacenter dc1
â”‚   â”œâ”€â”€ room room1
â”‚   â”‚   â”œâ”€â”€ row row1
â”‚   â”‚   â”‚   â”œâ”€â”€ rack rack1
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ host node1
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ osd.0 (weight: 1.82TiB)
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ osd.1 (weight: 1.82TiB)
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ osd.2 (weight: 1.82TiB)
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ host node2
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ osd.3 (weight: 1.82TiB)
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ osd.4 (weight: 1.82TiB)
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ osd.5 (weight: 1.82TiB)
â”‚   â”‚   â”‚   â””â”€â”€ rack rack2
â”‚   â”‚   â”‚       â”œâ”€â”€ host node3
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ osd.6 (weight: 1.82TiB)
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ osd.7 (weight: 1.82TiB)
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ osd.8 (weight: 1.82TiB)
â”‚   â”‚   â”‚       â””â”€â”€ host node4
â”‚   â”‚   â”‚           â”œâ”€â”€ osd.9 (weight: 1.82TiB)
â”‚   â”‚   â”‚           â”œâ”€â”€ osd.10 (weight: 1.82TiB)
â”‚   â”‚   â”‚           â””â”€â”€ osd.11 (weight: 1.82TiB)
â”‚   â”‚   â””â”€â”€ row row2
â”‚   â”‚       â””â”€â”€ ... (æ›´å¤šæœºæ¶å’Œä¸»æœº)
â”‚   â””â”€â”€ room room2
â”‚       â””â”€â”€ ... (æ›´å¤šæœºæ¶å’Œä¸»æœº)
â””â”€â”€ datacenter dc2
    â””â”€â”€ ... (æ›´å¤šæ•°æ®ä¸­å¿ƒ)

æ•…éšœåŸŸå±‚çº§:
  - osd: å•ä¸ªOSDæ•…éšœ
  - host: ä¸»æœºæ•…éšœ
  - rack: æœºæ¶æ•…éšœ (ç”µæº/ç½‘ç»œ)
  - row: è¡Œæ•…éšœ
  - room: æœºæˆ¿æ•…éšœ
  - datacenter: æ•°æ®ä¸­å¿ƒæ•…éšœ
```

**CRUSHè§„åˆ™ç¤ºä¾‹**:

```yaml
# å‰¯æœ¬æ”¾ç½®è§„åˆ™ (3å‰¯æœ¬, è·¨æœºæ¶)
rule replicated_rule {
  id 0
  type replicated
  min_size 1
  max_size 10
  step take default            # ä»rootå¼€å§‹
  step chooseleaf firstn 0 type rack  # é€‰æ‹©3ä¸ªä¸åŒrack
  step emit                    # è¾“å‡ºç»“æœ
}

# çº åˆ ç æ”¾ç½®è§„åˆ™ (4+2, è·¨ä¸»æœº)
rule erasure_rule {
  id 1
  type erasure
  min_size 3
  max_size 6
  step set_chooseleaf_tries 5
  step set_choose_tries 100
  step take default
  step chooseleaf indep 0 type host  # é€‰æ‹©6ä¸ªä¸åŒhost
  step emit
}

å‚æ•°è¯´æ˜:
  type:
    - replicated: å‰¯æœ¬æ¨¡å¼
    - erasure: çº åˆ ç æ¨¡å¼

  chooseleaf:
    - firstn: é€‰æ‹©å‰Nä¸ª (å‰¯æœ¬)
    - indep: ç‹¬ç«‹é€‰æ‹© (çº åˆ ç )

  type:
    - osd: æŒ‰OSDéš”ç¦»
    - host: æŒ‰ä¸»æœºéš”ç¦»
    - rack: æŒ‰æœºæ¶éš”ç¦»
    - datacenter: æŒ‰æ•°æ®ä¸­å¿ƒéš”ç¦»
```

**æ•°æ®å®šä½æµç¨‹**:

```yaml
æ•°æ®å®šä½æ­¥éª¤:

1. Object -> PGæ˜ å°„:
   PG = hash(object_name + pool_id) % pg_num

   ç¤ºä¾‹:
     object: "image1.raw"
     pool_id: 1
     pg_num: 128
     hash("image1.raw" + 1) = 0xABCD1234
     PG = 0xABCD1234 % 128 = 52
     ç»“æœ: PG 1.52

2. PG -> OSDæ˜ å°„ (CRUSH):
   CRUSH(PG, cluster_map, rule) -> [primary_osd, replica_osd...]

   è¾“å…¥:
     - PG: 1.52
     - cluster_map: å½“å‰é›†ç¾¤çŠ¶æ€
     - rule: replicated_rule

   è¾“å‡º:
     - [osd.3, osd.7, osd.11] (3å‰¯æœ¬)

   ä¿è¯:
     âœ… 3ä¸ªOSDåœ¨ä¸åŒrack
     âœ… ç¡®å®šæ€§ (æ¯æ¬¡è®¡ç®—ç»“æœç›¸åŒ)
     âœ… å‡åŒ€åˆ†å¸ƒ

3. Clientç›´æ¥è®¿é—®Primary OSD:
   Client -> osd.3 (primary)
     osd.3 -> osd.7 (replica)
     osd.3 -> osd.11 (replica)

   ä¼˜åŠ¿:
     âœ… æ— ä¸­å¿ƒç“¶é¢ˆ
     âœ… é«˜æ€§èƒ½
     âœ… è‡ªåŠ¨failover
```

**æŸ¥çœ‹CRUSH Map**:

```bash
# è·å–CRUSH Map (äºŒè¿›åˆ¶)
ceph osd getcrushmap -o /tmp/crushmap.bin

# åç¼–è¯‘ä¸ºæ–‡æœ¬
crushtool -d /tmp/crushmap.bin -o /tmp/crushmap.txt

# æŸ¥çœ‹CRUSH Map
cat /tmp/crushmap.txt

# æŸ¥çœ‹CRUSHæ ‘
ceph osd tree

# è¾“å‡ºç¤ºä¾‹:
ID  CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-1         10.92188  root default
-3          5.46094      host node1
 0    ssd   1.82031          osd.0     up   1.00000  1.00000
 1    ssd   1.82031          osd.1     up   1.00000  1.00000
 2    ssd   1.82031          osd.2     up   1.00000  1.00000
-5          5.46094      host node2
 3    ssd   1.82031          osd.3     up   1.00000  1.00000
 4    ssd   1.82031          osd.4     up   1.00000  1.00000
 5    ssd   1.82031          osd.5     up   1.00000  1.00000

# æŸ¥çœ‹CRUSHè§„åˆ™
ceph osd crush rule ls
ceph osd crush rule dump replicated_rule
```

**ä¿®æ”¹CRUSH Map**:

```bash
# æ·»åŠ æ–°OSDåˆ°æŒ‡å®šä½ç½®
ceph osd crush add osd.12 1.82 host=node4 rack=rack2 room=room1 datacenter=dc1

# è°ƒæ•´OSDæƒé‡ (å½±å“æ•°æ®åˆ†å¸ƒ)
ceph osd crush reweight osd.0 1.50

# ç§»é™¤OSD
ceph osd crush remove osd.0

# åˆ›å»ºæ–°è§„åˆ™ (è·¨æœºæ¶3å‰¯æœ¬)
ceph osd crush rule create-replicated my-rule default rack

# ä¿®æ”¹Poolçš„CRUSHè§„åˆ™
ceph osd pool set my-pool crush_rule my-rule
```

---

### 1.3 æ•°æ®åˆ†å¸ƒä¸å‰¯æœ¬

**å‰¯æœ¬æ¨¡å¼ vs çº åˆ ç **:

```yaml
å‰¯æœ¬æ¨¡å¼ (Replication):
  åŸç†:
    - æ¯ä¸ªå¯¹è±¡å®Œæ•´å¤åˆ¶Nä»½
    - å…¸å‹: 3å‰¯æœ¬
    - å­˜å‚¨å¼€é”€: 3x

  ä¼˜åŠ¿:
    âœ… é«˜æ€§èƒ½ (æ— éœ€ç¼–è§£ç )
    âœ… ä½å»¶è¿Ÿ
    âœ… æ•…éšœæ¢å¤å¿«
    âœ… æ”¯æŒéƒ¨åˆ†å†™å…¥

  åŠ£åŠ¿:
    âŒ å­˜å‚¨æ•ˆç‡ä½ (3x)
    âŒ æˆæœ¬é«˜

  é€‚ç”¨åœºæ™¯:
    âœ… æ•°æ®åº“ (é«˜IOPS)
    âœ… è™šæ‹Ÿæœºç£ç›˜
    âœ… çƒ­æ•°æ®
    âœ… å°æ–‡ä»¶

çº åˆ ç  (Erasure Coding):
  åŸç†:
    - å°†å¯¹è±¡åˆ†æˆKä¸ªæ•°æ®å—
    - ç”ŸæˆMä¸ªæ ¡éªŒå—
    - æ€»å…±K+Mä¸ªå—
    - å¯å®¹å¿Mä¸ªå—ä¸¢å¤±
    - å…¸å‹: 4+2 (å¯ä¸¢2å—)

  å­˜å‚¨å¼€é”€:
    4+2: 1.5x (vs 3xå‰¯æœ¬)
    8+3: 1.375x
    8+4: 1.5x

  ä¼˜åŠ¿:
    âœ… å­˜å‚¨æ•ˆç‡é«˜ (1.5x vs 3x)
    âœ… èŠ‚çœæˆæœ¬ (50%)
    âœ… æ›´é«˜å¯é æ€§ (å¯ä¸¢æ›´å¤šå—)

  åŠ£åŠ¿:
    âŒ æ€§èƒ½è¾ƒä½ (ç¼–è§£ç å¼€é”€)
    âŒ å»¶è¿Ÿè¾ƒé«˜
    âŒ æ¢å¤æ…¢
    âŒ ä¸æ”¯æŒéƒ¨åˆ†å†™å…¥

  é€‚ç”¨åœºæ™¯:
    âœ… å¯¹è±¡å­˜å‚¨ (å†·æ•°æ®)
    âœ… å¤‡ä»½å½’æ¡£
    âœ… å¤§æ–‡ä»¶
    âœ… ä½æˆæœ¬éœ€æ±‚

å¯¹æ¯”è¡¨æ ¼:
| ç‰¹æ€§ | 3å‰¯æœ¬ | 4+2 EC | 8+3 EC |
|------|-------|--------|--------|
| å­˜å‚¨å¼€é”€ | 3x | 1.5x | 1.375x |
| å¯å®¹å¿æ•…éšœ | 2 | 2 | 3 |
| è¯»æ€§èƒ½ | â­â­â­â­â­ | â­â­â­ | â­â­ |
| å†™æ€§èƒ½ | â­â­â­â­â­ | â­â­â­ | â­â­ |
| æ¢å¤é€Ÿåº¦ | â­â­â­â­â­ | â­â­â­ | â­â­ |
| æˆæœ¬ | é«˜ | ä¸­ | ä½ |
| é€‚ç”¨åœºæ™¯ | çƒ­æ•°æ®/æ•°æ®åº“ | æ¸©æ•°æ® | å†·æ•°æ®/å½’æ¡£ |
```

**PG (Placement Group)**:

```yaml
PGæ¦‚å¿µ:
  å®šä¹‰:
    - é€»è¾‘åˆ†ç»„
    - å¯¹è±¡é›†åˆ
    - CRUSHçš„æœ€å°å•ä½
    - ä¸€ä¸ªPoolåŒ…å«å¤šä¸ªPG
    - ä¸€ä¸ªPGæ˜ å°„åˆ°å¤šä¸ªOSD

  ä½œç”¨:
    âœ… ç®€åŒ–æ•°æ®ç®¡ç†
    âœ… åŠ é€Ÿæ¢å¤
    âœ… è´Ÿè½½å‡è¡¡
    âœ… å‡å°‘å…ƒæ•°æ®

PGæ•°é‡è®¡ç®—:
  å…¬å¼:
    Total PGs = (OSDs Ã— 100) / replicas

  ç¤ºä¾‹:
    12 OSDs, 3å‰¯æœ¬:
      PGs = (12 Ã— 100) / 3 = 400
      å–æœ€æ¥è¿‘çš„2çš„å¹‚: 512

  æ¨è:
    < 5 OSDs: 128 PGs
    5-10 OSDs: 512 PGs
    10-50 OSDs: 1024 PGs
    > 50 OSDs: 2048+ PGs

  æ³¨æ„:
    âš ï¸ PGè¿‡å°‘: è´Ÿè½½ä¸å‡
    âš ï¸ PGè¿‡å¤š: å…ƒæ•°æ®å¼€é”€å¤§
    âœ… å¹³è¡¡: æ¯ä¸ªOSDçº¦100ä¸ªPG

PGçŠ¶æ€:
  active: æ­£å¸¸æœåŠ¡
  clean: æ‰€æœ‰å‰¯æœ¬æ­£å¸¸
  degraded: å‰¯æœ¬ä¸è¶³
  recovering: æ•°æ®æ¢å¤ä¸­
  backfilling: æ•°æ®å›å¡«ä¸­
  remapped: é‡æ–°æ˜ å°„
  peering: å‰¯æœ¬é—´åŒæ­¥çŠ¶æ€
  incomplete: PGä¸å®Œæ•´
  stale: PGé€šä¿¡ä¸­æ–­
```

**ç¤ºä¾‹: åˆ›å»ºä¸åŒç±»å‹çš„Pool**:

```bash
# 1. åˆ›å»º3å‰¯æœ¬Pool
ceph osd pool create rbd-pool 128 128 replicated
ceph osd pool set rbd-pool size 3
ceph osd pool set rbd-pool min_size 2  # æœ€å°‘2å‰¯æœ¬å¯å†™

# 2. åˆ›å»ºçº åˆ ç Profile
ceph osd erasure-code-profile set ec-profile-4-2 \
  k=4 \
  m=2 \
  crush-failure-domain=host \
  crush-device-class=ssd

# æŸ¥çœ‹EC Profile
ceph osd erasure-code-profile get ec-profile-4-2

# 3. åˆ›å»ºçº åˆ ç Pool
ceph osd pool create ec-pool 128 128 erasure ec-profile-4-2

# 4. åˆ›å»ºCephFSéœ€è¦çš„Pool (æ•°æ®+å…ƒæ•°æ®)
ceph osd pool create cephfs-data 128
ceph osd pool create cephfs-metadata 64

# 5. æŸ¥çœ‹Pool
ceph osd pool ls detail

# 6. æŸ¥çœ‹PGåˆ†å¸ƒ
ceph pg dump
ceph pg stat

# 7. æŸ¥çœ‹Poolä½¿ç”¨æƒ…å†µ
ceph df
rados df

# 8. è°ƒæ•´PGæ•°é‡ (è°¨æ…æ“ä½œ)
ceph osd pool set rbd-pool pg_num 256
ceph osd pool set rbd-pool pgp_num 256
```

---

### 1.4 Poolç®¡ç†

**Poolå‚æ•°é…ç½®**:

```bash
# åˆ›å»ºPool
ceph osd pool create my-pool <pg_num> <pgp_num> <type>

# å¸¸ç”¨å‚æ•°è®¾ç½®
ceph osd pool set my-pool size 3              # å‰¯æœ¬æ•°
ceph osd pool set my-pool min_size 2          # æœ€å°‘å‰¯æœ¬
ceph osd pool set my-pool crush_rule replicated_rule  # CRUSHè§„åˆ™
ceph osd pool set my-pool compression_mode aggressive  # å‹ç¼©
ceph osd pool set my-pool compression_algorithm snappy # å‹ç¼©ç®—æ³•

# é…é¢è®¾ç½®
ceph osd pool set-quota my-pool max_objects 10000  # æœ€å¤§å¯¹è±¡æ•°
ceph osd pool set-quota my-pool max_bytes 1099511627776  # æœ€å¤§å®¹é‡(1TB)

# Poolé‡å‘½å
ceph osd pool rename old-name new-name

# åˆ é™¤Pool (éœ€è¦ç¡®è®¤)
ceph osd pool delete my-pool my-pool --yes-i-really-really-mean-it

# å¯ç”¨Poolåº”ç”¨
ceph osd pool application enable my-pool rbd  # RBD
ceph osd pool application enable my-pool cephfs  # CephFS
ceph osd pool application enable my-pool rgw  # RGW
```

**Poolå‹ç¼©**:

```yaml
å‹ç¼©æ¨¡å¼:
  none: ä¸å‹ç¼©
  passive: å®¢æˆ·ç«¯hintå‹ç¼©
  aggressive: æ‰€æœ‰æ•°æ®å‹ç¼©
  force: å¼ºåˆ¶å‹ç¼©

å‹ç¼©ç®—æ³•:
  snappy: å¿«é€Ÿ, ä½å‹ç¼©æ¯” (æ¨èé€šç”¨)
  zlib: ä¸­ç­‰, ä¸­å‹ç¼©æ¯”
  zstd: å¯è°ƒ, é«˜å‹ç¼©æ¯” (æ¨èå†·æ•°æ®)
  lz4: æœ€å¿«, æœ€ä½å‹ç¼©æ¯”

é…ç½®ç¤ºä¾‹:
  # å¯ç”¨å‹ç¼©
  ceph osd pool set my-pool compression_mode aggressive
  ceph osd pool set my-pool compression_algorithm snappy
  ceph osd pool set my-pool compression_required_ratio 0.875  # å‹ç¼©æ¯”é˜ˆå€¼

  # å‹ç¼©ç»Ÿè®¡
  ceph osd pool stats my-pool
```

## 2. Rook Operator

### 2.1 Rookæ¶æ„

**Rookæ¦‚è¿°**:

```yaml
Rookç‰¹ç‚¹:
  âœ… KubernetesåŸç”Ÿ
  âœ… Operatoræ¨¡å¼
  âœ… è‡ªåŠ¨åŒ–ç®¡ç†
  âœ… å£°æ˜å¼é…ç½®
  âœ… è‡ªæ„ˆèƒ½åŠ›
  âœ… å¤šå­˜å‚¨åç«¯ (Ceph, Cassandra, NFS)

Rook vs ä¼ ç»ŸCeph:
  ä¼ ç»ŸCeph:
    - æ‰‹åŠ¨éƒ¨ç½²
    - é…ç½®å¤æ‚
    - éš¾ä»¥æ‰©å±•
    - è¿ç»´è´Ÿæ‹…é‡

  Rook:
    âœ… ä¸€é”®éƒ¨ç½²
    âœ… CRDå£°æ˜å¼
    âœ… è‡ªåŠ¨æ‰©å±•
    âœ… è‡ªåŠ¨è¿ç»´
```

**Rookæ¶æ„å›¾**:

```text
Rook-Cephæ¶æ„:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Kubernetes Cluster                       â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              Rook Operator (Deployment)                 â”‚ â”‚
â”‚  â”‚  - Watch CRDs                                           â”‚ â”‚
â”‚  â”‚  - Reconcile Ceph Cluster                              â”‚ â”‚
â”‚  â”‚  - Manage Ceph Components                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                   â”‚                                           â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚         â”‚         â”‚          â”‚          â”‚          â”‚         â”‚
â”‚    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”  â”‚
â”‚    â”‚CephClusterâ”‚â”‚CephBlockâ”‚â”‚CephFileâ”‚â”‚CephObjectâ”‚â”‚CephNFS â”‚  â”‚
â”‚    â”‚   (CRD)   â”‚â”‚Pool(CRD)â”‚â”‚System   â”‚â”‚Store(CRD)â”‚â”‚(CRD)   â”‚  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ (CRD)  â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    Ceph Components                      â”‚ â”‚
â”‚  â”‚                                                         â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚ â”‚
â”‚  â”‚  â”‚ MON  â”‚ â”‚ MON  â”‚ â”‚ MON  â”‚  â”‚   MGR    â”‚             â”‚ â”‚
â”‚  â”‚  â”‚ Pod  â”‚ â”‚ Pod  â”‚ â”‚ Pod  â”‚  â”‚   Pod    â”‚             â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚ â”‚
â”‚  â”‚                                                         â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”                 â”‚ â”‚
â”‚  â”‚  â”‚ OSD  â”‚ â”‚ OSD  â”‚ â”‚ OSD  â”‚ â”‚ OSD  â”‚  ...            â”‚ â”‚
â”‚  â”‚  â”‚ Pod  â”‚ â”‚ Pod  â”‚ â”‚ Pod  â”‚ â”‚ Pod  â”‚                 â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜                 â”‚ â”‚
â”‚  â”‚     â”‚        â”‚        â”‚        â”‚                      â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”                 â”‚ â”‚
â”‚  â”‚  â”‚/dev/â”‚  â”‚/dev/â”‚  â”‚/dev/â”‚  â”‚/dev/â”‚                 â”‚ â”‚
â”‚  â”‚  â”‚ sdb â”‚  â”‚ sdc â”‚  â”‚ sdd â”‚  â”‚ sde â”‚                 â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜                 â”‚ â”‚
â”‚  â”‚                                                         â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”                â”‚ â”‚
â”‚  â”‚  â”‚ MDS  â”‚ â”‚ MDS  â”‚           â”‚ RGW  â”‚                â”‚ â”‚
â”‚  â”‚  â”‚ Pod  â”‚ â”‚ Pod  â”‚           â”‚ Pod  â”‚                â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”˜                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚               CSI Drivers (DaemonSet)                   â”‚ â”‚
â”‚  â”‚  - RBD CSI Plugin                                       â”‚ â”‚
â”‚  â”‚  - CephFS CSI Plugin                                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.2 CRDè¯¦è§£

**Rookæ ¸å¿ƒCRD**:

```yaml
1. CephCluster:
   ä½œç”¨: å®šä¹‰Cephé›†ç¾¤

   ç¤ºä¾‹:
     apiVersion: ceph.rook.io/v1
     kind: CephCluster
     metadata:
       name: rook-ceph
       namespace: rook-ceph
     spec:
       dataDirHostPath: /var/lib/rook
       mon:
         count: 3
         allowMultiplePerNode: false
       mgr:
         count: 2
       storage:
         useAllNodes: true
         useAllDevices: true

2. CephBlockPool:
   ä½œç”¨: å®šä¹‰RBDå­˜å‚¨æ± 

   ç¤ºä¾‹:
     apiVersion: ceph.rook.io/v1
     kind: CephBlockPool
     metadata:
       name: replicapool
       namespace: rook-ceph
     spec:
       replicated:
         size: 3
         requireSafeReplicaSize: true

3. CephFilesystem:
   ä½œç”¨: å®šä¹‰CephFSæ–‡ä»¶ç³»ç»Ÿ

   ç¤ºä¾‹:
     apiVersion: ceph.rook.io/v1
     kind: CephFilesystem
     metadata:
       name: myfs
       namespace: rook-ceph
     spec:
       metadataPool:
         replicated:
           size: 3
       dataPools:
       - replicated:
           size: 3
       metadataServer:
         activeCount: 1
         activeStandby: true

4. CephObjectStore:
   ä½œç”¨: å®šä¹‰RGWå¯¹è±¡å­˜å‚¨

   ç¤ºä¾‹:
     apiVersion: ceph.rook.io/v1
     kind: CephObjectStore
     metadata:
       name: my-store
       namespace: rook-ceph
     spec:
       metadataPool:
         replicated:
           size: 3
       dataPool:
         replicated:
           size: 3
       gateway:
         instances: 2
         port: 80

5. CephNFS:
   ä½œç”¨: å®šä¹‰NFS Gateway

   ç¤ºä¾‹:
     apiVersion: ceph.rook.io/v1
     kind: CephNFS
     metadata:
       name: my-nfs
       namespace: rook-ceph
     spec:
       rados:
         pool: myfs-data0
         namespace: nfs-ns
       server:
         active: 2
```

---

### 2.3 Operatoréƒ¨ç½²

**å®Œæ•´éƒ¨ç½²æµç¨‹**:

```bash
# 1. ä¸‹è½½Rookä»“åº“
git clone --single-branch --branch v1.12.0 https://github.com/rook/rook.git
cd rook/deploy/examples

# 2. éƒ¨ç½²Rook Operator
kubectl create -f crds.yaml
kubectl create -f common.yaml
kubectl create -f operator.yaml

# 3. éªŒè¯Operator
kubectl -n rook-ceph get pod
# NAME                                  READY   STATUS    AGE
# rook-ceph-operator-xxx                1/1     Running   30s
# rook-discover-xxx                     1/1     Running   30s
# rook-discover-yyy                     1/1     Running   30s

# 4. å‡†å¤‡èŠ‚ç‚¹ç£ç›˜ (æ¯ä¸ªèŠ‚ç‚¹)
# åˆ—å‡ºå¯ç”¨ç£ç›˜
lsblk -f

# æ¸…ç†ç£ç›˜ (å¦‚æœä¹‹å‰ä½¿ç”¨è¿‡)
sudo sgdisk --zap-all /dev/sdb
sudo dd if=/dev/zero of=/dev/sdb bs=1M count=100 oflag=direct,dsync
sudo blkdiscard /dev/sdb

# 5. åˆ›å»ºCephé›†ç¾¤
kubectl create -f cluster.yaml

# 6. ç­‰å¾…é›†ç¾¤å°±ç»ª
kubectl -n rook-ceph get cephcluster
# NAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE   HEALTH
# rook-ceph   /var/lib/rook     3          60s   Ready             HEALTH_OK

# 7. æŸ¥çœ‹Ceph Pods
kubectl -n rook-ceph get pod
# NAME                                     READY   STATUS      AGE
# csi-cephfsplugin-provisioner-xxx         6/6     Running     3m
# csi-cephfsplugin-xxx                     3/3     Running     3m
# csi-rbdplugin-provisioner-xxx            6/6     Running     3m
# csi-rbdplugin-xxx                        3/3     Running     3m
# rook-ceph-crashcollector-node1-xxx       1/1     Running     2m
# rook-ceph-mgr-a-xxx                      1/1     Running     2m
# rook-ceph-mon-a-xxx                      1/1     Running     3m
# rook-ceph-mon-b-xxx                      1/1     Running     3m
# rook-ceph-mon-c-xxx                      1/1     Running     3m
# rook-ceph-operator-xxx                   1/1     Running     10m
# rook-ceph-osd-0-xxx                      1/1     Running     2m
# rook-ceph-osd-1-xxx                      1/1     Running     2m
# rook-ceph-osd-2-xxx                      1/1     Running     2m
# rook-ceph-osd-prepare-node1-xxx          0/1     Completed   2m
# rook-ceph-osd-prepare-node2-xxx          0/1     Completed   2m
# rook-ceph-osd-prepare-node3-xxx          0/1     Completed   2m

# 8. éƒ¨ç½²Toolbox (ç®¡ç†å·¥å…·)
kubectl create -f toolbox.yaml

# 9. ä½¿ç”¨Toolboxæ‰§è¡ŒCephå‘½ä»¤
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash
# åœ¨toolboxä¸­:
ceph status
ceph osd status
ceph osd tree
ceph df
rados df
```

**å®Œæ•´cluster.yamlç¤ºä¾‹**:

```yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v17.2.6  # Ceph Quincy
    allowUnsupported: false

  dataDirHostPath: /var/lib/rook

  # è·³è¿‡OSDè®¾å¤‡å‡çº§ç¡®è®¤
  skipUpgradeChecks: false

  # æŒç»­å¥åº·æ£€æŸ¥
  continueUpgradeAfterChecksEvenIfNotHealthy: false

  # MONé…ç½®
  mon:
    count: 3  # å¥‡æ•°ä¸ª
    allowMultiplePerNode: false
    volumeClaimTemplate:
      spec:
        storageClassName: local-storage
        resources:
          requests:
            storage: 10Gi

  # MGRé…ç½®
  mgr:
    count: 2  # é«˜å¯ç”¨
    allowMultiplePerNode: false
    modules:
    - name: pg_autoscaler
      enabled: true
    - name: rook
      enabled: true

  # Dashboard
  dashboard:
    enabled: true
    ssl: true
    port: 8443

  # Prometheusç›‘æ§
  monitoring:
    enabled: true
    createPrometheusRules: true

  # ç½‘ç»œé…ç½®
  network:
    provider: host  # hostæˆ–multus
    connections:
      encryption:
        enabled: false
      compression:
        enabled: false

    # åŒç½‘ç»œé…ç½® (å¯é€‰)
    # hostNetwork: true
    # provider: multus
    # selectors:
    #   public: public-network
    #   cluster: cluster-network

  # Crashæ”¶é›†å™¨
  crashCollector:
    disable: false

  # æ—¥å¿—æ”¶é›†å™¨
  logCollector:
    enabled: true
    periodicity: daily
    maxLogSize: 500M

  # æ¸…ç†ç­–ç•¥
  cleanupPolicy:
    confirmation: ""  # yes-really-destroy-data (åˆ é™¤é›†ç¾¤æ—¶æ¸…ç†æ•°æ®)
    sanitizeDisks:
      method: quick  # quickæˆ–complete
      dataSource: zero  # zeroæˆ–random
      iteration: 1
    allowUninstallWithVolumes: false

  # OSDå­˜å‚¨é…ç½®
  storage:
    useAllNodes: true
    useAllDevices: false  # ä¸è‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰è®¾å¤‡

    # è®¾å¤‡é€‰æ‹©
    deviceFilter: "^sd[b-z]"  # æ­£åˆ™åŒ¹é…è®¾å¤‡å
    devicePathFilter: "^/dev/disk/by-path/.*-ssd.*"  # è·¯å¾„è¿‡æ»¤

    # èŠ‚ç‚¹é…ç½®
    nodes:
    - name: "node1"
      devices:
      - name: "/dev/sdb"
      - name: "/dev/sdc"
    - name: "node2"
      devices:
      - name: "/dev/sdb"
      - name: "/dev/sdc"
    - name: "node3"
      devices:
      - name: "/dev/sdb"
      - name: "/dev/sdc"

    # OSDé…ç½®
    config:
      osdsPerDevice: "1"  # æ¯ä¸ªè®¾å¤‡1ä¸ªOSD
      encryptedDevice: "false"  # åŠ å¯†
      metadataDevice: ""  # å…ƒæ•°æ®è®¾å¤‡ (SSD)
      databaseSizeMB: "1024"  # RocksDBå¤§å°
      walSizeMB: "576"  # WALå¤§å°
      journalSizeMB: "5120"  # Journalå¤§å°

    # StorageClassè®¾å¤‡åˆ†ç±»
    storageClassDeviceSets:
    - name: set1
      count: 3  # åˆ›å»º3ä¸ªOSD
      portable: true
      tuneDeviceClass: true
      encrypted: false
      placement:
        tolerations:
        - effect: NoSchedule
          key: storage-node
          operator: Exists
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - rook-ceph-osd
            topologyKey: kubernetes.io/hostname
      resources:
        limits:
          cpu: "2"
          memory: "4Gi"
        requests:
          cpu: "1"
          memory: "2Gi"
      volumeClaimTemplates:
      - metadata:
          name: data
        spec:
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 100Gi
          storageClassName: local-storage
          volumeMode: Block

  # èµ„æºé™åˆ¶
  resources:
    mgr:
      limits:
        cpu: "1"
        memory: "2Gi"
      requests:
        cpu: "500m"
        memory: "1Gi"
    mon:
      limits:
        cpu: "2"
        memory: "4Gi"
      requests:
        cpu: "1"
        memory: "2Gi"
    osd:
      limits:
        cpu: "2"
        memory: "4Gi"
      requests:
        cpu: "1"
        memory: "2Gi"
    prepareosd:
      limits:
        cpu: "1"
        memory: "1Gi"
      requests:
        cpu: "500m"
        memory: "500Mi"
    crashcollector:
      limits:
        cpu: "100m"
        memory: "60Mi"
      requests:
        cpu: "50m"
        memory: "60Mi"

  # ä¼˜å…ˆçº§
  priorityClassNames:
    mgr: system-cluster-critical
    mon: system-cluster-critical
    osd: system-node-critical

  # å¥åº·æ£€æŸ¥
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    livenessProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false
```

---

### 2.4 é›†ç¾¤ç®¡ç†

**å¸¸ç”¨ç®¡ç†å‘½ä»¤**:

```bash
# è¿›å…¥Toolbox
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash

# === é›†ç¾¤çŠ¶æ€ ===
ceph status
ceph health detail

# === OSDç®¡ç† ===
ceph osd status
ceph osd tree
ceph osd df

# === Poolç®¡ç† ===
ceph osd pool ls detail
ceph df
```

---

## 3. å—å­˜å‚¨ (RBD)

### 3.1 RBDæ¦‚è¿°ä¸StorageClass

**åˆ›å»ºRBD StorageClass**:

```yaml
# 1. CephBlockPool
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
    size: 3
---
# 2. StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: replicapool
  imageFormat: "2"
  imageFeatures: layering
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Delete
allowVolumeExpansion: true
```

---

## 4. æ–‡ä»¶å­˜å‚¨ (CephFS)

### 4.1 CephFSéƒ¨ç½²

```yaml
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: myfs
  namespace: rook-ceph
spec:
  metadataPool:
    replicated:
      size: 3
  dataPools:
  - replicated:
      size: 3
  metadataServer:
    activeCount: 1
    activeStandby: true
```

---

## 5. å¯¹è±¡å­˜å‚¨ (RGW)

### 5.1 RGWéƒ¨ç½²

```yaml
apiVersion: ceph.rook.io/v1
kind: CephObjectStore
metadata:
  name: my-store
  namespace: rook-ceph
spec:
  metadataPool:
    replicated:
      size: 3
  dataPool:
    replicated:
      size: 3
  gateway:
    instances: 2
```

---

## 6. ç›‘æ§ä¸å‘Šè­¦

### 6.1 Prometheusé›†æˆ

```yaml
monitoring:
  enabled: true
  createPrometheusRules: true
```

---

## 7. ç”Ÿäº§çº§è°ƒä¼˜

### 7.1 OSDè°ƒä¼˜

```yaml
# BlueStoreé…ç½®
osd_memory_target: 4294967296  # 4GB
bluestore_cache_size_ssd: 3221225472  # 3GB
```

---

## 8. æ€»ç»“

### 8.1 æœ¬ç« è¦ç‚¹

```yaml
æ ¸å¿ƒçŸ¥è¯†:
  âœ… Cephæ¶æ„ (MON/OSD/MGR/MDS/RGW)
  âœ… CRUSHç®—æ³•
  âœ… Rook Operatoréƒ¨ç½²
  âœ… å—/æ–‡ä»¶/å¯¹è±¡å­˜å‚¨
  âœ… ç›‘æ§è°ƒä¼˜
```

---

**å®Œæˆæ—¥æœŸ**: 2025-10-19
**ç‰ˆæœ¬**: v1.0
**ä½œè€…**: äº‘åŸç”Ÿå­˜å‚¨ä¸“å®¶å›¢é˜Ÿ

**Tags**: `#RookCeph` `#Ceph` `#DistributedStorage` `#CloudNativeStorage`

---

## ç›¸å…³æ–‡æ¡£

### æœ¬æ¨¡å—ç›¸å…³

- [äº‘åŸç”Ÿå­˜å‚¨æ¦‚è¿°ä¸æ¶æ„](./01_äº‘åŸç”Ÿå­˜å‚¨æ¦‚è¿°ä¸æ¶æ„.md) - äº‘åŸç”Ÿå­˜å‚¨æ¦‚è¿°ä¸æ¶æ„
- [Kuberneteså­˜å‚¨åŸºç¡€](./02_Kuberneteså­˜å‚¨åŸºç¡€.md) - Kuberneteså­˜å‚¨åŸºç¡€
- [Veleroå¤‡ä»½æ¢å¤](./04_Veleroå¤‡ä»½æ¢å¤.md) - Veleroå¤‡ä»½æ¢å¤
- [CSIé©±åŠ¨è¯¦è§£](./05_CSIé©±åŠ¨è¯¦è§£.md) - CSIé©±åŠ¨è¯¦è§£
- [å­˜å‚¨æ€§èƒ½ä¼˜åŒ–](./06_å­˜å‚¨æ€§èƒ½ä¼˜åŒ–.md) - å­˜å‚¨æ€§èƒ½ä¼˜åŒ–
- [å¤šäº‘å­˜å‚¨](./07_å¤šäº‘å­˜å‚¨.md) - å¤šäº‘å­˜å‚¨
- [å­˜å‚¨å®‰å…¨](./08_å­˜å‚¨å®‰å…¨.md) - å­˜å‚¨å®‰å…¨
- [å®æˆ˜æ¡ˆä¾‹](./09_å®æˆ˜æ¡ˆä¾‹.md) - å®æˆ˜æ¡ˆä¾‹
- [æœ€ä½³å®è·µ](./10_æœ€ä½³å®è·µ.md) - æœ€ä½³å®è·µ
- [README.md](./README.md) - æœ¬æ¨¡å—å¯¼èˆª

### å…¶ä»–æ¨¡å—ç›¸å…³

- [å®¹å™¨å­˜å‚¨æŠ€æœ¯](../05_å®¹å™¨å­˜å‚¨æŠ€æœ¯/README.md) - å®¹å™¨å­˜å‚¨æŠ€æœ¯
- [é«˜çº§å­˜å‚¨æŠ€æœ¯](../05_å®¹å™¨å­˜å‚¨æŠ€æœ¯/02_é«˜çº§å­˜å‚¨æŠ€æœ¯.md) - é«˜çº§å­˜å‚¨æŠ€æœ¯
- [Kuberneteså­˜å‚¨ç®¡ç†](../03_KubernetesæŠ€æœ¯è¯¦è§£/04_å­˜å‚¨ç®¡ç†æŠ€æœ¯.md) - K8så­˜å‚¨ç®¡ç†
- [å®¹å™¨ç¼–æ’æŠ€æœ¯](../04_å®¹å™¨ç¼–æ’æŠ€æœ¯/README.md) - å®¹å™¨ç¼–æ’æŠ€æœ¯

---

**æœ€åæ›´æ–°**: 2025å¹´11æœˆ11æ—¥
**ç»´æŠ¤çŠ¶æ€**: æŒç»­æ›´æ–°
