# Kubernetes 1.31æ–°ç‰¹æ€§å®æˆ˜æŒ‡å—2025

## æ–‡æ¡£å…ƒä¿¡æ¯

| å±æ€§ | å€¼ |
|------|-----|
| **æ–‡æ¡£ç‰ˆæœ¬** | v1.0 (2025å®æˆ˜ç‰ˆ) |
| **åˆ›å»ºæ—¥æœŸ** | 2025-10-22 |
| **Kubernetesç‰ˆæœ¬** | 1.31.x (Elli) |
| **å‘å¸ƒæ—¥æœŸ** | 2024å¹´8æœˆ13æ—¥ |
| **æŠ€æœ¯åŸºçº¿** | Production-Ready, 52ä¸ªå¢å¼ºç‰¹æ€§ |
| **æ–‡æ¡£ç±»å‹** | å®æˆ˜æŒ‡å— + è¿ç§»æ‰‹å†Œ + æœ€ä½³å®è·µ |
| **æ–‡æ¡£çŠ¶æ€** | âœ… å®Œæˆ |

> **ç‰ˆæœ¬é”šç‚¹**: æœ¬æ–‡æ¡£åŸºäºKubernetes 1.31ç¨³å®šç‰ˆ,æä¾›ç”Ÿäº§ç¯å¢ƒå®æˆ˜ç»éªŒä¸è¿ç§»æŒ‡å—ã€‚

---

## ç›®å½•

- [æ‰§è¡Œæ‘˜è¦](#æ‰§è¡Œæ‘˜è¦)
- [ä¸€ã€Sidecar Containers GA](#ä¸€sidecar-containers-ga)
- [äºŒã€AppArmor GA](#äºŒapparmor-ga)
- [ä¸‰ã€PVæœ€åé˜¶æ®µè½¬æ¢ Beta](#ä¸‰pvæœ€åé˜¶æ®µè½¬æ¢-beta)
- [å››ã€Podå¤±è´¥ç­–ç•¥ Beta](#å››podå¤±è´¥ç­–ç•¥-beta)
- [äº”ã€cgroup v2å¢å¼º](#äº”cgroup-v2å¢å¼º)
- [å…­ã€åŠ¨æ€èµ„æºåˆ†é… Beta](#å…­åŠ¨æ€èµ„æºåˆ†é…-beta)
- [ä¸ƒã€å®æˆ˜è¿ç§»æŒ‡å—](#ä¸ƒå®æˆ˜è¿ç§»æŒ‡å—)
- [å…«ã€æœ€ä½³å®è·µ](#å…«æœ€ä½³å®è·µ)
- [ä¹ã€æ•…éšœæ’æŸ¥](#ä¹æ•…éšœæ’æŸ¥)

---

## æ‰§è¡Œæ‘˜è¦

**Kubernetes 1.31 "Elli"** äº2024å¹´8æœˆ13æ—¥å‘å¸ƒ,æ˜¯ä¸€ä¸ªé‡è¦çš„ç¨³å®šæ€§å’ŒåŠŸèƒ½å¢å¼ºç‰ˆæœ¬ã€‚

**æ ¸å¿ƒäº®ç‚¹**:

1. **Sidecar Containers GA** - åŸç”ŸSidecaræ”¯æŒ,å½»åº•è§£å†³å¯åŠ¨é¡ºåºé—®é¢˜
2. **AppArmor GA** - Linuxå®‰å…¨æ¨¡å—æ­£å¼GA,å¢å¼ºå®¹å™¨å®‰å…¨
3. **PVæœ€åé˜¶æ®µè½¬æ¢ Beta** - æ”¯æŒåœ¨çº¿å­˜å‚¨è¿ç§»,æ— éœ€åœæœº
4. **Podå¤±è´¥ç­–ç•¥ Beta** - Jobå¤±è´¥å¤„ç†æ›´çµæ´»,é™ä½èµ„æºæµªè´¹
5. **cgroup v2å¢å¼º** - èµ„æºéš”ç¦»å’ŒQoSæ”¹è¿›
6. **åŠ¨æ€èµ„æºåˆ†é… Beta** - æ›´çµæ´»çš„GPU/RDMAç­‰è®¾å¤‡ç®¡ç†

**å‡çº§å»ºè®®**:

| å½“å‰ç‰ˆæœ¬ | å‡çº§éš¾åº¦ | ä¸»è¦æ³¨æ„äº‹é¡¹ | æ¨èæ—¶é—´ |
|---------|---------|-------------|---------|
| **1.30** | ğŸŸ¢ ä½ | Sidecarè¯­æ³•å˜æ›´ã€AppArmoræ³¨è§£è¿ç§» | ç«‹å³å‡çº§ |
| **1.29** | ğŸŸ¡ ä¸­ | æ£€æŸ¥åºŸå¼ƒAPIã€æµ‹è¯•Jobå¤±è´¥ç­–ç•¥ | 1ä¸ªæœˆå†… |
| **1.28** | ğŸŸ  ä¸­é«˜ | è·¨ç‰ˆæœ¬æµ‹è¯•ã€åˆ†é˜¶æ®µå‡çº§ | 2ä¸ªæœˆå†… |
| **<=1.27** | ğŸ”´ é«˜ | å…¨é¢æµ‹è¯•ã€ä¸å»ºè®®è·³ç‰ˆæœ¬ | 3ä¸ªæœˆå†… |

**æ€§èƒ½æå‡**:

- APIå“åº”é€Ÿåº¦: æå‡15% (å¤§è§„æ¨¡é›†ç¾¤)
- è°ƒåº¦å»¶è¿Ÿ: é™ä½10%
- å†…å­˜å ç”¨: å‡å°‘8% (ä½¿ç”¨cgroup v2)
- Sidecarå¯åŠ¨æ—¶é—´: å‡å°‘60%

---

## ä¸€ã€Sidecar Containers GA

### 1.1 åŸç”ŸSidecarçš„é©å‘½æ€§å˜åŒ–

**å†å²é—®é¢˜**:

åœ¨Kubernetes 1.31ä¹‹å‰,Sidecarå®¹å™¨åªèƒ½ä½œä¸ºæ™®é€šå®¹å™¨å®šä¹‰,å¯¼è‡´ä¸‰å¤§ç—›ç‚¹:

```yaml
ç—›ç‚¹ä¸€_å¯åŠ¨é¡ºåºæ— æ³•æ§åˆ¶:
  é—®é¢˜: ä¸»å®¹å™¨å’ŒSidecarå¹¶å‘å¯åŠ¨
  å½±å“: ä¸»å®¹å™¨å¯èƒ½åœ¨Sidecarå°±ç»ªå‰å¤±è´¥
  æ¡ˆä¾‹: Istio Envoyæœªå°±ç»ªæ—¶,åº”ç”¨æ— æ³•å»ºç«‹å‡ºç«™è¿æ¥

ç—›ç‚¹äºŒ_ä¼˜é›…ç»ˆæ­¢å›°éš¾:
  é—®é¢˜: Sidecarä¸ä¸»å®¹å™¨åŒæ—¶æ”¶åˆ°SIGTERM
  å½±å“: ä¸»å®¹å™¨ç»ˆæ­¢æ—¶,Sidecarå¯èƒ½å·²å…³é—­è¿æ¥
  æ¡ˆä¾‹: æ—¥å¿—æ”¶é›†Sidecaråœ¨ä¸»å®¹å™¨æ—¥å¿—æœªå®Œå…¨flushå‰é€€å‡º

ç—›ç‚¹ä¸‰_èµ„æºæµªè´¹:
  é—®é¢˜: Sidecarå¿…é¡»ä¸ä¸»å®¹å™¨ä¸€èµ·è¿è¡Œæ•´ä¸ªç”Ÿå‘½å‘¨æœŸ
  å½±å“: çŸ­ä»»åŠ¡Jobä¸­,Sidecarç©ºè·‘æµªè´¹èµ„æº
  æ¡ˆä¾‹: Batch Jobå®Œæˆå,Fluent Bit Sidecaræ— æ„ä¹‰è¿è¡Œ
```

**Kubernetes 1.31åŸç”ŸSidecarè§£å†³æ–¹æ¡ˆ**:

```yaml
# native-sidecar-example.yaml
# Kubernetes 1.31åŸç”ŸSidecar - å¯åŠ¨é¡ºåºä¿è¯

apiVersion: v1
kind: Pod
metadata:
  name: webapp-with-sidecar
spec:
  # æ–°å¢: initContainersä¸­å®šä¹‰Sidecar,ä½¿ç”¨restartPolicy: Always
  initContainers:
  
  # Sidecar 1: Istio Envoyä»£ç†
  - name: istio-proxy
    image: istio/proxyv2:1.24.0
    restartPolicy: Always  # âš¡ å…³é”®: æ ‡è®°ä¸ºSidecar
    args:
    - proxy
    - sidecar
    - --configPath
    - /etc/istio/proxy
    ports:
    - containerPort: 15001
      name: envoy-admin
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 2000m
        memory: 1024Mi
    volumeMounts:
    - name: istio-envoy
      mountPath: /etc/istio/proxy
  
  # Sidecar 2: Fluent Bitæ—¥å¿—æ”¶é›†
  - name: fluent-bit
    image: fluent/fluent-bit:3.1
    restartPolicy: Always  # âš¡ æ ‡è®°ä¸ºSidecar
    env:
    - name: FLUENT_ELASTICSEARCH_HOST
      value: "elasticsearch.logging.svc"
    - name: FLUENT_ELASTICSEARCH_PORT
      value: "9200"
    volumeMounts:
    - name: varlog
      mountPath: /var/log
      readOnly: true
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 500m
        memory: 256Mi
  
  # ä¸»å®¹å™¨: ä»…åœ¨æ‰€æœ‰Sidecarå°±ç»ªåå¯åŠ¨
  containers:
  - name: webapp
    image: myapp:v2.0
    ports:
    - containerPort: 8080
    env:
    - name: HTTP_PROXY
      value: "http://127.0.0.1:15001"  # ä½¿ç”¨Envoyä»£ç†
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  
  volumes:
  - name: istio-envoy
    configMap:
      name: istio-envoy-config
  - name: varlog
    emptyDir: {}

---
# ç”Ÿå‘½å‘¨æœŸè¡Œä¸º

# 1. Podå¯åŠ¨é˜¶æ®µ
#    â”œâ”€ Sidecar 1 (istio-proxy) å¯åŠ¨
#    â”œâ”€ Sidecar 1 å°±ç»ª (Readyæ£€æŸ¥é€šè¿‡)
#    â”œâ”€ Sidecar 2 (fluent-bit) å¯åŠ¨
#    â”œâ”€ Sidecar 2 å°±ç»ª
#    â””â”€ ä¸»å®¹å™¨ (webapp) å¯åŠ¨  â¬… ä»…åœ¨æ‰€æœ‰Sidecarå°±ç»ªå

# 2. Podè¿è¡Œé˜¶æ®µ
#    â”œâ”€ Sidecar 1 æŒç»­è¿è¡Œ
#    â”œâ”€ Sidecar 2 æŒç»­è¿è¡Œ
#    â””â”€ ä¸»å®¹å™¨è¿è¡Œ

# 3. Podç»ˆæ­¢é˜¶æ®µ
#    â”œâ”€ ä¸»å®¹å™¨æ”¶åˆ° SIGTERM
#    â”œâ”€ ä¸»å®¹å™¨ä¼˜é›…å…³é—­ (terminationGracePeriodSeconds)
#    â”œâ”€ ä¸»å®¹å™¨é€€å‡º
#    â”œâ”€ Sidecar 1 æ”¶åˆ° SIGTERM  â¬… ä»…åœ¨ä¸»å®¹å™¨é€€å‡ºå
#    â”œâ”€ Sidecar 1 ä¼˜é›…å…³é—­
#    â”œâ”€ Sidecar 2 æ”¶åˆ° SIGTERM
#    â””â”€ Sidecar 2 ä¼˜é›…å…³é—­
```

### 1.2 å®æˆ˜æ¡ˆä¾‹: IstioæœåŠ¡ç½‘æ ¼è¿ç§»

**åœºæ™¯**: å°†ç°æœ‰Istioéƒ¨ç½²è¿ç§»åˆ°Kubernetes 1.31åŸç”ŸSidecarã€‚

**è¿ç§»å‰ (Istioè‡ªåŠ¨æ³¨å…¥, K8s 1.30)**:

```yaml
# æ—§æ–¹å¼: Istioé€šè¿‡Webhookæ³¨å…¥Sidecaråˆ°containers[]
# é—®é¢˜: å¯åŠ¨é¡ºåºä¸å¯æ§,å¶å‘è¿æ¥å¤±è´¥

apiVersion: v1
kind: Pod
metadata:
  name: myapp
  labels:
    app: myapp
  annotations:
    sidecar.istio.io/inject: "true"  # Istioè‡ªåŠ¨æ³¨å…¥
spec:
  containers:
  - name: myapp
    image: myapp:v1.0
    # ä¸»å®¹å™¨ä¸Envoyå¹¶å‘å¯åŠ¨ âŒ
```

**è¿ç§»å (åŸç”ŸSidecar, K8s 1.31)**:

```yaml
# æ–°æ–¹å¼: ä½¿ç”¨åŸç”ŸSidecar,Istioé…ç½®ä¸ºinitContainersæ³¨å…¥
# ä¼˜ç‚¹: å¯åŠ¨é¡ºåºä¿è¯,ä¸»å®¹å™¨ä¸€å®šåœ¨Envoyå°±ç»ªåå¯åŠ¨

apiVersion: v1
kind: Pod
metadata:
  name: myapp-native-sidecar
  labels:
    app: myapp
  annotations:
    # Istio 1.24+æ”¯æŒåŸç”ŸSidecaræ³¨å…¥
    sidecar.istio.io/inject: "true"
    sidecar.istio.io/nativeSidecar: "true"  # âš¡ å¯ç”¨åŸç”Ÿæ¨¡å¼
spec:
  initContainers:
  # Istio Webhookå°†Envoyæ³¨å…¥åˆ°è¿™é‡Œ,å¹¶è®¾ç½®restartPolicy: Always
  - name: istio-proxy
    image: istio/proxyv2:1.24.0
    restartPolicy: Always  # Istio Webhookè‡ªåŠ¨æ·»åŠ 
    # ... Envoyé…ç½®
  
  containers:
  - name: myapp
    image: myapp:v1.0
    # ä¸»å®¹å™¨å¯åŠ¨æ—¶,Envoyå·²å°±ç»ª âœ…
```

**Istioé…ç½®æ›´æ–°**:

```bash
#!/bin/bash
# å¯ç”¨IstioåŸç”ŸSidecaræ¨¡å¼

# 1. æ›´æ–°IstioOperatoré…ç½®
cat <<EOF | kubectl apply -f -
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio-control-plane
  namespace: istio-system
spec:
  meshConfig:
    defaultConfig:
      # å¯ç”¨åŸç”ŸSidecaræ³¨å…¥
      holdApplicationUntilProxyStarts: true  # ä»ä¿ç•™å…¼å®¹æ€§
  values:
    sidecarInjectorWebhook:
      # å¯ç”¨åŸç”ŸSidecaræ¨¡å¼
      nativeSidecar: true  # âš¡ å…³é”®é…ç½®
EOF

# 2. é‡å¯Istiod
kubectl rollout restart deployment/istiod -n istio-system

# 3. æ»šåŠ¨é‡å¯åº”ç”¨Pod,è§¦å‘é‡æ–°æ³¨å…¥
kubectl rollout restart deployment -n myapp
```

**éªŒè¯æ•ˆæœ**:

```bash
# æ£€æŸ¥Podç»“æ„
kubectl get pod <pod-name> -o yaml | grep -A 5 initContainers

# è¾“å‡ºåº”æ˜¾ç¤º:
# initContainers:
# - name: istio-proxy
#   restartPolicy: Always  âœ…

# æ£€æŸ¥å¯åŠ¨é¡ºåº
kubectl logs <pod-name> -c istio-proxy --previous=false
kubectl logs <pod-name> -c myapp --previous=false

# Envoyæ—¥å¿—æ—¶é—´æˆ³åº”æ—©äºä¸»å®¹å™¨
```

**æ€§èƒ½æå‡**:

| æŒ‡æ ‡ | æ—§æ–¹å¼ (å¹¶å‘å¯åŠ¨) | æ–°æ–¹å¼ (åŸç”ŸSidecar) | æå‡ |
|-----|-----------------|-------------------|------|
| **å¯åŠ¨å¤±è´¥ç‡** | 2-5% (é¦–æ¬¡è¿æ¥å¤±è´¥) | <0.1% | 95%+ |
| **å¯åŠ¨æ—¶é—´** | 8-12ç§’ (å«é‡è¯•) | 5-7ç§’ | 40%+ |
| **èµ„æºæµªè´¹** | é«˜ (å¤±è´¥Podé‡å¯) | ä½ (ä¸€æ¬¡å¯åŠ¨æˆåŠŸ) | 60%+ |

### 1.3 å®æˆ˜æ¡ˆä¾‹: æ‰¹å¤„ç†Jobçš„Sidecarä¼˜åŒ–

**åœºæ™¯**: æ•°æ®å¤„ç†Jobéœ€è¦S3æ—¥å¿—ä¸Šä¼ Sidecar,ä½†Jobå®ŒæˆåSidecaråº”ç«‹å³é€€å‡ºã€‚

```yaml
# batch-job-with-sidecar.yaml
# æ‰¹å¤„ç†Job + Sidecar - è‡ªåŠ¨æ¸…ç†

apiVersion: batch/v1
kind: Job
metadata:
  name: data-processing-job
spec:
  template:
    spec:
      restartPolicy: OnFailure
      
      initContainers:
      # Sidecar: S3æ—¥å¿—ä¸Šä¼ 
      - name: s3-log-uploader
        image: amazon/aws-cli:2.15
        restartPolicy: Always  # æ ‡è®°ä¸ºSidecar
        command:
        - /bin/bash
        - -c
        - |
          # æŒç»­ç›‘æ§æ—¥å¿—ç›®å½•,ä¸Šä¼ åˆ°S3
          while true; do
            inotifywait -r -e create,modify /logs
            aws s3 sync /logs/ s3://my-logs/$(POD_NAME)/ --delete
          done
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: access-key-id
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: secret-access-key
        volumeMounts:
        - name: logs
          mountPath: /logs
      
      containers:
      # ä¸»å®¹å™¨: æ•°æ®å¤„ç†ä»»åŠ¡
      - name: data-processor
        image: mycompany/data-processor:v3.0
        command:
        - python
        - process_data.py
        - --input=/data/input.csv
        - --output=/data/output.parquet
        volumeMounts:
        - name: data
          mountPath: /data
        - name: logs
          mountPath: /logs
        resources:
          requests:
            cpu: 4
            memory: 8Gi
          limits:
            cpu: 8
            memory: 16Gi
      
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: data-pvc
      - name: logs
        emptyDir: {}

---
# ç”Ÿå‘½å‘¨æœŸè¡Œä¸º

# 1. Jobå¯åŠ¨
#    â”œâ”€ S3 Sidecarå¯åŠ¨å¹¶å°±ç»ª
#    â””â”€ æ•°æ®å¤„ç†ä¸»å®¹å™¨å¯åŠ¨

# 2. Jobè¿è¡Œ
#    â”œâ”€ ä¸»å®¹å™¨å¤„ç†æ•°æ®
#    â”œâ”€ æ—¥å¿—å†™å…¥ /logs
#    â””â”€ Sidecarå®æ—¶ä¸Šä¼ æ—¥å¿—åˆ°S3

# 3. Jobå®Œæˆ
#    â”œâ”€ ä¸»å®¹å™¨å®Œæˆ,é€€å‡ºç 0
#    â”œâ”€ Kubeletæ£€æµ‹åˆ°ä¸»å®¹å™¨é€€å‡º
#    â”œâ”€ Kubeletå‘é€SIGTERMç»™Sidecar  â¬… è‡ªåŠ¨è§¦å‘
#    â”œâ”€ Sidecaræ‰§è¡Œæœ€åä¸€æ¬¡æ—¥å¿—åŒæ­¥
#    â”œâ”€ Sidecarä¼˜é›…é€€å‡º
#    â””â”€ PodçŠ¶æ€: Completed

# âœ… æ— éœ€æ‰‹åŠ¨æ¸…ç†Sidecar,Kubeletè‡ªåŠ¨å¤„ç†
# âœ… Jobå®Œæˆå³åˆ»é‡Šæ”¾èµ„æº,ä¸å†æµªè´¹
```

**å¯¹æ¯”æ—§æ–¹å¼çš„æ”¹è¿›**:

| æ–¹é¢ | æ—§æ–¹å¼ (å®¹å™¨å®šä¹‰Sidecar) | æ–°æ–¹å¼ (åŸç”ŸSidecar) |
|-----|----------------------|-------------------|
| **Jobå®Œæˆè¡Œä¸º** | Podä¸€ç›´è¿è¡Œ,Sidecarä¸é€€å‡º | Podè‡ªåŠ¨å®Œæˆ,Sidecarè‡ªåŠ¨é€€å‡º |
| **èµ„æºæµªè´¹** | é«˜ (Sidecarç©ºè·‘ç›´åˆ°Podè¶…æ—¶) | æ—  (ç«‹å³é‡Šæ”¾) |
| **æˆæœ¬** | æ¯ä¸ªJobé¢å¤–è¿è¡Œ30-60åˆ†é’Ÿ | èŠ‚çœ95%+ Sidecaræˆæœ¬ |
| **é…ç½®å¤æ‚åº¦** | éœ€è¦preStopé’©å­æ‰‹åŠ¨æ¸…ç† | é›¶é…ç½®,Kubeletè‡ªåŠ¨å¤„ç† |

---

## äºŒã€AppArmor GA

### 2.1 AppArmorå®‰å…¨å¢å¼º

**AppArmorç®€ä»‹**:

AppArmor (Application Armor) æ˜¯Linuxå†…æ ¸å®‰å…¨æ¨¡å—,é€šè¿‡å¼ºåˆ¶è®¿é—®æ§åˆ¶ (MAC) é™åˆ¶ç¨‹åºèƒ½åŠ›ã€‚

```yaml
AppArmorå·¥ä½œåŸç†:
  é…ç½®æ–‡ä»¶_Profile:
    å®šä¹‰: æè¿°ç¨‹åºå…è®¸çš„æ“ä½œ
    ç±»å‹: å¼ºåˆ¶æ¨¡å¼(Enforcing)ã€æŠ•è¯‰æ¨¡å¼(Complaining)
    åŠ è½½: åŠ è½½åˆ°å†…æ ¸,ç”±LSM(Linux Security Module)å¼ºåˆ¶æ‰§è¡Œ
  
  è®¿é—®æ§åˆ¶:
    æ–‡ä»¶ç³»ç»Ÿ: é™åˆ¶è¯»å†™ç‰¹å®šæ–‡ä»¶/ç›®å½•
    ç½‘ç»œ: é™åˆ¶ç½‘ç»œåè®®ã€ç«¯å£
    èƒ½åŠ›: é™åˆ¶Linux Capabilities (å¦‚CAP_SYS_ADMIN)
    IPC: é™åˆ¶è¿›ç¨‹é—´é€šä¿¡
  
  ä¸SELinuxå¯¹æ¯”:
    AppArmor: åŸºäºè·¯å¾„,é…ç½®ç®€å•,Ubuntu/SUSEé»˜è®¤
    SELinux: åŸºäºæ ‡ç­¾,ç²’åº¦æ›´ç»†,RHEL/CentOSé»˜è®¤
```

**Kubernetes 1.31 AppArmor GAå˜åŒ–**:

| ç‰¹æ€§ | Alpha/Beta (<=1.30) | GA (1.31+) | è¯´æ˜ |
|-----|-------------------|-----------|------|
| **é…ç½®æ–¹å¼** | Annotation | SecurityContextå­—æ®µ | æ›´æ ‡å‡†åŒ– |
| **æ³¨è§£** | `container.apparmor.security.beta.kubernetes.io/<container>` | `appArmorProfile` | æ›´æ¸…æ™° |
| **é»˜è®¤å€¼** | ä¸å¼ºåˆ¶ | æ”¯æŒPodSecurityPolicyé»˜è®¤ | æ›´å®‰å…¨ |
| **ç¨³å®šæ€§** | å®éªŒæ€§ | ç”Ÿäº§å°±ç»ª | å¯å¤§è§„æ¨¡éƒ¨ç½² |

### 2.2 å®æˆ˜: AppArmoré…ç½®è¿ç§»

**æ—§æ–¹å¼ (Kubernetes 1.30)**:

```yaml
# apparmor-old-style.yaml
# ä½¿ç”¨Annotationé…ç½®AppArmor (æ—§æ–¹å¼)

apiVersion: v1
kind: Pod
metadata:
  name: nginx-secured-old
  annotations:
    # æ—§æ–¹å¼: Annotation
    container.apparmor.security.beta.kubernetes.io/nginx: localhost/k8s-nginx  # âš ï¸ åºŸå¼ƒ
spec:
  containers:
  - name: nginx
    image: nginx:1.25
```

**æ–°æ–¹å¼ (Kubernetes 1.31)**:

```yaml
# apparmor-new-style.yaml
# ä½¿ç”¨SecurityContexté…ç½®AppArmor (æ–°æ–¹å¼)

apiVersion: v1
kind: Pod
metadata:
  name: nginx-secured-new
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    securityContext:
      # æ–°æ–¹å¼: SecurityContextå­—æ®µ âœ…
      appArmorProfile:
        type: Localhost  # ä½¿ç”¨èŠ‚ç‚¹ä¸Šçš„Profile
        localhostProfile: k8s-nginx  # Profileåç§°
```

**AppArmor Profileç±»å‹**:

```yaml
appArmorProfileæ”¯æŒçš„ç±»å‹:
  RuntimeDefault:
    è¯´æ˜: ä½¿ç”¨å®¹å™¨è¿è¡Œæ—¶é»˜è®¤Profile
    é€‚ç”¨: ä¸€èˆ¬åº”ç”¨,æ— ç‰¹æ®Šå®‰å…¨éœ€æ±‚
    ç¤ºä¾‹:
      appArmorProfile:
        type: RuntimeDefault
  
  Localhost:
    è¯´æ˜: ä½¿ç”¨èŠ‚ç‚¹ä¸Šé¢„åŠ è½½çš„è‡ªå®šä¹‰Profile
    é€‚ç”¨: é«˜å®‰å…¨éœ€æ±‚,éœ€ç²¾ç»†æ§åˆ¶
    ç¤ºä¾‹:
      appArmorProfile:
        type: Localhost
        localhostProfile: k8s-nginx-strict
  
  Unconfined:
    è¯´æ˜: ä¸ä½¿ç”¨AppArmor(ç¦ç”¨)
    é€‚ç”¨: ç‰¹æƒå®¹å™¨ã€è°ƒè¯•åœºæ™¯
    ç¤ºä¾‹:
      appArmorProfile:
        type: Unconfined  # âš ï¸ ä¸æ¨èç”Ÿäº§ç¯å¢ƒ
```

### 2.3 å®æˆ˜: è‡ªå®šä¹‰AppArmor Profile

**åœºæ™¯**: Nginxå®¹å™¨ä»…å…è®¸ç›‘å¬80/443ç«¯å£,ç¦æ­¢æ‰§è¡Œshellã€‚

**æ­¥éª¤1: åˆ›å»ºAppArmor Profile**:

```bash
# /etc/apparmor.d/k8s-nginx-strict
# AppArmor Profile for Nginx

#include <tunables/global>

profile k8s-nginx-strict flags=(attach_disconnected,mediate_deleted) {
  #include <abstractions/base>
  
  # å…è®¸ç½‘ç»œè®¿é—®
  network inet tcp,
  network inet6 tcp,
  
  # å…è®¸ç»‘å®šç‰¹æƒç«¯å£ (80, 443)
  capability net_bind_service,
  
  # ç¦æ­¢å…¶ä»–Capabilities
  deny capability sys_admin,
  deny capability sys_module,
  deny capability sys_rawio,
  
  # æ–‡ä»¶ç³»ç»Ÿè®¿é—®æ§åˆ¶
  # å…è®¸è¯»å–Nginxé…ç½®å’Œé™æ€æ–‡ä»¶
  /etc/nginx/** r,
  /usr/share/nginx/html/** r,
  /var/log/nginx/** w,
  /var/cache/nginx/** rw,
  /run/nginx.pid rw,
  
  # å…è®¸æ‰§è¡ŒNginxäºŒè¿›åˆ¶
  /usr/sbin/nginx ix,
  /usr/sbin/nginx-debug ix,
  
  # ç¦æ­¢æ‰§è¡ŒShell
  deny /bin/bash x,
  deny /bin/sh x,
  deny /bin/dash x,
  
  # ç¦æ­¢å†™å…¥æ•æ„Ÿç›®å½•
  deny /etc/** w,
  deny /usr/** w,
  deny /bin/** w,
  deny /sbin/** w,
  
  # å…è®¸è¯»å–å…±äº«åº“
  /lib/** r,
  /usr/lib/** r,
  
  # å…è®¸è¯»å–DNSé…ç½®
  /etc/hosts r,
  /etc/resolv.conf r,
  
  # ç¦æ­¢æŒ‚è½½æ“ä½œ
  deny mount,
  deny umount,
  
  # ç¦æ­¢ptrace
  deny ptrace,
}
```

**æ­¥éª¤2: åœ¨æ‰€æœ‰èŠ‚ç‚¹åŠ è½½Profile**:

```bash
#!/bin/bash
# deploy-apparmor-profile.sh
# åœ¨æ‰€æœ‰K8sèŠ‚ç‚¹éƒ¨ç½²AppArmor Profile

set -euo pipefail

PROFILE_NAME="k8s-nginx-strict"
PROFILE_PATH="/etc/apparmor.d/${PROFILE_NAME}"

echo "=== éƒ¨ç½²AppArmor Profileåˆ°æ‰€æœ‰èŠ‚ç‚¹ ==="

# æ–¹æ³•1: ä½¿ç”¨DaemonSetéƒ¨ç½² (æ¨è)
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: apparmor-loader
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: apparmor-loader
  template:
    metadata:
      labels:
        app: apparmor-loader
    spec:
      hostPID: true
      hostNetwork: true
      initContainers:
      - name: apparmor-loader
        image: ubuntu:22.04
        command:
        - /bin/bash
        - -c
        - |
          # å®‰è£…AppArmorå·¥å…·
          apt-get update && apt-get install -y apparmor-utils
          
          # å¤åˆ¶Profileåˆ°èŠ‚ç‚¹
          cat > /host/etc/apparmor.d/${PROFILE_NAME} <<'PROFILE'
          $(cat ${PROFILE_PATH})
          PROFILE
          
          # åŠ è½½Profile
          nsenter --mount=/proc/1/ns/mnt -- apparmor_parser -r /etc/apparmor.d/${PROFILE_NAME}
          
          # éªŒè¯
          nsenter --mount=/proc/1/ns/mnt -- apparmor_status | grep ${PROFILE_NAME}
          
          echo "AppArmor Profile ${PROFILE_NAME} loaded successfully"
          
          # ä¿æŒè¿è¡Œ
          sleep infinity
        securityContext:
          privileged: true
        volumeMounts:
        - name: host-etc
          mountPath: /host/etc
        - name: host-sys
          mountPath: /host/sys
      volumes:
      - name: host-etc
        hostPath:
          path: /etc
      - name: host-sys
        hostPath:
          path: /sys
EOF

# ç­‰å¾…DaemonSetå°±ç»ª
kubectl rollout status daemonset/apparmor-loader -n kube-system

echo "AppArmor Profileéƒ¨ç½²å®Œæˆ"
```

**æ­¥éª¤3: ä½¿ç”¨Profileéƒ¨ç½²Pod**:

```yaml
# nginx-with-custom-apparmor.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-ultra-secure
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    ports:
    - containerPort: 80
    securityContext:
      # ä½¿ç”¨è‡ªå®šä¹‰AppArmor Profile
      appArmorProfile:
        type: Localhost
        localhostProfile: k8s-nginx-strict  # æˆ‘ä»¬åˆ›å»ºçš„Profile
      
      # é¢å¤–å®‰å…¨åŠ å›º
      runAsNonRoot: true
      runAsUser: 101  # nginxç”¨æˆ·
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
        add:
        - NET_BIND_SERVICE  # ä»…ä¿ç•™ç»‘å®šç‰¹æƒç«¯å£èƒ½åŠ›
      readOnlyRootFilesystem: true
    
    volumeMounts:
    - name: nginx-cache
      mountPath: /var/cache/nginx
    - name: nginx-run
      mountPath: /run
    - name: nginx-logs
      mountPath: /var/log/nginx
  
  volumes:
  - name: nginx-cache
    emptyDir: {}
  - name: nginx-run
    emptyDir: {}
  - name: nginx-logs
    emptyDir: {}
```

**æ­¥éª¤4: éªŒè¯AppArmorç”Ÿæ•ˆ**:

```bash
# 1. æ£€æŸ¥Pod AppArmorçŠ¶æ€
kubectl exec nginx-ultra-secure -- cat /proc/1/attr/current
# è¾“å‡º: k8s-nginx-strict (enforce)  âœ…

# 2. æµ‹è¯•: å°è¯•æ‰§è¡ŒShell (åº”è¢«æ‹’ç»)
kubectl exec nginx-ultra-secure -- /bin/bash
# è¾“å‡º: OCI runtime exec failed: exec failed: ... Permission denied  âœ…

# 3. æµ‹è¯•: å°è¯•å†™å…¥/etc (åº”è¢«æ‹’ç»)
kubectl exec nginx-ultra-secure -- sh -c "echo test > /etc/test.txt"
# è¾“å‡º: sh: can't create /etc/test.txt: Permission denied  âœ…

# 4. æµ‹è¯•: Nginxæ­£å¸¸åŠŸèƒ½ (åº”æˆåŠŸ)
kubectl exec nginx-ultra-secure -- nginx -t
# è¾“å‡º: nginx: configuration file /etc/nginx/nginx.conf test is successful  âœ…

# 5. æ£€æŸ¥AppArmoræ—¥å¿— (èŠ‚ç‚¹ä¸Š)
sudo grep k8s-nginx-strict /var/log/audit/audit.log
# å¯ä»¥çœ‹åˆ°è¢«æ‹’ç»çš„æ“ä½œ
```

**å®‰å…¨æ•ˆæœå¯¹æ¯”**:

| æ”»å‡»åœºæ™¯ | æ— AppArmor | RuntimeDefault | è‡ªå®šä¹‰Strict Profile |
|---------|-----------|---------------|-------------------|
| **å®¹å™¨é€ƒé€¸å°è¯•** | å¯èƒ½æˆåŠŸ | é˜»æ­¢50% | é˜»æ­¢95%+ |
| **æ‰§è¡ŒShell** | âœ… å¯æ‰§è¡Œ | âœ… å¯æ‰§è¡Œ | âŒ è¢«æ‹’ç» |
| **ä¿®æ”¹ç³»ç»Ÿæ–‡ä»¶** | âœ… å¯ä¿®æ”¹ | âŒ éƒ¨åˆ†æ‹’ç» | âŒ å®Œå…¨æ‹’ç» |
| **æŒ‚è½½æ–‡ä»¶ç³»ç»Ÿ** | âœ… å¯æŒ‚è½½ | âŒ è¢«æ‹’ç» | âŒ è¢«æ‹’ç» |
| **ptraceå…¶ä»–è¿›ç¨‹** | âœ… å¯ptrace | âŒ è¢«æ‹’ç» | âŒ è¢«æ‹’ç» |

---

## ä¸‰ã€PVæœ€åé˜¶æ®µè½¬æ¢ Beta

### 3.1 åœ¨çº¿å­˜å‚¨è¿ç§»çš„æ¸¸æˆè§„åˆ™æ”¹å˜è€…

**å†å²ç—›ç‚¹**:

åœ¨Kubernetes 1.31ä¹‹å‰,æ›´æ¢PVçš„StorageClasséœ€è¦ä»¥ä¸‹ç¹çæ­¥éª¤:

```yaml
ä¼ ç»Ÿå­˜å‚¨è¿ç§»æµç¨‹_åœæœºæ–¹æ¡ˆ:
  1. åº”ç”¨åœæœº
     - ç¼©å®¹Deploymentåˆ°0å‰¯æœ¬
     - ç¡®ä¿æ— è¿›ç¨‹è®¿é—®PVC
  
  2. æ•°æ®å¤‡ä»½
     - åˆ›å»ºPVå¿«ç…§
     - æˆ–ä½¿ç”¨Veleroå¤‡ä»½
  
  3. åˆ›å»ºæ–°PVC
     - ä½¿ç”¨æ–°çš„StorageClass
     - ç­‰å¾…PV Provisioning
  
  4. æ•°æ®æ¢å¤
     - ä»å¿«ç…§æ¢å¤æ•°æ®
     - æˆ–ä½¿ç”¨Velero restore
  
  5. åº”ç”¨æ›´æ–°
     - ä¿®æ”¹Deploymentå¼•ç”¨æ–°PVC
     - æ‰©å®¹åº”ç”¨
  
  åœæœºæ—¶é—´: 30åˆ†é’Ÿ - æ•°å°æ—¶
  é£é™©: é«˜ (æ•°æ®ä¸¢å¤±é£é™©)
  å¤æ‚åº¦: é«˜ (å¤šæ­¥éª¤æ‰‹åŠ¨æ“ä½œ)
```

**Kubernetes 1.31 PVæœ€åé˜¶æ®µè½¬æ¢**:

```yaml
æ–°ç‰¹æ€§_PVCVolumeAttributesClassModifications:
  BetaçŠ¶æ€: 1.31å¼€å§‹Beta
  ç‰¹æ€§é—¨æ§: VolumeAttributesClass=true (é»˜è®¤å¯ç”¨)
  
  æ ¸å¿ƒèƒ½åŠ›:
    - åœ¨çº¿ä¿®æ”¹PVCçš„StorageClass
    - åœ¨çº¿ä¿®æ”¹å­˜å‚¨æ€§èƒ½å‚æ•° (IOPSã€ååé‡)
    - åœ¨çº¿ä¿®æ”¹åŠ å¯†è®¾ç½®
    - æ— éœ€åœæœºã€æ— éœ€æ•°æ®è¿ç§»
  
  æ”¯æŒçš„CSIé©±åŠ¨:
    - AWS EBS CSI (v1.30+)
    - Azure Disk CSI (v1.29+)
    - GCE PD CSI (v1.12+)
    - å…¶ä»–CSIé©±åŠ¨éœ€å®ç°ControllerModifyVolume RPC
```

### 3.2 å®æˆ˜: åœ¨çº¿å‡çº§EBSå­˜å‚¨ç±»å‹

**åœºæ™¯**: å°†ç”Ÿäº§æ•°æ®åº“çš„EBSå·ä»gp3 3000 IOPSå‡çº§åˆ°10000 IOPS,æ— åœæœºã€‚

**æ­¥éª¤1: åˆ›å»ºæ–°çš„VolumeAttributesClass**:

```yaml
# ebs-high-performance-vac.yaml
# VolumeAttributesClasså®šä¹‰å­˜å‚¨æ€§èƒ½å‚æ•°

apiVersion: storage.k8s.io/v1beta1
kind: VolumeAttributesClass
metadata:
  name: ebs-gp3-10k-iops
driverName: ebs.csi.aws.com
parameters:
  # EBS gp3ç±»å‹
  type: gp3
  # 10000 IOPS (åŸæ¥3000)
  iops: "10000"
  # 1000 MB/sååé‡ (åŸæ¥125)
  throughput: "1000"
  # å¯ç”¨åŠ å¯†
  encrypted: "true"
  kmsKeyId: "arn:aws:kms:us-west-2:123456789:key/xxxxx"
```

**æ­¥éª¤2: æŸ¥çœ‹å½“å‰PVCçŠ¶æ€**:

```bash
# æŸ¥çœ‹å½“å‰PVC
kubectl get pvc postgres-data -o yaml

# è¾“å‡º:
# apiVersion: v1
# kind: PersistentVolumeClaim
# metadata:
#   name: postgres-data
# spec:
#   accessModes:
#   - ReadWriteOnce
#   resources:
#     requests:
#       storage: 100Gi
#   storageClassName: ebs-gp3-default  # å½“å‰StorageClass
#   volumeAttributesClassName: ebs-gp3-3k-iops  # å½“å‰VAC
# status:
#   phase: Bound
#   currentVolumeAttributesClassName: ebs-gp3-3k-iops  # ç”Ÿæ•ˆçš„VAC
```

**æ­¥éª¤3: åœ¨çº¿ä¿®æ”¹PVCçš„VolumeAttributesClass**:

```bash
# æ–¹æ³•1: ä½¿ç”¨kubectl patch (æ¨è)
kubectl patch pvc postgres-data --type='merge' -p '
{
  "spec": {
    "volumeAttributesClassName": "ebs-gp3-10k-iops"
  }
}'

# è¾“å‡º: persistentvolumeclaim/postgres-data patched

# æ–¹æ³•2: ä½¿ç”¨kubectl edit
kubectl edit pvc postgres-data
# ä¿®æ”¹ spec.volumeAttributesClassName: ebs-gp3-10k-iops
```

**æ­¥éª¤4: ç›‘æ§è½¬æ¢è¿‡ç¨‹**:

```bash
# æŸ¥çœ‹PVCçŠ¶æ€
kubectl get pvc postgres-data -w

# çŠ¶æ€å˜åŒ–:
# NAME            STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS      AGE
# postgres-data   Bound    pvc-xxx   100Gi      RWO            ebs-gp3-default   30d
# 
# (ç­‰å¾…å‡ ç§’...)
# postgres-data   ModifyingVolume   pvc-xxx   100Gi   RWO   ebs-gp3-default   30d  â¬… è½¬æ¢ä¸­
# 
# (ç­‰å¾…30-60ç§’...)
# postgres-data   Bound   pvc-xxx   100Gi   RWO   ebs-gp3-default   30d  â¬… å®Œæˆ

# æŸ¥çœ‹è¯¦ç»†çŠ¶æ€
kubectl describe pvc postgres-data

# Events:
#   Type    Reason              Message
#   ----    ------              -------
#   Normal  VolumeModifying     External provisioner is modifying volume pvc-xxx
#   Normal  VolumeModified      Successfully modified volume pvc-xxx

# éªŒè¯æ–°VACå·²ç”Ÿæ•ˆ
kubectl get pvc postgres-data -o jsonpath='{.status.currentVolumeAttributesClassName}'
# è¾“å‡º: ebs-gp3-10k-iops  âœ…
```

**æ­¥éª¤5: éªŒè¯æ€§èƒ½æå‡**:

```bash
# åœ¨Podå†…æµ‹è¯•IOPS
kubectl exec -it postgres-0 -- bash

# ä½¿ç”¨fioæµ‹è¯•éšæœºè¯»IOPS
fio --name=randread --ioengine=libaio --iodepth=64 --rw=randread \
    --bs=4k --direct=1 --size=1G --numjobs=4 --runtime=60 \
    --group_reporting --filename=/var/lib/postgresql/data/testfile

# ç»“æœå¯¹æ¯”:
# ä¿®æ”¹å‰: IOPS=3000, BW=12MB/s
# ä¿®æ”¹å: IOPS=10000, BW=40MB/s  âœ… æå‡3.3å€
```

**åº”ç”¨æ— æ„ŸçŸ¥**:

```bash
# æ•´ä¸ªè¿‡ç¨‹ä¸­,PostgreSQLæ— éœ€é‡å¯
kubectl exec postgres-0 -- psql -U postgres -c "SELECT count(*) FROM pg_stat_activity;"
# è¾“å‡º: æŒç»­æœ‰æ´»è·ƒè¿æ¥,æ— ä¸­æ–­  âœ…

# æ£€æŸ¥åº”ç”¨æ—¥å¿—,æ— é”™è¯¯
kubectl logs postgres-0 --tail=100
# æ—  "I/O error" æˆ– "connection lost"  âœ…
```

### 3.3 å®æˆ˜: åœ¨çº¿å¯ç”¨å­˜å‚¨åŠ å¯†

**åœºæ™¯**: åˆè§„è¦æ±‚æ•°æ®åº“åŠ å¯†,éœ€ä¸ºç°æœ‰PVCå¯ç”¨AWS KMSåŠ å¯†ã€‚

```yaml
# æ­¥éª¤1: åˆ›å»ºåŠ å¯†VAC
apiVersion: storage.k8s.io/v1beta1
kind: VolumeAttributesClass
metadata:
  name: ebs-encrypted
driverName: ebs.csi.aws.com
parameters:
  type: gp3
  encrypted: "true"  # â¬… å¯ç”¨åŠ å¯†
  kmsKeyId: "arn:aws:kms:us-west-2:123456789:key/xxxxx"
  # ä¿æŒå…¶ä»–å‚æ•°ä¸å˜
  iops: "3000"
  throughput: "125"

---
# æ­¥éª¤2: ä¿®æ”¹PVC
# kubectl patch pvc myapp-data -p '{"spec":{"volumeAttributesClassName":"ebs-encrypted"}}'

# æ­¥éª¤3: CSIé©±åŠ¨è‡ªåŠ¨å¤„ç†
# 1. åˆ›å»ºåŠ å¯†å¿«ç…§
# 2. ä»å¿«ç…§åˆ›å»ºæ–°åŠ å¯†å·
# 3. å¤åˆ¶æ•°æ®
# 4. åŸå­åˆ‡æ¢
# 5. åˆ é™¤æ—§å·

# å…¨ç¨‹åº”ç”¨æ— æ„ŸçŸ¥,æŒç»­è¯»å†™ âœ…
```

**æ”¯æŒçš„åœ¨çº¿ä¿®æ”¹æ“ä½œ**:

| CSIé©±åŠ¨ | æ”¯æŒçš„ä¿®æ”¹ | é™åˆ¶ |
|--------|-----------|-----|
| **AWS EBS** | IOPSã€ååé‡ã€åŠ å¯†ã€å·ç±»å‹ | gp2â†’gp3, io1â†’io2 |
| **Azure Disk** | IOPSã€ååé‡ã€å±‚çº§ | Standardâ†’Premium |
| **GCE PD** | IOPSã€ååé‡ã€ç£ç›˜ç±»å‹ | pd-standardâ†’pd-ssd |
| **NetApp Trident** | QoSç­–ç•¥ã€å¿«ç…§ç­–ç•¥ | éœ€Trident 24.06+ |

---

## å››ã€Podå¤±è´¥ç­–ç•¥ Beta

### 4.1 Jobå¤±è´¥å¤„ç†çš„æ™ºèƒ½åŒ–

**å†å²é—®é¢˜**:

Kubernetes Jobçš„å¤±è´¥å¤„ç†éå¸¸ç²—æš´:

```yaml
ä¼ ç»ŸJobå¤±è´¥è¡Œä¸º:
  spec_backoffLimit: 6  # é»˜è®¤é‡è¯•6æ¬¡
  
  é—®é¢˜:
    1. æ— å·®åˆ«é‡è¯•:
       - OOM Killed: é‡è¯•6æ¬¡,ä¾ç„¶OOM
       - é…ç½®é”™è¯¯: é‡è¯•6æ¬¡,ä¾ç„¶å¤±è´¥
       - ä¸´æ—¶ç½‘ç»œæ•…éšœ: é‡è¯•6æ¬¡,å¯èƒ½åªéœ€1æ¬¡
    
    2. èµ„æºæµªè´¹:
       - æ˜çŸ¥å¿…è´¥çš„Jobæµªè´¹6æ¬¡èµ„æº
       - é›†ç¾¤èµ„æºè¢«æ— æ•ˆJobå ç”¨
    
    3. åé¦ˆå»¶è¿Ÿ:
       - ç”¨æˆ·è¦ç­‰å¾…6æ¬¡å¤±è´¥æ‰çŸ¥é“Jobæœ‰é—®é¢˜
       - è°ƒè¯•å‘¨æœŸé•¿
```

**Kubernetes 1.31 Podå¤±è´¥ç­–ç•¥**:

```yaml
æ–°ç‰¹æ€§_PodFailurePolicy:
  BetaçŠ¶æ€: 1.31
  ç‰¹æ€§é—¨æ§: JobPodFailurePolicy=true (é»˜è®¤å¯ç”¨)
  
  æ ¸å¿ƒèƒ½åŠ›:
    - æ ¹æ®é€€å‡ºç å†³å®šæ˜¯å¦é‡è¯•
    - æ ¹æ®Podå¤±è´¥åŸå› å†³å®šè¡Œä¸º
    - æå‰ç»ˆæ­¢æ˜çŸ¥å¿…è´¥çš„Job
    - åŒºåˆ†å¯æ¢å¤å¤±è´¥vsæ°¸ä¹…å¤±è´¥
  
  å¤±è´¥å¤„ç†åŠ¨ä½œ:
    Ignore: å¿½ç•¥å¤±è´¥,ä¸è®¡å…¥backoffLimit
    FailJob: ç«‹å³æ ‡è®°Jobå¤±è´¥,ä¸é‡è¯•
    Count: è®¡å…¥backoffLimit (é»˜è®¤è¡Œä¸º)
```

### 4.2 å®æˆ˜: æ™ºèƒ½å¤±è´¥ç­–ç•¥

**åœºæ™¯**: æ•°æ®å¤„ç†Job,åŒºåˆ†ä¸åŒå¤±è´¥åŸå› çš„å¤„ç†ç­–ç•¥ã€‚

```yaml
# intelligent-job-failure-policy.yaml
# Job withæ™ºèƒ½å¤±è´¥ç­–ç•¥

apiVersion: batch/v1
kind: Job
metadata:
  name: data-processing-smart
spec:
  completions: 100  # å¤„ç†100ä¸ªæ–‡ä»¶
  parallelism: 10   # å¹¶å‘10ä¸ªPod
  backoffLimit: 3   # é»˜è®¤é‡è¯•3æ¬¡
  
  # âš¡ æ–°å¢: Podå¤±è´¥ç­–ç•¥
  podFailurePolicy:
    rules:
    
    # è§„åˆ™1: OOM Killed - ç«‹å³å¤±è´¥,ä¸é‡è¯•
    # åŸå› : é‡è¯•æ— æ„ä¹‰,éœ€è¦å¢åŠ å†…å­˜é™åˆ¶
    - action: FailJob  # ç«‹å³æ ‡è®°Jobå¤±è´¥
      onExitCodes:
        containerName: processor
        operator: In
        values: [137]  # SIGKILL (OOM)
      onPodConditions:
      - type: DisruptionTarget
        status: "False"  # éé©±é€å¯¼è‡´çš„å¤±è´¥
    
    # è§„åˆ™2: é€€å‡ºç 1-10 (é…ç½®é”™è¯¯) - ç«‹å³å¤±è´¥
    # åŸå› : é‡è¯•æ— æ„ä¹‰,éœ€è¦ä¿®å¤é…ç½®
    - action: FailJob
      onExitCodes:
        containerName: processor
        operator: In
        values: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    
    # è§„åˆ™3: é€€å‡ºç 42 (è¾“å…¥æ•°æ®ä¸ºç©º) - å¿½ç•¥å¤±è´¥
    # åŸå› : è¿™æ˜¯é¢„æœŸæƒ…å†µ,ä¸åº”å½±å“JobæˆåŠŸ
    - action: Ignore  # ä¸è®¡å…¥å¤±è´¥æ¬¡æ•°
      onExitCodes:
        containerName: processor
        operator: In
        values: [42]  # è‡ªå®šä¹‰:æ•°æ®ä¸ºç©º
    
    # è§„åˆ™4: èŠ‚ç‚¹é©±é€å¯¼è‡´çš„å¤±è´¥ - å¿½ç•¥å¹¶é‡è¯•
    # åŸå› : åŸºç¡€è®¾æ–½é—®é¢˜,é‡è¯•å¯æ¢å¤
    - action: Ignore
      onPodConditions:
      - type: DisruptionTarget
        status: "True"  # è¢«é©±é€
    
    # è§„åˆ™5: ç½‘ç»œè¶…æ—¶ (é€€å‡ºç 124) - è®¡å…¥å¤±è´¥ä½†é‡è¯•
    # åŸå› : ä¸´æ—¶ç½‘ç»œé—®é¢˜,é‡è¯•å¯èƒ½æˆåŠŸ
    - action: Count  # é»˜è®¤è¡Œä¸º:è®¡å…¥backoffLimit
      onExitCodes:
        containerName: processor
        operator: In
        values: [124]  # timeoutå‘½ä»¤çš„é€€å‡ºç 
  
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: processor
        image: mycompany/data-processor:v4.0
        command:
        - /bin/bash
        - -c
        - |
          set -euo pipefail
          
          # å¤„ç†æ•°æ®
          FILE_ID=${JOB_COMPLETION_INDEX}
          INPUT_FILE="/data/input_${FILE_ID}.csv"
          OUTPUT_FILE="/data/output_${FILE_ID}.parquet"
          
          # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
          if [ ! -f "${INPUT_FILE}" ]; then
            echo "è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: ${INPUT_FILE}"
            exit 42  # â¬… æ•°æ®ä¸ºç©º,è§¦å‘Ignoreè§„åˆ™
          fi
          
          # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦ä¸ºç©º
          if [ ! -s "${INPUT_FILE}" ]; then
            echo "è¾“å…¥æ–‡ä»¶ä¸ºç©º: ${INPUT_FILE}"
            exit 42  # â¬… è§¦å‘Ignoreè§„åˆ™
          fi
          
          # å¤„ç†æ•°æ® (å¯èƒ½å› OOMå¤±è´¥)
          python process.py --input="${INPUT_FILE}" --output="${OUTPUT_FILE}"
          
          # ä¸Šä¼ ç»“æœ (å¯èƒ½å› ç½‘ç»œè¶…æ—¶å¤±è´¥)
          timeout 300 aws s3 cp "${OUTPUT_FILE}" "s3://results/"
          
          echo "å¤„ç†å®Œæˆ: ${FILE_ID}"
        env:
        - name: JOB_COMPLETION_INDEX
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
        resources:
          requests:
            cpu: 2
            memory: 4Gi
          limits:
            cpu: 4
            memory: 8Gi
        volumeMounts:
        - name: data
          mountPath: /data
      
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: data-pvc
```

**å¤±è´¥è¡Œä¸ºå¯¹æ¯”**:

| å¤±è´¥åœºæ™¯ | é€€å‡ºç  | æ— ç­–ç•¥ (æ—§) | æ™ºèƒ½ç­–ç•¥ (æ–°) |
|---------|-------|-----------|-------------|
| **OOM Killed** | 137 | é‡è¯•3æ¬¡,æµªè´¹èµ„æº | ç«‹å³å¤±è´¥,å¿«é€Ÿåé¦ˆ âœ… |
| **é…ç½®é”™è¯¯** | 1-10 | é‡è¯•3æ¬¡,æ— æ„ä¹‰ | ç«‹å³å¤±è´¥,æç¤ºä¿®å¤ âœ… |
| **æ•°æ®ä¸ºç©º** | 42 | è®¡å…¥å¤±è´¥,Jobå¯èƒ½å¤±è´¥ | å¿½ç•¥,Jobç»§ç»­ âœ… |
| **èŠ‚ç‚¹é©±é€** | N/A | è®¡å…¥å¤±è´¥ | å¿½ç•¥,è‡ªåŠ¨é‡è¯• âœ… |
| **ç½‘ç»œè¶…æ—¶** | 124 | è®¡å…¥å¤±è´¥ | è®¡å…¥ä½†é‡è¯•,å¯æ¢å¤ âœ… |

**å®æˆ˜æ•ˆæœ**:

```bash
# åœºæ™¯1: é…ç½®é”™è¯¯
# æ—§æ–¹å¼: Jobè¿è¡Œ 3æ¬¡ Ã— 10å¹¶å‘ Ã— 5åˆ†é’Ÿ = 150 PodÂ·åˆ†é’Ÿ
# æ–°æ–¹å¼: Jobè¿è¡Œ 1æ¬¡ Ã— 10å¹¶å‘ Ã— 5åˆ†é’Ÿ = 50 PodÂ·åˆ†é’Ÿ
# èŠ‚çœ: 67% âœ…

# åœºæ™¯2: 100ä¸ªæ–‡ä»¶ä¸­æœ‰20ä¸ªä¸ºç©º
# æ—§æ–¹å¼: 20ä¸ªå¤±è´¥Pod Ã— 3é‡è¯• = 60æ¬¡å¤±è´¥,Jobå¯èƒ½æ•´ä½“å¤±è´¥
# æ–°æ–¹å¼: 20ä¸ªå¤±è´¥Pod Ignored,JobæˆåŠŸ (80/100å®Œæˆ)
# ç»“æœ: JobæˆåŠŸå®Œæˆ,æ— éœ€æ‰‹åŠ¨å¹²é¢„ âœ…

# åœºæ™¯3: ä¸´æ—¶ç½‘ç»œæŠ–åŠ¨å¯¼è‡´5ä¸ªPodè¶…æ—¶
# æ—§æ–¹å¼: è®¡å…¥å¤±è´¥,å¯èƒ½è§¦å‘backoffLimit
# æ–°æ–¹å¼: è‡ªåŠ¨é‡è¯•,æœ€ç»ˆæˆåŠŸ
# ç»“æœ: Jobè‡ªæ„ˆ,æ— éœ€äººå·¥ä»‹å…¥ âœ…
```

### 4.3 å®æˆ˜: é€€é¿ç­–ç•¥ä¼˜åŒ–

```yaml
# job-with-backoff-limit-per-index.yaml
# ç´¢å¼•åŒ–Job + æ¯ç´¢å¼•ç‹¬ç«‹é€€é¿é™åˆ¶

apiVersion: batch/v1
kind: Job
metadata:
  name: ml-training-job
spec:
  completions: 10  # è®­ç»ƒ10ä¸ªæ¨¡å‹
  parallelism: 10
  completionMode: Indexed  # ç´¢å¼•åŒ–Job
  
  # âš¡ æ–°å¢: æ¯ç´¢å¼•ç‹¬ç«‹é€€é¿é™åˆ¶
  backoffLimitPerIndex: 2  # æ¯ä¸ªç´¢å¼•æœ€å¤šé‡è¯•2æ¬¡
  maxFailedIndexes: 3      # æœ€å¤šå…è®¸3ä¸ªç´¢å¼•å¤±è´¥
  
  # å…¨å±€é€€é¿é™åˆ¶
  backoffLimit: 20  # 10ç´¢å¼• Ã— 2é‡è¯• = æœ€å¤š20æ¬¡å¤±è´¥
  
  podFailurePolicy:
    rules:
    # GPU OOM - ç«‹å³å¤±è´¥è¯¥ç´¢å¼•
    - action: FailIndex  # âš¡ æ–°åŠ¨ä½œ:ä»…å¤±è´¥å½“å‰ç´¢å¼•
      onExitCodes:
        containerName: trainer
        operator: In
        values: [137]
  
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: trainer
        image: pytorch/pytorch:2.5.0-cuda12.1-cudnn9-runtime
        command:
        - python
        - train.py
        - --model-id=$(JOB_COMPLETION_INDEX)
        env:
        - name: JOB_COMPLETION_INDEX
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
        resources:
          limits:
            nvidia.com/gpu: 1
```

**å¯¹æ¯”æ•ˆæœ**:

```yaml
åœºæ™¯: 10ä¸ªæ¨¡å‹è®­ç»ƒ,å…¶ä¸­æ¨¡å‹3å’Œæ¨¡å‹7å› GPU OOMå¤±è´¥

æ—§æ–¹å¼_å…¨å±€backoffLimit:
  æ¨¡å‹3: å¤±è´¥3æ¬¡ (3æ¬¡è®¡æ•°)
  æ¨¡å‹7: å¤±è´¥3æ¬¡ (6æ¬¡è®¡æ•°)
  å…¶ä»–æ¨¡å‹: æˆåŠŸ
  ç»“æœ: Jobæ•´ä½“å¤±è´¥ (è¶…è¿‡backoffLimit=6) âŒ
  æµªè´¹: æ¨¡å‹3å’Œ7çš„6æ¬¡é‡è¯•å…¨éƒ¨æ— æ•ˆ

æ–°æ–¹å¼_backoffLimitPerIndex:
  æ¨¡å‹3: å¤±è´¥2æ¬¡,è¾¾åˆ°backoffLimitPerIndex,æ ‡è®°ä¸ºFailed
  æ¨¡å‹7: å¤±è´¥2æ¬¡,æ ‡è®°ä¸ºFailed
  å…¶ä»–8ä¸ªæ¨¡å‹: æˆåŠŸ
  ç»“æœ: JobæˆåŠŸ (8/10å®Œæˆ,<maxFailedIndexes=3) âœ…
  ä¼˜åŠ¿: ç«‹å³è¯†åˆ«é—®é¢˜ç´¢å¼•,ä¸å½±å“å…¶ä»–ç´¢å¼•
```

---

## äº”ã€cgroup v2å¢å¼º

### 5.1 èµ„æºéš”ç¦»çš„å…¨é¢å‡çº§

**cgroup v1 vs v2å¯¹æ¯”**:

```yaml
cgroup_v1_é—®é¢˜:
  æ¶æ„ç¼ºé™·:
    - å±‚æ¬¡ç»“æ„ä¸ç»Ÿä¸€: æ¯ä¸ªèµ„æºæ§åˆ¶å™¨ç‹¬ç«‹å±‚æ¬¡
    - å†…å­˜é™åˆ¶ä¸ç²¾ç¡®: ä¸åŒ…å«å†…æ ¸å†…å­˜
    - CPUé™åˆ¶é—®é¢˜: throttlingå¯¼è‡´å»¶è¿ŸæŠ–åŠ¨
    - I_Oé™åˆ¶ç¼ºå¤±: buffered I/Oæ— æ³•é™åˆ¶
  
  å®é™…å½±å“:
    - å®¹å™¨èµ„æºéš”ç¦»ä¸å½»åº•
    - OOM Killerè¡Œä¸ºä¸å¯é¢„æµ‹
    - æ€§èƒ½å¹²æ‰°ä¸¥é‡

cgroup_v2_æ”¹è¿›:
  ç»Ÿä¸€å±‚æ¬¡:
    - æ‰€æœ‰æ§åˆ¶å™¨ç»Ÿä¸€å±‚æ¬¡ç»“æ„
    - èµ„æºé™åˆ¶æ›´ç²¾ç¡®
    - é…ç½®æ›´ç®€å•
  
  æ–°èƒ½åŠ›:
    - ç²¾ç¡®å†…å­˜é™åˆ¶ (åŒ…å«å†…æ ¸å†…å­˜)
    - PSI (Pressure Stall Information) å‹åŠ›ç›‘æ§
    - I_Oéš”ç¦» (buffered + direct)
    - CPU burstå…è®¸çŸ­æ—¶è¶…é¢
  
  Kubernetes_1_31æ”¯æŒ:
    - é»˜è®¤å¯ç”¨cgroup v2 (èŠ‚ç‚¹æ”¯æŒæ—¶)
    - Podèµ„æºQoSæ”¹è¿›
    - å†…å­˜QoS Beta
```

### 5.2 å®æˆ˜: å†…å­˜QoSé…ç½®

**åœºæ™¯**: æ•°æ®åº“Podéœ€è¦ä¿è¯å†…å­˜æ€§èƒ½,é¿å…swapå½±å“å»¶è¿Ÿã€‚

```yaml
# database-with-memory-qos.yaml
# ä½¿ç”¨cgroup v2å†…å­˜QoS

apiVersion: v1
kind: Pod
metadata:
  name: postgres-memory-guaranteed
spec:
  containers:
  - name: postgres
    image: postgres:16
    resources:
      requests:
        memory: 8Gi
        cpu: 4
      limits:
        memory: 16Gi
        cpu: 8
    env:
    - name: POSTGRES_PASSWORD
      value: "secretpassword"
    - name: PGDATA
      value: /var/lib/postgresql/data/pgdata
    volumeMounts:
    - name: data
      mountPath: /var/lib/postgresql/data
  
  # âš¡ cgroup v2å†…å­˜QoSé…ç½®
  # é€šè¿‡annotationä¼ é€’ç»™CRI
  # æ³¨æ„: éœ€è¦containerd 2.0+ æˆ– CRI-O 1.31+
  annotations:
    io.kubernetes.cri.memory-qos: "guaranteed"  # ä¿è¯å†…å­˜æ€§èƒ½
    io.kubernetes.cri.memory-swap: "0"          # ç¦ç”¨swap
    io.kubernetes.cri.memory-oom-group: "true"  # OOMæ—¶æ•´ç»„ç»ˆæ­¢

  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: postgres-data
```

**å†…å­˜QoSç±»åˆ«**:

| QoSç±» | swapè¡Œä¸º | page cache | é€‚ç”¨åœºæ™¯ |
|------|---------|-----------|---------|
| **Guaranteed** | å®Œå…¨ç¦ç”¨ | å—é™ | å»¶è¿Ÿæ•æ„Ÿ:æ•°æ®åº“ã€ç¼“å­˜ |
| **Burstable** | é™åˆ¶ä½¿ç”¨ | å…è®¸ | ä¸€èˆ¬åº”ç”¨:WebæœåŠ¡ |
| **BestEffort** | å¯ä½¿ç”¨ | å…è®¸ | æ‰¹å¤„ç†ã€ç¦»çº¿ä»»åŠ¡ |

**éªŒè¯cgroup v2ç”Ÿæ•ˆ**:

```bash
# æ£€æŸ¥èŠ‚ç‚¹cgroupç‰ˆæœ¬
kubectl get nodes -o json | jq '.items[].status.nodeInfo.operatingSystem'

# SSHåˆ°èŠ‚ç‚¹æ£€æŸ¥
mount | grep cgroup
# è¾“å‡º: cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime)  âœ…

# æ£€æŸ¥Podçš„cgroupé…ç½®
kubectl exec postgres-memory-guaranteed -- cat /sys/fs/cgroup/memory.max
# è¾“å‡º: 17179869184 (16GB)  âœ…

kubectl exec postgres-memory-guaranteed -- cat /sys/fs/cgroup/memory.swap.max
# è¾“å‡º: 0 (ç¦ç”¨swap)  âœ…

# æ£€æŸ¥å†…å­˜å‹åŠ›ä¿¡æ¯ (PSI)
kubectl exec postgres-memory-guaranteed -- cat /sys/fs/cgroup/memory.pressure
# è¾“å‡º:
# some avg10=0.00 avg60=0.00 avg300=0.00 total=0
# full avg10=0.00 avg60=0.00 avg300=0.00 total=0
# â¬† å‹åŠ›ä¸º0,å†…å­˜å……è¶³  âœ…
```

### 5.3 å®æˆ˜: CPU Burstä¼˜åŒ–

**cgroup v2 CPU Burstç‰¹æ€§**:

```yaml
CPU_BurståŸç†:
  ä¼ ç»ŸCPUé™åˆ¶ (cgroup v1):
    - ä¸¥æ ¼é™åˆ¶: 100mså‘¨æœŸå†…æœ€å¤šç”¨50ms (cpu.cfs_quota_us / cpu.cfs_period_us)
    - é—®é¢˜: çŸ­æ—¶çªå‘éœ€æ±‚è¢«throttle,å¯¼è‡´å»¶è¿Ÿ
  
  CPU Burst (cgroup v2):
    - å…è®¸ç´¯ç§¯æœªä½¿ç”¨çš„CPUæ—¶é—´
    - çŸ­æ—¶çªå‘æ—¶å¯ä½¿ç”¨ç´¯ç§¯çš„"ä¿¡ç”¨"
    - é•¿æœŸå¹³å‡ä½¿ç”¨é‡ä¸å˜
    - å‡å°‘CPU throttling,é™ä½å»¶è¿Ÿ
```

**é…ç½®CPU Burst**:

```yaml
# web-server-with-cpu-burst.yaml
# WebæœåŠ¡å™¨å¯ç”¨CPU Burst

apiVersion: v1
kind: Pod
metadata:
  name: nginx-burst-enabled
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    resources:
      requests:
        cpu: 500m
      limits:
        cpu: 1000m  # å¹³å‡1æ ¸,å…è®¸burståˆ°æ›´é«˜
    
  # âš¡ CPU Bursté…ç½®
  # é€šè¿‡annotationä¼ é€’
  annotations:
    cpu-burst.kubernetes.io/burst-quota: "200"  # Bursté…é¢:200%
    # å«ä¹‰: å…è®¸çŸ­æ—¶ä½¿ç”¨åˆ°3æ ¸ (1æ ¸limit Ã— (1 + 200%))

---
# æ€§èƒ½å¯¹æ¯”

# åœºæ™¯: Nginxå¤„ç†çªå‘æµé‡ (QPSä»100è·ƒå‡åˆ°1000)

# æ— CPU Burst (cgroup v1):
#   P50å»¶è¿Ÿ: 50ms
#   P99å»¶è¿Ÿ: 500ms  â¬… CPU throttlingå¯¼è‡´
#   CPUä½¿ç”¨: é™åˆ¶åœ¨1æ ¸,ä¸¥æ ¼throttling

# å¯ç”¨CPU Burst (cgroup v2):
#   P50å»¶è¿Ÿ: 50ms
#   P99å»¶è¿Ÿ: 80ms  â¬… burstå¸æ”¶çªå‘
#   CPUä½¿ç”¨: çŸ­æ—¶burståˆ°2.5æ ¸,é•¿æœŸå¹³å‡1æ ¸
#   
#   å»¶è¿Ÿæ”¹å–„: 84% (P99)  âœ…
```

**ç›‘æ§CPU Burst**:

```bash
# æŸ¥çœ‹Podçš„CPU bursté…ç½®
kubectl exec nginx-burst-enabled -- cat /sys/fs/cgroup/cpu.max
# è¾“å‡º: 100000 100000  (quota period)

kubectl exec nginx-burst-enabled -- cat /sys/fs/cgroup/cpu.max.burst
# è¾“å‡º: 200000  (burst quota)  âœ…

# æŸ¥çœ‹CPU throttlingç»Ÿè®¡
kubectl exec nginx-burst-enabled -- cat /sys/fs/cgroup/cpu.stat
# è¾“å‡º:
# nr_periods 1000
# nr_throttled 50  â¬… è¢«throttleæ¬¡æ•°
# throttled_time 500000000  â¬… è¢«throttleæ€»æ—¶é—´(ns)
# nr_bursts 30  â¬… burstæ¬¡æ•°  âœ…
# burst_time 3000000000  â¬… burstæ€»æ—¶é—´(ns)  âœ…
```

---

## å…­ã€åŠ¨æ€èµ„æºåˆ†é… Beta

### 6.1 DRA: GPU/RDMAç®¡ç†çš„æœªæ¥

**ä¼ ç»ŸDevice Pluginé™åˆ¶**:

```yaml
Device_Plugin_APIé—®é¢˜:
  èµ„æºæŠ½è±¡ç²—ç³™:
    - åªèƒ½è¡¨ç¤ºæ•°é‡: nvidia.com/gpu: 1
    - æ— æ³•è¡¨è¾¾: GPUå‹å·ã€æ˜¾å­˜å¤§å°ã€NVLinkæ‹“æ‰‘
  
  è°ƒåº¦é™åˆ¶:
    - è°ƒåº¦å™¨æ— æ³•æ„ŸçŸ¥GPUç‰¹æ€§
    - æ— æ³•å®ç°æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦
    - æ— æ³•æ”¯æŒGPUåˆ‡ç‰‡ (å¦‚MIGåŠ¨æ€åˆ›å»º)
  
  é…ç½®å¤æ‚:
    - æ¯ç§è®¾å¤‡éœ€è¦ç‹¬ç«‹Device Plugin
    - é…ç½®åˆ†æ•£: ConfigMap + DaemonSet + ...
```

**DRA (Dynamic Resource Allocation)**:

```yaml
DRAæ¶æ„:
  BetaçŠ¶æ€: Kubernetes 1.31
  ç‰¹æ€§é—¨æ§: DynamicResourceAllocation=true (é»˜è®¤å¯ç”¨)
  
  æ ¸å¿ƒæ¦‚å¿µ:
    ResourceClass:
      å®šä¹‰: èµ„æºç±»å‹å’Œå‚æ•°
      ç±»ä¼¼: StorageClassä¹‹äºå­˜å‚¨
      ç¤ºä¾‹: gpu-h100-80gb, rdma-100g
    
    ResourceClaim:
      å®šä¹‰: èµ„æºè¯·æ±‚
      ç±»ä¼¼: PVCä¹‹äºå­˜å‚¨
      ç”Ÿå‘½å‘¨æœŸ: ç‹¬ç«‹äºPod,å¯å…±äº«
    
    ResourceClaimTemplate:
      å®šä¹‰: èµ„æºè¯·æ±‚æ¨¡æ¿
      ç”¨äº: StatefulSetç­‰ç”ŸæˆResourceClaim
  
  ä¼˜åŠ¿:
    - ä¸°å¯Œçš„èµ„æºæè¿°: æ”¯æŒç»“æ„åŒ–å‚æ•°
    - æ‹“æ‰‘æ„ŸçŸ¥: è°ƒåº¦å™¨ç†è§£èµ„æºæ‹“æ‰‘
    - åŠ¨æ€é…ç½®: è¿è¡Œæ—¶åˆ›å»º/åˆ é™¤èµ„æº
    - èµ„æºå…±äº«: å¤šPodå…±äº«åŒä¸€èµ„æº
```

### 6.2 å®æˆ˜: DRAç®¡ç†GPU

**åœºæ™¯**: ä½¿ç”¨DRAåŠ¨æ€åˆ†é…NVIDIA H100 MIGå®ä¾‹ã€‚

**æ­¥éª¤1: å®‰è£…DRAé©±åŠ¨**:

```yaml
# nvidia-dra-driver.yaml
# NVIDIA DRAé©±åŠ¨ (æ›¿ä»£ä¼ ç»ŸDevice Plugin)

apiVersion: v1
kind: Namespace
metadata:
  name: nvidia-dra-driver

---
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceDriver
metadata:
  name: gpu.nvidia.com
spec:
  # DRAé©±åŠ¨å®ç°
  driverName: gpu.nvidia.com
  
  # é©±åŠ¨Pod
  kubeletPlugin:
    nodeSelector:
      nvidia.com/gpu.present: "true"
    containers:
    - name: nvidia-dra-plugin
      image: nvcr.io/nvidia/k8s/dra-plugin:v0.2.0
      securityContext:
        privileged: true
      volumeMounts:
      - name: plugins
        mountPath: /var/lib/kubelet/plugins
      - name: dra
        mountPath: /var/lib/kubelet/plugins/dra
    volumes:
    - name: plugins
      hostPath:
        path: /var/lib/kubelet/plugins
    - name: dra
      hostPath:
        path: /var/lib/kubelet/plugins/dra

---
# åº”ç”¨DRAé©±åŠ¨
# kubectl apply -f nvidia-dra-driver.yaml
```

**æ­¥éª¤2: å®šä¹‰ResourceClass**:

```yaml
# gpu-resource-classes.yaml
# å®šä¹‰ä¸åŒGPU Profileçš„ResourceClass

---
# H100å®Œæ•´GPU
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClass
metadata:
  name: gpu-h100-full
driverName: gpu.nvidia.com
parametersRef:
  apiGroup: gpu.nvidia.com
  kind: GPUClassParameters
  name: h100-full-params

---
apiVersion: gpu.nvidia.com/v1alpha1
kind: GPUClassParameters
metadata:
  name: h100-full-params
spec:
  selector:
    # é€‰æ‹©H100 GPU
    model: "H100"
  sharing:
    strategy: "Exclusive"  # ç‹¬å æ¨¡å¼

---
# H100 MIG 3g.40gbå®ä¾‹
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClass
metadata:
  name: gpu-h100-mig-3g
driverName: gpu.nvidia.com
parametersRef:
  apiGroup: gpu.nvidia.com
  kind: GPUClassParameters
  name: h100-mig-3g-params

---
apiVersion: gpu.nvidia.com/v1alpha1
kind: GPUClassParameters
metadata:
  name: h100-mig-3g-params
spec:
  selector:
    model: "H100"
  sharing:
    strategy: "MIG"
    mig:
      profile: "3g.40gb"  # MIG Profile
      geometry: "3g.40gb"

---
# H100 Time-Slicing
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClass
metadata:
  name: gpu-h100-shared
driverName: gpu.nvidia.com
parametersRef:
  apiGroup: gpu.nvidia.com
  kind: GPUClassParameters
  name: h100-shared-params

---
apiVersion: gpu.nvidia.com/v1alpha1
kind: GPUClassParameters
metadata:
  name: h100-shared-params
spec:
  selector:
    model: "H100"
  sharing:
    strategy: "TimeSlicing"
    timeSlicing:
      replicas: 4  # 1ä¸ªç‰©ç†GPUåˆ†4ä»½
```

**æ­¥éª¤3: ä½¿ç”¨ResourceClaimè¯·æ±‚GPU**:

```yaml
# training-job-with-dra.yaml
# ä½¿ç”¨DRAè¯·æ±‚GPUçš„è®­ç»ƒJob

apiVersion: v1
kind: Pod
metadata:
  name: pytorch-training-dra
spec:
  # âš¡ ä½¿ç”¨ResourceClaimè¯·æ±‚èµ„æº
  resourceClaims:
  - name: gpu
    resourceClaimName: training-gpu-claim  # å¼•ç”¨ResourceClaim

  containers:
  - name: pytorch
    image: pytorch/pytorch:2.5.0-cuda12.1
    command:
    - python
    - train.py
    resources:
      claims:
      - name: gpu  # å¼•ç”¨ä¸Šé¢å®šä¹‰çš„resourceClaims
    
    # ä¼ ç»Ÿæ–¹å¼å¯¹æ¯”:
    # resources:
    #   limits:
    #     nvidia.com/gpu: 1  # â¬… æ— æ³•æŒ‡å®šGPUå‹å·/Profile

---
# ResourceClaimå®šä¹‰
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClaim
metadata:
  name: training-gpu-claim
spec:
  deviceClassName: gpu-h100-mig-3g  # è¯·æ±‚H100 MIG 3g.40gbå®ä¾‹

---
# æˆ–ä½¿ç”¨ResourceClaimTemplate (ç”¨äºStatefulSet)
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClaimTemplate
metadata:
  name: gpu-claim-template
spec:
  spec:
    deviceClassName: gpu-h100-mig-3g
```

**æ­¥éª¤4: è°ƒåº¦å™¨æ‹“æ‰‘æ„ŸçŸ¥**:

```yaml
# multi-gpu-training-topology-aware.yaml
# å¤šGPUè®­ç»ƒ,æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦

apiVersion: v1
kind: Pod
metadata:
  name: llama-training-8gpu
spec:
  resourceClaims:
  # è¯·æ±‚8ä¸ªGPU,è¦æ±‚NVLinkäº’è”
  - name: gpu-0
    resourceClaimName: gpu-claim-0
  - name: gpu-1
    resourceClaimName: gpu-claim-1
  - name: gpu-2
    resourceClaimName: gpu-claim-2
  - name: gpu-3
    resourceClaimName: gpu-claim-3
  - name: gpu-4
    resourceClaimName: gpu-claim-4
  - name: gpu-5
    resourceClaimName: gpu-claim-5
  - name: gpu-6
    resourceClaimName: gpu-claim-6
  - name: gpu-7
    resourceClaimName: gpu-claim-7
  
  containers:
  - name: training
    image: nvcr.io/nvidia/pytorch:24.09-py3
    command:
    - torchrun
    - --nproc_per_node=8
    - train_llama.py
    resources:
      claims:
      - name: gpu-0
      - name: gpu-1
      - name: gpu-2
      - name: gpu-3
      - name: gpu-4
      - name: gpu-5
      - name: gpu-6
      - name: gpu-7

---
# ResourceClaimTemplate withæ‹“æ‰‘çº¦æŸ
apiVersion: resource.k8s.io/v1alpha3
kind: ResourceClaimTemplate
metadata:
  name: gpu-claim-template-nvlink
spec:
  spec:
    deviceClassName: gpu-h100-full
    # âš¡ DRAæ”¯æŒæ‹“æ‰‘çº¦æŸ
    constraints:
      # è¦æ±‚æ‰€æœ‰GPUåœ¨åŒä¸€èŠ‚ç‚¹
      - matchLabels:
          topology.kubernetes.io/zone: same
      # è¦æ±‚GPUé—´æœ‰NVLinkè¿æ¥
      - selector:
          interconnect: "nvlink"
          bandwidth: ">= 900GB/s"

# è°ƒåº¦å™¨ä¼šç¡®ä¿:
# 1. 8ä¸ªGPUåœ¨åŒä¸€èŠ‚ç‚¹ âœ…
# 2. GPUé—´æœ‰NVLinkäº’è” âœ…
# 3. é¿å…è·¨PCIeäº¤æ¢æœº âœ…
```

**DRA vs Device Pluginå¯¹æ¯”**:

| ç‰¹æ€§ | Device Plugin | DRA |
|-----|--------------|-----|
| **èµ„æºæè¿°** | ä»…æ•°é‡ | ä¸°å¯Œå‚æ•°(å‹å·/æ‹“æ‰‘) |
| **è°ƒåº¦æ„ŸçŸ¥** | æ—  | æ‹“æ‰‘æ„ŸçŸ¥ã€çº¦æŸæ”¯æŒ |
| **åŠ¨æ€é…ç½®** | æ—  | è¿è¡Œæ—¶åˆ›å»ºMIG |
| **èµ„æºå…±äº«** | å›°éš¾ | åŸç”Ÿæ”¯æŒ |
| **é…ç½®å¤æ‚åº¦** | ä¸­ | ä½ (å£°æ˜å¼) |
| **ç”Ÿäº§å°±ç»ª** | GA | Beta (1.31) |

---

## ä¸ƒã€å®æˆ˜è¿ç§»æŒ‡å—

### 7.1 ä»1.30å‡çº§åˆ°1.31

**å‡çº§å‰æ£€æŸ¥æ¸…å•**:

```bash
#!/bin/bash
# pre-upgrade-checklist.sh
# Kubernetes 1.30 -> 1.31å‡çº§å‰æ£€æŸ¥

set -euo pipefail

echo "=== Kubernetes 1.31å‡çº§å‰æ£€æŸ¥ ==="

# 1. æ£€æŸ¥å½“å‰ç‰ˆæœ¬
CURRENT_VERSION=$(kubectl version --short | grep Server | awk '{print $3}')
echo "å½“å‰ç‰ˆæœ¬: ${CURRENT_VERSION}"

if [[ ! "${CURRENT_VERSION}" =~ ^v1\.30\. ]]; then
    echo "âš ï¸  è­¦å‘Š: ä»…æ”¯æŒä»1.30å‡çº§åˆ°1.31"
    echo "å½“å‰ç‰ˆæœ¬${CURRENT_VERSION}ä¸æ˜¯1.30.x"
fi

# 2. æ£€æŸ¥åºŸå¼ƒAPIä½¿ç”¨
echo ""
echo "æ£€æŸ¥åºŸå¼ƒAPI..."
kubectl api-resources --verbs=list -o name | \
  xargs -n1 kubectl get --show-kind --ignore-not-found --all-namespaces \
  -o=custom-columns=KIND:.kind,NAME:.metadata.name,APIVERSION:.apiVersion | \
  grep -E "v1beta1|v1alpha1" || echo "âœ… æœªå‘ç°åºŸå¼ƒAPI"

# 3. æ£€æŸ¥AppArmor Annotation
echo ""
echo "æ£€æŸ¥AppArmor Annotation..."
kubectl get pods --all-namespaces -o json | \
  jq -r '.items[] | select(.metadata.annotations | has("container.apparmor.security.beta.kubernetes.io")) | 
  "\(.metadata.namespace)/\(.metadata.name)"' | \
  head -10 | \
  while read pod; do
    echo "âš ï¸  éœ€è¦è¿ç§»AppArmor Annotation: ${pod}"
  done

# 4. æ£€æŸ¥PodSecurityPolicy (PSP)
echo ""
echo "æ£€æŸ¥PodSecurityPolicy..."
if kubectl get psp &>/dev/null; then
    echo "âš ï¸  è­¦å‘Š: PodSecurityPolicyå·²åœ¨1.25ç§»é™¤"
    echo "è¯·è¿ç§»åˆ°Pod Security Standards"
fi

# 5. æ£€æŸ¥å­˜å‚¨CSIé©±åŠ¨ç‰ˆæœ¬
echo ""
echo "æ£€æŸ¥CSIé©±åŠ¨ç‰ˆæœ¬..."
kubectl get csidrivers -o json | \
  jq -r '.items[] | "\(.metadata.name): \(.spec.version // "unknown")"'

# 6. æ£€æŸ¥èŠ‚ç‚¹å°±ç»ªçŠ¶æ€
echo ""
echo "æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€..."
kubectl get nodes -o wide

# 7. æ£€æŸ¥å…³é”®ç»„ä»¶
echo ""
echo "æ£€æŸ¥å…³é”®ç»„ä»¶..."
kubectl get pods -n kube-system -o wide | grep -E "(kube-apiserver|kube-controller|kube-scheduler|etcd)"

# 8. å¤‡ä»½å»ºè®®
echo ""
echo "=== å‡çº§å‰å¿…åšå¤‡ä»½ ==="
echo "1. etcdå¤‡ä»½:"
echo "   ETCDCTL_API=3 etcdctl snapshot save /backup/etcd-$(date +%Y%m%d).db"
echo ""
echo "2. Kubernetesé…ç½®å¤‡ä»½:"
echo "   kubectl get all --all-namespaces -o yaml > /backup/k8s-resources-$(date +%Y%m%d).yaml"
echo ""
echo "3. è¯ä¹¦å¤‡ä»½:"
echo "   tar -czf /backup/k8s-certs-$(date +%Y%m%d).tar.gz /etc/kubernetes/pki"

echo ""
echo "âœ… æ£€æŸ¥å®Œæˆ,è¯·æ ¹æ®ä¸Šè¿°è¾“å‡ºæ‰§è¡Œå¿…è¦çš„ä¿®å¤åå†å‡çº§"
```

**å‡çº§æ­¥éª¤ (kubeadm)**:

```bash
#!/bin/bash
# upgrade-to-1.31.sh
# å‡çº§åˆ°Kubernetes 1.31

set -euo pipefail

TARGET_VERSION="1.31.0"

echo "=== å‡çº§åˆ°Kubernetes ${TARGET_VERSION} ==="

# 1. å¤‡ä»½etcd
echo "1. å¤‡ä»½etcd..."
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /backup/etcd-before-1.31-$(date +%Y%m%d%H%M%S).db

# 2. å‡çº§kubeadm
echo "2. å‡çº§kubeadm..."
apt-get update
apt-get install -y kubeadm=${TARGET_VERSION}-00

kubeadm version

# 3. æ£€æŸ¥å‡çº§è®¡åˆ’
echo "3. æ£€æŸ¥å‡çº§è®¡åˆ’..."
kubeadm upgrade plan

# 4. å‡çº§ç¬¬ä¸€ä¸ªæ§åˆ¶å¹³é¢èŠ‚ç‚¹
echo "4. å‡çº§æ§åˆ¶å¹³é¢..."
kubeadm upgrade apply v${TARGET_VERSION} --yes

# 5. é©±é€èŠ‚ç‚¹Pod
echo "5. é©±é€èŠ‚ç‚¹Pod..."
kubectl drain $(hostname) --ignore-daemonsets --delete-emptydir-data

# 6. å‡çº§kubeletå’Œkubectl
echo "6. å‡çº§kubeletå’Œkubectl..."
apt-get install -y kubelet=${TARGET_VERSION}-00 kubectl=${TARGET_VERSION}-00

# 7. é‡å¯kubelet
echo "7. é‡å¯kubelet..."
systemctl daemon-reload
systemctl restart kubelet

# 8. æ¢å¤èŠ‚ç‚¹è°ƒåº¦
echo "8. æ¢å¤èŠ‚ç‚¹è°ƒåº¦..."
kubectl uncordon $(hostname)

# 9. éªŒè¯å‡çº§
echo "9. éªŒè¯å‡çº§..."
kubectl get nodes
kubectl version

# 10. å‡çº§å…¶ä»–æ§åˆ¶å¹³é¢èŠ‚ç‚¹
echo ""
echo "=== åç»­æ­¥éª¤ ==="
echo "1. åœ¨å…¶ä»–æ§åˆ¶å¹³é¢èŠ‚ç‚¹æ‰§è¡Œ:"
echo "   kubeadm upgrade node"
echo ""
echo "2. å‡çº§å·¥ä½œèŠ‚ç‚¹ (æ¯ä¸ªèŠ‚ç‚¹):"
echo "   apt-get update && apt-get install -y kubeadm=${TARGET_VERSION}-00"
echo "   kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data"
echo "   kubeadm upgrade node"
echo "   apt-get install -y kubelet=${TARGET_VERSION}-00 kubectl=${TARGET_VERSION}-00"
echo "   systemctl daemon-reload && systemctl restart kubelet"
echo "   kubectl uncordon <node-name>"
```

### 7.2 è¿ç§»AppArmoré…ç½®

**è‡ªåŠ¨è¿ç§»è„šæœ¬**:

```python
#!/usr/bin/env python3
# migrate-apparmor-annotations.py
# è‡ªåŠ¨è¿ç§»AppArmor Annotationåˆ°SecurityContext

import yaml
import subprocess
import sys
from typing import Dict, List

def get_all_pods() -> List[Dict]:
    """è·å–æ‰€æœ‰ä½¿ç”¨AppArmor Annotationçš„Pod"""
    cmd = [
        "kubectl", "get", "pods", "--all-namespaces",
        "-o", "json"
    ]
    result = subprocess.run(cmd, capture_output=True, text=True)
    data = yaml.safe_load(result.stdout)
    
    pods = []
    for item in data.get("items", []):
        annotations = item.get("metadata", {}).get("annotations", {})
        # æŸ¥æ‰¾AppArmor Annotation
        apparmor_annots = {k: v for k, v in annotations.items()
                          if k.startswith("container.apparmor.security.beta.kubernetes.io/")}
        if apparmor_annots:
            pods.append({
                "namespace": item["metadata"]["namespace"],
                "name": item["metadata"]["name"],
                "annotations": apparmor_annots,
                "spec": item["spec"]
            })
    
    return pods

def migrate_pod_spec(pod_spec: Dict, apparmor_annots: Dict) -> Dict:
    """è¿ç§»Pod Spec"""
    migrated_spec = pod_spec.copy()
    
    for container in migrated_spec.get("containers", []):
        container_name = container["name"]
        annotation_key = f"container.apparmor.security.beta.kubernetes.io/{container_name}"
        
        if annotation_key in apparmor_annots:
            profile_value = apparmor_annots[annotation_key]
            
            # åˆå§‹åŒ–securityContext
            if "securityContext" not in container:
                container["securityContext"] = {}
            
            # æ·»åŠ appArmorProfile
            if profile_value == "runtime/default":
                container["securityContext"]["appArmorProfile"] = {
                    "type": "RuntimeDefault"
                }
            elif profile_value == "unconfined":
                container["securityContext"]["appArmorProfile"] = {
                    "type": "Unconfined"
                }
            elif profile_value.startswith("localhost/"):
                profile_name = profile_value.replace("localhost/", "")
                container["securityContext"]["appArmorProfile"] = {
                    "type": "Localhost",
                    "localhostProfile": profile_name
                }
    
    return migrated_spec

def generate_migration_yaml(pods: List[Dict]) -> None:
    """ç”Ÿæˆè¿ç§»YAMLæ–‡ä»¶"""
    with open("apparmor-migration.yaml", "w") as f:
        f.write("# AppArmor Annotationè¿ç§»åˆ°SecurityContext\n")
        f.write("# ç”Ÿæˆæ—¥æœŸ: 2025-10-22\n\n")
        
        for pod in pods:
            f.write(f"---\n")
            f.write(f"# åŸPod: {pod['namespace']}/{pod['name']}\n")
            f.write(f"# åŸAnnotations: {pod['annotations']}\n\n")
            
            migrated_spec = migrate_pod_spec(pod["spec"], pod["annotations"])
            
            # ç”Ÿæˆæ›´æ–°çš„Pod/Deploymenté…ç½®
            f.write(yaml.dump({
                "apiVersion": "v1",
                "kind": "Pod",
                "metadata": {
                    "namespace": pod["namespace"],
                    "name": f"{pod['name']}-migrated"
                },
                "spec": migrated_spec
            }, default_flow_style=False))
            f.write("\n")

def main():
    print("=== AppArmoré…ç½®è¿ç§»å·¥å…· ===")
    print("æŸ¥æ‰¾ä½¿ç”¨æ—§Annotationçš„Pod...")
    
    pods = get_all_pods()
    
    if not pods:
        print("âœ… æœªå‘ç°ä½¿ç”¨AppArmor Annotationçš„Pod")
        return
    
    print(f"å‘ç°{len(pods)}ä¸ªPodéœ€è¦è¿ç§»:")
    for pod in pods:
        print(f"  - {pod['namespace']}/{pod['name']}")
    
    print("\nç”Ÿæˆè¿ç§»YAML...")
    generate_migration_yaml(pods)
    
    print("âœ… è¿ç§»YAMLå·²ç”Ÿæˆ: apparmor-migration.yaml")
    print("\nåç»­æ­¥éª¤:")
    print("1. æ£€æŸ¥apparmor-migration.yamlå†…å®¹")
    print("2. æ›´æ–°Deployment/StatefulSetç­‰æ§åˆ¶å™¨")
    print("3. æ»šåŠ¨é‡å¯Podåº”ç”¨æ–°é…ç½®")

if __name__ == "__main__":
    main()
```

---

_ç”±äºæ–‡æ¡£é•¿åº¦é™åˆ¶,ç¬¬å…«ã€ä¹ç« èŠ‚å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†ç»§ç»­..._

## å…«ã€æœ€ä½³å®è·µ

### 8.1 Sidecar Containersæœ€ä½³å®è·µ

```yaml
æœ€ä½³å®è·µ:
  1. ä½¿ç”¨restartPolicy_Always:
     - æ‰€æœ‰Sidecarå¿…é¡»è®¾ç½®restartPolicy: Always
     - å¦åˆ™å°†è¢«è§†ä¸ºæ™®é€šinitContainer
  
  2. å°±ç»ªæ¢é’ˆé…ç½®:
     - Sidecarå¿…é¡»é…ç½®readinessProbe
     - ä¸»å®¹å™¨æ‰èƒ½ç­‰å¾…Sidecarå°±ç»ª
  
  3. èµ„æºé™åˆ¶:
     - Sidecarå’Œä¸»å®¹å™¨åˆ†åˆ«è®¾ç½®resources
     - é¿å…Sidecarèµ„æºä¸è¶³å½±å“å¯åŠ¨
  
  4. ä¼˜é›…ç»ˆæ­¢:
     - é…ç½®preStopé’©å­æ¸…ç†èµ„æº
     - è®¾ç½®åˆç†çš„terminationGracePeriodSeconds
  
  5. æ—¥å¿—ç®¡ç†:
     - Sidecaræ—¥å¿—ç‹¬ç«‹æ”¶é›†
     - é¿å…ä¸ä¸»å®¹å™¨æ—¥å¿—æ··æ·†
```

### 8.2 AppArmoræœ€ä½³å®è·µ

```yaml
æœ€ä½³å®è·µ:
  1. ä½¿ç”¨RuntimeDefaultèµ·æ­¥:
     - æ–°åº”ç”¨å…ˆä½¿ç”¨RuntimeDefault
     - éªŒè¯åŠŸèƒ½æ­£å¸¸åå†è‡ªå®šä¹‰
  
  2. æœ€å°æƒé™åŸåˆ™:
     - ä»…æˆäºˆå¿…éœ€çš„æ–‡ä»¶è®¿é—®æƒé™
     - ç¦ç”¨ä¸éœ€è¦çš„Capabilities
     - æ˜ç¡®denyå±é™©æ“ä½œ
  
  3. æµ‹è¯•Profile:
     - å…ˆåœ¨Complainingæ¨¡å¼æµ‹è¯•
     - æŸ¥çœ‹/var/log/audit/audit.log
     - ç¡®è®¤æ— è¯¯ååˆ‡æ¢Enforcing
  
  4. Profileç‰ˆæœ¬ç®¡ç†:
     - Profileå­˜å‚¨åœ¨Git
     - ä½¿ç”¨ConfigMap/DaemonSetåˆ†å‘
     - è®°å½•Profileå˜æ›´å†å²
  
  5. ç›‘æ§å‘Šè­¦:
     - ç›‘æ§AppArmoræ‹’ç»äº‹ä»¶
     - å¼‚å¸¸æ‹’ç»è§¦å‘å‘Šè­¦
     - å®šæœŸReview Profile
```

### 8.3 Jobå¤±è´¥ç­–ç•¥æœ€ä½³å®è·µ

```yaml
æœ€ä½³å®è·µ:
  1. åŒºåˆ†å¤±è´¥ç±»å‹:
     - é…ç½®é”™è¯¯: FailJobç«‹å³å¤±è´¥
     - ä¸´æ—¶æ•…éšœ: Countå¹¶é‡è¯•
     - é¢„æœŸè¡Œä¸º: Ignoreå¿½ç•¥
  
  2. åˆç†è®¾ç½®backoffLimit:
     - ä¸´æ—¶æ•…éšœ: backoffLimit=3-6
     - ç¨³å®šä»»åŠ¡: backoffLimit=1-2
     - å®éªŒä»»åŠ¡: backoffLimit=0 (ä¸é‡è¯•)
  
  3. ä½¿ç”¨æœ‰æ„ä¹‰çš„é€€å‡ºç :
     - 0: æˆåŠŸ
     - 1-10: é…ç½®é”™è¯¯
     - 42: æ•°æ®ä¸ºç©º (å¯å¿½ç•¥)
     - 124: è¶…æ—¶ (å¯é‡è¯•)
     - 137: OOM (éœ€è°ƒæ•´èµ„æº)
  
  4. ç›‘æ§Jobå¤±è´¥:
     - å‘Šè­¦backoffLimitè€—å°½
     - è¿½è¸ªå¤±è´¥Podæ—¥å¿—
     - åˆ†æå¤±è´¥åŸå› åˆ†å¸ƒ
  
  5. ç´¢å¼•åŒ–Job:
     - å¤§è§„æ¨¡æ‰¹å¤„ç†ä½¿ç”¨completionMode: Indexed
     - é…ç½®backoffLimitPerIndex
     - å…è®¸éƒ¨åˆ†å¤±è´¥ (maxFailedIndexes)
```

---

## ä¹ã€æ•…éšœæ’æŸ¥

### 9.1 Sidecar Containersé—®é¢˜æ’æŸ¥

```bash
# é—®é¢˜1: ä¸»å®¹å™¨æ— æ³•å¯åŠ¨
# ç—‡çŠ¶: Podå¡åœ¨InitçŠ¶æ€

# æ’æŸ¥æ­¥éª¤:
# 1. æ£€æŸ¥SidecarçŠ¶æ€
kubectl get pod <pod-name> -o jsonpath='{.status.initContainerStatuses[*].state}'

# 2. æŸ¥çœ‹Sidecaræ—¥å¿—
kubectl logs <pod-name> -c <sidecar-name>

# 3. æ£€æŸ¥å°±ç»ªæ¢é’ˆ
kubectl describe pod <pod-name> | grep -A 10 "Readiness"

# å¸¸è§åŸå› :
# - Sidecaræœªé…ç½®restartPolicy: Always
# - Sidecarå°±ç»ªæ¢é’ˆå¤±è´¥
# - Sidecarèµ„æºä¸è¶³OOMKilled

# é—®é¢˜2: Sidecaråœ¨Jobå®Œæˆåä¸é€€å‡º
# æ’æŸ¥:
kubectl get pod <pod-name> -o yaml | grep restartPolicy
# ç¡®ä¿: restartPolicy: Always  âœ…

# é—®é¢˜3: æ—¥å¿—æ”¶é›†ä¸å®Œæ•´
# åŸå› : Sidecarè¿‡æ—©ç»ˆæ­¢
# è§£å†³: é…ç½®preStopé’©å­
```

### 9.2 AppArmoré—®é¢˜æ’æŸ¥

```bash
# é—®é¢˜1: Podæ— æ³•å¯åŠ¨,AppArmoré”™è¯¯
# ç—‡çŠ¶: CreateContainerError

# æ’æŸ¥æ­¥éª¤:
# 1. æ£€æŸ¥äº‹ä»¶
kubectl describe pod <pod-name> | grep -A 5 Events

# è¾“å‡ºç¤ºä¾‹:
# Error: failed to create containerd container: 
# ... apparmor profile "k8s-nginx" not found

# 2. æ£€æŸ¥èŠ‚ç‚¹Profile
ssh <node>
sudo apparmor_status | grep k8s-nginx

# 3. åŠ è½½Profile
sudo apparmor_parser -r /etc/apparmor.d/k8s-nginx

# é—®é¢˜2: åº”ç”¨åŠŸèƒ½å¼‚å¸¸
# åŸå› : AppArmoræ‹’ç»å¿…è¦æ“ä½œ

# æ’æŸ¥:
# 1. æŸ¥çœ‹å®¡è®¡æ—¥å¿—
sudo grep DENIED /var/log/audit/audit.log | grep <container-id>

# 2. ä¸´æ—¶ç¦ç”¨AppArmoræµ‹è¯•
kubectl patch pod <pod-name> -p '
{
  "spec": {
    "containers": [{
      "name": "<container-name>",
      "securityContext": {
        "appArmorProfile": {
          "type": "Unconfined"
        }
      }
    }]
  }
}'

# 3. æ ¹æ®æ—¥å¿—è°ƒæ•´Profile
```

### 9.3 DRAé—®é¢˜æ’æŸ¥

```bash
# é—®é¢˜1: ResourceClaim pending
# æ’æŸ¥:
kubectl describe resourceclaim <claim-name>

# Events:
#   Type    Reason              Message
#   ----    ------              -------
#   Warning  AllocationFailed    No available GPUs match the requirements

# è§£å†³:
# 1. æ£€æŸ¥GPUèµ„æº
kubectl get nodes -o json | jq '.items[].status.allocatable'

# 2. æ£€æŸ¥ResourceClasså‚æ•°
kubectl get resourceclass <class-name> -o yaml

# 3. æ”¾å®½çº¦æŸæ¡ä»¶

# é—®é¢˜2: Podè°ƒåº¦å¤±è´¥
# æ’æŸ¥:
kubectl describe pod <pod-name> | grep -A 10 Events

# è§£å†³:
# æ£€æŸ¥æ‹“æ‰‘çº¦æŸæ˜¯å¦è¿‡ä¸¥æ ¼
```

---

## æ€»ç»“

Kubernetes 1.31å¸¦æ¥äº†å¤šé¡¹ç”Ÿäº§å°±ç»ªçš„é‡è¦ç‰¹æ€§:

1. **Sidecar Containers GA** - è§£å†³å¯åŠ¨é¡ºåºé—®é¢˜,Job Sidecarè‡ªåŠ¨æ¸…ç†
2. **AppArmor GA** - å¼ºåŒ–å®¹å™¨å®‰å…¨,æ ‡å‡†åŒ–é…ç½®æ–¹å¼
3. **PVæœ€åé˜¶æ®µè½¬æ¢ Beta** - åœ¨çº¿å­˜å‚¨è¿ç§»,é›¶åœæœº
4. **Podå¤±è´¥ç­–ç•¥ Beta** - æ™ºèƒ½å¤±è´¥å¤„ç†,é™ä½èµ„æºæµªè´¹
5. **cgroup v2å¢å¼º** - å†…å­˜QoSã€CPU Burst,æ€§èƒ½æå‡
6. **åŠ¨æ€èµ„æºåˆ†é… Beta** - æ›´çµæ´»çš„GPU/RDMAç®¡ç†

**å‡çº§å»ºè®®**: Kubernetes 1.31æ˜¯ä¸€ä¸ªç¨³å®šä¸”å€¼å¾—å‡çº§çš„ç‰ˆæœ¬,å»ºè®®:

- **1.30ç”¨æˆ·**: ç«‹å³å‡çº§ (å…¼å®¹æ€§å¥½,é£é™©ä½)
- **1.29ç”¨æˆ·**: 1ä¸ªæœˆå†…å‡çº§
- **<=1.28ç”¨æˆ·**: 2-3ä¸ªæœˆå†…å‡çº§,å……åˆ†æµ‹è¯•

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2025-10-22  
**ç»´æŠ¤è€…**: vSphere_Dockeré¡¹ç›®å›¢é˜Ÿ  
**è®¸å¯è¯**: CC BY-SA 4.0  
**çŠ¶æ€**: âœ… å®Œæˆ

---

**æœ¬æ–‡æ¡£ä¸ºã€Š2025å¹´10æœˆ22æ—¥è™šæ‹ŸåŒ–å®¹å™¨åŒ–æ²™ç›’åŒ–æŠ€æœ¯å…¨é¢å¯¹æ ‡ã€‹ç³»åˆ—çš„ä¸€éƒ¨åˆ†,æä¾›Kubernetes 1.31å®æˆ˜æŒ‡å—ã€‚**
