# 可观测性与监控

## 目录

- [可观测性与监控](#可观测性与监控)
  - [目录](#目录)
  - [1. 可观测性概述](#1-可观测性概述)
    - [1.1 可观测性三大支柱](#11-可观测性三大支柱)
    - [1.2 服务网格可观测性优势](#12-服务网格可观测性优势)
    - [1.3 可观测性架构](#13-可观测性架构)
  - [2. Prometheus监控](#2-prometheus监控)
    - [2.1 Prometheus架构](#21-prometheus架构)
    - [2.2 Istio Prometheus集成](#22-istio-prometheus集成)
      - [2.2.1 部署Prometheus](#221-部署prometheus)
      - [2.2.2 Istio暴露的指标](#222-istio暴露的指标)
    - [2.3 Linkerd Prometheus集成](#23-linkerd-prometheus集成)
    - [2.4 关键指标](#24-关键指标)
    - [2.5 PromQL查询](#25-promql查询)
  - [3. Jaeger分布式追踪](#3-jaeger分布式追踪)
    - [3.1 分布式追踪原理](#31-分布式追踪原理)
    - [3.2 Istio Jaeger集成](#32-istio-jaeger集成)
    - [3.3 追踪分析](#33-追踪分析)
    - [3.4 性能优化](#34-性能优化)
  - [4. Grafana可视化](#4-grafana可视化)
    - [4.1 Grafana Dashboard](#41-grafana-dashboard)
    - [4.2 Istio Dashboard](#42-istio-dashboard)
    - [4.3 Linkerd Dashboard](#43-linkerd-dashboard)
    - [4.4 自定义Dashboard](#44-自定义dashboard)
  - [5. Loki日志聚合](#5-loki日志聚合)
    - [5.1 Loki架构](#51-loki架构)
    - [5.2 日志收集](#52-日志收集)
    - [5.3 日志查询](#53-日志查询)
    - [5.4 日志关联](#54-日志关联)
  - [6. OpenTelemetry标准](#6-opentelemetry标准)
    - [6.1 OpenTelemetry概述](#61-opentelemetry概述)
    - [6.2 服务网格集成](#62-服务网格集成)
    - [6.3 统一可观测性](#63-统一可观测性)
  - [7. SLI/SLO/SLA](#7-slislosla)
    - [7.1 概念与区别](#71-概念与区别)
    - [7.2 定义SLI](#72-定义sli)
    - [7.3 设置SLO](#73-设置slo)
    - [7.4 错误预算](#74-错误预算)
  - [8. 告警配置](#8-告警配置)
    - [8.1 Alertmanager配置](#81-alertmanager配置)
    - [8.2 告警规则](#82-告警规则)
    - [8.3 通知集成](#83-通知集成)
    - [8.4 告警最佳实践](#84-告警最佳实践)
  - [9. 可观测性实战](#9-可观测性实战)
    - [9.1 完整可观测性栈](#91-完整可观测性栈)
    - [9.2 故障排查案例](#92-故障排查案例)
    - [9.3 性能分析案例](#93-性能分析案例)
  - [10. 总结](#10-总结)
    - [10.1 可观测性三大支柱](#101-可观测性三大支柱)
    - [10.2 服务网格可观测性优势](#102-服务网格可观测性优势)
    - [10.3 最佳实践总结](#103-最佳实践总结)
    - [10.4 工具对比](#104-工具对比)
    - [10.5 未来展望](#105-未来展望)

---

## 1. 可观测性概述

### 1.1 可观测性三大支柱

```yaml
可观测性三大支柱:

1. Metrics (指标):
   定义: 时间序列数据，量化系统状态
   
   类型:
     - Counter (计数器): 累计值（请求数、错误数）
     - Gauge (测量值): 瞬时值（CPU、内存、活跃连接数）
     - Histogram (直方图): 分布情况（延迟分布）
     - Summary (摘要): 百分位数（P50、P95、P99）
   
   价值:
     ✅ 实时监控
     ✅ 趋势分析
     ✅ 告警触发
     ✅ 容量规划
   
   示例:
     - 请求成功率
     - 请求延迟
     - 吞吐量
     - 错误率

2. Tracing (追踪):
   定义: 请求在分布式系统中的完整路径
   
   概念:
     - Trace (跟踪): 完整的请求链路
     - Span (跨度): 单个操作（服务调用）
     - Parent/Child Span: 父子关系
   
   价值:
     ✅ 调用链可视化
     ✅ 性能瓶颈定位
     ✅ 依赖关系分析
     ✅ 故障根因分析
   
   示例:
     - 用户请求 → Gateway → Service A → Service B → Database
     - 每个环节的耗时
     - 调用关系图

3. Logging (日志):
   定义: 事件的离散记录
   
   类型:
     - 应用日志: 业务逻辑日志
     - 访问日志: HTTP请求日志
     - 系统日志: 基础设施日志
     - 审计日志: 安全审计日志
   
   价值:
     ✅ 详细上下文
     ✅ 问题诊断
     ✅ 审计追踪
     ✅ 安全分析
   
   示例:
     - 错误堆栈
     - 请求详情
     - 用户操作
     - 系统事件

三者关系:
  Metrics: 告诉你"有问题"（What）
  Tracing: 告诉你"问题在哪"（Where）
  Logging: 告诉你"问题详情"（Why）

完整流程:
  1. Metrics发现异常（请求成功率下降）
  2. Tracing定位问题服务（Service B慢）
  3. Logging查看详细错误（数据库连接超时）
```

### 1.2 服务网格可观测性优势

```yaml
服务网格 vs 传统方式:

传统方式:
  ❌ 每个服务独立实现
  ❌ 指标格式不一致
  ❌ 难以关联
  ❌ 侵入式埋点
  ❌ 维护成本高

服务网格:
  ✅ 自动采集
  ✅ 统一格式
  ✅ 自动关联
  ✅ 非侵入式
  ✅ 开箱即用

服务网格可观测性优势:

1. 自动指标采集:
   ✅ Sidecar自动采集
   ✅ 无需修改代码
   ✅ 标准化指标
   ✅ 全局一致

2. 分布式追踪:
   ✅ 自动注入Trace Header
   ✅ 完整调用链
   ✅ 性能分析
   ✅ 依赖关系图

3. 访问日志:
   ✅ 统一格式
   ✅ 丰富上下文
   ✅ 自动收集
   ✅ 结构化日志

4. 拓扑可视化:
   ✅ 服务依赖图
   ✅ 实时流量
   ✅ 健康状态
   ✅ 性能热图

5. 统一管理:
   ✅ 集中配置
   ✅ 统一查询
   ✅ 关联分析
   ✅ 一站式平台
```

### 1.3 可观测性架构

```yaml
服务网格可观测性架构:

┌─────────────────────────────────────────────────────────────┐
│                     应用与服务                               │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐  │
│  │Service A │  │Service B │  │Service C │  │Service D │  │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘  │
│       │             │             │             │         │
│  ┌────▼─────┐  ┌───▼──────┐  ┌───▼──────┐  ┌───▼──────┐  │
│  │ Sidecar  │  │ Sidecar  │  │ Sidecar  │  │ Sidecar  │  │
│  │  Proxy   │  │  Proxy   │  │  Proxy   │  │  Proxy   │  │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘  │
└───────┼─────────────┼─────────────┼─────────────┼─────────┘
        │             │             │             │
        │ Metrics     │ Traces      │ Logs        │
        ▼             ▼             ▼             ▼
┌─────────────────────────────────────────────────────────────┐
│                    采集与存储层                              │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐  │
│  │Prometheus│  │  Jaeger  │  │   Loki   │  │  Other   │  │
│  │  (指标)  │  │  (追踪)  │  │  (日志)  │  │          │  │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘  │
└───────┼─────────────┼─────────────┼─────────────┼─────────┘
        │             │             │             │
        ▼             ▼             ▼             ▼
┌─────────────────────────────────────────────────────────────┐
│                    可视化与告警层                            │
│  ┌─────────────────────────────────────────────────────┐  │
│  │                    Grafana                          │  │
│  │  - 统一可视化                                        │  │
│  │  - 多数据源                                          │  │
│  │  │  - Dashboard                                     │  │
│  └─────────────────────────────────────────────────────┘  │
│  ┌─────────────────────────────────────────────────────┐  │
│  │                 Alertmanager                        │  │
│  │  - 告警聚合                                          │  │
│  │  - 通知路由                                          │  │
│  │  - 抑制与静默                                        │  │
│  └─────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘

数据流:
  1. Sidecar采集指标、追踪、日志
  2. 数据发送到对应存储（Prometheus/Jaeger/Loki）
  3. Grafana统一查询和可视化
  4. Alertmanager处理告警和通知

关键组件:
  - Envoy/Linkerd Proxy: 数据采集
  - Prometheus: 指标存储
  - Jaeger: 追踪存储
  - Loki: 日志聚合
  - Grafana: 统一可视化
  - Alertmanager: 告警管理
```

---

## 2. Prometheus监控

### 2.1 Prometheus架构

```yaml
Prometheus架构:

核心组件:
  1. Prometheus Server:
     - 数据采集（Pull模式）
     - 时间序列数据库
     - PromQL查询引擎
  
  2. Exporters:
     - 暴露指标的HTTP端点
     - /metrics
  
  3. Pushgateway:
     - 短期任务指标推送
     - Push模式支持
  
  4. Alertmanager:
     - 告警处理
     - 通知路由
  
  5. Service Discovery:
     - Kubernetes SD
     - Consul SD
     - 静态配置

数据模型:
  <metric name>{<label name>=<label value>, ...} value timestamp
  
  示例:
  istio_requests_total{
    destination_service="my-service",
    destination_version="v1",
    response_code="200"
  } 1234 1634567890

特点:
  ✅ 多维数据模型
  ✅ 强大的查询语言（PromQL）
  ✅ 高效存储
  ✅ 拉取模式（Pull）
  ✅ 服务发现
```

### 2.2 Istio Prometheus集成

#### 2.2.1 部署Prometheus

```yaml
# 使用Istio addon部署Prometheus
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.21/samples/addons/prometheus.yaml

# 访问Prometheus
kubectl port-forward -n istio-system svc/prometheus 9090:9090

# 浏览器访问: http://localhost:9090
```

#### 2.2.2 Istio暴露的指标

```yaml
Istio核心指标:

1. 请求指标:
   istio_requests_total: 请求总数
   istio_request_duration_milliseconds: 请求延迟
   istio_request_bytes: 请求大小
   istio_response_bytes: 响应大小

2. TCP指标:
   istio_tcp_connections_opened_total: TCP连接打开
   istio_tcp_connections_closed_total: TCP连接关闭
   istio_tcp_sent_bytes_total: 发送字节数
   istio_tcp_received_bytes_total: 接收字节数

3. Pilot指标:
   pilot_xds_pushes: 配置推送次数
   pilot_proxy_convergence_time: 配置收敛时间

4. Galley指标:
   galley_validation_passed: 验证通过数
   galley_validation_failed: 验证失败数

标签维度:
  - source_workload: 源工作负载
  - destination_workload: 目标工作负载
  - source_version: 源版本
  - destination_version: 目标版本
  - response_code: 响应码
  - connection_security_policy: mTLS状态
```

### 2.3 Linkerd Prometheus集成

```yaml
# Linkerd内置Prometheus

# 安装Linkerd Viz（包含Prometheus）
linkerd viz install | kubectl apply -f -

# 访问Prometheus
linkerd viz dashboard

# 或直接访问
kubectl port-forward -n linkerd-viz svc/prometheus 9090:9090
```

```yaml
Linkerd核心指标:

1. 黄金指标:
   request_total: 请求总数
   response_total: 响应总数
   response_latency_ms: 响应延迟
   
2. 成功率:
   success_rate = 
     sum(response_total{classification="success"}) 
     / 
     sum(response_total)

3. 流量指标:
   inbound_http_requests_total
   outbound_http_requests_total

4. TCP指标:
   tcp_open_connections
   tcp_read_bytes_total
   tcp_write_bytes_total

标签维度:
  - namespace
  - deployment
  - pod
  - direction (inbound/outbound)
  - tls (true/false)
```

### 2.4 关键指标

```yaml
生产环境关键指标:

1. 四大黄金信号 (Google SRE):
   
   a. Latency (延迟):
      P50延迟: 中位数
      P95延迟: 95%请求
      P99延迟: 99%请求
      
   b. Traffic (流量):
      QPS (Queries Per Second)
      RPS (Requests Per Second)
      
   c. Errors (错误):
      错误率 = 错误请求数 / 总请求数
      5xx错误率
      4xx错误率
      
   d. Saturation (饱和度):
      CPU使用率
      内存使用率
      连接数

2. RED指标 (Rate, Errors, Duration):
   Rate: 请求速率
   Errors: 错误率
   Duration: 延迟分布

3. USE指标 (Utilization, Saturation, Errors):
   Utilization: 资源使用率
   Saturation: 资源饱和度
   Errors: 错误数

4. 服务网格特定指标:
   mTLS覆盖率
   配置推送延迟
   Sidecar资源消耗
   连接池状态
```

### 2.5 PromQL查询

```promql
# 1. 成功率查询
sum(rate(istio_requests_total{response_code=~"2.*"}[5m])) 
/ 
sum(rate(istio_requests_total[5m])) * 100

# 2. P99延迟查询
histogram_quantile(0.99, 
  sum(rate(istio_request_duration_milliseconds_bucket[5m])) 
  by (le, destination_service)
)

# 3. QPS查询
sum(rate(istio_requests_total[1m])) by (destination_service)

# 4. 错误率查询
sum(rate(istio_requests_total{response_code=~"5.*"}[5m])) 
/ 
sum(rate(istio_requests_total[5m])) * 100

# 5. 按版本对比延迟
histogram_quantile(0.95,
  sum(rate(istio_request_duration_milliseconds_bucket[5m])) 
  by (le, destination_version)
)

# 6. mTLS使用率
sum(istio_requests_total{connection_security_policy="mutual_tls"}) 
/ 
sum(istio_requests_total) * 100

# 7. Top 5慢服务
topk(5, 
  histogram_quantile(0.99,
    sum(rate(istio_request_duration_milliseconds_bucket[5m])) 
    by (le, destination_service)
  )
)

# 8. 服务间流量
sum(rate(istio_requests_total[1m])) 
by (source_workload, destination_workload)

# 9. CPU使用率
sum(rate(container_cpu_usage_seconds_total[5m])) 
by (pod) * 100

# 10. 内存使用
container_memory_working_set_bytes / 1024 / 1024
```

---

## 3. Jaeger分布式追踪

### 3.1 分布式追踪原理

```yaml
分布式追踪原理:

核心概念:

1. Trace (跟踪):
   - 完整的请求链路
   - 唯一Trace ID
   - 包含多个Span

2. Span (跨度):
   - 单个操作或服务调用
   - 唯一Span ID
   - 父Span ID（Parent ID）
   - 开始时间
   - 持续时间
   - Tags（标签）
   - Logs（日志事件）

3. Context Propagation (上下文传播):
   - HTTP Headers传递Trace信息
   - 标准Headers:
     - x-request-id
     - x-b3-traceid
     - x-b3-spanid
     - x-b3-parentspanid
     - x-b3-sampled
     - x-b3-flags

示例Trace:

User Request
  │
  └─> Gateway (Span 1, 100ms)
        │
        ├─> Service A (Span 2, 50ms)
        │     │
        │     └─> Database (Span 3, 30ms)
        │
        └─> Service B (Span 4, 40ms)
              │
              └─> Cache (Span 5, 10ms)

Trace ID: abc123
Spans:
  - Span 1: Gateway, 100ms, parent=null
  - Span 2: Service A, 50ms, parent=Span 1
  - Span 3: Database, 30ms, parent=Span 2
  - Span 4: Service B, 40ms, parent=Span 1
  - Span 5: Cache, 10ms, parent=Span 4

采样策略:
  - Always: 100%采样
  - Never: 0%采样
  - Probabilistic: 概率采样（1%, 10%）
  - Rate Limiting: 限速采样（100 traces/s）
```

### 3.2 Istio Jaeger集成

```yaml
# 部署Jaeger
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.21/samples/addons/jaeger.yaml

# 配置Istio启用追踪
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  meshConfig:
    enableTracing: true
    defaultConfig:
      tracing:
        sampling: 100.0  # 100%采样（生产环境建议1-10%）
        zipkin:
          address: jaeger-collector.istio-system.svc:9411

# 访问Jaeger UI
kubectl port-forward -n istio-system svc/jaeger-query 16686:16686

# 浏览器访问: http://localhost:16686
```

```yaml
# 应用需要传播Trace Headers

# Python Flask示例
from flask import Flask, request
import requests

app = Flask(__name__)

# 需要传播的Headers
TRACE_HEADERS = [
    'x-request-id',
    'x-b3-traceid',
    'x-b3-spanid',
    'x-b3-parentspanid',
    'x-b3-sampled',
    'x-b3-flags',
    'x-ot-span-context'
]

@app.route('/api/users')
def get_users():
    # 提取Trace Headers
    headers = {}
    for header in TRACE_HEADERS:
        if header in request.headers:
            headers[header] = request.headers[header]
    
    # 调用下游服务，传递Headers
    response = requests.get(
        'http://user-service/users',
        headers=headers
    )
    
    return response.json()
```

### 3.3 追踪分析

```yaml
Jaeger追踪分析:

1. 查看Trace:
   - 选择Service
   - 设置时间范围
   - 筛选Tags
   - 查看Trace列表

2. Trace详情:
   - 完整调用链
   - 每个Span的耗时
   - 时间瀑布图
   - Tags和Logs

3. 性能分析:
   - 识别慢Span
   - 找出瓶颈服务
   - 分析串行/并行调用
   - 优化机会

4. 依赖关系:
   - Service Graph
   - 服务依赖图
   - 调用频率
   - 错误率

5. 异常追踪:
   - 错误Span标记
   - 错误堆栈
   - 错误传播路径
```

### 3.4 性能优化

```yaml
追踪性能优化:

1. 采样率调整:
   生产环境建议:
     - 高流量服务: 1-5%
     - 中流量服务: 10%
     - 低流量服务: 100%
   
   原因:
     - 降低存储压力
     - 减少网络开销
     - 保持性能

2. 自适应采样:
   apiVersion: install.istio.io/v1alpha1
   kind: IstioOperator
   spec:
     meshConfig:
       defaultConfig:
         tracing:
           sampling: 1.0  # 基础采样率1%
           custom_tags:
             user_type:
               header:
                 name: x-user-type
           # 重要用户100%采样
           # 普通用户1%采样

3. 异步上报:
   - 使用异步客户端
   - 批量上报
   - 本地缓冲

4. 存储优化:
   - 设置TTL（Time To Live）
   - 定期清理旧数据
   - 使用Elasticsearch集群
```

---

## 4. Grafana可视化

### 4.1 Grafana Dashboard

```yaml
# 部署Grafana
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.21/samples/addons/grafana.yaml

# 访问Grafana
kubectl port-forward -n istio-system svc/grafana 3000:3000

# 浏览器访问: http://localhost:3000
# 默认用户名/密码: admin/admin
```

```yaml
Grafana核心功能:

1. 数据源管理:
   - Prometheus
   - Jaeger
   - Loki
   - Elasticsearch
   - InfluxDB

2. Dashboard:
   - 预置Dashboard
   - 自定义Dashboard
   - 变量支持
   - 模板化

3. 告警:
   - 可视化告警规则
   - 多种通知渠道
   - 告警历史

4. 面板类型:
   - Graph (图表)
   - Stat (统计)
   - Gauge (仪表盘)
   - Table (表格)
   - Heatmap (热图)
   - Logs (日志)
```

### 4.2 Istio Dashboard

```yaml
Istio预置Dashboard:

1. Istio Mesh Dashboard:
   - 全局流量概览
   - 成功率
   - QPS
   - P50/P95/P99延迟

2. Istio Service Dashboard:
   - 单服务详情
   - 入站/出站流量
   - 客户端/服务端指标
   - 版本对比

3. Istio Workload Dashboard:
   - 工作负载详情
   - CPU/内存使用
   - 网络流量
   - 请求指标

4. Istio Performance Dashboard:
   - 控制平面性能
   - Pilot指标
   - 配置推送延迟
   - Envoy性能

5. Istio Wasm Extension Dashboard:
   - WebAssembly扩展指标
   - 执行时间
   - 内存使用
```

### 4.3 Linkerd Dashboard

```yaml
# Linkerd Dashboard（内置Grafana）
linkerd viz dashboard

# 或直接访问Grafana
kubectl port-forward -n linkerd-viz svc/grafana 3000:3000
```

```yaml
Linkerd预置Dashboard:

1. Linkerd Top Line:
   - 全局成功率
   - 全局RPS
   - 全局P95/P99延迟

2. Linkerd Deployment:
   - 部署级别指标
   - 成功率趋势
   - 流量趋势
   - 延迟趋势

3. Linkerd Pod:
   - Pod级别指标
   - 资源使用
   - 网络流量
   - TCP指标

4. Linkerd Authority:
   - 服务间流量
   - 路由指标
   - mTLS状态

5. Linkerd Namespace:
   - 命名空间级别汇总
   - 成功率
   - 流量分布
```

### 4.4 自定义Dashboard

```json
// Grafana Dashboard JSON示例
{
  "dashboard": {
    "title": "服务网格自定义监控",
    "panels": [
      {
        "id": 1,
        "title": "服务成功率",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(istio_requests_total{response_code=~\"2.*\"}[5m])) / sum(rate(istio_requests_total[5m])) * 100"
          }
        ]
      },
      {
        "id": 2,
        "title": "P99延迟",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket[5m])) by (le, destination_service))"
          }
        ]
      },
      {
        "id": 3,
        "title": "QPS",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(rate(istio_requests_total[1m]))"
          }
        ]
      },
      {
        "id": 4,
        "title": "错误率",
        "type": "gauge",
        "targets": [
          {
            "expr": "sum(rate(istio_requests_total{response_code=~\"5.*\"}[5m])) / sum(rate(istio_requests_total[5m])) * 100"
          }
        ],
        "thresholds": [
          {"value": 0, "color": "green"},
          {"value": 1, "color": "yellow"},
          {"value": 5, "color": "red"}
        ]
      }
    ]
  }
}
```

```yaml
# Dashboard最佳实践

1. 层次化监控:
   - L1: 全局概览（Mesh级别）
   - L2: 服务级别
   - L3: 工作负载级别
   - L4: Pod级别

2. 关键指标优先:
   - 成功率（最重要）
   - 延迟（P95/P99）
   - QPS/RPS
   - 错误率

3. 使用变量:
   - Namespace变量
   - Service变量
   - Version变量
   - 时间范围变量

4. 告警可视化:
   - 阈值线
   - 颜色编码
   - 告警状态面板

5. 链接关联:
   - Dashboard间跳转
   - Jaeger追踪链接
   - 日志链接
```

---

## 5. Loki日志聚合

### 5.1 Loki架构

```yaml
Loki架构:

核心组件:

1. Promtail (日志采集器):
   - 发现Pod
   - 采集日志
   - 标签提取
   - 推送到Loki

2. Loki (日志存储):
   - 日志索引（仅索引标签）
   - 日志存储
   - 查询API

3. Grafana (查询界面):
   - LogQL查询
   - 日志浏览
   - 与Metrics关联

特点:
  ✅ 只索引标签（降低成本）
  ✅ 使用与Prometheus相同的标签
  ✅ LogQL查询语言
  ✅ 与Grafana深度集成
  ✅ 支持分布式部署
```

### 5.2 日志收集

```yaml
# 部署Loki Stack
kubectl apply -f https://raw.githubusercontent.com/grafana/loki/main/production/ksonnet/loki-stack/loki-stack.yaml

# 或使用Helm
helm repo add grafana https://grafana.github.io/helm-charts
helm install loki grafana/loki-stack \
  --set promtail.enabled=true \
  --set grafana.enabled=true
```

```yaml
# Promtail配置示例
apiVersion: v1
kind: ConfigMap
metadata:
  name: promtail-config
  namespace: loki
data:
  promtail.yaml: |
    server:
      http_listen_port: 9080
      grpc_listen_port: 0

    positions:
      filename: /tmp/positions.yaml

    clients:
      - url: http://loki:3100/loki/api/v1/push

    scrape_configs:
      # Kubernetes Pod日志
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
          - role: pod
        
        relabel_configs:
          # 提取Pod标签
          - source_labels: [__meta_kubernetes_pod_label_app]
            target_label: app
          - source_labels: [__meta_kubernetes_pod_label_version]
            target_label: version
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          
          # 文件路径
          - source_labels: [__meta_kubernetes_pod_uid, __meta_kubernetes_pod_container_name]
            target_label: __path__
            separator: /
            replacement: /var/log/pods/*$1/*.log

      # Envoy访问日志
      - job_name: envoy-access-log
        kubernetes_sd_configs:
          - role: pod
        
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_container_name]
            action: keep
            regex: istio-proxy
          
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
```

### 5.3 日志查询

```logql
# LogQL查询示例

# 1. 查看特定服务的日志
{app="my-service"}

# 2. 查看错误日志
{app="my-service"} |= "error"

# 3. 查看HTTP 500错误
{app="my-service"} | json | status_code="500"

# 4. 正则过滤
{app="my-service"} |~ "ERROR|WARN"

# 5. 排除特定日志
{app="my-service"} != "health check"

# 6. 聚合查询（QPS）
rate({app="my-service"}[1m])

# 7. 错误率
sum(rate({app="my-service"} |= "error"[5m])) 
/ 
sum(rate({app="my-service"}[5m]))

# 8. Top 10错误
topk(10, 
  sum by (error_message) (
    count_over_time({app="my-service"} |= "error" [1h])
  )
)

# 9. 延迟统计
quantile_over_time(0.99, 
  {app="my-service"} | json | unwrap latency [5m]
)

# 10. 多标签查询
{namespace="prod", app="my-service", version="v2"}
```

### 5.4 日志关联

```yaml
日志与Metrics/Traces关联:

1. Trace ID关联:
   # 日志中包含Trace ID
   {app="my-service"} | json | trace_id="abc123"
   
   # 在Grafana中:
   - Metrics发现异常
   - 点击Trace查看调用链
   - 点击Span查看对应日志
   - 一键跳转

2. 时间关联:
   # 在Grafana中选择时间范围
   # 自动同步Metrics、Traces、Logs
   
3. 标签关联:
   # 使用相同的标签
   - namespace
   - app
   - version
   - pod
   
   # Prometheus标签
   istio_requests_total{destination_service="my-service"}
   
   # Loki标签
   {app="my-service"}
   
   # 一致性保证关联

4. 告警关联:
   # 告警触发时
   - 显示Metrics图表
   - 显示相关Traces
   - 显示错误日志
   - 一键故障排查
```

---

## 6. OpenTelemetry标准

### 6.1 OpenTelemetry概述

```yaml
OpenTelemetry (OTel):

定义:
  - CNCF项目
  - 统一的可观测性标准
  - 合并OpenTracing和OpenCensus
  - 供应商中立

核心组件:

1. API/SDK:
   - 应用程序集成
   - 多语言支持（Java、Python、Go、Node.js...）
   - Metrics、Traces、Logs

2. Collector:
   - 数据接收
   - 数据处理
   - 数据导出
   - 多协议支持

3. Protocol (OTLP):
   - 统一协议
   - gRPC/HTTP
   - 高效传输

优势:
  ✅ 统一标准
  ✅ 供应商中立
  ✅ 自动化Instrumentation
  ✅ 多后端支持
  ✅ 社区活跃
```

### 6.2 服务网格集成

```yaml
# 部署OpenTelemetry Collector
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: istio-system
data:
  config.yaml: |
    receivers:
      # Zipkin协议（Istio默认）
      zipkin:
        endpoint: 0.0.0.0:9411
      
      # OTLP协议
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

    processors:
      # 批处理
      batch:
        timeout: 10s
        send_batch_size: 1024
      
      # 采样
      probabilistic_sampler:
        sampling_percentage: 10

    exporters:
      # 导出到Jaeger
      jaeger:
        endpoint: jaeger-collector:14250
        tls:
          insecure: true
      
      # 导出到Prometheus
      prometheus:
        endpoint: 0.0.0.0:8889
      
      # 导出到Loki
      loki:
        endpoint: http://loki:3100/loki/api/v1/push

    service:
      pipelines:
        traces:
          receivers: [zipkin, otlp]
          processors: [batch, probabilistic_sampler]
          exporters: [jaeger]
        
        metrics:
          receivers: [otlp]
          processors: [batch]
          exporters: [prometheus]

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: istio-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector:0.91.0
        args:
          - "--config=/etc/otel-collector-config.yaml"
        volumeMounts:
        - name: config
          mountPath: /etc/otel-collector-config.yaml
          subPath: config.yaml
        ports:
        - containerPort: 9411  # Zipkin
        - containerPort: 4317  # OTLP gRPC
        - containerPort: 4318  # OTLP HTTP
        - containerPort: 8889  # Prometheus
      volumes:
      - name: config
        configMap:
          name: otel-collector-config
```

```yaml
# Istio配置使用OpenTelemetry Collector
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  meshConfig:
    enableTracing: true
    defaultConfig:
      tracing:
        sampling: 100.0
        zipkin:
          address: otel-collector.istio-system.svc:9411
```

### 6.3 统一可观测性

```yaml
OpenTelemetry统一可观测性:

优势:

1. 统一采集:
   ✅ Metrics、Traces、Logs统一API
   ✅ 自动关联
   ✅ 一致的标签

2. 灵活导出:
   ✅ 多后端支持
   - Prometheus
   - Jaeger
   - Zipkin
   - Elasticsearch
   - Datadog
   - New Relic
   - 云厂商（AWS X-Ray、Google Cloud Trace）

3. 供应商中立:
   ✅ 避免供应商锁定
   ✅ 自由切换后端
   ✅ 多后端并行

4. 自动化Instrumentation:
   ✅ 无需修改代码
   ✅ 自动注入
   ✅ 多语言支持

架构:

Application (auto-instrumented)
    │
    │ OTLP
    ▼
OpenTelemetry Collector
    │
    ├─> Jaeger (Traces)
    ├─> Prometheus (Metrics)
    ├─> Loki (Logs)
    ├─> Elasticsearch (All)
    └─> Cloud Providers
```

---

## 7. SLI/SLO/SLA

### 7.1 概念与区别

```yaml
SLI/SLO/SLA概念:

1. SLI (Service Level Indicator) - 服务级别指标:
   定义: 量化服务质量的指标
   
   示例:
     - 请求成功率
     - P95延迟
     - 可用性
     - 吞吐量
   
   表达:
     成功率 SLI = 成功请求数 / 总请求数

2. SLO (Service Level Objective) - 服务级别目标:
   定义: SLI的目标值
   
   示例:
     - 成功率 ≥ 99.9%
     - P95延迟 ≤ 500ms
     - 可用性 ≥ 99.95%
   
   表达:
     "过去30天，成功率≥99.9%"

3. SLA (Service Level Agreement) - 服务级别协议:
   定义: 与客户的正式合同
   
   包含:
     - SLO目标
     - 测量方法
     - 违反后果（赔偿）
   
   示例:
     "月度可用性≥99.9%，否则赔偿10%费用"

关系:
  SLI ⊂ SLO ⊂ SLA
  
  SLI: 测量什么
  SLO: 目标是多少
  SLA: 违反了怎么办
```

### 7.2 定义SLI

```yaml
常见SLI定义:

1. 可用性 SLI:
   成功率 = 成功请求数 / 总请求数
   
   PromQL:
   sum(rate(istio_requests_total{response_code=~"2.*|3.*"}[1h])) 
   / 
   sum(rate(istio_requests_total[1h]))

2. 延迟 SLI:
   P95延迟 < 500ms
   
   PromQL:
   histogram_quantile(0.95,
     sum(rate(istio_request_duration_milliseconds_bucket[5m])) 
     by (le)
   ) < 500

3. 吞吐量 SLI:
   QPS ≥ 1000
   
   PromQL:
   sum(rate(istio_requests_total[1m])) >= 1000

4. 错误率 SLI:
   错误率 < 0.1%
   
   PromQL:
   sum(rate(istio_requests_total{response_code=~"5.*"}[5m])) 
   / 
   sum(rate(istio_requests_total[5m])) < 0.001
```

### 7.3 设置SLO

```yaml
SLO设置示例:

服务: 支付服务
SLI: 成功率
SLO: 99.9% (30天)

计算:
  30天 = 43,200分钟
  允许故障时间 = 43,200 * (1 - 0.999) = 43.2分钟

Prometheus Recording Rule:
```

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-slo-rules
  namespace: istio-system
data:
  slo-rules.yaml: |
    groups:
      - name: slo
        interval: 30s
        rules:
          # 成功率 SLI
          - record: sli:request_success_rate:ratio
            expr: |
              sum(rate(istio_requests_total{response_code=~"2.*"}[5m]))
              /
              sum(rate(istio_requests_total[5m]))
          
          # P95延迟 SLI
          - record: sli:request_latency_p95:milliseconds
            expr: |
              histogram_quantile(0.95,
                sum(rate(istio_request_duration_milliseconds_bucket[5m])) 
                by (le)
              )
          
          # SLO告警（成功率<99.9%）
          - alert: SLOViolation_SuccessRate
            expr: sli:request_success_rate:ratio < 0.999
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "成功率低于SLO"
              description: "当前成功率: {{ $value | humanizePercentage }}"
          
          # SLO告警（P95延迟>500ms）
          - alert: SLOViolation_Latency
            expr: sli:request_latency_p95:milliseconds > 500
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "P95延迟超过SLO"
              description: "当前P95延迟: {{ $value }}ms"
```

### 7.4 错误预算

```yaml
错误预算 (Error Budget):

定义:
  允许的故障时间或失败请求数

计算:
  错误预算 = 1 - SLO
  
  SLO = 99.9%
  错误预算 = 0.1% = 1 - 99.9%

示例:
  30天SLO = 99.9%
  总时间 = 30 * 24 * 60 = 43,200分钟
  错误预算 = 43,200 * 0.001 = 43.2分钟

用途:

1. 发布决策:
   错误预算充足 → 可以发布新功能
   错误预算耗尽 → 暂停发布，修复稳定性

2. 平衡创新与稳定:
   ✅ 100% SLO = 没有创新空间
   ✅ 99.9% SLO = 0.1%错误预算可用于实验

3. 监控错误预算消耗:
```

```promql
# 错误预算消耗率
(1 - sli:request_success_rate:ratio) / 0.001

# 错误预算剩余（基于30天）
1 - (
  sum(increase(istio_requests_total{response_code!~"2.*"}[30d]))
  /
  sum(increase(istio_requests_total[30d]))
) / 0.001

# 错误预算剩余天数
(
  1 - (
    sum(increase(istio_requests_total{response_code!~"2.*"}[30d]))
    /
    sum(increase(istio_requests_total[30d]))
  ) / 0.001
) * 30
```

```yaml
错误预算策略:

1. 绿色（>75%剩余）:
   ✅ 正常发布
   ✅ 可以实验新功能
   ✅ 性能优化

2. 黄色（25%-75%剩余）:
   ⚠️ 谨慎发布
   ⚠️ 增加测试
   ⚠️ 关注监控

3. 红色（<25%剩余）:
   🚫 暂停发布
   🚫 只修复Bug
   🚫 回滚可疑更改
   🚫 专注稳定性
```

---

## 8. 告警配置

### 8.1 Alertmanager配置

```yaml
# Alertmanager ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: istio-system
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      
      # Slack配置
      slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'

    # 路由规则
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s        # 等待10秒聚合告警
      group_interval: 10s    # 聚合后告警间隔
      repeat_interval: 12h   # 重复告警间隔
      
      receiver: 'default'
      
      routes:
        # 严重告警 → Slack + PagerDuty
        - match:
            severity: critical
          receiver: 'critical-alerts'
          continue: true
        
        # 警告 → Slack
        - match:
            severity: warning
          receiver: 'warning-alerts'
        
        # 信息 → 邮件
        - match:
            severity: info
          receiver: 'info-alerts'

    # 抑制规则
    inhibit_rules:
      # 如果有critical告警，抑制warning告警
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        equal: ['alertname', 'service']

    receivers:
      - name: 'default'
        slack_configs:
          - channel: '#alerts'
            title: '{{ .GroupLabels.alertname }}'
            text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}\n{{ end }}'

      - name: 'critical-alerts'
        slack_configs:
          - channel: '#critical-alerts'
            title: '🚨 Critical Alert'
            text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}\n{{ end }}'
        pagerduty_configs:
          - service_key: 'YOUR_PAGERDUTY_KEY'

      - name: 'warning-alerts'
        slack_configs:
          - channel: '#warnings'
            title: '⚠️ Warning Alert'
            text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}\n{{ end }}'

      - name: 'info-alerts'
        email_configs:
          - to: 'team@example.com'
            from: 'alertmanager@example.com'
            smarthost: 'smtp.gmail.com:587'
            auth_username: 'alertmanager@example.com'
            auth_password: 'your-password'
```

### 8.2 告警规则

```yaml
# Prometheus告警规则
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alerts
  namespace: istio-system
data:
  alerts.yaml: |
    groups:
      - name: service-mesh-alerts
        rules:
          # 1. 成功率低
          - alert: HighErrorRate
            expr: |
              (
                sum(rate(istio_requests_total{response_code=~"5.*"}[5m])) 
                / 
                sum(rate(istio_requests_total[5m]))
              ) > 0.05
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "服务错误率过高"
              description: "{{ $labels.destination_service }} 错误率: {{ $value | humanizePercentage }}"

          # 2. 延迟过高
          - alert: HighLatency
            expr: |
              histogram_quantile(0.99,
                sum(rate(istio_request_duration_milliseconds_bucket[5m])) 
                by (le, destination_service)
              ) > 1000
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "服务延迟过高"
              description: "{{ $labels.destination_service }} P99延迟: {{ $value }}ms"

          # 3. QPS异常下降
          - alert: QPSDropped
            expr: |
              (
                sum(rate(istio_requests_total[5m])) 
                / 
                sum(rate(istio_requests_total[5m] offset 1h))
              ) < 0.5
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "QPS异常下降"
              description: "QPS下降超过50%"

          # 4. Pilot配置推送延迟
          - alert: PilotPushLatencyHigh
            expr: |
              histogram_quantile(0.99, 
                sum(rate(pilot_proxy_convergence_time_bucket[5m])) by (le)
              ) > 10000
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Pilot配置推送延迟过高"
              description: "P99配置推送延迟: {{ $value }}ms"

          # 5. mTLS覆盖率低
          - alert: LowMTLSCoverage
            expr: |
              (
                sum(istio_requests_total{connection_security_policy="mutual_tls"})
                /
                sum(istio_requests_total)
              ) < 0.95
            for: 15m
            labels:
              severity: info
            annotations:
              summary: "mTLS覆盖率低于95%"
              description: "当前mTLS覆盖率: {{ $value | humanizePercentage }}"

          # 6. Pod重启频繁
          - alert: PodRestartingFrequently
            expr: |
              rate(kube_pod_container_status_restarts_total[15m]) > 0.1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Pod频繁重启"
              description: "{{ $labels.pod }} 重启率: {{ $value }}/分钟"

          # 7. Sidecar资源消耗过高
          - alert: SidecarHighMemory
            expr: |
              container_memory_working_set_bytes{container="istio-proxy"} 
              / 
              1024 / 1024 > 500
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Sidecar内存使用过高"
              description: "{{ $labels.pod }} Sidecar内存: {{ $value }}MB"
```

### 8.3 通知集成

```yaml
告警通知渠道:

1. Slack:
   slack_configs:
     - api_url: 'https://hooks.slack.com/services/...'
       channel: '#alerts'
       title: '{{ .GroupLabels.alertname }}'
       text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

2. Email:
   email_configs:
     - to: 'team@example.com'
       from: 'alertmanager@example.com'
       smarthost: 'smtp.gmail.com:587'
       auth_username: 'user'
       auth_password: 'password'
       headers:
         Subject: 'Alert: {{ .GroupLabels.alertname }}'

3. PagerDuty:
   pagerduty_configs:
     - service_key: 'YOUR_SERVICE_KEY'
       description: '{{ .GroupLabels.alertname }}'

4. Webhook:
   webhook_configs:
     - url: 'http://your-webhook-service/alert'
       send_resolved: true

5. 企业微信:
   wechat_configs:
     - corp_id: 'YOUR_CORP_ID'
       agent_id: 'YOUR_AGENT_ID'
       to_user: '@all'
       api_secret: 'YOUR_API_SECRET'
```

### 8.4 告警最佳实践

```yaml
告警最佳实践:

1. 可操作性:
   ❌ 差: "服务有问题"
   ✅ 好: "订单服务P99延迟1200ms，超过SLO 500ms，检查数据库连接池"

2. 避免告警疲劳:
   - 合理设置阈值
   - 聚合相关告警
   - 设置抑制规则
   - 定期review告警

3. 分级告警:
   Critical: 需要立即行动（页面级告警）
   Warning: 需要关注（Slack）
   Info: 仅记录（邮件）

4. 告警上下文:
   ✅ 当前值
   ✅ SLO目标
   ✅ Runbook链接
   ✅ Dashboard链接
   ✅ 相关服务

5. 自愈机制:
   - 自动扩容
   - 自动重启
   - 自动降级
   - 告警后确认效果
```

---

## 9. 可观测性实战

### 9.1 完整可观测性栈

```bash
#!/bin/bash
# 一键部署完整可观测性栈

# 1. 部署Prometheus
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.21/samples/addons/prometheus.yaml

# 2. 部署Jaeger
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.21/samples/addons/jaeger.yaml

# 3. 部署Grafana
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.21/samples/addons/grafana.yaml

# 4. 部署Kiali（可选）
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.21/samples/addons/kiali.yaml

# 5. 部署Loki Stack
helm repo add grafana https://grafana.github.io/helm-charts
helm install loki grafana/loki-stack \
  --namespace loki --create-namespace \
  --set promtail.enabled=true \
  --set grafana.enabled=false

# 6. 配置Grafana数据源
kubectl apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: istio-system
data:
  datasources.yaml: |
    apiVersion: 1
    datasources:
      - name: Prometheus
        type: prometheus
        access: proxy
        url: http://prometheus:9090
        isDefault: true
      
      - name: Jaeger
        type: jaeger
        access: proxy
        url: http://jaeger-query:16686
      
      - name: Loki
        type: loki
        access: proxy
        url: http://loki.loki:3100
EOF

# 7. 等待所有Pod就绪
kubectl wait --for=condition=ready pod -l app=prometheus -n istio-system --timeout=300s
kubectl wait --for=condition=ready pod -l app=jaeger -n istio-system --timeout=300s
kubectl wait --for=condition=ready pod -l app=grafana -n istio-system --timeout=300s

# 8. 访问服务
echo "Prometheus: http://localhost:9090"
kubectl port-forward -n istio-system svc/prometheus 9090:9090 &

echo "Jaeger: http://localhost:16686"
kubectl port-forward -n istio-system svc/jaeger-query 16686:16686 &

echo "Grafana: http://localhost:3000 (admin/admin)"
kubectl port-forward -n istio-system svc/grafana 3000:3000 &

echo "可观测性栈部署完成！"
```

### 9.2 故障排查案例

```yaml
案例1: 服务延迟突增

步骤1: Metrics发现问题
  - Grafana Dashboard显示P99延迟从100ms突增到2000ms
  - 时间点: 14:32

步骤2: 确定问题范围
  PromQL:
  histogram_quantile(0.99,
    sum(rate(istio_request_duration_milliseconds_bucket[5m])) 
    by (le, destination_service)
  )
  
  结果: order-service P99延迟异常

步骤3: Jaeger追踪分析
  - 选择order-service
  - 时间范围: 14:30-14:35
  - 排序: Duration (降序)
  
  发现:
    - order-service → database-service耗时1800ms
    - 正常情况应该<50ms

步骤4: 查看日志
  LogQL:
  {app="database-service"} |= "error" | json
  
  发现:
    - 14:32开始大量"connection timeout"错误
    - 数据库连接池耗尽

步骤5: 根因分析
  PromQL:
  sum(rate(istio_requests_total{destination_service="database-service"}[1m]))
  
  发现:
    - QPS从100突增到1000
    - 触发原因: 营销活动开始

步骤6: 解决方案
  - 立即扩容database-service: 3 → 10副本
  - 增加数据库连接池大小
  - 启用熔断保护

步骤7: 验证
  - 5分钟后P99延迟恢复到150ms
  - 错误率从5%降到0.1%
  - 告警解除

总耗时: 15分钟
```

```yaml
案例2: 间歇性503错误

步骤1: 告警触发
  - HighErrorRate告警
  - payment-service 503错误率2%
  - 间歇性发生

步骤2: Metrics分析
  PromQL:
  sum(rate(istio_requests_total{
    destination_service="payment-service",
    response_code="503"
  }[1m])) by (destination_version)
  
  发现:
    - 仅v2版本有503
    - v1版本正常

步骤3: Trace分析
  - 筛选response_code=503
  - 查看v2版本Traces
  
  发现:
    - 503发生在payment-service → redis-service调用
    - 错误: "connection refused"

步骤4: 日志查看
  LogQL:
  {app="payment-service", version="v2"} |= "redis"
  
  发现:
    - v2版本Redis地址配置错误
    - redis-service.default → redis-service.prod

步骤5: 解决
  - 修复ConfigMap中Redis地址
  - 滚动重启v2版本

步骤6: 验证
  - 503错误率降为0
  - v2版本成功率恢复99.9%

根因: 配置错误
总耗时: 10分钟
```

### 9.3 性能分析案例

```yaml
案例3: 资源优化

目标: 降低Sidecar资源消耗

步骤1: 当前状态评估
  PromQL:
  # CPU使用
  sum(rate(container_cpu_usage_seconds_total{container="istio-proxy"}[5m])) 
  by (pod)
  
  # 内存使用
  container_memory_working_set_bytes{container="istio-proxy"} / 1024 / 1024
  
  发现:
    - 每个Sidecar CPU: 200m
    - 每个Sidecar内存: 250MB
    - 集群100个Pod, 总消耗: 20 CPU, 25GB内存

步骤2: 追踪采样率优化
  当前: 100%采样
  优化: 1%采样（高流量服务）
  
  配置:
  apiVersion: install.istio.io/v1alpha1
  kind: IstioOperator
  spec:
    meshConfig:
      defaultConfig:
        tracing:
          sampling: 1.0  # 1%

步骤3: 访问日志优化
  关闭详细访问日志，仅保留错误日志
  
  配置:
  apiVersion: telemetry.istio.io/v1alpha1
  kind: Telemetry
  metadata:
    name: mesh-logging
  spec:
    accessLogging:
      - providers:
        - name: envoy
        filter:
          expression: response.code >= 400

步骤4: Metrics过滤
  仅采集关键指标
  
  配置:
  apiVersion: telemetry.istio.io/v1alpha1
  kind: Telemetry
  metadata:
    name: mesh-metrics
  spec:
    metrics:
      - providers:
        - name: prometheus
        dimensions:
          request_protocol: request.protocol
          response_code: response.code
        tags_to_remove:
          - request_host
          - source_principal
          - destination_principal

步骤5: 结果验证
  PromQL:
  # 优化后CPU
  sum(rate(container_cpu_usage_seconds_total{container="istio-proxy"}[5m])) 
  by (pod)
  
  # 优化后内存
  container_memory_working_set_bytes{container="istio-proxy"} / 1024 / 1024
  
  结果:
    - CPU: 200m → 50m (降低75%)
    - 内存: 250MB → 100MB (降低60%)
    - 集群总节省: 15 CPU, 15GB内存

步骤6: 性能验证
  PromQL:
  histogram_quantile(0.99,
    sum(rate(istio_request_duration_milliseconds_bucket[5m])) 
    by (le)
  )
  
  结果:
    - P99延迟: 优化前150ms, 优化后145ms
    - 性能无明显影响

总结:
  ✅ 资源消耗降低70%
  ✅ 性能无明显影响
  ✅ 年节省成本: $50,000+
```

---

## 10. 总结

### 10.1 可观测性三大支柱

```yaml
Metrics (指标):
  工具: Prometheus + Grafana
  价值: ✅ 实时监控, ✅ 趋势分析, ✅ 告警触发
  核心: 四大黄金信号（Latency、Traffic、Errors、Saturation）

Tracing (追踪):
  工具: Jaeger + OpenTelemetry
  价值: ✅ 调用链可视化, ✅ 性能瓶颈定位, ✅ 依赖关系分析
  核心: Trace、Span、Context Propagation

Logging (日志):
  工具: Loki + Promtail
  价值: ✅ 详细上下文, ✅ 问题诊断, ✅ 审计追踪
  核心: 结构化日志、标签索引、关联查询
```

### 10.2 服务网格可观测性优势

```yaml
自动化:
  ✅ 无需修改代码
  ✅ Sidecar自动采集
  ✅ 统一标准格式

全面性:
  ✅ L7 HTTP/gRPC指标
  ✅ L4 TCP指标
  ✅ mTLS状态
  ✅ 拓扑关系

关联性:
  ✅ Metrics → Traces → Logs
  ✅ 统一标签体系
  ✅ 一键故障排查

标准化:
  ✅ Prometheus标准
  ✅ OpenTelemetry标准
  ✅ 供应商中立
```

### 10.3 最佳实践总结

```yaml
1. SLI/SLO驱动:
   ✅ 定义清晰的SLI
   ✅ 设置合理的SLO
   ✅ 监控错误预算
   ✅ 基于数据决策

2. 分层监控:
   L1: 全局Mesh监控
   L2: 服务级别监控
   L3: 工作负载监控
   L4: Pod/Container监控

3. 告警策略:
   ✅ 可操作性
   ✅ 避免疲劳
   ✅ 分级处理
   ✅ 自愈机制

4. 性能优化:
   ✅ 采样率调整（1-10%）
   ✅ 日志过滤
   ✅ Metrics精简
   ✅ 资源限制

5. 故障排查:
   ✅ Metrics发现问题
   ✅ Tracing定位瓶颈
   ✅ Logging查看详情
   ✅ 快速根因分析
```

### 10.4 工具对比

```yaml
Istio可观测性:
  优势:
    ✅ 功能丰富
    ✅ 深度集成
    ✅ Envoy强大
  
  劣势:
    ❌ 资源消耗高
    ❌ 配置复杂
  
  适用:
    - 大型企业
    - 复杂场景
    - 深度定制

Linkerd可观测性:
  优势:
    ✅ 轻量级
    ✅ 开箱即用
    ✅ 黄金指标
  
  劣势:
    ❌ 功能相对简单
    ❌ 可定制性低
  
  适用:
    - 中小型企业
    - 快速上手
    - 资源受限

OpenTelemetry:
  优势:
    ✅ 统一标准
    ✅ 供应商中立
    ✅ 社区活跃
  
  适用:
    - 多云环境
    - 避免锁定
    - 未来趋势
```

### 10.5 未来展望

```yaml
趋势:

1. eBPF可观测性:
   - 更低开销
   - 内核级别采集
   - 无需Sidecar

2. AIOps:
   - 异常检测
   - 根因分析
   - 自动修复

3. 成本优化:
   - 智能采样
   - 数据压缩
   - 边缘计算

4. 统一可观测性平台:
   - 一站式解决方案
   - 自动关联
   - 智能告警
```

---

**相关章节**:

- [02_Istio深度解析](./02_Istio深度解析.md)
- [03_Linkerd轻量级服务网格](./03_Linkerd轻量级服务网格.md)
- [04_服务网格安全](./04_服务网格安全.md)
- [05_流量管理与灰度发布](./05_流量管理与灰度发布.md)

**下一章**: [07_多集群服务网格](./07_多集群服务网格.md)

---

**完成日期**: 2025-10-19  
**版本**: v1.0  
**作者**: 服务网格技术专家团队

**Tags**: `#ServiceMesh` `#Observability` `#Prometheus` `#Jaeger` `#Grafana` `#Loki` `#OpenTelemetry` `#SLO` `#Monitoring`
