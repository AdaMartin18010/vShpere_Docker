# 边缘AI与推理优化

## 目录

- [边缘AI与推理优化](#边缘ai与推理优化)
  - [目录](#目录)
  - [边缘AI概述](#边缘ai概述)
    - [边缘AI挑战](#边缘ai挑战)
    - [推理vs训练](#推理vs训练)
    - [性能指标](#性能指标)
  - [推理框架](#推理框架)
    - [TensorRT](#tensorrt)
    - [ONNX Runtime](#onnx-runtime)
    - [OpenVINO](#openvino)
    - [TensorFlow Lite](#tensorflow-lite)
    - [NCNN](#ncnn)
  - [模型优化技术](#模型优化技术)
    - [量化](#量化)
    - [剪枝](#剪枝)
    - [知识蒸馏](#知识蒸馏)
    - [神经架构搜索](#神经架构搜索)
  - [硬件加速](#硬件加速)
    - [GPU优化](#gpu优化)
    - [国产GPU](#国产gpu)
    - [NPU加速器](#npu加速器)
    - [ARM CPU优化](#arm-cpu优化)
  - [实战案例](#实战案例)
    - [YOLOv8目标检测](#yolov8目标检测)
    - [人脸识别](#人脸识别)
    - [语音识别](#语音识别)
  - [边缘大模型](#边缘大模型)
    - [模型压缩](#模型压缩)
    - [分布式推理](#分布式推理)
    - [LLM边缘部署](#llm边缘部署)
  - [性能优化](#性能优化)
    - [批处理优化](#批处理优化)
    - [内存优化](#内存优化)
    - [延迟优化](#延迟优化)
  - [参考资料](#参考资料)
    - [推理框架文档](#推理框架文档)
    - [模型优化](#模型优化)
    - [硬件优化](#硬件优化)
    - [学习资源](#学习资源)

---

## 边缘AI概述

### 边缘AI挑战

**资源约束**:

```yaml
计算资源:
  边缘设备:
    - CPU: 2-8核 (ARM/x86)
    - 内存: 2-16GB
    - GPU: 集成显卡或入门级独显
    - 功耗: 10-50W
  
  vs 云端:
    - CPU: 数十核
    - 内存: 数百GB
    - GPU: A100/H100
    - 功耗: 数千W

模型约束:
  大小限制: <500MB (理想<100MB)
  推理延迟: <100ms (实时应用<30ms)
  吞吐量: 10-100 fps
  内存占用: <2GB

网络约束:
  带宽有限: 无法实时传输高清视频到云端
  不稳定: 网络中断影响服务
  延迟高: 往返100-300ms不可接受
  隐私: 敏感数据不能上云
```

**边缘AI优势**:

```yaml
低延迟:
  - 本地推理: <10ms
  - 云端推理: 100-300ms
  - 适用: 自动驾驶、工业控制

隐私保护:
  - 数据不出边缘
  - 符合GDPR/等保要求
  - 敏感场景: 医疗、金融

离线工作:
  - 网络中断仍可用
  - 适用: 偏远地区、地下场景

成本节省:
  - 减少云端传输
  - 降低带宽成本
  - 一次部署长期使用
```

### 推理vs训练

**对比分析**:

```yaml
训练 (Training):
  目标: 学习模型参数
  数据: 大规模标注数据集
  计算: 大量矩阵运算 (前向+反向)
  时间: 小时/天/周
  硬件: 高性能GPU/TPU集群
  精度: FP32/FP16
  位置: 数据中心/云端

推理 (Inference):
  目标: 使用模型预测
  数据: 单个/小批量输入
  计算: 前向传播
  时间: 毫秒级
  硬件: 边缘设备 (CPU/GPU/NPU)
  精度: INT8/FP16 (量化后)
  位置: 边缘/终端设备

推理优化方向:
  1. 减少模型大小 (压缩/剪枝)
  2. 降低计算复杂度 (量化)
  3. 加速推理引擎 (TensorRT/ONNX)
  4. 硬件加速 (GPU/NPU)
  5. 批处理优化
```

### 性能指标

**关键指标**:

```yaml
延迟 (Latency):
  定义: 单次推理耗时
  单位: ms (毫秒)
  目标:
    - 实时视频: <33ms (30fps)
    - 交互应用: <100ms
    - 自动驾驶: <10ms
  
  组成:
    - 预处理: 5-10ms
    - 推理: 20-50ms
    - 后处理: 5-10ms

吞吐量 (Throughput):
  定义: 单位时间处理样本数
  单位: fps (帧/秒) 或 samples/s
  目标:
    - 视频分析: 30+ fps
    - 批量处理: 100+ samples/s

内存占用 (Memory):
  模型大小: <500MB
  运行时内存: <2GB
  显存 (GPU): <4GB

功耗 (Power):
  边缘设备: <50W
  移动设备: <5W
  嵌入式: <10W

精度 (Accuracy):
  量化后精度损失: <1%
  mAP (目标检测): >0.5
  Top-1 Accuracy (分类): >70%
```

---

## 推理框架

### TensorRT

**NVIDIA TensorRT简介**:

```yaml
特点:
  - NVIDIA官方推理引擎
  - 针对NVIDIA GPU优化
  - 支持INT8/FP16/FP32
  - 图优化 (层融合/常量折叠)
  - 动态Shape支持
  - 最高性能 (GPU)

支持模型:
  - TensorFlow
  - PyTorch
  - ONNX
  - Caffe

性能提升:
  - vs PyTorch: 3-10x
  - vs TensorFlow: 2-5x
```

**安装TensorRT**:

```bash
# 1. 安装CUDA (假设CUDA 12.0)
# 下载并安装: https://developer.nvidia.com/cuda-downloads

# 2. 安装cuDNN
# 下载并安装: https://developer.nvidia.com/cudnn

# 3. 安装TensorRT
# 方法1: pip安装
pip install nvidia-tensorrt

# 方法2: 下载tar包
# https://developer.nvidia.com/tensorrt
# 解压并设置环境变量

# 4. 验证安装
python -c "import tensorrt; print(tensorrt.__version__)"
```

**PyTorch模型转TensorRT**:

```python
# pytorch_to_tensorrt.py
import torch
import torch.onnx
import tensorrt as trt
import numpy as np

class YOLOv8(torch.nn.Module):
    """示例模型"""
    def __init__(self):
        super().__init__()
        # 加载预训练模型
        self.model = torch.hub.load('ultralytics/yolov8', 'yolov8n', pretrained=True)
    
    def forward(self, x):
        return self.model(x)

def export_to_onnx(model, onnx_path, input_shape=(1, 3, 640, 640)):
    """PyTorch → ONNX"""
    model.eval()
    dummy_input = torch.randn(*input_shape)
    
    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        export_params=True,
        opset_version=14,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )
    print(f"Exported ONNX: {onnx_path}")

def build_tensorrt_engine(onnx_path, engine_path, fp16_mode=True, int8_mode=False):
    """ONNX → TensorRT Engine"""
    logger = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(logger)
    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
    parser = trt.OnnxParser(network, logger)
    
    # 解析ONNX
    with open(onnx_path, 'rb') as model:
        if not parser.parse(model.read()):
            for error in range(parser.num_errors):
                print(parser.get_error(error))
            raise RuntimeError("Failed to parse ONNX")
    
    # 构建配置
    config = builder.create_builder_config()
    config.max_workspace_size = 2 << 30  # 2GB
    
    # FP16优化
    if fp16_mode and builder.platform_has_fast_fp16:
        config.set_flag(trt.BuilderFlag.FP16)
        print("FP16 mode enabled")
    
    # INT8优化 (需要校准数据)
    if int8_mode and builder.platform_has_fast_int8:
        config.set_flag(trt.BuilderFlag.INT8)
        # config.int8_calibrator = MyCalibrator()  # 需要实现
        print("INT8 mode enabled")
    
    # 动态Shape配置
    profile = builder.create_optimization_profile()
    profile.set_shape(
        "input",
        (1, 3, 640, 640),   # min
        (1, 3, 640, 640),   # opt
        (8, 3, 640, 640)    # max
    )
    config.add_optimization_profile(profile)
    
    # 构建引擎
    print("Building TensorRT engine... (this may take a while)")
    engine = builder.build_engine(network, config)
    
    # 保存引擎
    with open(engine_path, 'wb') as f:
        f.write(engine.serialize())
    
    print(f"TensorRT engine saved: {engine_path}")
    return engine

class TensorRTInference:
    """TensorRT推理封装"""
    def __init__(self, engine_path):
        # 加载引擎
        self.logger = trt.Logger(trt.Logger.WARNING)
        with open(engine_path, 'rb') as f:
            self.runtime = trt.Runtime(self.logger)
            self.engine = self.runtime.deserialize_cuda_engine(f.read())
        
        self.context = self.engine.create_execution_context()
        
        # 分配GPU内存
        self.inputs = []
        self.outputs = []
        self.bindings = []
        self.stream = None
        
        import pycuda.driver as cuda
        import pycuda.autoinit
        
        self.stream = cuda.Stream()
        
        for binding in self.engine:
            size = trt.volume(self.engine.get_binding_shape(binding))
            dtype = trt.nptype(self.engine.get_binding_dtype(binding))
            
            # 分配内存
            host_mem = cuda.pagelocked_empty(size, dtype)
            device_mem = cuda.mem_alloc(host_mem.nbytes)
            
            self.bindings.append(int(device_mem))
            
            if self.engine.binding_is_input(binding):
                self.inputs.append({'host': host_mem, 'device': device_mem})
            else:
                self.outputs.append({'host': host_mem, 'device': device_mem})
    
    def infer(self, input_data):
        """执行推理"""
        import pycuda.driver as cuda
        
        # 复制输入数据到GPU
        np.copyto(self.inputs[0]['host'], input_data.ravel())
        cuda.memcpy_htod_async(
            self.inputs[0]['device'],
            self.inputs[0]['host'],
            self.stream
        )
        
        # 执行推理
        self.context.execute_async_v2(
            bindings=self.bindings,
            stream_handle=self.stream.handle
        )
        
        # 复制输出数据到CPU
        cuda.memcpy_dtoh_async(
            self.outputs[0]['host'],
            self.outputs[0]['device'],
            self.stream
        )
        
        # 同步
        self.stream.synchronize()
        
        return self.outputs[0]['host']

# 使用示例
if __name__ == '__main__':
    # 1. PyTorch → ONNX
    model = YOLOv8()
    export_to_onnx(model, 'yolov8n.onnx')
    
    # 2. ONNX → TensorRT
    build_tensorrt_engine(
        'yolov8n.onnx',
        'yolov8n_fp16.engine',
        fp16_mode=True
    )
    
    # 3. 推理
    trt_model = TensorRTInference('yolov8n_fp16.engine')
    
    # 准备输入
    input_data = np.random.randn(1, 3, 640, 640).astype(np.float32)
    
    # 推理
    import time
    start = time.time()
    output = trt_model.infer(input_data)
    elapsed = (time.time() - start) * 1000
    
    print(f"Inference time: {elapsed:.2f} ms")
```

**性能对比**:

```python
# benchmark.py
import torch
import time
import numpy as np

def benchmark_pytorch(model, input_shape, iterations=100):
    """PyTorch基准测试"""
    model.eval()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    dummy_input = torch.randn(*input_shape).to(device)
    
    # Warmup
    for _ in range(10):
        with torch.no_grad():
            _ = model(dummy_input)
    
    # Benchmark
    torch.cuda.synchronize()
    start = time.time()
    
    for _ in range(iterations):
        with torch.no_grad():
            _ = model(dummy_input)
    
    torch.cuda.synchronize()
    elapsed = time.time() - start
    
    avg_time = (elapsed / iterations) * 1000  # ms
    fps = iterations / elapsed
    
    return avg_time, fps

def benchmark_tensorrt(trt_model, input_shape, iterations=100):
    """TensorRT基准测试"""
    input_data = np.random.randn(*input_shape).astype(np.float32)
    
    # Warmup
    for _ in range(10):
        _ = trt_model.infer(input_data)
    
    # Benchmark
    start = time.time()
    
    for _ in range(iterations):
        _ = trt_model.infer(input_data)
    
    elapsed = time.time() - start
    
    avg_time = (elapsed / iterations) * 1000  # ms
    fps = iterations / elapsed
    
    return avg_time, fps

# 对比测试
print("=" * 60)
print("Performance Comparison: PyTorch vs TensorRT")
print("=" * 60)

# PyTorch
pytorch_model = YOLOv8()
pt_time, pt_fps = benchmark_pytorch(pytorch_model, (1, 3, 640, 640))
print(f"PyTorch FP32: {pt_time:.2f} ms/frame ({pt_fps:.1f} FPS)")

# TensorRT FP16
trt_fp16 = TensorRTInference('yolov8n_fp16.engine')
trt_time, trt_fps = benchmark_tensorrt(trt_fp16, (1, 3, 640, 640))
print(f"TensorRT FP16: {trt_time:.2f} ms/frame ({trt_fps:.1f} FPS)")

speedup = pt_time / trt_time
print(f"\nSpeedup: {speedup:.2f}x")
```

### ONNX Runtime

**ONNX Runtime简介**:

```yaml
特点:
  - 跨平台 (CPU/GPU/NPU)
  - 支持多种硬件后端
  - 轻量级 (<100MB)
  - 广泛兼容性

执行提供器 (Execution Provider):
  - CPU: OpenMP, MLAS
  - GPU: CUDA, TensorRT, DirectML
  - 国产硬件: 华为昇腾, 寒武纪
  - 移动: CoreML (iOS), NNAPI (Android)

性能:
  - vs PyTorch CPU: 2-3x
  - vs TensorFlow CPU: 1.5-2x
```

**安装与使用**:

```bash
# 安装ONNX Runtime
# CPU版本
pip install onnxruntime

# GPU版本 (CUDA)
pip install onnxruntime-gpu

# 验证
python -c "import onnxruntime as ort; print(ort.__version__)"
```

**ONNX Runtime推理**:

```python
# onnx_runtime_inference.py
import onnxruntime as ort
import numpy as np
import time

class ONNXInference:
    def __init__(self, model_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider']):
        """
        初始化ONNX Runtime推理
        
        providers优先级:
        1. CUDAExecutionProvider (NVIDIA GPU)
        2. TensorrtExecutionProvider (TensorRT)
        3. CPUExecutionProvider (CPU)
        """
        # 创建推理会话
        self.session = ort.InferenceSession(
            model_path,
            providers=providers
        )
        
        # 获取输入输出信息
        self.input_name = self.session.get_inputs()[0].name
        self.output_names = [output.name for output in self.session.get_outputs()]
        
        # 打印模型信息
        print(f"Model loaded: {model_path}")
        print(f"Input: {self.input_name}")
        print(f"Outputs: {self.output_names}")
        print(f"Providers: {self.session.get_providers()}")
    
    def infer(self, input_data):
        """执行推理"""
        outputs = self.session.run(
            self.output_names,
            {self.input_name: input_data}
        )
        return outputs[0]
    
    def benchmark(self, input_shape, iterations=100):
        """性能测试"""
        input_data = np.random.randn(*input_shape).astype(np.float32)
        
        # Warmup
        for _ in range(10):
            _ = self.infer(input_data)
        
        # Benchmark
        start = time.time()
        for _ in range(iterations):
            _ = self.infer(input_data)
        elapsed = time.time() - start
        
        avg_time = (elapsed / iterations) * 1000
        fps = iterations / elapsed
        
        print(f"Average time: {avg_time:.2f} ms")
        print(f"Throughput: {fps:.1f} FPS")
        
        return avg_time, fps

# 使用示例
if __name__ == '__main__':
    # CPU推理
    print("\n" + "="*60)
    print("CPU Inference")
    print("="*60)
    cpu_model = ONNXInference('yolov8n.onnx', providers=['CPUExecutionProvider'])
    cpu_model.benchmark((1, 3, 640, 640))
    
    # GPU推理
    print("\n" + "="*60)
    print("GPU Inference (CUDA)")
    print("="*60)
    gpu_model = ONNXInference('yolov8n.onnx', providers=['CUDAExecutionProvider'])
    gpu_model.benchmark((1, 3, 640, 640))
```

**性能优化选项**:

```python
# onnx_optimization.py
import onnxruntime as ort

def create_optimized_session(model_path, use_gpu=True):
    """创建优化的推理会话"""
    
    # 会话选项
    sess_options = ort.SessionOptions()
    
    # 1. 图优化级别
    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
    
    # 2. 线程数 (CPU)
    sess_options.intra_op_num_threads = 4
    sess_options.inter_op_num_threads = 4
    
    # 3. 启用内存模式优化
    sess_options.enable_mem_pattern = True
    sess_options.enable_mem_reuse = True
    
    # 4. 启用CPU加速
    sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
    
    # 5. 日志级别
    sess_options.log_severity_level = 3  # 0=Verbose, 3=Error
    
    # 选择执行提供器
    if use_gpu:
        providers = [
            ('CUDAExecutionProvider', {
                'device_id': 0,
                'gpu_mem_limit': 2 * 1024 * 1024 * 1024,  # 2GB
                'arena_extend_strategy': 'kSameAsRequested',
                'cudnn_conv_algo_search': 'HEURISTIC',
            }),
            'CPUExecutionProvider'
        ]
    else:
        providers = ['CPUExecutionProvider']
    
    # 创建会话
    session = ort.InferenceSession(
        model_path,
        sess_options=sess_options,
        providers=providers
    )
    
    return session

# 使用
optimized_session = create_optimized_session('yolov8n.onnx', use_gpu=True)
```

### OpenVINO

**Intel OpenVINO简介**:

```yaml
特点:
  - Intel官方推理工具包
  - 针对Intel CPU/GPU/VPU优化
  - 支持多种框架
  - 模型优化器 (Model Optimizer)
  - 推理引擎 (Inference Engine)

硬件支持:
  - Intel CPU (Xeon, Core)
  - Intel集成显卡 (iGPU)
  - Intel独立显卡 (Arc)
  - Intel Movidius VPU
  - Intel FPGA

性能提升:
  - vs TensorFlow CPU: 3-5x
  - vs PyTorch CPU: 2-4x
```

**安装OpenVINO**:

```bash
# 安装OpenVINO Runtime
pip install openvino

# 安装开发工具 (模型转换)
pip install openvino-dev

# 验证
python -c "from openvino.runtime import Core; print(Core().available_devices)"
```

**模型转换与推理**:

```python
# openvino_inference.py
from openvino.runtime import Core
import numpy as np
import time

def convert_to_openvino(onnx_path, output_dir='./openvino_model'):
    """ONNX → OpenVINO IR"""
    import os
    os.system(f'mo --input_model {onnx_path} --output_dir {output_dir}')
    print(f"OpenVINO IR saved to: {output_dir}")

class OpenVINOInference:
    def __init__(self, model_xml, device='CPU'):
        """
        初始化OpenVINO推理
        
        device选项:
        - CPU: Intel CPU
        - GPU: Intel iGPU/Arc
        - MYRIAD: Intel Movidius VPU
        - AUTO: 自动选择最佳设备
        """
        # 创建Core
        self.core = Core()
        
        # 读取模型
        self.model = self.core.read_model(model=model_xml)
        
        # 编译模型
        self.compiled_model = self.core.compile_model(
            model=self.model,
            device_name=device
        )
        
        # 获取输入输出层
        self.input_layer = self.compiled_model.input(0)
        self.output_layer = self.compiled_model.output(0)
        
        print(f"Model loaded on device: {device}")
        print(f"Input shape: {self.input_layer.shape}")
        print(f"Output shape: {self.output_layer.shape}")
    
    def infer(self, input_data):
        """执行推理"""
        result = self.compiled_model([input_data])[self.output_layer]
        return result
    
    def benchmark(self, input_shape, iterations=100):
        """性能测试"""
        input_data = np.random.randn(*input_shape).astype(np.float32)
        
        # Warmup
        for _ in range(10):
            _ = self.infer(input_data)
        
        # Benchmark
        start = time.time()
        for _ in range(iterations):
            _ = self.infer(input_data)
        elapsed = time.time() - start
        
        avg_time = (elapsed / iterations) * 1000
        fps = iterations / elapsed
        
        print(f"Average time: {avg_time:.2f} ms")
        print(f"Throughput: {fps:.1f} FPS")
        
        return avg_time, fps

# 使用示例
if __name__ == '__main__':
    # 1. 转换模型 (首次运行)
    # convert_to_openvino('yolov8n.onnx', './openvino_model')
    
    # 2. CPU推理
    print("\n" + "="*60)
    print("OpenVINO CPU Inference")
    print("="*60)
    cpu_model = OpenVINOInference('./openvino_model/yolov8n.xml', device='CPU')
    cpu_model.benchmark((1, 3, 640, 640))
    
    # 3. GPU推理 (如果有Intel GPU)
    try:
        print("\n" + "="*60)
        print("OpenVINO GPU Inference")
        print("="*60)
        gpu_model = OpenVINOInference('./openvino_model/yolov8n.xml', device='GPU')
        gpu_model.benchmark((1, 3, 640, 640))
    except Exception as e:
        print(f"GPU not available: {e}")
```

**性能优化配置**:

```python
# openvino_optimization.py
from openvino.runtime import Core, properties

def create_optimized_core(device='CPU'):
    """创建优化的OpenVINO Core"""
    core = Core()
    
    if device == 'CPU':
        # CPU优化配置
        config = {
            properties.enable_profiling(): False,
            properties.inference_num_threads(): 4,
            properties.streams.num(): 2,  # 推理流数量
        }
    elif device == 'GPU':
        # GPU优化配置
        config = {
            properties.enable_profiling(): False,
            properties.cache_dir(): './gpu_cache',
        }
    else:
        config = {}
    
    return core, config

# 使用
core, config = create_optimized_core('CPU')
compiled_model = core.compile_model(model, device_name='CPU', config=config)
```

### TensorFlow Lite

**TensorFlow Lite简介**:

```yaml
特点:
  - Google官方移动/嵌入式推理框架
  - 轻量级 (<1MB)
  - 针对ARM优化
  - 支持量化模型
  - 跨平台 (Android/iOS/Linux/MCU)

硬件加速:
  - ARM NEON (CPU)
  - Android NNAPI
  - iOS CoreML
  - GPU Delegate
  - DSP/NPU

典型应用:
  - 移动应用
  - 树莓派/Jetson Nano
  - 物联网设备
```

**模型转换**:

```python
# tflite_conversion.py
import tensorflow as tf
import numpy as np

def convert_to_tflite(saved_model_dir, output_path, quantize=False):
    """TensorFlow SavedModel → TFLite"""
    
    # 创建转换器
    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
    
    if quantize:
        # INT8量化
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        converter.inference_input_type = tf.uint8
        converter.inference_output_type = tf.uint8
        
        # 提供校准数据
        def representative_dataset():
            for _ in range(100):
                data = np.random.randn(1, 224, 224, 3).astype(np.float32)
                yield [data]
        
        converter.representative_dataset = representative_dataset
    
    # 转换
    tflite_model = converter.convert()
    
    # 保存
    with open(output_path, 'wb') as f:
        f.write(tflite_model)
    
    print(f"TFLite model saved: {output_path}")
    print(f"Size: {len(tflite_model) / 1024:.2f} KB")

# PyTorch → ONNX → TensorFlow → TFLite (跨框架转换)
def pytorch_to_tflite(pytorch_model, input_shape, output_path):
    """PyTorch → TFLite (通过ONNX)"""
    import onnx
    from onnx_tf.backend import prepare
    
    # 1. PyTorch → ONNX
    dummy_input = torch.randn(*input_shape)
    torch.onnx.export(
        pytorch_model,
        dummy_input,
        'temp.onnx',
        opset_version=12
    )
    
    # 2. ONNX → TensorFlow
    onnx_model = onnx.load('temp.onnx')
    tf_rep = prepare(onnx_model)
    tf_rep.export_graph('temp_tf')
    
    # 3. TensorFlow → TFLite
    convert_to_tflite('temp_tf', output_path)

# 使用
# convert_to_tflite('./saved_model', 'model.tflite', quantize=True)
```

**TFLite推理**:

```python
# tflite_inference.py
import numpy as np
import tensorflow as tf
import time

class TFLiteInference:
    def __init__(self, model_path, use_gpu=False):
        """初始化TFLite推理"""
        
        # 加载模型
        self.interpreter = tf.lite.Interpreter(
            model_path=model_path,
            num_threads=4  # CPU线程数
        )
        
        # GPU加速 (如果支持)
        if use_gpu:
            try:
                self.interpreter = tf.lite.Interpreter(
                    model_path=model_path,
                    experimental_delegates=[tf.lite.experimental.load_delegate('libGLES_mali.so')]
                )
                print("GPU delegate loaded")
            except:
                print("GPU delegate not available, using CPU")
        
        self.interpreter.allocate_tensors()
        
        # 获取输入输出张量
        self.input_details = self.interpreter.get_input_details()
        self.output_details = self.interpreter.get_output_details()
        
        print(f"Model loaded: {model_path}")
        print(f"Input shape: {self.input_details[0]['shape']}")
        print(f"Output shape: {self.output_details[0]['shape']}")
    
    def infer(self, input_data):
        """执行推理"""
        # 设置输入
        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)
        
        # 推理
        self.interpreter.invoke()
        
        # 获取输出
        output = self.interpreter.get_tensor(self.output_details[0]['index'])
        return output
    
    def benchmark(self, input_shape, iterations=100):
        """性能测试"""
        # 生成随机输入
        input_dtype = self.input_details[0]['dtype']
        input_data = np.random.randn(*input_shape).astype(input_dtype)
        
        # Warmup
        for _ in range(10):
            _ = self.infer(input_data)
        
        # Benchmark
        start = time.time()
        for _ in range(iterations):
            _ = self.infer(input_data)
        elapsed = time.time() - start
        
        avg_time = (elapsed / iterations) * 1000
        fps = iterations / elapsed
        
        print(f"Average time: {avg_time:.2f} ms")
        print(f"Throughput: {fps:.1f} FPS")
        
        return avg_time, fps

# 使用示例
if __name__ == '__main__':
    # FP32模型
    print("\n" + "="*60)
    print("TFLite FP32 Inference")
    print("="*60)
    fp32_model = TFLiteInference('model_fp32.tflite')
    fp32_model.benchmark((1, 224, 224, 3))
    
    # INT8量化模型
    print("\n" + "="*60)
    print("TFLite INT8 Inference")
    print("="*60)
    int8_model = TFLiteInference('model_int8.tflite')
    int8_model.benchmark((1, 224, 224, 3))
```

### NCNN

**NCNN简介**:

```yaml
特点:
  - 腾讯开源移动端推理框架
  - 针对ARM优化 (NEON/Vulkan)
  - 极致轻量 (<500KB)
  - 无第三方依赖
  - 支持量化模型

平台支持:
  - Android
  - iOS
  - Linux (ARM/x86)
  - Windows
  - macOS

性能:
  - ARM CPU: 优于TFLite
  - 内存占用: 极小
  - 启动速度: 快
```

**模型转换**:

```bash
# 安装NCNN
git clone https://github.com/Tencent/ncnn.git
cd ncnn
mkdir build && cd build
cmake -DCMAKE_BUILD_TYPE=Release ..
make -j4
sudo make install

# ONNX → NCNN
./onnx2ncnn model.onnx model.param model.bin

# 量化
./ncnn2int8 model.param model.bin quantized.param quantized.bin calibration_images/
```

**NCNN推理 (C++)**:

```cpp
// ncnn_inference.cpp
#include <ncnn/net.h>
#include <opencv2/opencv.hpp>

class NCNNInference {
public:
    NCNNInference(const char* param_path, const char* bin_path) {
        // 加载模型
        net.load_param(param_path);
        net.load_model(bin_path);
        
        // 使用Vulkan GPU加速 (可选)
        net.opt.use_vulkan_compute = true;
        
        printf("NCNN model loaded\n");
    }
    
    cv::Mat infer(const cv::Mat& input_image) {
        // 预处理
        ncnn::Mat input = ncnn::Mat::from_pixels_resize(
            input_image.data,
            ncnn::Mat::PIXEL_BGR2RGB,
            input_image.cols,
            input_image.rows,
            640, 640
        );
        
        // 归一化
        const float mean_vals[3] = {0.0f, 0.0f, 0.0f};
        const float norm_vals[3] = {1/255.0f, 1/255.0f, 1/255.0f};
        input.substract_mean_normalize(mean_vals, norm_vals);
        
        // 推理
        ncnn::Extractor ex = net.create_extractor();
        ex.input("input", input);
        
        ncnn::Mat output;
        ex.extract("output", output);
        
        // 后处理
        // ... (根据具体模型)
        
        return cv::Mat();  // 返回结果
    }
    
private:
    ncnn::Net net;
};

int main() {
    NCNNInference model("yolov8n.param", "yolov8n.bin");
    
    cv::Mat image = cv::imread("test.jpg");
    cv::Mat result = model.infer(image);
    
    return 0;
}
```

**NCNN推理 (Python)**:

```python
# ncnn_python_inference.py
import ncnn
import numpy as np
import cv2

class NCNNInference:
    def __init__(self, param_path, bin_path):
        """初始化NCNN推理"""
        self.net = ncnn.Net()
        
        # 加载模型
        self.net.load_param(param_path)
        self.net.load_model(bin_path)
        
        print(f"NCNN model loaded")
    
    def infer(self, image):
        """执行推理"""
        # 预处理
        mat_in = ncnn.Mat.from_pixels_resize(
            image,
            ncnn.Mat.PixelType.PIXEL_BGR2RGB,
            image.shape[1],
            image.shape[0],
            640, 640
        )
        
        # 归一化
        mean_vals = [0, 0, 0]
        norm_vals = [1/255.0, 1/255.0, 1/255.0]
        mat_in.substract_mean_normalize(mean_vals, norm_vals)
        
        # 推理
        ex = self.net.create_extractor()
        ex.input("input", mat_in)
        
        mat_out = ncnn.Mat()
        ex.extract("output", mat_out)
        
        # 转换为numpy
        output = np.array(mat_out)
        
        return output

# 使用
if __name__ == '__main__':
    model = NCNNInference('yolov8n.param', 'yolov8n.bin')
    
    image = cv2.imread('test.jpg')
    output = model.infer(image)
    print(f"Output shape: {output.shape}")
```

---

## 模型优化技术

### 量化

**量化原理**:

```yaml
什么是量化:
  - 降低模型精度 (FP32 → INT8/FP16)
  - 减少模型大小 (4x-8x)
  - 加速推理 (2x-4x)
  - 轻微精度损失 (<1-2%)

量化类型:
  1. 训练后量化 (Post-Training Quantization):
     - 静态量化: 需要校准数据
     - 动态量化: 运行时量化
     - 简单快速，轻微精度损失
  
  2. 量化感知训练 (Quantization-Aware Training):
     - 训练时模拟量化
     - 精度损失最小
     - 训练成本高

精度对比:
  FP32: 32-bit浮点 (基准)
  FP16: 16-bit浮点 (2x加速, 模型减半)
  INT8: 8-bit整数 (4x加速, 模型减至1/4)
  INT4: 4-bit整数 (8x加速, 实验性)
```

**PyTorch量化**:

```python
# pytorch_quantization.py
import torch
import torch.quantization as quantization
import torchvision.models as models

class ModelQuantizer:
    """模型量化工具类"""
    
    @staticmethod
    def dynamic_quantization(model):
        """动态量化 (最简单)"""
        quantized_model = torch.quantization.quantize_dynamic(
            model,
            {torch.nn.Linear, torch.nn.Conv2d},  # 量化层类型
            dtype=torch.qint8
        )
        return quantized_model
    
    @staticmethod
    def static_quantization(model, calibration_loader):
        """静态量化 (需要校准数据)"""
        # 1. 设置量化配置
        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
        
        # 2. 准备模型
        model_prepared = torch.quantization.prepare(model)
        
        # 3. 校准 (用代表性数据运行模型)
        model_prepared.eval()
        with torch.no_grad():
            for data, _ in calibration_loader:
                model_prepared(data)
        
        # 4. 转换为量化模型
        quantized_model = torch.quantization.convert(model_prepared)
        
        return quantized_model
    
    @staticmethod
    def quantization_aware_training(model, train_loader, epochs=10):
        """量化感知训练"""
        # 1. 设置量化配置
        model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
        
        # 2. 准备QAT模型
        model_prepared = torch.quantization.prepare_qat(model)
        
        # 3. 训练
        model_prepared.train()
        optimizer = torch.optim.Adam(model_prepared.parameters(), lr=0.001)
        criterion = torch.nn.CrossEntropyLoss()
        
        for epoch in range(epochs):
            for data, target in train_loader:
                optimizer.zero_grad()
                output = model_prepared(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
        
        # 4. 转换为量化模型
        model_prepared.eval()
        quantized_model = torch.quantization.convert(model_prepared)
        
        return quantized_model

# 使用示例
if __name__ == '__main__':
    # 加载模型
    model = models.resnet18(pretrained=True)
    model.eval()
    
    # 1. 动态量化
    print("Dynamic Quantization...")
    dynamic_quantized = ModelQuantizer.dynamic_quantization(model)
    
    # 2. 静态量化
    print("Static Quantization...")
    # 准备校准数据 (假设)
    calibration_loader = torch.utils.data.DataLoader(...)
    static_quantized = ModelQuantizer.static_quantization(model, calibration_loader)
    
    # 比较模型大小
    def get_model_size(model):
        torch.save(model.state_dict(), "temp.pth")
        size = os.path.getsize("temp.pth") / 1e6  # MB
        os.remove("temp.pth")
        return size
    
    original_size = get_model_size(model)
    quantized_size = get_model_size(dynamic_quantized)
    
    print(f"Original model: {original_size:.2f} MB")
    print(f"Quantized model: {quantized_size:.2f} MB")
    print(f"Compression ratio: {original_size/quantized_size:.2f}x")
```

**TensorRT INT8量化**:

```python
# tensorrt_int8_calibration.py
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np

class Int8Calibrator(trt.IInt8EntropyCalibrator2):
    """INT8校准器"""
    
    def __init__(self, calibration_data, cache_file='calibration.cache'):
        trt.IInt8EntropyCalibrator2.__init__(self)
        
        self.calibration_data = calibration_data
        self.cache_file = cache_file
        self.current_index = 0
        
        # 分配GPU内存
        self.device_input = cuda.mem_alloc(calibration_data[0].nbytes)
    
    def get_batch_size(self):
        return 1
    
    def get_batch(self, names):
        """获取一批校准数据"""
        if self.current_index < len(self.calibration_data):
            batch = self.calibration_data[self.current_index]
            cuda.memcpy_htod(self.device_input, batch)
            self.current_index += 1
            return [int(self.device_input)]
        else:
            return None
    
    def read_calibration_cache(self):
        """读取缓存的校准数据"""
        if os.path.exists(self.cache_file):
            with open(self.cache_file, 'rb') as f:
                return f.read()
        return None
    
    def write_calibration_cache(self, cache):
        """写入校准缓存"""
        with open(self.cache_file, 'wb') as f:
            f.write(cache)

def build_int8_engine(onnx_path, calibration_data, engine_path):
    """构建INT8 TensorRT引擎"""
    logger = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(logger)
    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
    parser = trt.OnnxParser(network, logger)
    
    # 解析ONNX
    with open(onnx_path, 'rb') as model:
        parser.parse(model.read())
    
    # 配置
    config = builder.create_builder_config()
    config.max_workspace_size = 2 << 30
    
    # 启用INT8
    config.set_flag(trt.BuilderFlag.INT8)
    
    # 设置校准器
    calibrator = Int8Calibrator(calibration_data)
    config.int8_calibrator = calibrator
    
    # 构建引擎
    engine = builder.build_engine(network, config)
    
    # 保存
    with open(engine_path, 'wb') as f:
        f.write(engine.serialize())
    
    print(f"INT8 engine saved: {engine_path}")
    return engine

# 准备校准数据
calibration_data = []
for i in range(100):  # 100张图像用于校准
    img = np.random.randn(1, 3, 640, 640).astype(np.float32)
    calibration_data.append(img)

# 构建INT8引擎
build_int8_engine('yolov8n.onnx', calibration_data, 'yolov8n_int8.engine')
```

### 剪枝

**剪枝原理**:

```yaml
什么是剪枝:
  - 移除不重要的神经元/连接
  - 减少模型参数量
  - 加速推理
  - 保持精度

剪枝类型:
  1. 非结构化剪枝:
     - 剪除单个权重
     - 高压缩比
     - 需要特殊硬件支持
  
  2. 结构化剪枝:
     - 剪除整个通道/层
     - 通用硬件支持
     - 实际加速

剪枝策略:
  - 幅度剪枝 (Magnitude Pruning)
  - 敏感度剪枝
  - 迭代剪枝
```

**PyTorch剪枝**:

```python
# pytorch_pruning.py
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune

class ModelPruner:
    """模型剪枝工具类"""
    
    @staticmethod
    def magnitude_pruning(model, amount=0.3):
        """
        幅度剪枝
        amount: 剪枝比例 (0.3 = 30%)
        """
        for name, module in model.named_modules():
            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):
                # L1非结构化剪枝
                prune.l1_unstructured(module, name='weight', amount=amount)
        
        return model
    
    @staticmethod
    def structured_pruning(model, amount=0.3):
        """结构化剪枝 (剪除整个通道)"""
        for name, module in model.named_modules():
            if isinstance(module, nn.Conv2d):
                # 按通道剪枝
                prune.ln_structured(
                    module,
                    name='weight',
                    amount=amount,
                    n=2,
                    dim=0  # 输出通道维度
                )
        
        return model
    
    @staticmethod
    def iterative_pruning(model, train_fn, prune_amount=0.2, iterations=5):
        """迭代剪枝"""
        for i in range(iterations):
            print(f"Pruning iteration {i+1}/{iterations}")
            
            # 1. 剪枝
            model = ModelPruner.magnitude_pruning(model, amount=prune_amount)
            
            # 2. 微调 (恢复精度)
            model = train_fn(model, epochs=5)
            
            # 3. 评估
            accuracy = evaluate(model)
            sparsity = get_sparsity(model)
            print(f"Accuracy: {accuracy:.2f}%, Sparsity: {sparsity:.2f}%")
        
        return model
    
    @staticmethod
    def remove_pruning_masks(model):
        """移除剪枝掩码 (使剪枝永久化)"""
        for name, module in model.named_modules():
            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):
                if prune.is_pruned(module):
                    prune.remove(module, 'weight')
        
        return model

def get_sparsity(model):
    """计算模型稀疏度"""
    total_params = 0
    zero_params = 0
    
    for param in model.parameters():
        total_params += param.numel()
        zero_params += (param == 0).sum().item()
    
    sparsity = 100.0 * zero_params / total_params
    return sparsity

# 使用示例
if __name__ == '__main__':
    model = models.resnet18(pretrained=True)
    
    print(f"Original sparsity: {get_sparsity(model):.2f}%")
    
    # 剪枝30%
    pruned_model = ModelPruner.magnitude_pruning(model, amount=0.3)
    
    print(f"After pruning sparsity: {get_sparsity(pruned_model):.2f}%")
    
    # 移除掩码
    pruned_model = ModelPruner.remove_pruning_masks(pruned_model)
```

### 知识蒸馏

**知识蒸馏原理**:

```yaml
什么是知识蒸馏:
  - 大模型 (Teacher) 教小模型 (Student)
  - 小模型学习大模型的"软标签"
  - 保持精度同时减小模型

蒸馏类型:
  1. Response-based: 学习输出分布
  2. Feature-based: 学习中间特征
  3. Relation-based: 学习样本关系

优势:
  - 小模型获得更好性能
  - 适合边缘部署
  - 不需要重新训练大模型
```

**知识蒸馏实现**:

```python
# knowledge_distillation.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class DistillationLoss(nn.Module):
    """蒸馏损失函数"""
    
    def __init__(self, temperature=3.0, alpha=0.7):
        """
        temperature: 温度参数 (软化概率分布)
        alpha: 蒸馏损失权重 (1-alpha为硬标签损失权重)
        """
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.criterion = nn.KLDivLoss(reduction='batchmean')
    
    def forward(self, student_logits, teacher_logits, labels):
        # 1. 蒸馏损失 (软标签)
        soft_student = F.log_softmax(student_logits / self.temperature, dim=1)
        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=1)
        
        distillation_loss = self.criterion(soft_student, soft_teacher) * (self.temperature ** 2)
        
        # 2. 硬标签损失
        hard_loss = F.cross_entropy(student_logits, labels)
        
        # 3. 组合损失
        total_loss = self.alpha * distillation_loss + (1 - self.alpha) * hard_loss
        
        return total_loss

def distill_model(teacher_model, student_model, train_loader, epochs=10):
    """知识蒸馏训练"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    teacher_model = teacher_model.to(device)
    student_model = student_model.to(device)
    
    teacher_model.eval()  # 教师模型冻结
    student_model.train()
    
    optimizer = torch.optim.Adam(student_model.parameters(), lr=0.001)
    criterion = DistillationLoss(temperature=3.0, alpha=0.7)
    
    for epoch in range(epochs):
        total_loss = 0
        
        for data, labels in train_loader:
            data, labels = data.to(device), labels.to(device)
            
            # 教师模型预测
            with torch.no_grad():
                teacher_logits = teacher_model(data)
            
            # 学生模型预测
            student_logits = student_model(data)
            
            # 计算蒸馏损失
            loss = criterion(student_logits, teacher_logits, labels)
            
            # 反向传播
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")
    
    return student_model

# 使用示例
if __name__ == '__main__':
    # 教师模型 (大)
    teacher = models.resnet50(pretrained=True)
    
    # 学生模型 (小)
    student = models.resnet18(pretrained=False)
    
    # 蒸馏训练
    distilled_student = distill_model(
        teacher,
        student,
        train_loader,
        epochs=20
    )
    
    # 比较性能
    teacher_acc = evaluate(teacher, test_loader)
    student_acc = evaluate(distilled_student, test_loader)
    
    print(f"Teacher accuracy: {teacher_acc:.2f}%")
    print(f"Student accuracy: {student_acc:.2f}%")
```

### 神经架构搜索

**NAS概述**:

```yaml
神经架构搜索 (NAS):
  - 自动设计神经网络架构
  - 为特定硬件优化
  - 平衡精度和效率

NAS方法:
  1. 强化学习NAS
  2. 进化算法NAS
  3. 可微分NAS (DARTS)
  4. Once-for-All (OFA)

边缘优化NAS:
  - EfficientNet: 复合缩放
  - MobileNet: 深度可分离卷积
  - ShuffleNet: 通道重组
  - GhostNet: 廉价操作
```

**MobileNet架构**:

```python
# mobilenet_efficient.py
import torch
import torch.nn as nn

class DepthwiseSeparableConv(nn.Module):
    """深度可分离卷积 (MobileNet核心)"""
    
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        # 1. Depthwise卷积 (每个通道独立卷积)
        self.depthwise = nn.Conv2d(
            in_channels,
            in_channels,
            kernel_size=3,
            stride=stride,
            padding=1,
            groups=in_channels,  # 关键: groups=in_channels
            bias=False
        )
        self.bn1 = nn.BatchNorm2d(in_channels)
        
        # 2. Pointwise卷积 (1x1卷积混合通道)
        self.pointwise = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size=1,
            bias=False
        )
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        self.relu = nn.ReLU6(inplace=True)
    
    def forward(self, x):
        x = self.depthwise(x)
        x = self.bn1(x)
        x = self.relu(x)
        
        x = self.pointwise(x)
        x = self.bn2(x)
        x = self.relu(x)
        
        return x

# 计算量对比
def compare_ops():
    """标准卷积 vs 深度可分离卷积"""
    h, w = 224, 224
    in_ch, out_ch = 128, 256
    k = 3  # 卷积核大小
    
    # 标准卷积计算量
    standard_ops = h * w * in_ch * out_ch * k * k
    
    # 深度可分离卷积计算量
    depthwise_ops = h * w * in_ch * k * k  # Depthwise
    pointwise_ops = h * w * in_ch * out_ch  # Pointwise
    separable_ops = depthwise_ops + pointwise_ops
    
    reduction = standard_ops / separable_ops
    
    print(f"Standard Conv ops: {standard_ops / 1e9:.2f} G")
    print(f"Separable Conv ops: {separable_ops / 1e9:.2f} G")
    print(f"Reduction: {reduction:.2f}x")

compare_ops()
```

---

## 硬件加速

### GPU优化

**CUDA优化技巧**:

```python
# gpu_optimization.py
import torch

class GPUOptimizer:
    """GPU推理优化"""
    
    @staticmethod
    def optimize_model_for_gpu(model):
        """GPU模型优化"""
        model = model.cuda()
        model.eval()
        
        # 1. 启用cuDNN自动调优
        torch.backends.cudnn.benchmark = True
        
        # 2. 使用TF32 (A100+)
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        
        # 3. FP16混合精度
        model = model.half()
        
        # 4. Torch compile (PyTorch 2.0+)
        # model = torch.compile(model, mode='reduce-overhead')
        
        return model
    
    @staticmethod
    def batch_inference_gpu(model, data_loader, device='cuda'):
        """批量GPU推理"""
        model = model.to(device)
        model.eval()
        
        results = []
        
        with torch.no_grad():
            with torch.cuda.amp.autocast():  # 自动混合精度
                for batch in data_loader:
                    batch = batch.to(device, non_blocking=True)  # 异步传输
                    output = model(batch)
                    results.append(output.cpu())
        
        return torch.cat(results)
    
    @staticmethod
    def profile_gpu(model, input_shape):
        """GPU性能分析"""
        import torch.profiler as profiler
        
        model = model.cuda()
        dummy_input = torch.randn(*input_shape).cuda()
        
        with profiler.profile(
            activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],
            record_shapes=True,
            with_stack=True
        ) as prof:
            with profiler.record_function("model_inference"):
                model(dummy_input)
        
        # 打印统计信息
        print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
        
        # 导出Chrome trace
        prof.export_chrome_trace("trace.json")

# GPU内存优化
def optimize_gpu_memory(model):
    """减少GPU内存占用"""
    # 1. Gradient checkpointing (训练时)
    # model.gradient_checkpointing_enable()
    
    # 2. 清理缓存
    torch.cuda.empty_cache()
    
    # 3. 使用更小的batch size
    
    # 4. 使用in-place操作
    # model.relu = nn.ReLU(inplace=True)
    
    return model
```

### 国产GPU

**国产GPU支持**:

```yaml
国产GPU厂商:
  1. 天数智芯 (Iluvatar CoreX):
     - 架构: 自研GPU架构
     - 计算: FP32/FP16/INT8
     - 生态: PyTorch/TensorFlow适配
     - 应用: AI训练/推理
  
  2. 摩尔线程 (Moore Threads):
     - GPU: MTT S系列
     - 特点: 图形+计算
     - 软件栈: MUSA (类CUDA)
     - 应用: 视频处理/AI推理
  
  3. 壁仞科技 (Biren):
     - GPU: BR100系列
     - 算力: 超高性能
     - 特点: 云端AI芯片
  
  4. 海光DCU:
     - 架构: 兼容ROCm
     - 生态: HIP (类CUDA)
     - 应用: HPC/AI

软件适配:
  - PyTorch插件
  - TensorFlow后端
  - ONNX Runtime EP
  - 自研推理框架
```

**国产GPU使用示例 (天数智芯)**:

```python
# iluvatar_gpu_inference.py
import torch

# 检测天数智芯GPU
if torch.cuda.is_available():
    device_name = torch.cuda.get_device_name(0)
    print(f"GPU Device: {device_name}")
    
    # 如果是天数智芯GPU
    if 'Iluvatar' in device_name or 'CoreX' in device_name:
        print("Iluvatar GPU detected")
        
        # 使用方式与CUDA相同
        model = YourModel().cuda()
        input_data = torch.randn(1, 3, 224, 224).cuda()
        
        with torch.no_grad():
            output = model(input_data)
        
        print(f"Output shape: {output.shape}")
```

### NPU加速器

**NPU概述**:

```yaml
神经网络处理单元 (NPU):
  - 专用AI芯片
  - 低功耗高效率
  - 移动/边缘设备

主流NPU:
  1. 华为昇腾 (Ascend):
     - 310: 推理 (边缘)
     - 910: 训练 (云端)
     - CANN工具链
  
  2. 寒武纪 (Cambricon):
     - MLU系列
     - Bang语言
     - 适用边缘AI
  
  3. 地平线 (Horizon):
     - 征程系列
     - 自动驾驶专用
     - BPU架构
  
  4. Qualcomm NPU:
     - 移动端
     - Android设备
     - QNN框架
```

**华为昇腾NPU使用**:

```python
# ascend_npu_inference.py
# 需要安装: pip install torch-npu

import torch
import torch_npu

# 检测昇腾NPU
if torch_npu.npu.is_available():
    device = 'npu:0'
    print(f"Ascend NPU available")
    print(f"Device count: {torch_npu.npu.device_count()}")
    
    # 模型推理
    model = YourModel().to(device)
    input_data = torch.randn(1, 3, 224, 224).to(device)
    
    model.eval()
    with torch.no_grad():
        output = model(input_data)
    
    print(f"Output: {output.shape}")
```

### ARM CPU优化

**ARM NEON优化**:

```yaml
ARM NEON:
  - ARM的SIMD指令集
  - 向量化计算
  - 移动设备标配

优化策略:
  1. 使用优化库:
     - ARM Compute Library
     - XNNPACK
     - QNNPACK
  
  2. 编译器优化:
     - -O3 -march=native
     - -mfpu=neon
     - -ftree-vectorize
  
  3. 手写NEON汇编 (极致性能)
```

**ARM优化实践**:

```python
# arm_optimization.py
import numpy as np

def optimize_for_arm():
    """ARM CPU优化配置"""
    # 1. NumPy使用ARM优化BLAS
    # 编译numpy时链接OpenBLAS (ARM优化版本)
    
    # 2. PyTorch ARM优化
    torch.set_num_threads(4)  # 根据CPU核数
    
    # 3. ONNX Runtime ARM优化
    session_options = onnxruntime.SessionOptions()
    session_options.intra_op_num_threads = 4
    session_options.execution_mode = onnxruntime.ExecutionMode.ORT_SEQUENTIAL
    
    # 4. TFLite使用XNNPACK
    interpreter = tf.lite.Interpreter(
        model_path='model.tflite',
        num_threads=4
    )
    interpreter.allocate_tensors()

# Raspberry Pi 4 优化示例
def optimize_for_raspberry_pi():
    """树莓派4优化"""
    # CPU: ARM Cortex-A72 (4核)
    # 内存: 2GB/4GB/8GB
    
    # 推荐配置
    config = {
        'num_threads': 4,
        'use_xnnpack': True,
        'batch_size': 1,
        'input_size': 320,  # 减小输入尺寸
        'model_precision': 'INT8',  # 使用量化模型
    }
    
    return config
```

---

## 实战案例

### YOLOv8目标检测

**YOLOv8边缘部署**:

```python
# yolov8_edge_deployment.py
from ultralytics import YOLO
import cv2
import numpy as np
import time

class YOLOv8EdgeInference:
    """YOLOv8边缘推理"""
    
    def __init__(self, model_path='yolov8n.pt', device='cuda'):
        """
        model_path: 模型路径
          - yolov8n.pt: Nano (最小最快)
          - yolov8s.pt: Small
          - yolov8m.pt: Medium
          - yolov8l.pt: Large
          - yolov8x.pt: XLarge
        """
        self.model = YOLO(model_path)
        self.device = device
        
        # 导出为优化格式
        self.export_model()
    
    def export_model(self):
        """导出优化模型"""
        # 1. 导出ONNX
        self.model.export(format='onnx', dynamic=True, simplify=True)
        
        # 2. 导出TensorRT (NVIDIA GPU)
        if self.device == 'cuda':
            self.model.export(format='engine', half=True)  # FP16
        
        # 3. 导出TFLite (移动端)
        # self.model.export(format='tflite', int8=True)
    
    def infer_image(self, image_path, conf=0.25, iou=0.45):
        """单图推理"""
        results = self.model.predict(
            image_path,
            conf=conf,
            iou=iou,
            device=self.device,
            verbose=False
        )
        
        return results[0]
    
    def infer_video(self, video_path, output_path='output.mp4'):
        """视频推理"""
        cap = cv2.VideoCapture(video_path)
        
        # 视频参数
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # 输出视频
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        frame_count = 0
        total_time = 0
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            # 推理
            start = time.time()
            results = self.model.predict(frame, device=self.device, verbose=False)[0]
            elapsed = time.time() - start
            
            # 绘制结果
            annotated_frame = results.plot()
            
            # 显示FPS
            cv2.putText(
                annotated_frame,
                f"FPS: {1/elapsed:.1f}",
                (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX,
                1,
                (0, 255, 0),
                2
            )
            
            out.write(annotated_frame)
            
            frame_count += 1
            total_time += elapsed
            
            if frame_count % 30 == 0:
                avg_fps = frame_count / total_time
                print(f"Processed {frame_count} frames, Avg FPS: {avg_fps:.1f}")
        
        cap.release()
        out.release()
        
        avg_fps = frame_count / total_time
        print(f"\nTotal frames: {frame_count}")
        print(f"Average FPS: {avg_fps:.1f}")
    
    def infer_realtime(self, camera_id=0):
        """实时摄像头推理"""
        cap = cv2.VideoCapture(camera_id)
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # 推理
            start = time.time()
            results = self.model.predict(frame, device=self.device, verbose=False)[0]
            elapsed = time.time() - start
            
            # 绘制
            annotated_frame = results.plot()
            
            # 显示FPS
            fps = 1 / elapsed
            cv2.putText(
                annotated_frame,
                f"FPS: {fps:.1f}",
                (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX,
                1,
                (0, 255, 0),
                2
            )
            
            cv2.imshow('YOLOv8 Real-time', annotated_frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()

# 使用示例
if __name__ == '__main__':
    # 初始化
    yolo = YOLOv8EdgeInference('yolov8n.pt', device='cuda')
    
    # 图像推理
    result = yolo.infer_image('test.jpg')
    print(f"Detected {len(result.boxes)} objects")
    
    # 视频推理
    yolo.infer_video('input.mp4', 'output.mp4')
    
    # 实时推理
    # yolo.infer_realtime(camera_id=0)
```

**性能基准测试**:

```python
# yolov8_benchmark.py
import torch
from ultralytics import YOLO
import time
import numpy as np

def benchmark_yolov8():
    """YOLOv8性能测试"""
    models = ['yolov8n', 'yolov8s', 'yolov8m']
    devices = ['cpu', 'cuda']
    input_sizes = [320, 480, 640]
    
    results = []
    
    for model_name in models:
        model = YOLO(f'{model_name}.pt')
        
        for device in devices:
            if device == 'cuda' and not torch.cuda.is_available():
                continue
            
            for size in input_sizes:
                # 生成假数据
                dummy_image = np.random.randint(0, 255, (size, size, 3), dtype=np.uint8)
                
                # Warmup
                for _ in range(10):
                    _ = model.predict(dummy_image, device=device, verbose=False)
                
                # Benchmark
                iterations = 100
                start = time.time()
                
                for _ in range(iterations):
                    _ = model.predict(dummy_image, device=device, verbose=False)
                
                elapsed = time.time() - start
                avg_time = (elapsed / iterations) * 1000  # ms
                fps = iterations / elapsed
                
                result = {
                    'model': model_name,
                    'device': device,
                    'input_size': size,
                    'avg_time_ms': avg_time,
                    'fps': fps
                }
                
                results.append(result)
                print(f"{model_name} | {device} | {size}x{size} | {avg_time:.2f}ms | {fps:.1f} FPS")
    
    return results

# 运行测试
results = benchmark_yolov8()
```

### 人脸识别

**人脸识别边缘部署**:

```python
# face_recognition_edge.py
import cv2
import numpy as np
from facenet_pytorch import MTCNN, InceptionResnetV1
import torch

class EdgeFaceRecognition:
    """边缘人脸识别系统"""
    
    def __init__(self, device='cuda'):
        self.device = device
        
        # 1. 人脸检测 (MTCNN)
        self.mtcnn = MTCNN(
            image_size=160,
            margin=0,
            min_face_size=20,
            thresholds=[0.6, 0.7, 0.7],
            factor=0.709,
            post_process=True,
            device=device
        )
        
        # 2. 人脸识别 (FaceNet)
        self.facenet = InceptionResnetV1(pretrained='vggface2').eval().to(device)
        
        # 3. 人脸库
        self.face_database = {}
        
        # 4. 导出优化模型 (可选)
        self.export_onnx()
    
    def export_onnx(self):
        """导出ONNX模型"""
        dummy_input = torch.randn(1, 3, 160, 160).to(self.device)
        
        torch.onnx.export(
            self.facenet,
            dummy_input,
            'facenet.onnx',
            input_names=['input'],
            output_names=['embedding'],
            dynamic_axes={'input': {0: 'batch'}, 'embedding': {0: 'batch'}}
        )
        print("FaceNet exported to ONNX")
    
    def register_face(self, image, name):
        """注册人脸到数据库"""
        # 检测人脸
        boxes, _ = self.mtcnn.detect(image)
        
        if boxes is None:
            print(f"No face detected for {name}")
            return False
        
        # 提取人脸
        face = self.mtcnn(image)
        
        if face is None:
            return False
        
        # 生成embedding
        with torch.no_grad():
            embedding = self.facenet(face.unsqueeze(0).to(self.device))
        
        # 保存到数据库
        self.face_database[name] = embedding.cpu().numpy()
        
        print(f"Registered face: {name}")
        return True
    
    def recognize_face(self, image, threshold=0.6):
        """识别人脸"""
        # 检测人脸
        boxes, _ = self.mtcnn.detect(image)
        
        if boxes is None:
            return None, None
        
        # 提取人脸
        face = self.mtcnn(image)
        
        if face is None:
            return None, None
        
        # 生成embedding
        with torch.no_grad():
            embedding = self.facenet(face.unsqueeze(0).to(self.device))
            embedding = embedding.cpu().numpy()
        
        # 与数据库比对
        best_match = None
        best_distance = float('inf')
        
        for name, db_embedding in self.face_database.items():
            # 计算欧氏距离
            distance = np.linalg.norm(embedding - db_embedding)
            
            if distance < best_distance:
                best_distance = distance
                best_match = name
        
        # 判断是否匹配
        if best_distance < threshold:
            return best_match, best_distance
        else:
            return "Unknown", best_distance
    
    def process_video(self, video_path):
        """处理视频流"""
        cap = cv2.VideoCapture(video_path)
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            # 转换为RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # 识别
            start = time.time()
            name, distance = self.recognize_face(frame_rgb)
            elapsed = (time.time() - start) * 1000
            
            # 显示结果
            if name:
                text = f"{name} ({distance:.2f})"
                cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            
            cv2.putText(frame, f"{elapsed:.1f}ms", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
            
            cv2.imshow('Face Recognition', frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()

# 使用示例
if __name__ == '__main__':
    # 初始化
    face_rec = EdgeFaceRecognition(device='cuda')
    
    # 注册人脸
    person1 = cv2.imread('person1.jpg')
    face_rec.register_face(person1, 'Alice')
    
    person2 = cv2.imread('person2.jpg')
    face_rec.register_face(person2, 'Bob')
    
    # 识别
    test_image = cv2.imread('test.jpg')
    name, distance = face_rec.recognize_face(test_image)
    print(f"Recognized: {name} (distance: {distance:.2f})")
```

### 语音识别

**Whisper语音识别边缘部署**:

```python
# whisper_edge_asr.py
import whisper
import torch
import numpy as np

class EdgeASR:
    """边缘语音识别"""
    
    def __init__(self, model_size='base', device='cuda'):
        """
        model_size选项:
        - tiny: 39M params (最快)
        - base: 74M params
        - small: 244M params
        - medium: 769M params
        - large: 1550M params (最准确)
        """
        self.device = device
        self.model = whisper.load_model(model_size, device=device)
        
        print(f"Whisper {model_size} loaded on {device}")
    
    def transcribe(self, audio_path, language='en'):
        """转录音频"""
        result = self.model.transcribe(
            audio_path,
            language=language,
            fp16=(self.device == 'cuda')  # GPU使用FP16
        )
        
        return result['text']
    
    def transcribe_realtime(self, audio_stream):
        """实时转录 (流式)"""
        # Whisper不支持流式，需要分段处理
        segment_length = 30  # 30秒段
        
        # 实现细节略...
        pass
    
    def export_onnx(self, output_path='whisper.onnx'):
        """导出ONNX (用于进一步优化)"""
        # Whisper导出较复杂，需要分别导出encoder和decoder
        # 实现细节略...
        pass

# 使用示例
if __name__ == '__main__':
    asr = EdgeASR(model_size='base', device='cuda')
    
    # 转录
    text = asr.transcribe('audio.mp3', language='en')
    print(f"Transcription: {text}")
```

---

## 边缘大模型

### 模型压缩

**大模型压缩技术**:

```yaml
压缩方法:
  1. 量化:
     - INT8: 标准
     - INT4/INT3: 激进
     - GPTQ/AWQ: 大模型专用量化
  
  2. 剪枝:
     - 结构化剪枝
     - 非结构化剪枝
     - 动态剪枝
  
  3. 蒸馏:
     - DistilBERT (BERT → 小BERT)
     - TinyBERT
     - MobileBERT
  
  4. 低秩分解:
     - LoRA (Low-Rank Adaptation)
     - 分解大矩阵为小矩阵乘积
```

**GPTQ量化 (LLM)**:

```python
# gptq_quantization.py
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def quantize_llm_gptq(model_name, bits=4):
    """
    GPTQ量化大语言模型
    bits: 4 or 8
    """
    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
    
    # 量化配置
    quantize_config = BaseQuantizeConfig(
        bits=bits,
        group_size=128,
        desc_act=False,
    )
    
    # 加载模型
    model = AutoGPTQForCausalLM.from_pretrained(
        model_name,
        quantize_config=quantize_config
    )
    
    # 量化 (需要校准数据)
    # model.quantize(calibration_dataset)
    
    # 保存量化模型
    model.save_quantized(f"{model_name}-gptq-{bits}bit")
    
    return model

# 使用
# quantized_model = quantize_llm_gptq('llama-7b', bits=4)
```

### 分布式推理

**模型并行推理**:

```python
# distributed_inference.py
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

class DistributedInference:
    """分布式推理 (多GPU/多节点)"""
    
    def __init__(self, model, world_size=2):
        # 初始化分布式环境
        dist.init_process_group(backend='nccl', world_size=world_size)
        
        self.rank = dist.get_rank()
        self.device = torch.device(f'cuda:{self.rank}')
        
        # 模型分布
        self.model = model.to(self.device)
        self.model = DDP(self.model, device_ids=[self.rank])
    
    def infer(self, input_data):
        """分布式推理"""
        input_data = input_data.to(self.device)
        
        with torch.no_grad():
            output = self.model(input_data)
        
        # 收集所有GPU结果
        gathered_outputs = [torch.zeros_like(output) for _ in range(dist.get_world_size())]
        dist.all_gather(gathered_outputs, output)
        
        return torch.cat(gathered_outputs)

# 启动多进程推理
# torchrun --nproc_per_node=2 distributed_inference.py
```

### LLM边缘部署

**Llama.cpp边缘部署**:

```bash
# 1. 克隆llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# 2. 编译
make

# 3. 转换模型为GGUF格式
python convert.py path/to/llama-model --outtype f16

# 4. 量化模型
./quantize llama-7b-f16.gguf llama-7b-q4_0.gguf q4_0

# 5. 运行推理
./main -m llama-7b-q4_0.gguf -p "Hello, how are you?" -n 128
```

**Python调用llama.cpp**:

```python
# llama_cpp_python.py
from llama_cpp import Llama

class EdgeLLM:
    """边缘大语言模型"""
    
    def __init__(self, model_path, n_ctx=2048, n_gpu_layers=0):
        """
        n_ctx: 上下文长度
        n_gpu_layers: 使用GPU的层数 (0=纯CPU)
        """
        self.llm = Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_batch=512,
            n_gpu_layers=n_gpu_layers,  # 根据GPU内存调整
            use_mlock=True,  # 锁定内存
            verbose=False
        )
    
    def generate(self, prompt, max_tokens=256, temperature=0.7):
        """生成文本"""
        output = self.llm(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=0.95,
            top_k=40,
            repeat_penalty=1.1,
            stop=["</s>", "\n\n"]
        )
        
        return output['choices'][0]['text']
    
    def chat(self, messages, max_tokens=256):
        """对话模式"""
        output = self.llm.create_chat_completion(
            messages=messages,
            max_tokens=max_tokens,
            temperature=0.7
        )
        
        return output['choices'][0]['message']['content']

# 使用示例
if __name__ == '__main__':
    # 加载量化模型
    llm = EdgeLLM(
        model_path='llama-7b-q4_0.gguf',
        n_gpu_layers=32  # 部分层用GPU
    )
    
    # 生成
    prompt = "Explain edge computing in simple terms:"
    response = llm.generate(prompt, max_tokens=256)
    print(response)
    
    # 对话
    messages = [
        {"role": "system", "content": "You are a helpful AI assistant."},
        {"role": "user", "content": "What is edge AI?"}
    ]
    response = llm.chat(messages)
    print(response)
```

---

## 性能优化

### 批处理优化

**动态批处理**:

```python
# dynamic_batching.py
import torch
import asyncio
from collections import deque
import time

class DynamicBatcher:
    """动态批处理推理"""
    
    def __init__(self, model, max_batch_size=16, max_wait_ms=10):
        self.model = model
        self.max_batch_size = max_batch_size
        self.max_wait_ms = max_wait_ms / 1000  # 转为秒
        
        self.request_queue = deque()
        self.processing = False
    
    async def infer(self, input_data):
        """异步推理请求"""
        # 创建Future用于异步返回
        future = asyncio.Future()
        
        # 加入队列
        self.request_queue.append((input_data, future))
        
        # 启动批处理 (如果未运行)
        if not self.processing:
            asyncio.create_task(self._process_batch())
        
        # 等待结果
        return await future
    
    async def _process_batch(self):
        """处理一批请求"""
        self.processing = True
        
        await asyncio.sleep(self.max_wait_ms)  # 等待更多请求
        
        # 收集批次
        batch_inputs = []
        batch_futures = []
        
        while len(batch_inputs) < self.max_batch_size and self.request_queue:
            input_data, future = self.request_queue.popleft()
            batch_inputs.append(input_data)
            batch_futures.append(future)
        
        if not batch_inputs:
            self.processing = False
            return
        
        # 批量推理
        batch_tensor = torch.stack(batch_inputs)
        
        with torch.no_grad():
            batch_outputs = self.model(batch_tensor)
        
        # 分发结果
        for i, future in enumerate(batch_futures):
            future.set_result(batch_outputs[i])
        
        self.processing = False
        
        # 如果还有请求，继续处理
        if self.request_queue:
            asyncio.create_task(self._process_batch())

# 使用示例
async def main():
    model = YourModel()
    batcher = DynamicBatcher(model, max_batch_size=8, max_wait_ms=10)
    
    # 并发请求
    tasks = []
    for i in range(20):
        input_data = torch.randn(3, 224, 224)
        task = batcher.infer(input_data)
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    print(f"Processed {len(results)} requests")

# asyncio.run(main())
```

### 内存优化

**内存高效推理**:

```python
# memory_optimization.py
import torch
from torch.utils.checkpoint import checkpoint

class MemoryEfficientModel:
    """内存优化推理"""
    
    @staticmethod
    def use_gradient_checkpointing(model):
        """Gradient Checkpointing (训练时节省内存)"""
        # 仅在训练时有效
        if hasattr(model, 'gradient_checkpointing_enable'):
            model.gradient_checkpointing_enable()
        return model
    
    @staticmethod
    def use_8bit_inference(model):
        """8-bit推理 (减少内存占用)"""
        # 需要: pip install bitsandbytes
        from bitsandbytes import Linear8bitLt
        
        # 替换Linear层
        for name, module in model.named_children():
            if isinstance(module, torch.nn.Linear):
                setattr(model, name, Linear8bitLt(
                    module.in_features,
                    module.out_features,
                    module.bias is not None
                ))
        
        return model
    
    @staticmethod
    def optimize_kv_cache(max_batch_size=1, max_seq_len=2048):
        """优化KV Cache (LLM)"""
        # 预分配KV Cache内存
        kv_cache_shape = (max_batch_size, num_heads, max_seq_len, head_dim)
        
        k_cache = torch.zeros(kv_cache_shape, dtype=torch.float16, device='cuda')
        v_cache = torch.zeros(kv_cache_shape, dtype=torch.float16, device='cuda')
        
        return k_cache, v_cache
    
    @staticmethod
    def use_model_offloading(model):
        """模型卸载 (CPU ↔ GPU)"""
        from accelerate import load_checkpoint_and_dispatch
        
        # 自动在CPU/GPU间调度层
        model = load_checkpoint_and_dispatch(
            model,
            checkpoint=model_path,
            device_map="auto",  # 自动分配
            no_split_module_classes=["Block"]
        )
        
        return model

# 清理GPU内存
def clear_gpu_memory():
    """强制清理GPU内存"""
    import gc
    gc.collect()
    torch.cuda.empty_cache()
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
```

### 延迟优化

**推理延迟分析**:

```python
# latency_profiling.py
import torch
import time
import numpy as np

class LatencyProfiler:
    """推理延迟分析"""
    
    @staticmethod
    def profile_model(model, input_shape, iterations=100):
        """分析模型各部分延迟"""
        device = next(model.parameters()).device
        dummy_input = torch.randn(*input_shape).to(device)
        
        # Warmup
        for _ in range(10):
            _ = model(dummy_input)
        
        # 整体延迟
        torch.cuda.synchronize() if device.type == 'cuda' else None
        start = time.time()
        
        for _ in range(iterations):
            _ = model(dummy_input)
        
        torch.cuda.synchronize() if device.type == 'cuda' else None
        total_time = (time.time() - start) / iterations * 1000
        
        # 逐层分析
        layer_times = {}
        
        for name, module in model.named_children():
            # 钩子记录时间
            start_time = [0]
            end_time = [0]
            
            def forward_pre_hook(module, input):
                torch.cuda.synchronize() if device.type == 'cuda' else None
                start_time[0] = time.time()
            
            def forward_hook(module, input, output):
                torch.cuda.synchronize() if device.type == 'cuda' else None
                end_time[0] = time.time()
            
            handle_pre = module.register_forward_pre_hook(forward_pre_hook)
            handle = module.register_forward_hook(forward_hook)
            
            # 测试
            _ = model(dummy_input)
            
            layer_times[name] = (end_time[0] - start_time[0]) * 1000
            
            # 移除钩子
            handle_pre.remove()
            handle.remove()
        
        # 打印结果
        print(f"\n{'Layer':<30} {'Time (ms)':<15} {'Percentage':<10}")
        print("=" * 55)
        
        for name, t in sorted(layer_times.items(), key=lambda x: x[1], reverse=True):
            percentage = (t / total_time) * 100
            print(f"{name:<30} {t:<15.2f} {percentage:<10.1f}%")
        
        print(f"\nTotal time: {total_time:.2f} ms")
        
        return layer_times
    
    @staticmethod
    def analyze_bottleneck(layer_times):
        """分析瓶颈"""
        total = sum(layer_times.values())
        
        # 找出占比>10%的层
        bottlenecks = {
            name: time
            for name, time in layer_times.items()
            if time / total > 0.1
        }
        
        if bottlenecks:
            print("\nBottlenecks (>10% of total time):")
            for name, time in bottlenecks.items():
                print(f"  - {name}: {time:.2f} ms ({time/total*100:.1f}%)")
        
        return bottlenecks

# 使用
profiler = LatencyProfiler()
layer_times = profiler.profile_model(model, (1, 3, 224, 224))
profiler.analyze_bottleneck(layer_times)
```

---

## 参考资料

### 推理框架文档

**官方文档**:

- [TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/)
- [ONNX Runtime](https://onnxruntime.ai/docs/)
- [OpenVINO Toolkit](https://docs.openvino.ai/)
- [TensorFlow Lite](https://www.tensorflow.org/lite)
- [NCNN](https://github.com/Tencent/ncnn/wiki)

### 模型优化

**量化与剪枝**:

- [PyTorch Quantization](https://pytorch.org/docs/stable/quantization.html)
- [TensorFlow Model Optimization](https://www.tensorflow.org/model_optimization)
- [Neural Network Distiller](https://intellabs.github.io/distiller/)

**模型压缩**:

- [GPTQ](https://github.com/IST-DASLab/gptq)
- [AWQ](https://github.com/mit-han-lab/llm-awq)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)

### 硬件优化

**GPU优化**:

- [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit)
- [NVIDIA cuDNN](https://developer.nvidia.com/cudnn)
- [PyTorch Performance Tuning](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)

**国产GPU**:

- [天数智芯](https://www.iluvatar.com/)
- [摩尔线程](https://www.mthreads.com/)
- [华为昇腾](https://www.hiascend.com/)

### 学习资源

**在线课程**:

- [Stanford CS231n: Deep Learning for Computer Vision](http://cs231n.stanford.edu/)
- [Deep Learning Specialization (Andrew Ng)](https://www.deeplearning.ai/)
- [TinyML Specialization](https://www.edx.org/professional-certificate/harvardx-tiny-machine-learning)

**论文**:

- [EfficientNet](https://arxiv.org/abs/1905.11946)
- [MobileNetV3](https://arxiv.org/abs/1905.02244)
- [YOLOv8](https://github.com/ultralytics/ultralytics)
- [Whisper](https://arxiv.org/abs/2212.04356)

---

**文档版本**: v1.0  
**最后更新**: 2025-10-19  
**维护者**: 虚拟化容器化技术知识库项目组

**下一步阅读**:

- [05_边缘存储与数据管理](./05_边缘存储与数据管理.md)
- [07_边缘网络与通信](./07_边缘网络与通信.md)
- [08_边缘安全与运维](./08_边缘安全与运维.md)
