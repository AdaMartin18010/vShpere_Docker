# 08_边缘安全与运维

## 目录

- [08_边缘安全与运维](#08_边缘安全与运维)

---

## 边缘安全概述

### 边缘安全挑战

```yaml
物理安全:
  - 设备分散 (难以集中保护)
  - 环境恶劣 (工业/户外)
  - 物理访问风险
  - 设备被盗/篡改

网络安全:
  - 边缘节点暴露
  - 不可信网络环境
  - DDoS攻击面大
  - 中间人攻击

数据安全:
  - 敏感数据本地存储
  - 数据传输加密
  - 数据泄露风险
  - 合规要求 (GDPR/CCPA)

身份认证:
  - 设备身份管理
  - 用户认证
  - 服务间认证
  - 证书管理

运维安全:
  - 远程运维风险
  - 补丁更新困难
  - 配置漂移
  - 审计追踪
```

### 安全防护体系

```yaml
纵深防御:
  物理层:
    - TPM (Trusted Platform Module)
    - Secure Boot
    - 物理防篡改

  网络层:
    - 零信任网络
    - mTLS双向认证
    - 微分段 (Micro-segmentation)
    - 网络策略

  主机层:
    - 最小化OS (Hardened)
    - SELinux/AppArmor
    - 内核安全 (Seccomp/eBPF)
    - 入侵检测

  应用层:
    - 容器安全
    - 镜像扫描
    - 运行时保护
    - SBOM (Software Bill of Materials)

  数据层:
    - 加密存储
    - 加密传输
    - 机密计算 (TEE)
    - 密钥管理
```

---

## 零信任架构

### SPIFFE/SPIRE

```yaml
SPIFFE:
  - Secure Production Identity Framework For Everyone
  - 工作负载身份标准
  - 平台无关
  - 自动身份轮换

SPIRE:
  - SPIFFE实现
  - 身份发放
  - 证书自动化
  - 工作负载认证

架构:
  SPIRE Server (控制平面):
    - 身份注册
    - 证书颁发
    - 策略管理

  SPIRE Agent (数据平面):
    - 节点认证
    - 工作负载发现
    - SVID (SPIFFE Verifiable Identity Document) 获取
```

#### SPIRE部署

```bash
# Kubernetes部署SPIRE

# 1. 安装SPIRE Server
kubectl apply -f https://github.com/spiffe/spire-tutorials/raw/main/k8s/quickstart/server-statefulset.yaml
kubectl apply -f https://github.com/spiffe/spire-tutorials/raw/main/k8s/quickstart/server-service.yaml
kubectl apply -f https://github.com/spiffe/spire-tutorials/raw/main/k8s/quickstart/server-configmap.yaml

# 2. 安装SPIRE Agent (DaemonSet)
kubectl apply -f https://github.com/spiffe/spire-tutorials/raw/main/k8s/quickstart/agent-daemonset.yaml
kubectl apply -f https://github.com/spiffe/spire-tutorials/raw/main/k8s/quickstart/agent-configmap.yaml

# 3. 创建工作负载注册条目
kubectl exec -n spire spire-server-0 -- \
  /opt/spire/bin/spire-server entry create \
  -spiffeID spiffe://example.org/my-workload \
  -parentID spiffe://example.org/k8s-node \
  -selector k8s:ns:default \
  -selector k8s:pod-label:app:my-app

# 4. 验证
kubectl exec -n spire spire-server-0 -- \
  /opt/spire/bin/spire-server entry show
```

#### SPIFFE集成示例

```python
# spiffe_client.py
from spiffe import WorkloadClient
import grpc
from grpc import ssl_channel_credentials

class SecureEdgeClient:
    """使用SPIFFE身份的安全客户端"""

    def __init__(self, spiffe_socket='/tmp/spire-agent/public/api.sock'):
        # 连接SPIRE Agent
        self.workload_client = WorkloadClient(spiffe_socket)

        # 获取SVID (X.509证书)
        self.x509_context = self.workload_client.fetch_x509_context()
        print(f"SPIFFE ID: {self.x509_context.default_svid.spiffe_id}")

    def connect_to_service(self, target, service_spiffe_id):
        """使用mTLS连接到服务"""
        # 获取证书和私钥
        cert_chain = self.x509_context.default_svid.cert_chain_pem
        private_key = self.x509_context.default_svid.private_key_pem
        trust_bundle = self.x509_context.default_bundle.x509_bundle_pem

        # 创建mTLS credentials
        credentials = ssl_channel_credentials(
            root_certificates=trust_bundle,
            private_key=private_key,
            certificate_chain=cert_chain
        )

        # 连接
        channel = grpc.secure_channel(target, credentials)

        # 验证对端SPIFFE ID
        # (实际实现需要gRPC拦截器)

        return channel

    def refresh_identity(self):
        """刷新SVID (自动轮换)"""
        self.x509_context = self.workload_client.fetch_x509_context()
        print("Identity refreshed")

# 使用示例
if __name__ == '__main__':
    client = SecureEdgeClient()

    # 连接到后端服务
    channel = client.connect_to_service(
        target='backend.example.com:443',
        service_spiffe_id='spiffe://example.org/backend'
    )

    # 使用channel进行gRPC调用
    # stub = MyServiceStub(channel)
    # response = stub.MyMethod(request)
```

### 零信任网络策略

```yaml
# zero-trust-policy.yaml
# Cilium零信任网络策略

apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: zero-trust-default-deny
  namespace: default
spec:
  endpointSelector: {}

  # 默认拒绝所有入站流量
  ingress:
  - {}

  # 默认拒绝所有出站流量
  egress:
  - {}

---
# 允许frontend到backend的流量
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-frontend-to-backend
spec:
  endpointSelector:
    matchLabels:
      app: backend

  ingress:
  - fromEndpoints:
    - matchLabels:
        app: frontend
    toPorts:
    - ports:
      - port: "8080"
        protocol: TCP
      rules:
        http:
        - method: GET
          path: /api/.*
        - method: POST
          path: /api/.*

---
# 允许backend到数据库的流量
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-backend-to-db
spec:
  endpointSelector:
    matchLabels:
      app: backend

  egress:
  - toEndpoints:
    - matchLabels:
        app: postgres
    toPorts:
    - ports:
      - port: "5432"
        protocol: TCP

---
# 允许所有Pod访问DNS
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: allow-dns
spec:
  endpointSelector: {}

  egress:
  - toEndpoints:
    - matchLabels:
        k8s:io.kubernetes.pod.namespace: kube-system
        k8s-app: kube-dns
    toPorts:
    - ports:
      - port: "53"
        protocol: UDP
      rules:
        dns:
        - matchPattern: "*"
```

---

## 机密计算

### Intel TDX

```yaml
Intel TDX (Trust Domain Extensions):
  - 基于硬件的可信执行环境 (TEE)
  - 虚拟机级别隔离
  - 内存加密 (MKTME)
  - 远程证明
  - 支持: 第四代Intel Xeon (Sapphire Rapids+)

工作原理:
  1. TDX Module (SEAM模块)
  2. Trust Domain (TD) - 隔离的VM
  3. 内存加密 (每个TD独立密钥)
  4. CPU状态加密
  5. 远程证明 (Attestation)

应用场景:
  - 多租户云环境
  - 敏感数据处理
  - 金融计算
  - 医疗数据分析
```

#### TDX虚拟机部署

```yaml
# tdx-vm.yaml
# Kubernetes + KubeVirt 部署TDX虚拟机

apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: tdx-confidential-vm
spec:
  running: true
  template:
    metadata:
      labels:
        kubevirt.io/domain: tdx-vm
    spec:
      domain:
        cpu:
          cores: 4
          model: host-passthrough

        memory:
          guest: 8Gi

        devices:
          disks:
          - disk:
              bus: virtio
            name: rootdisk

          interfaces:
          - name: default
            masquerade: {}

        # Intel TDX配置
        features:
          tdx:
            enabled: true
            # 远程证明配置
            attestation:
              enabled: true
              quote_generation_service: "https://attestation.example.com"

        # UEFI Secure Boot
        firmware:
          bootloader:
            efi:
              secureBoot: true

      networks:
      - name: default
        pod: {}

      volumes:
      - name: rootdisk
        containerDisk:
          image: quay.io/kubevirt/fedora-cloud-container-disk-demo:latest
```

### AMD SEV-SNP

```yaml
AMD SEV-SNP (Secure Encrypted Virtualization - Secure Nested Paging):
  - VM内存加密
  - 完整性保护
  - 重放攻击防护
  - 远程证明
  - 支持: AMD EPYC 第三代 (Milan+)

特性:
  - 每个VM独立加密密钥
  - 页表完整性校验
  - RMP (Reverse Map Table)
  - 无需Hypervisor信任

工作流程:
  1. VM启动时生成唯一密钥
  2. 内存加密 (ASID独立)
  3. 完整性度量 (VMSA/VMCB)
  4. 远程证明 (Platform Security Processor)
```

#### SEV-SNP QEMU配置

```bash
# QEMU启动SEV-SNP虚拟机

qemu-system-x86_64 \
  -enable-kvm \
  -cpu EPYC-v4 \
  -machine q35,memory-encryption=sev0,vmport=off \
  -object sev-snp-guest,id=sev0,cbitpos=51,reduced-phys-bits=1,policy=0x30000 \
  -object memory-backend-memfd-private,id=ram1,size=4G,share=true \
  -machine memory-backend=ram1 \
  -smp 4 \
  -m 4G \
  -drive if=pflash,format=raw,unit=0,file=/usr/share/OVMF/OVMF_CODE.fd,readonly=on \
  -drive if=pflash,format=raw,unit=1,file=OVMF_VARS.fd \
  -drive file=ubuntu-22.04.img,if=none,id=disk0,format=qcow2 \
  -device virtio-scsi-pci,id=scsi0 \
  -device scsi-hd,drive=disk0 \
  -netdev user,id=vmnic \
  -device virtio-net-pci,netdev=vmnic \
  -vnc :0

# 远程证明
sev-guest-get-report \
  --firmware /dev/sev-guest \
  --vmpl 1 \
  --nonce "random-nonce" \
  --output attestation-report.bin
```

### Confidential Containers (CoCo)

```yaml
CoCo项目:
  - CNCF沙箱项目
  - 容器级机密计算
  - 支持多种TEE (TDX/SEV/SGX/TrustZone)
  - Kubernetes原生集成

组件:
  - Kata Containers (隔离运行时)
  - Attestation Agent
  - Key Broker Service (KBS)
  - Image Encryption

工作流程:
  1. 容器镜像加密
  2. Pod调度到TEE节点
  3. 远程证明 (Attestation)
  4. 密钥获取 (KBS)
  5. 镜像解密
  6. 在TEE中运行
```

#### CoCo部署

```bash
# Kubernetes部署Confidential Containers

# 1. 安装Kata Containers Runtime
kubectl apply -f https://raw.githubusercontent.com/kata-containers/kata-containers/main/tools/packaging/kata-deploy/kata-rbac/base/kata-rbac.yaml
kubectl apply -f https://raw.githubusercontent.com/kata-containers/kata-containers/main/tools/packaging/kata-deploy/kata-deploy/base/kata-deploy.yaml

# 2. 安装CoCo Operator
kubectl apply -f https://github.com/confidential-containers/operator/releases/latest/download/confidential-containers-operator.yaml

# 3. 创建RuntimeClass
kubectl apply -f - << EOF
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: kata-qemu-sev
handler: kata-qemu-sev
overhead:
  podFixed:
    memory: "512Mi"
    cpu: "500m"
EOF

# 4. 部署加密镜像
kubectl apply -f - << EOF
apiVersion: v1
kind: Pod
metadata:
  name: confidential-pod
spec:
  runtimeClassName: kata-qemu-sev
  containers:
  - name: app
    image: ghcr.io/confidential-containers/encrypted-image:latest
    env:
    - name: AA_KBC_PARAMS
      value: "cc_kbc::https://kbs.example.com:8080"
EOF
```

#### 镜像加密工具

```python
# encrypt_image.py
import subprocess
import json
import base64
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.backends import default_backend
import os

class ImageEncryptor:
    """容器镜像加密工具"""

    def __init__(self, kbs_url):
        self.kbs_url = kbs_url

    def generate_key(self):
        """生成加密密钥"""
        key = os.urandom(32)  # AES-256
        key_id = base64.b64encode(os.urandom(16)).decode('utf-8')
        return key, key_id

    def upload_key_to_kbs(self, key, key_id):
        """上传密钥到Key Broker Service"""
        import requests

        payload = {
            'key_id': key_id,
            'key': base64.b64encode(key).decode('utf-8'),
            'metadata': {
                'algorithm': 'AES-256-GCM',
                'created_at': '2025-10-19'
            }
        }

        response = requests.post(
            f"{self.kbs_url}/keys",
            json=payload,
            headers={'Content-Type': 'application/json'}
        )

        if response.status_code == 201:
            print(f"Key uploaded: {key_id}")
            return True
        else:
            print(f"Error: {response.status_code}")
            return False

    def encrypt_layer(self, layer_tar, key):
        """加密镜像层"""
        # 生成随机IV
        iv = os.urandom(12)  # GCM mode

        # 创建加密器
        cipher = Cipher(
            algorithms.AES(key),
            modes.GCM(iv),
            backend=default_backend()
        )
        encryptor = cipher.encryptor()

        # 读取并加密数据
        with open(layer_tar, 'rb') as f:
            plaintext = f.read()

        ciphertext = encryptor.update(plaintext) + encryptor.finalize()

        # 保存加密层
        encrypted_layer = layer_tar + '.enc'
        with open(encrypted_layer, 'wb') as f:
            f.write(iv)  # 前12字节: IV
            f.write(encryptor.tag)  # 接下来16字节: GCM Tag
            f.write(ciphertext)  # 剩余: 加密数据

        return encrypted_layer

    def encrypt_image(self, image_name, output_image):
        """加密整个镜像"""
        # 1. Pull镜像
        subprocess.run(['skopeo', 'copy', f'docker://{image_name}', f'oci:temp-image'], check=True)

        # 2. 生成密钥
        key, key_id = self.generate_key()

        # 3. 上传密钥到KBS
        if not self.upload_key_to_kbs(key, key_id):
            raise Exception("Failed to upload key")

        # 4. 加密镜像层
        # (使用ocicrypt或skopeo内置加密)
        subprocess.run([
            'skopeo', 'copy',
            '--encryption-key', f'provider:kbs:{self.kbs_url}:key:{key_id}',
            'oci:temp-image',
            f'docker://{output_image}'
        ], check=True)

        print(f"Image encrypted: {output_image}")
        print(f"Key ID: {key_id}")
        print(f"KBS: {self.kbs_url}")

# 使用示例
if __name__ == '__main__':
    encryptor = ImageEncryptor(kbs_url='https://kbs.example.com')
    encryptor.encrypt_image(
        image_name='nginx:latest',
        output_image='myregistry.io/encrypted/nginx:latest'
    )
```

---

## 供应链安全

### SBOM (Software Bill of Materials)

```yaml
SBOM重要性:
  - 软件成分清单
  - 依赖透明化
  - 漏洞追踪
  - 合规要求 (美国EO 14028)

格式:
  - SPDX (Linux Foundation)
  - CycloneDX (OWASP)

工具:
  - Syft (Anchore)
  - Tern (VMware)
  - sbom-tool (Microsoft)
```

#### 生成SBOM

```bash
# 使用Syft生成容器镜像SBOM

# 1. 安装Syft
curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin

# 2. 扫描Docker镜像
syft nginx:latest -o spdx-json > nginx-sbom.spdx.json
syft nginx:latest -o cyclonedx-json > nginx-sbom.cdx.json

# 3. 扫描文件系统
syft dir:/path/to/project -o spdx-json > project-sbom.spdx.json

# 4. 扫描Git仓库
syft github:kubernetes/kubernetes -o spdx-json > k8s-sbom.spdx.json

# 5. 查看SBOM内容
cat nginx-sbom.spdx.json | jq '.packages | length'
cat nginx-sbom.spdx.json | jq '.packages[] | select(.name == "openssl")'
```

#### Kubernetes自动生成SBOM

```yaml
# sbom-generator.yaml
# Kubernetes CronJob自动生成SBOM

apiVersion: batch/v1
kind: CronJob
metadata:
  name: sbom-generator
spec:
  schedule: "0 2 * * *"  # 每天凌晨2点
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: sbom-generator
          containers:
          - name: syft
            image: anchore/syft:latest
            command:
            - /bin/sh
            - -c
            - |
              # 获取所有运行中的镜像
              kubectl get pods --all-namespaces -o jsonpath='{range .items[*]}{.spec.containers[*].image}{"\n"}{end}' | sort -u > images.txt

              # 为每个镜像生成SBOM
              while read image; do
                echo "Generating SBOM for $image"
                syft "$image" -o spdx-json > "/sbom/$(echo $image | tr '/:' '_').spdx.json"
              done < images.txt

              # 上传到存储
              aws s3 sync /sbom/ s3://my-bucket/sbom/$(date +%Y-%m-%d)/

            volumeMounts:
            - name: sbom-output
              mountPath: /sbom

            env:
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: secret-access-key

          volumes:
          - name: sbom-output
            emptyDir: {}

          restartPolicy: OnFailure

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sbom-generator

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: sbom-generator
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: sbom-generator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: sbom-generator
subjects:
- kind: ServiceAccount
  name: sbom-generator
  namespace: default
```

### Sigstore (签名与验证)

```yaml
Sigstore项目:
  - 软件供应链安全
  - 无密钥签名 (Keyless signing)
  - 透明日志 (Rekor)
  - 证书颁发 (Fulcio)

组件:
  Cosign:
    - 容器镜像签名
    - OCI artifact签名
    - SBOM签名

  Rekor:
    - 透明日志
    - 不可篡改
    - 公开审计

  Fulcio:
    - 短期证书颁发
    - OIDC身份绑定
```

#### Cosign签名镜像

```bash
# 使用Cosign签名容器镜像

# 1. 安装Cosign
curl -O -L "https://github.com/sigstore/cosign/releases/latest/download/cosign-linux-amd64"
mv cosign-linux-amd64 /usr/local/bin/cosign
chmod +x /usr/local/bin/cosign

# 2. 生成密钥对 (可选，keyless签名不需要)
cosign generate-key-pair

# 3. 签名镜像 (keyless方式，使用OIDC)
cosign sign \
  --oidc-issuer=https://token.actions.githubusercontent.com \
  myregistry.io/myapp:v1.0.0

# 4. 签名镜像 (使用密钥对)
cosign sign --key cosign.key myregistry.io/myapp:v1.0.0

# 5. 验证签名
cosign verify myregistry.io/myapp:v1.0.0

# 6. 签名SBOM
cosign attach sbom --sbom myapp-sbom.spdx.json myregistry.io/myapp:v1.0.0
cosign sign myregistry.io/myapp:sha256-<digest>.sbom

# 7. 验证SBOM
cosign verify --attachment sbom myregistry.io/myapp:v1.0.0

# 8. 生成签名策略
cat > image-policy.yaml << EOF
apiVersion: policy.sigstore.dev/v1beta1
kind: ClusterImagePolicy
metadata:
  name: require-signed-images
spec:
  images:
  - glob: "myregistry.io/**"
  authorities:
  - keyless:
      url: https://fulcio.sigstore.dev
      identities:
      - issuer: https://token.actions.githubusercontent.com
        subject: https://github.com/myorg/myrepo/.github/workflows/build.yaml@refs/heads/main
EOF
```

#### Kubernetes准入控制 (Policy Controller)

```yaml
# policy-controller.yaml
# Sigstore Policy Controller - 只允许签名镜像运行

apiVersion: policy.sigstore.dev/v1beta1
kind: ClusterImagePolicy
metadata:
  name: signed-images-only
spec:
  images:
  - glob: "**"  # 所有镜像

  authorities:
  - name: official-signer
    keyless:
      url: https://fulcio.sigstore.dev
      identities:
      - issuerRegExp: ".*"
        subjectRegExp: ".*@example\\.com$"

  - name: github-actions
    keyless:
      url: https://fulcio.sigstore.dev
      identities:
      - issuer: https://token.actions.githubusercontent.com
        subjectRegExp: "https://github.com/myorg/.*"

  # 公钥签名 (备用)
  - key:
      data: |
        -----BEGIN PUBLIC KEY-----
        MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE...
        -----END PUBLIC KEY-----

---
# 豁免系统命名空间
apiVersion: policy.sigstore.dev/v1beta1
kind: ClusterImagePolicy
metadata:
  name: allow-system-images
spec:
  match:
  - excludeNamespaces:
    - kube-system
    - kube-public
    - kube-node-lease
```

---

## 容器安全

### Falco运行时检测

```yaml
Falco:
  - CNCF毕业项目
  - 运行时威胁检测
  - 基于eBPF/内核模块
  - 规则引擎

检测能力:
  - 异常进程执行
  - 文件系统篡改
  - 网络连接
  - 系统调用异常
  - 权限提升
  - 容器逃逸
```

#### Falco部署

```bash
# Kubernetes Helm部署Falco

# 1. 添加Falco Helm仓库
helm repo add falcosecurity https://falcosecurity.github.io/charts
helm repo update

# 2. 安装Falco (eBPF模式)
helm install falco falcosecurity/falco \
  --namespace falco \
  --create-namespace \
  --set driver.kind=ebpf \
  --set falcosidekick.enabled=true \
  --set falcosidekick.webui.enabled=true

# 3. 查看日志
kubectl logs -n falco -l app.kubernetes.io/name=falco -f

# 4. 访问Falco Sidekick UI
kubectl port-forward -n falco svc/falco-falcosidekick-ui 2802:2802
# 访问 http://localhost:2802
```

#### 自定义Falco规则

```yaml
# custom-falco-rules.yaml

customRules:
  custom-rules.yaml: |-
    # 检测在容器中运行shell
    - rule: Shell Spawned in Container
      desc: Detect shell process started in a container
      condition: >
        spawned_process and
        container and
        shell_procs and
        proc.pname exists and
        not user_known_shell_spawn_activity
      output: >
        Shell spawned in container (user=%user.name
        container_id=%container.id container_name=%container.name
        shell=%proc.name parent=%proc.pname cmdline=%proc.cmdline)
      priority: WARNING
      tags: [container, shell, mitre_execution]

    # 检测敏感文件读取
    - rule: Read Sensitive File in Container
      desc: Detect读取敏感文件
      condition: >
        open_read and
        container and
        sensitive_files and
        not trusted_containers
      output: >
        Sensitive file opened for reading (user=%user.name
        file=%fd.name container=%container.name
        image=%container.image.repository)
      priority: WARNING
      tags: [filesystem, mitre_credential_access]

    # 检测权限提升
    - rule: Privilege Escalation Detected
      desc: Detect setuid/setgid system calls
      condition: >
        evt.type in (setuid, setgid) and
        container and
        evt.arg.uid=0
      output: >
        Privilege escalation detected (user=%user.name
        container=%container.name process=%proc.name
        command=%proc.cmdline target_uid=%evt.arg.uid)
      priority: CRITICAL
      tags: [privilege_escalation, mitre_privilege_escalation]

    # 检测容器逃逸尝试
    - rule: Container Escape Attempt
      desc: Detect potential container escape
      condition: >
        spawned_process and
        container and
        (
          proc.name in (nsenter, unshare, capsh) or
          (proc.name = runc and proc.args contains "exec") or
          proc.cmdline contains "docker.sock"
        )
      output: >
        Potential container escape attempt (user=%user.name
        container=%container.name process=%proc.name
        command=%proc.cmdline)
      priority: CRITICAL
      tags: [container_escape, mitre_escape]

    # 检测加密货币挖矿
    - rule: Cryptocurrency Mining Detected
      desc: Detect crypto mining processes
      condition: >
        spawned_process and
        container and
        (proc.name in (xmrig, minerd, cpuminer, ethminer) or
         proc.cmdline contains "stratum+tcp")
      output: >
        Cryptocurrency mining detected (container=%container.name
        process=%proc.name command=%proc.cmdline)
      priority: CRITICAL
      tags: [cryptomining, malware]
```

### Trivy镜像扫描

```bash
# Trivy容器镜像安全扫描

# 1. 安装Trivy
wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
echo "deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main" | sudo tee /etc/apt/sources.list.d/trivy.list
sudo apt update
sudo apt install trivy

# 2. 扫描镜像
trivy image nginx:latest

# 3. 只显示高危和严重漏洞
trivy image --severity HIGH,CRITICAL nginx:latest

# 4. 生成JSON报告
trivy image -f json -o nginx-scan.json nginx:latest

# 5. 扫描并输出SBOM
trivy image --format cyclonedx nginx:latest > nginx-sbom.cdx.json

# 6. 扫描文件系统
trivy fs /path/to/project

# 7. 扫描Kubernetes集群
trivy k8s --report summary cluster

# 8. CI/CD集成 (GitHub Actions)
# .github/workflows/scan.yml
name: Trivy Scan
on: [push]
jobs:
  scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3

    - name: Build image
      run: docker build -t myapp:${{ github.sha }} .

    - name: Run Trivy scan
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: 'myapp:${{ github.sha }}'
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy results to GitHub Security
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'
```

---

## 监控告警

### Prometheus边缘监控

```yaml
边缘场景特点:
  - 节点分散
  - 网络不稳定
  - 存储受限
  - 本地采集+联邦

架构:
  边缘节点:
    - Prometheus Agent (采集)
    - Node Exporter (节点指标)
    - cAdvisor (容器指标)

  中心聚合:
    - Prometheus Server (联邦查询)
    - Thanos/Cortex (长期存储)
    - Grafana (可视化)
```

#### Prometheus Agent部署

```yaml
# prometheus-agent.yaml
# 边缘节点轻量级Prometheus Agent

apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-agent-config
data:
  prometheus.yml: |
    global:
      scrape_interval: 30s
      external_labels:
        cluster: 'edge-site-01'
        region: 'apac'

    # Agent模式配置
    remote_write:
    - url: https://prometheus-central.example.com/api/v1/write
      queue_config:
        max_samples_per_send: 10000
        batch_send_deadline: 5s
        max_shards: 10
      # 失败重试
      retry_on_http_status_429: true
      # 身份认证
      basic_auth:
        username: edge-agent
        password_file: /etc/prometheus/password

    scrape_configs:
    # Node Exporter
    - job_name: 'node'
      static_configs:
      - targets: ['localhost:9100']

    # Kubelet cAdvisor
    - job_name: 'kubernetes-cadvisor'
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

    # 应用指标
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-agent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-agent
  template:
    metadata:
      labels:
        app: prometheus-agent
    spec:
      serviceAccountName: prometheus
      containers:
      - name: prometheus
        image: prom/prometheus:latest
        args:
        - --config.file=/etc/prometheus/prometheus.yml
        - --storage.agent.path=/prometheus
        - --enable-feature=agent  # Agent模式
        - --web.enable-lifecycle
        ports:
        - containerPort: 9090
        volumeMounts:
        - name: config
          mountPath: /etc/prometheus
        - name: storage
          mountPath: /prometheus
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"

      - name: node-exporter
        image: prom/node-exporter:latest
        args:
        - --path.rootfs=/host
        ports:
        - containerPort: 9100
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true

      volumes:
      - name: config
        configMap:
          name: prometheus-agent-config
      - name: storage
        emptyDir: {}
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
```

#### Grafana仪表板

```json
{
  "dashboard": {
    "title": "Edge Node Monitoring",
    "panels": [
      {
        "title": "CPU Usage",
        "targets": [
          {
            "expr": "100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "{{instance}}"
          }
        ]
      },
      {
        "title": "Memory Usage",
        "targets": [
          {
            "expr": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100",
            "legendFormat": "{{instance}}"
          }
        ]
      },
      {
        "title": "Disk I/O",
        "targets": [
          {
            "expr": "rate(node_disk_read_bytes_total[5m])",
            "legendFormat": "Read {{device}}"
          },
          {
            "expr": "rate(node_disk_written_bytes_total[5m])",
            "legendFormat": "Write {{device}}"
          }
        ]
      },
      {
        "title": "Network Traffic",
        "targets": [
          {
            "expr": "rate(node_network_receive_bytes_total{device!=\"lo\"}[5m]) * 8",
            "legendFormat": "RX {{device}}"
          },
          {
            "expr": "rate(node_network_transmit_bytes_total{device!=\"lo\"}[5m]) * 8",
            "legendFormat": "TX {{device}}"
          }
        ]
      },
      {
        "title": "Container CPU",
        "targets": [
          {
            "expr": "sum(rate(container_cpu_usage_seconds_total{container!=\"\"}[5m])) by (pod, container) * 100",
            "legendFormat": "{{pod}}/{{container}}"
          }
        ]
      },
      {
        "title": "Container Memory",
        "targets": [
          {
            "expr": "sum(container_memory_working_set_bytes{container!=\"\"}) by (pod, container) / 1024 / 1024",
            "legendFormat": "{{pod}}/{{container}}"
          }
        ]
      }
    ]
  }
}
```

### Alertmanager告警

```yaml
# alertmanager-config.yaml

global:
  resolve_timeout: 5m
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alerts@example.com'
  smtp_auth_username: 'alerts@example.com'
  smtp_auth_password: 'password'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'

  routes:
  # 严重告警 -> PagerDuty
  - match:
      severity: critical
    receiver: 'pagerduty'
    continue: true

  # 边缘节点告警 -> 运维团队
  - match_re:
      cluster: edge-.*
    receiver: 'ops-team'

  # 数据库告警 -> DBA
  - match:
      service: database
    receiver: 'dba-team'

receivers:
- name: 'default'
  email_configs:
  - to: 'ops@example.com'
    headers:
      Subject: '[{{ .Status | toUpper }}] {{ .GroupLabels.alertname }}'

- name: 'pagerduty'
  pagerduty_configs:
  - service_key: 'xxxxx'
    description: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ end }}'

- name: 'ops-team'
  webhook_configs:
  - url: 'https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXX'
    send_resolved: true

- name: 'dba-team'
  email_configs:
  - to: 'dba@example.com'

inhibit_rules:
# 抑制同一节点的其他告警
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  equal: ['instance']

# 抑制节点宕机时的其他告警
- source_match:
    alertname: 'NodeDown'
  target_match_re:
    alertname: '.*'
  equal: ['instance']
```

#### 自定义告警规则

```yaml
# prometheus-rules.yaml

groups:
- name: edge-node-alerts
  interval: 30s
  rules:
  # 节点宕机
  - alert: NodeDown
    expr: up{job="node"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Node {{ $labels.instance }} is down"
      description: "Node {{ $labels.instance }} has been down for more than 5 minutes"

  # CPU高使用率
  - alert: HighCPUUsage
    expr: (100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage on {{ $labels.instance }}"
      description: "CPU usage is {{ $value | humanize }}%"

  # 内存不足
  - alert: LowMemory
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Low memory on {{ $labels.instance }}"
      description: "Memory usage is {{ $value | humanize }}%"

  # 磁盘空间不足
  - alert: DiskSpaceLow
    expr: (1 - (node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes)) * 100 > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Disk space low on {{ $labels.instance }}"
      description: "Disk {{ $labels.mountpoint }} is {{ $value | humanize }}% full"

  # 容器重启频繁
  - alert: ContainerRestartingFrequently
    expr: rate(kube_pod_container_status_restarts_total[1h]) > 0.1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Container {{ $labels.container }} in pod {{ $labels.pod }} restarting frequently"
      description: "Restart rate is {{ $value | humanize }} per second"

  # Pod CrashLoopBackOff
  - alert: PodCrashLooping
    expr: kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"} > 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Pod {{ $labels.pod }} in CrashLoopBackOff"
      description: "Container {{ $labels.container }} is crash looping"

  # 网络延迟高
  - alert: HighNetworkLatency
    expr: avg_over_time(ping_rtt_ms[5m]) > 100
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "High network latency to {{ $labels.target }}"
      description: "Average RTT is {{ $value | humanize }}ms"

- name: application-alerts
  interval: 30s
  rules:
  # HTTP错误率高
  - alert: HighHTTPErrorRate
    expr: (sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))) * 100 > 5
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "High HTTP 5xx error rate"
      description: "Error rate is {{ $value | humanize }}%"

  # 请求延迟高
  - alert: HighRequestLatency
    expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 1
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "High request latency"
      description: "P95 latency is {{ $value | humanize }}s"
```

---

## 日志管理

### Grafana Loki

```yaml
Loki优势:
  - 轻量级 (不索引日志内容)
  - 标签索引 (类似Prometheus)
  - 边缘友好 (低资源占用)
  - 与Grafana原生集成

架构:
  Promtail (Agent):
    - 日志采集
    - 标签提取
    - 推送到Loki

  Loki (Server):
    - 日志存储
    - 查询API
    - 标签索引

  Grafana:
    - 日志可视化
    - LogQL查询
    - 告警
```

#### Loki Stack部署

```yaml
# loki-stack.yaml

# 1. Loki配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
data:
  loki.yaml: |
    auth_enabled: false

    server:
      http_listen_port: 3100

    ingester:
      lifecycler:
        ring:
          kvstore:
            store: inmemory
          replication_factor: 1
      chunk_idle_period: 5m
      chunk_retain_period: 30s

    schema_config:
      configs:
      - from: 2020-10-24
        store: boltdb-shipper
        object_store: filesystem
        schema: v11
        index:
          prefix: index_
          period: 24h

    storage_config:
      boltdb_shipper:
        active_index_directory: /loki/index
        cache_location: /loki/cache
        shared_store: filesystem
      filesystem:
        directory: /loki/chunks

    limits_config:
      enforce_metric_name: false
      reject_old_samples: true
      reject_old_samples_max_age: 168h

    chunk_store_config:
      max_look_back_period: 168h

    table_manager:
      retention_deletes_enabled: true
      retention_period: 168h

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: loki
spec:
  serviceName: loki
  replicas: 1
  selector:
    matchLabels:
      app: loki
  template:
    metadata:
      labels:
        app: loki
    spec:
      containers:
      - name: loki
        image: grafana/loki:latest
        args:
        - -config.file=/etc/loki/loki.yaml
        ports:
        - containerPort: 3100
          name: http
        volumeMounts:
        - name: config
          mountPath: /etc/loki
        - name: storage
          mountPath: /loki
      volumes:
      - name: config
        configMap:
          name: loki-config
  volumeClaimTemplates:
  - metadata:
      name: storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi

---
# 2. Promtail配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: promtail-config
data:
  promtail.yaml: |
    server:
      http_listen_port: 9080
      grpc_listen_port: 0

    positions:
      filename: /tmp/positions.yaml

    clients:
    - url: http://loki:3100/loki/api/v1/push

    scrape_configs:
    # 容器日志
    - job_name: kubernetes-pods
      kubernetes_sd_configs:
      - role: pod

      pipeline_stages:
      # 提取JSON日志
      - json:
          expressions:
            level: level
            timestamp: ts
            message: msg

      # 标签
      - labels:
          level:

      # 时间戳
      - timestamp:
          source: timestamp
          format: RFC3339Nano

      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_node_name]
        target_label: node
      - source_labels: [__meta_kubernetes_namespace]
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        target_label: pod
      - source_labels: [__meta_kubernetes_pod_container_name]
        target_label: container
      - source_labels: [__meta_kubernetes_pod_label_app]
        target_label: app

      # 只采集特定namespace
      - source_labels: [__meta_kubernetes_namespace]
        regex: (default|kube-system|edge-apps)
        action: keep

    # 系统日志
    - job_name: syslog
      static_configs:
      - targets:
        - localhost
        labels:
          job: syslog
          __path__: /var/log/syslog

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: promtail
spec:
  selector:
    matchLabels:
      app: promtail
  template:
    metadata:
      labels:
        app: promtail
    spec:
      serviceAccountName: promtail
      containers:
      - name: promtail
        image: grafana/promtail:latest
        args:
        - -config.file=/etc/promtail/promtail.yaml
        volumeMounts:
        - name: config
          mountPath: /etc/promtail
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      volumes:
      - name: config
        configMap:
          name: promtail-config
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

#### LogQL查询示例

```logql
# 1. 查询特定Pod的日志
{namespace="default", pod="my-app-7d8c9f5b-xyz"}

# 2. 过滤错误日志
{namespace="default"} |= "error"

# 3. 正则表达式过滤
{app="nginx"} |~ "status=[45].."

# 4. JSON解析
{app="my-app"} | json | level="error"

# 5. 统计错误率
sum(rate({namespace="default"} |= "error" [5m])) by (pod)

# 6. 按级别聚合
sum by (level) (count_over_time({namespace="default"} | json | __error__="" [5m]))

# 7. 慢查询检测 (延迟>1s)
{app="database"} | json | duration > 1000

# 8. 统计不同HTTP状态码
sum by (status) (count_over_time({app="nginx"} | json | __error__="" [1h]))

# 9. 查找特定错误模式
{namespace="production"} |~ "(?i)(exception|panic|fatal)"

# 10. 计算P95延迟
quantile_over_time(0.95, {app="api"} | json | unwrap duration [5m])
```

---

## 分布式追踪

### Jaeger追踪系统

```yaml
Jaeger:
  - CNCF毕业项目
  - 分布式追踪标准 (OpenTracing/OpenTelemetry)
  - 微服务调用链追踪
  - 性能分析

组件:
  Jaeger Agent:
    - 收集Span
    - 缓冲
    - 批量发送

  Jaeger Collector:
    - 接收Span
    - 验证
    - 持久化

  Jaeger Query:
    - UI
    - API
    - 查询服务

  Storage:
    - Elasticsearch
    - Cassandra
    - Kafka
```

#### Jaeger Operator部署

```bash
# Kubernetes部署Jaeger

# 1. 安装Jaeger Operator
kubectl create namespace observability
kubectl apply -f https://github.com/jaegertracing/jaeger-operator/releases/latest/download/jaeger-operator.yaml -n observability

# 2. 部署Jaeger实例
kubectl apply -f - << EOF
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: jaeger-all-in-one
  namespace: observability
spec:
  strategy: allInOne  # 边缘简化部署

  allInOne:
    image: jaegertracing/all-in-one:latest
    options:
      log-level: info
      memory:
        max-traces: 10000

  storage:
    type: memory  # 边缘使用内存存储
    options:
      memory:
        max-traces: 100000

  ingress:
    enabled: true
    hosts:
    - jaeger.example.com

  agent:
    strategy: DaemonSet  # 每个节点运行Agent
EOF

# 3. 查看服务
kubectl get svc -n observability | grep jaeger
```

#### 应用集成OpenTelemetry

```python
# app_with_tracing.py
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from flask import Flask
import requests

# 初始化追踪
resource = Resource(attributes={
    "service.name": "edge-app",
    "service.version": "1.0.0",
    "deployment.environment": "production"
})

tracer_provider = TracerProvider(resource=resource)
trace.set_tracer_provider(tracer_provider)

# Jaeger导出器
jaeger_exporter = JaegerExporter(
    agent_host_name="jaeger-agent.observability.svc.cluster.local",
    agent_port=6831,
)

tracer_provider.add_span_processor(
    BatchSpanProcessor(jaeger_exporter)
)

# 自动化仪表化
RequestsInstrumentor().instrument()
FlaskInstrumentor().instrument()

# 创建应用
app = Flask(__name__)
tracer = trace.get_tracer(__name__)

@app.route('/api/data')
def get_data():
    # 自动追踪HTTP请求
    with tracer.start_as_current_span("get_data") as span:
        span.set_attribute("user.id", "12345")

        # 调用后端服务
        response = requests.get('http://backend-svc/data')

        span.set_attribute("http.status_code", response.status_code)
        span.add_event("data_fetched", {
            "size": len(response.content)
        })

        return response.json()

@app.route('/api/process')
def process_data():
    with tracer.start_as_current_span("process_data") as span:
        # 数据处理
        result = heavy_computation()

        span.set_attribute("result.count", len(result))

        return {"status": "success", "count": len(result)}

def heavy_computation():
    """模拟重度计算"""
    with tracer.start_as_current_span("heavy_computation") as span:
        span.set_attribute("computation.type", "ml_inference")

        # 实际计算...
        import time
        time.sleep(0.5)

        return [1, 2, 3, 4, 5]

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

---

## GitOps实践

### ArgoCD部署

```bash
# Kubernetes部署ArgoCD

# 1. 创建命名空间
kubectl create namespace argocd

# 2. 安装ArgoCD
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# 3. 暴露ArgoCD UI
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'

# 4. 获取初始密码
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d

# 5. 登录CLI
argocd login <ARGOCD_SERVER>

# 6. 修改密码
argocd account update-password
```

#### ArgoCD应用定义

```yaml
# edge-app-argocd.yaml

apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: edge-app
  namespace: argocd
spec:
  project: default

  source:
    repoURL: https://github.com/myorg/edge-app
    targetRevision: HEAD
    path: k8s/overlays/production

    # Kustomize配置
    kustomize:
      namePrefix: prod-
      commonLabels:
        environment: production
      images:
      - myregistry.io/edge-app:v1.2.3

  destination:
    server: https://kubernetes.default.svc
    namespace: edge-apps

  syncPolicy:
    automated:
      prune: true  # 删除不在Git中的资源
      selfHeal: true  # 自动修复漂移
      allowEmpty: false

    syncOptions:
    - CreateNamespace=true
    - PruneLast=true

    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m

---
# 多集群管理
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: edge-cluster-apps
  namespace: argocd
spec:
  generators:
  # Git目录生成器
  - git:
      repoURL: https://github.com/myorg/edge-deployments
      revision: HEAD
      directories:
      - path: clusters/*

  template:
    metadata:
      name: '{{path.basename}}-app'
    spec:
      project: default
      source:
        repoURL: https://github.com/myorg/edge-app
        targetRevision: HEAD
        path: '{{path}}'
      destination:
        server: '{{server}}'
        namespace: edge-apps
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
```

#### CI/CD流水线 (GitHub Actions + ArgoCD)

```yaml
# .github/workflows/deploy.yml

name: Build and Deploy

on:
  push:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=sha,prefix={{branch}}-
          type=ref,event=branch
          type=semver,pattern={{version}}

    - name: Build and push
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}

    - name: Sign image with Cosign
      run: |
        cosign sign --yes ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.meta.outputs.digest }}

    - name: Generate SBOM
      run: |
        syft ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.meta.outputs.digest }} -o spdx-json > sbom.spdx.json
        cosign attach sbom --sbom sbom.spdx.json ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.meta.outputs.digest }}

  deploy:
    needs: build
    runs-on: ubuntu-latest

    steps:
    - name: Checkout GitOps repo
      uses: actions/checkout@v3
      with:
        repository: myorg/edge-deployments
        token: ${{ secrets.GITOPS_TOKEN }}

    - name: Update image tag
      run: |
        cd clusters/edge-site-01
        kustomize edit set image myapp=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}

    - name: Commit and push
      run: |
        git config user.name "GitHub Actions"
        git config user.email "actions@github.com"
        git add .
        git commit -m "Update edge-site-01 image to ${{ github.sha }}"
        git push

    # ArgoCD will automatically sync the changes
```

---

## 故障自愈

### Chaos Engineering

```yaml
工具:
  - Chaos Mesh (CNCF)
  - Litmus Chaos
  - Chaos Toolkit

实验类型:
  - Pod故障 (PodChaos)
  - 网络故障 (NetworkChaos)
  - 压力测试 (StressChaos)
  - IO故障 (IOChaos)
  - 时间偏移 (TimeChaos)
```

#### Chaos Mesh实验

```yaml
# chaos-experiments.yaml

# 1. Pod杀死实验
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: pod-kill-experiment
spec:
  action: pod-kill
  mode: one  # 随机杀死一个Pod
  selector:
    namespaces:
    - edge-apps
    labelSelectors:
      app: edge-service
  scheduler:
    cron: "@every 30m"
  duration: "10s"

---
# 2. 网络延迟实验
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: network-delay
spec:
  action: delay
  mode: all
  selector:
    namespaces:
    - edge-apps
  delay:
    latency: "100ms"
    correlation: "25"
    jitter: "10ms"
  duration: "5m"
  scheduler:
    cron: "0 */2 * * *"  # 每2小时

---
# 3. CPU压力测试
apiVersion: chaos-mesh.org/v1alpha1
kind: StressChaos
metadata:
  name: cpu-stress
spec:
  mode: one
  selector:
    namespaces:
    - edge-apps
  stressors:
    cpu:
      workers: 2
      load: 80
  duration: "3m"

---
# 4. 磁盘IO故障
apiVersion: chaos-mesh.org/v1alpha1
kind: IOChaos
metadata:
  name: io-delay
spec:
  action: latency
  mode: one
  selector:
    namespaces:
    - edge-apps
  volumePath: /data
  path: /data/**/*
  delay: "100ms"
  percent: 50
  duration: "5m"

---
# 5. 网络分区
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: network-partition
spec:
  action: partition
  mode: all
  selector:
    namespaces:
    - edge-apps
    labelSelectors:
      app: edge-service
  direction: both
  target:
    mode: all
    selector:
      namespaces:
      - default
      labelSelectors:
        app: backend
  duration: "2m"
```

### 自动修复

```python
# auto_heal.py
import kubernetes
from kubernetes import client, config
import time

class AutoHealer:
    """自动故障修复"""

    def __init__(self):
        config.load_incluster_config()
        self.v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()

    def watch_pod_failures(self):
        """监控Pod故障"""
        w = kubernetes.watch.Watch()

        for event in w.stream(self.v1.list_pod_for_all_namespaces):
            pod = event['object']

            # CrashLoopBackOff
            if self._is_crash_looping(pod):
                self.handle_crash_loop(pod)

            # ImagePullBackOff
            elif self._is_image_pull_failed(pod):
                self.handle_image_pull_failure(pod)

            # OOMKilled
            elif self._is_oom_killed(pod):
                self.handle_oom(pod)

    def _is_crash_looping(self, pod):
        """检测CrashLoopBackOff"""
        for status in pod.status.container_statuses or []:
            if status.state.waiting and status.state.waiting.reason == "CrashLoopBackOff":
                return True
        return False

    def handle_crash_loop(self, pod):
        """处理崩溃循环"""
        print(f"Detected CrashLoopBackOff: {pod.metadata.namespace}/{pod.metadata.name}")

        # 1. 收集日志
        logs = self.v1.read_namespaced_pod_log(
            name=pod.metadata.name,
            namespace=pod.metadata.namespace,
            tail_lines=100
        )

        # 2. 分析错误
        if "OutOfMemoryError" in logs or "oom" in logs.lower():
            self.increase_memory_limit(pod)
        elif "connection refused" in logs.lower():
            self.check_dependencies(pod)

        # 3. 重启Pod (最后手段)
        else:
            print(f"Restarting pod {pod.metadata.name}")
            self.v1.delete_namespaced_pod(
                name=pod.metadata.name,
                namespace=pod.metadata.namespace
            )

    def increase_memory_limit(self, pod):
        """增加内存限制"""
        print(f"Increasing memory limit for {pod.metadata.name}")

        # 获取Deployment
        owner_refs = pod.metadata.owner_references
        for ref in owner_refs:
            if ref.kind == "ReplicaSet":
                rs = self.apps_v1.read_namespaced_replica_set(
                    name=ref.name,
                    namespace=pod.metadata.namespace
                )

                deploy_name = rs.metadata.owner_references[0].name
                deploy = self.apps_v1.read_namespaced_deployment(
                    name=deploy_name,
                    namespace=pod.metadata.namespace
                )

                # 增加内存限制 (2x)
                for container in deploy.spec.template.spec.containers:
                    if container.resources and container.resources.limits:
                        current_mem = container.resources.limits.get('memory', '1Gi')
                        # 解析并加倍内存
                        new_mem = self._double_memory(current_mem)
                        container.resources.limits['memory'] = new_mem
                        print(f"New memory limit: {new_mem}")

                # 更新Deployment
                self.apps_v1.patch_namespaced_deployment(
                    name=deploy_name,
                    namespace=pod.metadata.namespace,
                    body=deploy
                )

    def _double_memory(self, mem_str):
        """加倍内存值"""
        import re
        match = re.match(r'(\d+)([A-Za-z]+)', mem_str)
        if match:
            value = int(match.group(1))
            unit = match.group(2)
            return f"{value * 2}{unit}"
        return "2Gi"

    def _is_oom_killed(self, pod):
        """检测OOM"""
        for status in pod.status.container_statuses or []:
            if status.last_state.terminated and status.last_state.terminated.reason == "OOMKilled":
                return True
        return False

    def handle_oom(self, pod):
        """处理OOM"""
        print(f"Detected OOMKilled: {pod.metadata.namespace}/{pod.metadata.name}")
        self.increase_memory_limit(pod)

    def run(self):
        """启动自动修复"""
        print("Auto-healer started...")
        while True:
            try:
                self.watch_pod_failures()
            except Exception as e:
                print(f"Error: {e}")
                time.sleep(10)

if __name__ == '__main__':
    healer = AutoHealer()
    healer.run()
```

---

## 最佳实践

### 1. 安全加固清单

```yaml
镜像安全:
  ✅ 使用最小化基础镜像 (distroless/alpine)
  ✅ 定期扫描漏洞 (Trivy)
  ✅ 签名验证 (Cosign)
  ✅ SBOM生成
  ✅ 私有镜像仓库

容器运行时:
  ✅ 非root用户运行
  ✅ 只读根文件系统
  ✅ 最小权限 (capabilities)
  ✅ Seccomp/AppArmor配置
  ✅ 禁用特权容器

网络安全:
  ✅ 零信任网络策略
  ✅ mTLS加密通信
  ✅ 入口/出口控制
  ✅ 网络隔离 (微分段)

数据安全:
  ✅ 加密存储 (at rest)
  ✅ 加密传输 (in transit)
  ✅ 机密计算 (TEE)
  ✅ 密钥轮换
  ✅ 数据备份

身份认证:
  ✅ SPIFFE/SPIRE身份框架
  ✅ 服务账号权限最小化
  ✅ RBAC严格控制
  ✅ 审计日志启用
```

### 2. 运维自动化

```yaml
基础设施即代码 (IaC):
  - Terraform
  - Ansible
  - Pulumi

GitOps:
  - ArgoCD/Flux
  - Git作为单一事实来源
  - 自动化部署
  - 回滚机制

可观测性:
  - 指标 (Prometheus)
  - 日志 (Loki)
  - 追踪 (Jaeger)
  - 告警 (Alertmanager)

持续集成/交付:
  - GitHub Actions
  - GitLab CI
  - Jenkins
  - Tekton
```

### 3. 故障恢复

```yaml
备份策略:
  - 定期自动备份
  - 多地域冗余
  - 备份验证
  - 恢复演练

高可用:
  - 多副本部署
  - 跨区域分布
  - 自动故障切换
  - 负载均衡

灾难恢复:
  - RTO/RPO目标
  - DR演练
  - 数据恢复流程
  - 业务连续性计划
```

---

## 参考资料

### 标准与规范

```yaml
安全标准:
  - NIST SP 800-190 (容器安全)
  - CIS Benchmarks (Kubernetes/Docker)
  - ISO/IEC 27001 (信息安全)
  - OWASP Top 10

机密计算:
  - Intel TDX Specification
  - AMD SEV-SNP White Paper
  - Confidential Computing Consortium

供应链安全:
  - SLSA (Supply chain Levels for Software Artifacts)
  - SPDX 2.3 Specification
  - CycloneDX 1.4 Specification
  - NIST SP 800-161 (供应链风险管理)
```

### 开源项目

```yaml
安全:
  - SPIFFE/SPIRE: https://spiffe.io/
  - Falco: https://falco.org/
  - Trivy: https://trivy.dev/
  - Cosign: https://github.com/sigstore/cosign
  - Open Policy Agent: https://www.openpolicyagent.org/

可观测性:
  - Prometheus: https://prometheus.io/
  - Grafana: https://grafana.com/
  - Loki: https://grafana.com/oss/loki/
  - Jaeger: https://www.jaegertracing.io/
  - OpenTelemetry: https://opentelemetry.io/

GitOps:
  - ArgoCD: https://argo-cd.readthedocs.io/
  - Flux: https://fluxcd.io/
  - Tekton: https://tekton.dev/

混沌工程:
  - Chaos Mesh: https://chaos-mesh.org/
  - Litmus: https://litmuschaos.io/
```

### 学习资源

```yaml
书籍:
  - "Kubernetes Security and Observability" (Brendan Creane)
  - "Cloud Native Security" (Chris Binnie)
  - "GitOps and Kubernetes" (Billy Yuen)

认证:
  - CKS (Certified Kubernetes Security Specialist)
  - CKA (Certified Kubernetes Administrator)
  - CKAD (Certified Kubernetes Application Developer)

在线课程:
  - KodeKloud Kubernetes Security
  - Linux Foundation Training
  - CNCF KubeCon talks
```

---

**文档完成时间**: 2025-10-19
**技术覆盖**: 零信任/机密计算/供应链安全/容器安全/监控/日志/追踪/GitOps/自愈
**代码示例**: 30+个完整示例
**文档字数**: 约14,000字

**Secure Edge, Reliable Operations!** 🔒🛡️🚀

---

## 相关文档

### 本模块相关

- [边缘计算概述与架构](./01_边缘计算概述与架构.md) - 边缘计算概述与架构
- [KubeEdge技术详解](./02_KubeEdge技术详解.md) - KubeEdge技术详解
- [K3s轻量级Kubernetes](./03_K3s轻量级Kubernetes.md) - K3s轻量级Kubernetes详解
- [5G边缘计算MEC](./04_5G边缘计算MEC.md) - 5G边缘计算MEC详解
- [边缘存储与数据管理](./05_边缘存储与数据管理.md) - 边缘存储与数据管理
- [边缘AI与推理优化](./06_边缘AI与推理优化.md) - 边缘AI与推理优化
- [边缘网络与通信](./07_边缘网络与通信.md) - 边缘网络与通信
- [README.md](./README.md) - 本模块导航

### 其他模块相关

- [容器安全技术](../05_容器安全技术/README.md) - 容器安全技术
- [容器安全威胁分析](../05_容器安全技术/01_容器安全威胁分析.md) - 安全威胁分析
- [容器安全防护技术](../05_容器安全技术/02_容器安全防护技术.md) - 安全防护技术
- [容器监控与运维](../06_容器监控与运维/README.md) - 容器监控运维

---

**最后更新**: 2025年11月11日
**维护状态**: 持续更新
