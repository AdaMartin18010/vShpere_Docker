# å¸¸è§é—®é¢˜è§£ç­” (FAQ)

> **è¿”å›**: [é™„å½•èµ„æºé¦–é¡µ](../README.md) | [éƒ¨ç½²æŒ‡å—é¦–é¡µ](../../00_ç´¢å¼•å¯¼èˆª/README.md)

---

## ğŸ“‹ ç›®å½•

- [å¸¸è§é—®é¢˜è§£ç­” (FAQ)](#å¸¸è§é—®é¢˜è§£ç­”-faq)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [è™šæ‹ŸåŒ–å¸¸è§é—®é¢˜](#è™šæ‹ŸåŒ–å¸¸è§é—®é¢˜)
    - [VMwareç›¸å…³](#vmwareç›¸å…³)
    - [KVMç›¸å…³](#kvmç›¸å…³)
  - [å®¹å™¨åŒ–å¸¸è§é—®é¢˜](#å®¹å™¨åŒ–å¸¸è§é—®é¢˜)
    - [DockeråŸºç¡€](#dockeråŸºç¡€)
    - [Dockerç½‘ç»œ](#dockerç½‘ç»œ)
    - [Dockerå­˜å‚¨](#dockerå­˜å‚¨)
  - [Kuberneteså¸¸è§é—®é¢˜](#kuberneteså¸¸è§é—®é¢˜)
    - [é›†ç¾¤éƒ¨ç½²](#é›†ç¾¤éƒ¨ç½²)
    - [Podé—®é¢˜](#podé—®é¢˜)
    - [ç½‘ç»œé—®é¢˜](#ç½‘ç»œé—®é¢˜)
    - [å­˜å‚¨é—®é¢˜](#å­˜å‚¨é—®é¢˜)
    - [èµ„æºç®¡ç†](#èµ„æºç®¡ç†)
  - [ç½‘ç»œæ•…éšœæ’æŸ¥](#ç½‘ç»œæ•…éšœæ’æŸ¥)
    - [è¿é€šæ€§é—®é¢˜](#è¿é€šæ€§é—®é¢˜)
    - [DNSé—®é¢˜](#dnsé—®é¢˜)
    - [è´Ÿè½½å‡è¡¡é—®é¢˜](#è´Ÿè½½å‡è¡¡é—®é¢˜)
  - [å­˜å‚¨æ•…éšœæ’æŸ¥](#å­˜å‚¨æ•…éšœæ’æŸ¥)
    - [å­˜å‚¨æŒ‚è½½é—®é¢˜](#å­˜å‚¨æŒ‚è½½é—®é¢˜)
    - [æ€§èƒ½é—®é¢˜](#æ€§èƒ½é—®é¢˜)
  - [æ€§èƒ½ä¼˜åŒ–é—®é¢˜](#æ€§èƒ½ä¼˜åŒ–é—®é¢˜)
    - [CPUä¼˜åŒ–](#cpuä¼˜åŒ–)
    - [å†…å­˜ä¼˜åŒ–](#å†…å­˜ä¼˜åŒ–)
    - [IOä¼˜åŒ–](#ioä¼˜åŒ–)
  - [å®‰å…¨ç›¸å…³é—®é¢˜](#å®‰å…¨ç›¸å…³é—®é¢˜)
    - [é•œåƒå®‰å…¨](#é•œåƒå®‰å…¨)
    - [ç½‘ç»œå®‰å…¨](#ç½‘ç»œå®‰å…¨)
    - [æƒé™ç®¡ç†](#æƒé™ç®¡ç†)
  - [æ—¥å¸¸è¿ç»´é—®é¢˜](#æ—¥å¸¸è¿ç»´é—®é¢˜)
    - [å¤‡ä»½æ¢å¤](#å¤‡ä»½æ¢å¤)
    - [ç›‘æ§å‘Šè­¦](#ç›‘æ§å‘Šè­¦)
    - [æ—¥å¿—ç®¡ç†](#æ—¥å¿—ç®¡ç†)

---

## è™šæ‹ŸåŒ–å¸¸è§é—®é¢˜

### VMwareç›¸å…³

**Q1: ESXiæ— æ³•å¯åŠ¨ï¼Œå¡åœ¨"Relocating modules and starting up the kernel..."**-

**A**: å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ³•ï¼š

1. **ç¡¬ä»¶ä¸å…¼å®¹**

   ```bash
   # æ£€æŸ¥ç¡¬ä»¶å…¼å®¹æ€§
   - è®¿é—®VMware HCLç½‘ç«™éªŒè¯ç¡¬ä»¶å…¼å®¹æ€§
   - æ›´æ–°BIOSåˆ°æœ€æ–°ç‰ˆæœ¬
   ```

2. **å†…å­˜é—®é¢˜**
   - æµ‹è¯•å†…å­˜æ˜¯å¦æœ‰æ•…éšœ
   - å°è¯•ç§»é™¤éƒ¨åˆ†å†…å­˜æ¨¡å—

3. **BIOSè®¾ç½®**
   - ç¡®ä¿VT-x/AMD-Vå·²å¯ç”¨
   - ç¦ç”¨Secure Boot

---

**Q2: vMotionå¤±è´¥ï¼ŒæŠ¥é”™"A general system error occurred: PBM error occurred during PreMigrateCheckCallback"**-

**A**: è§£å†³æ­¥éª¤ï¼š

```bash
# 1. æ£€æŸ¥å­˜å‚¨ç­–ç•¥
- vCenterç•Œé¢æŸ¥çœ‹è™šæ‹Ÿæœºå­˜å‚¨ç­–ç•¥
- ç¡®ä¿ç›®æ ‡ä¸»æœºå­˜å‚¨æ»¡è¶³ç­–ç•¥è¦æ±‚

# 2. é‡å¯vCenterçš„spsæœåŠ¡
service-control --stop vmware-sps
service-control --start vmware-sps

# 3. æ¸…é™¤å­˜å‚¨ç­–ç•¥ç¼“å­˜
service-control --stop vmware-vpostgres
rm -rf /storage/db/vpostgres
service-control --start vmware-vpostgres
```

---

**Q3: å¦‚ä½•æ‰©å±•VMwareè™šæ‹Ÿæœºç£ç›˜ï¼Ÿ**

**A**: æ‰©å±•æ­¥éª¤ï¼š

1. **åœ¨vCenterä¸­æ‰©å±•è™šæ‹Ÿç£ç›˜**
   - å³é”®è™šæ‹Ÿæœº â†’ ç¼–è¾‘è®¾ç½® â†’ é€‰æ‹©ç¡¬ç›˜ â†’ å¢åŠ å®¹é‡

2. **åœ¨Guest OSä¸­æ‰©å±•åˆ†åŒº (Linux)**

   ```bash
   # æ‰«æç£ç›˜
   echo 1 > /sys/class/block/sda/device/rescan
   
   # ä½¿ç”¨growpartæ‰©å±•åˆ†åŒº
   growpart /dev/sda 1
   
   # æ‰©å±•æ–‡ä»¶ç³»ç»Ÿ
   # ext4
   resize2fs /dev/sda1
   
   # xfs
   xfs_growfs /
   
   # LVM
   pvresize /dev/sda1
   lvextend -l +100%FREE /dev/mapper/vg-lv
   resize2fs /dev/mapper/vg-lv
   ```

---

### KVMç›¸å…³

**Q4: KVMè™šæ‹Ÿæœºæ€§èƒ½å·®ï¼Œå¦‚ä½•ä¼˜åŒ–ï¼Ÿ**

**A**: æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼š

1. **ä½¿ç”¨virtioé©±åŠ¨**

   ```xml
   <!-- ç£ç›˜virtio -->
   <disk type='file' device='disk'>
     <driver name='qemu' type='qcow2' cache='none' io='native'/>
     <target dev='vda' bus='virtio'/>
   </disk>
   
   <!-- ç½‘å¡virtio -->
   <interface type='bridge'>
     <model type='virtio'/>
   </interface>
   ```

2. **CPUç»‘å®š**

   ```xml
   <vcpu placement='static' cpuset='0-3'>4</vcpu>
   <cputune>
     <vcpupin vcpu='0' cpuset='0'/>
     <vcpupin vcpu='1' cpuset='1'/>
     <vcpupin vcpu='2' cpuset='2'/>
     <vcpupin vcpu='3' cpuset='3'/>
   </cputune>
   ```

3. **å¤§é¡µå†…å­˜**

   ```bash
   # é…ç½®å¤§é¡µ
   echo 1024 > /proc/sys/vm/nr_hugepages
   
   # æ°¸ä¹…é…ç½®
   echo "vm.nr_hugepages = 1024" >> /etc/sysctl.conf
   sysctl -p
   ```

---

**Q5: å¦‚ä½•å°†VMwareè™šæ‹Ÿæœºè¿ç§»åˆ°KVMï¼Ÿ**

**A**: è¿ç§»æ­¥éª¤ï¼š

```bash
# 1. è½¬æ¢VMDKåˆ°qcow2
qemu-img convert -f vmdk -O qcow2 vm.vmdk vm.qcow2

# 2. åˆ›å»ºKVMè™šæ‹Ÿæœº
virt-install \
  --name vm-name \
  --ram 4096 \
  --vcpus 2 \
  --disk path=/var/lib/libvirt/images/vm.qcow2,bus=virtio \
  --import \
  --os-variant ubuntu20.04 \
  --network bridge=br0,model=virtio \
  --graphics vnc

# 3. å®‰è£…virtioé©±åŠ¨ (Windows guestéœ€è¦)
# ä¸‹è½½: https://fedorapeople.org/groups/virt/virtio-win/
```

---

## å®¹å™¨åŒ–å¸¸è§é—®é¢˜

### DockeråŸºç¡€

**Q6: Dockerå®¹å™¨æ— æ³•å¯åŠ¨ï¼ŒæŠ¥é”™"docker: Error response from daemon: OCI runtime create failed"**-

**A**: å¸¸è§åŸå› ï¼š

1. **SELinuxé—®é¢˜** (CentOS/RHEL)

   ```bash
   # ä¸´æ—¶ç¦ç”¨
   setenforce 0
   
   # æ°¸ä¹…ç¦ç”¨
   sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config
   ```

2. **cgroupé©±åŠ¨ä¸åŒ¹é…**

   ```bash
   # æ£€æŸ¥Docker cgroupé©±åŠ¨
   docker info | grep "Cgroup Driver"
   
   # é…ç½®systemd cgroupé©±åŠ¨
   cat > /etc/docker/daemon.json <<EOF
   {
     "exec-opts": ["native.cgroupdriver=systemd"]
   }
   EOF
   
   systemctl restart docker
   ```

3. **å­˜å‚¨é©±åŠ¨é—®é¢˜**

   ```bash
   # åˆ‡æ¢åˆ°overlay2
   cat > /etc/docker/daemon.json <<EOF
   {
     "storage-driver": "overlay2"
   }
   EOF
   ```

---

**Q7: å¦‚ä½•æ¸…ç†Dockerå ç”¨çš„ç£ç›˜ç©ºé—´ï¼Ÿ**

**A**: æ¸…ç†æ–¹æ³•ï¼š

```bash
# 1. æŸ¥çœ‹ç£ç›˜ä½¿ç”¨
docker system df

# 2. æ¸…ç†æœªä½¿ç”¨çš„é•œåƒ
docker image prune -a

# 3. æ¸…ç†æœªä½¿ç”¨çš„å®¹å™¨
docker container prune

# 4. æ¸…ç†æœªä½¿ç”¨çš„å·
docker volume prune

# 5. æ¸…ç†æœªä½¿ç”¨çš„ç½‘ç»œ
docker network prune

# 6. ä¸€é”®æ¸…ç†æ‰€æœ‰æœªä½¿ç”¨èµ„æº
docker system prune -a --volumes

# 7. å®šæœŸè‡ªåŠ¨æ¸…ç† (cron)
0 2 * * * docker system prune -af --volumes > /dev/null 2>&1
```

---

**Q8: å®¹å™¨å†…æ—¶é—´ä¸å®¿ä¸»æœºä¸ä¸€è‡´ï¼Ÿ**

**A**: è§£å†³æ–¹æ³•ï¼š

```bash
# æ–¹æ³•1: æŒ‚è½½å®¿ä¸»æœºæ—¶åŒº
docker run -v /etc/localtime:/etc/localtime:ro \
           -v /etc/timezone:/etc/timezone:ro \
           myimage

# æ–¹æ³•2: è®¾ç½®TZç¯å¢ƒå˜é‡
docker run -e TZ=Asia/Shanghai myimage

# æ–¹æ³•3: Dockerfileè®¾ç½®
FROM ubuntu:22.04
ENV TZ=Asia/Shanghai
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone
```

---

### Dockerç½‘ç»œ

**Q9: Dockerå®¹å™¨æ— æ³•è®¿é—®å¤–ç½‘ï¼Ÿ**

**A**: æ’æŸ¥æ­¥éª¤ï¼š

```bash
# 1. æ£€æŸ¥IPè½¬å‘
cat /proc/sys/net/ipv4/ip_forward  # åº”è¯¥è¿”å›1
sysctl -w net.ipv4.ip_forward=1

# 2. æ£€æŸ¥iptables NATè§„åˆ™
iptables -t nat -L -n | grep MASQUERADE

# 3. æ·»åŠ NATè§„åˆ™ (å¦‚æœç¼ºå¤±)
iptables -t nat -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE

# 4. æ£€æŸ¥DNSé…ç½®
docker run --rm alpine cat /etc/resolv.conf

# 5. è‡ªå®šä¹‰DNS
cat > /etc/docker/daemon.json <<EOF
{
  "dns": ["8.8.8.8", "8.8.4.4"]
}
EOF
systemctl restart docker
```

---

**Q10: Dockerå®¹å™¨é—´æ— æ³•é€šä¿¡ï¼Ÿ**

**A**: è§£å†³æ–¹æ³•ï¼š

```bash
# 1. ç¡®ä¿åœ¨åŒä¸€ç½‘ç»œ
docker network ls
docker network inspect bridge

# 2. åˆ›å»ºè‡ªå®šä¹‰ç½‘ç»œ
docker network create mynetwork

# 3. å¯åŠ¨å®¹å™¨åˆ°åŒä¸€ç½‘ç»œ
docker run --network mynetwork --name app1 nginx
docker run --network mynetwork --name app2 alpine ping app1

# 4. è¿æ¥å·²æœ‰å®¹å™¨åˆ°ç½‘ç»œ
docker network connect mynetwork existing-container

# 5. æ£€æŸ¥é˜²ç«å¢™è§„åˆ™
iptables -L DOCKER-USER -n
```

---

### Dockerå­˜å‚¨

**Q11: å¦‚ä½•æŒä¹…åŒ–å®¹å™¨æ•°æ®ï¼Ÿ**

**A**: ä¸‰ç§æ–¹å¼ï¼š

1. **Volume (æ¨è)**

   ```bash
   # åˆ›å»ºvolume
   docker volume create mydata
   
   # ä½¿ç”¨volume
   docker run -v mydata:/data nginx
   
   # æŸ¥çœ‹volume
   docker volume ls
   docker volume inspect mydata
   ```

2. **Bind Mount**

   ```bash
   # æŒ‚è½½ä¸»æœºç›®å½•
   docker run -v /host/path:/container/path nginx
   
   # åªè¯»æŒ‚è½½
   docker run -v /host/path:/container/path:ro nginx
   ```

3. **tmpfs Mount** (ä¸´æ—¶æ•°æ®)

   ```bash
   docker run --tmpfs /tmp nginx
   ```

---

**Q12: Dockerç£ç›˜ç©ºé—´ä¸è¶³ï¼Œå¦‚ä½•è¿ç§»åˆ°å…¶ä»–åˆ†åŒºï¼Ÿ**

**A**: è¿ç§»æ­¥éª¤ï¼š

```bash
# 1. åœæ­¢Docker
systemctl stop docker

# 2. è¿ç§»æ•°æ®ç›®å½•
rsync -aP /var/lib/docker/ /new/path/docker/

# 3. é…ç½®Dockerä½¿ç”¨æ–°è·¯å¾„
cat > /etc/docker/daemon.json <<EOF
{
  "data-root": "/new/path/docker"
}
EOF

# 4. å¯åŠ¨Docker
systemctl start docker

# 5. éªŒè¯
docker info | grep "Docker Root Dir"

# 6. åˆ é™¤æ—§æ•°æ® (ç¡®è®¤æ— è¯¯å)
rm -rf /var/lib/docker
```

---

## Kuberneteså¸¸è§é—®é¢˜

### é›†ç¾¤éƒ¨ç½²

**Q13: kubeadm initå¤±è´¥ï¼ŒæŠ¥é”™"[ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]"**

**A**: è§£å†³æ–¹æ³•ï¼š

```bash
# 1. åŠ è½½br_netfilteræ¨¡å—
modprobe br_netfilter

# 2. é…ç½®å†…æ ¸å‚æ•°
cat > /etc/sysctl.d/k8s.conf <<EOF
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF

sysctl --system

# 3. ç¦ç”¨Swap
swapoff -a
sed -i '/swap/d' /etc/fstab

# 4. é‡æ–°åˆå§‹åŒ–
kubeadm reset -f
kubeadm init --pod-network-cidr=10.244.0.0/16
```

---

**Q14: kubectlè¿æ¥é›†ç¾¤è¶…æ—¶ï¼Ÿ**

**A**: æ’æŸ¥æ­¥éª¤ï¼š

```bash
# 1. æ£€æŸ¥kubeconfig
cat ~/.kube/config
kubectl config view

# 2. æ£€æŸ¥API ServerçŠ¶æ€
systemctl status kube-apiserver  # äºŒè¿›åˆ¶éƒ¨ç½²
kubectl get pods -n kube-system | grep apiserver  # kubeadméƒ¨ç½²

# 3. æ£€æŸ¥è¯ä¹¦æ˜¯å¦è¿‡æœŸ
kubeadm certs check-expiration

# 4. æ›´æ–°è¯ä¹¦
kubeadm certs renew all
systemctl restart kubelet

# 5. æ£€æŸ¥é˜²ç«å¢™
firewall-cmd --list-all
# å¼€æ”¾6443ç«¯å£
firewall-cmd --permanent --add-port=6443/tcp
firewall-cmd --reload
```

---

### Podé—®é¢˜

**Q15: Podä¸€ç›´å¤„äºPendingçŠ¶æ€ï¼Ÿ**

**A**: å¸¸è§åŸå› ï¼š

```bash
# 1. æŸ¥çœ‹Podäº‹ä»¶
kubectl describe pod <pod-name>

# å¸¸è§åŸå› å’Œè§£å†³æ–¹æ³•ï¼š

# åŸå› 1: èµ„æºä¸è¶³
kubectl describe nodes  # æŸ¥çœ‹èŠ‚ç‚¹èµ„æº
kubectl get pods --all-namespaces  # æŸ¥çœ‹èµ„æºä½¿ç”¨

# åŸå› 2: NodeSelectorä¸åŒ¹é…
kubectl get nodes --show-labels
# åˆ é™¤æˆ–ä¿®æ”¹NodeSelector

# åŸå› 3: Taint/Toleration
kubectl describe node <node-name> | grep Taint
# æ·»åŠ tolerationæˆ–ç§»é™¤taint
kubectl taint nodes <node-name> key:NoSchedule-

# åŸå› 4: PVCæœªç»‘å®š
kubectl get pvc
# æ£€æŸ¥StorageClasså’ŒPV
```

---

**Q16: PodçŠ¶æ€CrashLoopBackOffï¼Ÿ**

**A**: æ’æŸ¥æ–¹æ³•ï¼š

```bash
# 1. æŸ¥çœ‹Podæ—¥å¿—
kubectl logs <pod-name>
kubectl logs <pod-name> --previous  # æŸ¥çœ‹ä¸Šä¸€ä¸ªå®¹å™¨æ—¥å¿—

# 2. æŸ¥çœ‹Podäº‹ä»¶
kubectl describe pod <pod-name>

# 3. è¿›å…¥å®¹å™¨è°ƒè¯•
kubectl exec -it <pod-name> -- sh

# 4. å¸¸è§åŸå› ï¼š
# - åº”ç”¨å¯åŠ¨å¤±è´¥ â†’ æ£€æŸ¥é…ç½®å’Œä¾èµ–
# - å¥åº·æ£€æŸ¥å¤±è´¥ â†’ è°ƒæ•´æ¢é’ˆé…ç½®
# - èµ„æºé™åˆ¶ â†’ å¢åŠ èµ„æºlimits
# - æƒé™é—®é¢˜ â†’ æ£€æŸ¥securityContext

# 5. è°ƒè¯•Pod (ä½¿ç”¨debugå®¹å™¨)
kubectl debug <pod-name> -it --image=busybox
```

---

**Q17: Podæ— æ³•æ‹‰å–é•œåƒï¼ŒæŠ¥é”™"ImagePullBackOff"ï¼Ÿ**

**A**: è§£å†³æ–¹æ³•ï¼š

```bash
# 1. æ£€æŸ¥é•œåƒåç§°
kubectl describe pod <pod-name> | grep Image

# 2. æ£€æŸ¥é•œåƒæ˜¯å¦å­˜åœ¨
docker pull <image-name>

# 3. é…ç½®ç§æœ‰é•œåƒä»“åº“è®¤è¯
kubectl create secret docker-registry regcred \
  --docker-server=<registry> \
  --docker-username=<username> \
  --docker-password=<password> \
  --docker-email=<email>

# åœ¨Podä¸­ä½¿ç”¨Secret
spec:
  imagePullSecrets:
  - name: regcred

# 4. ä½¿ç”¨é•œåƒä»£ç†
# ä¿®æ”¹containerdé…ç½®
cat >> /etc/containerd/config.toml <<EOF
[plugins."io.containerd.grpc.v1.cri".registry.mirrors]
  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
    endpoint = ["https://docker.mirrors.sjtug.sjtu.edu.cn"]
EOF

systemctl restart containerd
```

---

### ç½‘ç»œé—®é¢˜

**Q18: Podä¹‹é—´æ— æ³•é€šä¿¡ï¼Ÿ**

**A**: æ’æŸ¥æ­¥éª¤ï¼š

```bash
# 1. æ£€æŸ¥CNIæ’ä»¶
kubectl get pods -n kube-system | grep -E "calico|flannel|cilium"

# 2. æ£€æŸ¥NetworkPolicy
kubectl get networkpolicy --all-namespaces
kubectl describe networkpolicy <policy-name>

# 3. æµ‹è¯•Podé—´è¿é€šæ€§
# åˆ›å»ºæµ‹è¯•Pod
kubectl run test --image=busybox --rm -it -- sh
# åœ¨Podå†…æµ‹è¯•
ping <target-pod-ip>
wget -O- http://<service-name>

# 4. æ£€æŸ¥kube-proxy
kubectl get pods -n kube-system | grep kube-proxy
kubectl logs -n kube-system <kube-proxy-pod>

# 5. æ£€æŸ¥iptablesè§„åˆ™
iptables-save | grep <service-name>

# 6. é‡å¯ç½‘ç»œç»„ä»¶
kubectl delete pod -n kube-system -l k8s-app=kube-dns
kubectl delete pod -n kube-system -l k8s-app=calico-node
```

---

**Q19: Serviceæ— æ³•è®¿é—®ï¼Ÿ**

**A**: æ’æŸ¥æ–¹æ³•ï¼š

```bash
# 1. æ£€æŸ¥Service
kubectl get svc
kubectl describe svc <service-name>

# 2. æ£€æŸ¥Endpoints
kubectl get endpoints <service-name>
# å¦‚æœendpointsä¸ºç©ºï¼Œæ£€æŸ¥selector

# 3. æµ‹è¯•Service
# ClusterIP
kubectl run test --image=busybox --rm -it -- wget -O- http://<service-ip>

# NodePort
curl http://<node-ip>:<node-port>

# 4. æ£€æŸ¥DNS
kubectl run test --image=busybox --rm -it -- nslookup <service-name>

# 5. æŸ¥çœ‹kube-proxyæ—¥å¿—
kubectl logs -n kube-system -l k8s-app=kube-proxy

# 6. æ£€æŸ¥é˜²ç«å¢™
iptables -L -n | grep <service-port>
firewall-cmd --list-all
```

---

### å­˜å‚¨é—®é¢˜

**Q20: PVCä¸€ç›´å¤„äºPendingçŠ¶æ€ï¼Ÿ**

**A**: è§£å†³æ–¹æ³•ï¼š

```bash
# 1. æŸ¥çœ‹PVCè¯¦æƒ…
kubectl describe pvc <pvc-name>

# å¸¸è§åŸå› ï¼š

# åŸå› 1: æ²¡æœ‰å¯ç”¨çš„PV
kubectl get pv
# åˆ›å»ºPVæˆ–é…ç½®StorageClassåŠ¨æ€ä¾›åº”

# åŸå› 2: StorageClassä¸å­˜åœ¨
kubectl get storageclass
# åˆ›å»ºæˆ–æŒ‡å®šæ­£ç¡®çš„StorageClass

# åŸå› 3: è®¿é—®æ¨¡å¼ä¸åŒ¹é…
# PVCå’ŒPVçš„accessModeså¿…é¡»å…¼å®¹
# ReadWriteOnce, ReadOnlyMany, ReadWriteMany

# åŸå› 4: å®¹é‡ä¸è¶³
# PVå®¹é‡å¿…é¡» >= PVCè¯·æ±‚å®¹é‡

# ç¤ºä¾‹: åˆ›å»ºæœ¬åœ°PV
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
  - ReadWriteOnce
  hostPath:
    path: /mnt/data
```

---

**Q21: PodæŒ‚è½½å­˜å‚¨å¤±è´¥ï¼Ÿ**

**A**: æ’æŸ¥æ­¥éª¤ï¼š

```bash
# 1. æŸ¥çœ‹Podäº‹ä»¶
kubectl describe pod <pod-name>

# 2. æ£€æŸ¥PVCçŠ¶æ€
kubectl get pvc

# 3. æŸ¥çœ‹PVè¯¦æƒ…
kubectl describe pv <pv-name>

# 4. æ£€æŸ¥CSIé©±åŠ¨ (å¦‚æœä½¿ç”¨)
kubectl get pods -n kube-system | grep csi
kubectl logs -n kube-system <csi-pod>

# 5. æ£€æŸ¥èŠ‚ç‚¹æŒ‚è½½
# åœ¨Podæ‰€åœ¨èŠ‚ç‚¹ä¸Š
mount | grep <volume-path>
df -h

# 6. NFSæŒ‚è½½é—®é¢˜
# æ£€æŸ¥NFSæœåŠ¡å™¨
showmount -e <nfs-server>
# æ‰‹åŠ¨æµ‹è¯•æŒ‚è½½
mount -t nfs <nfs-server>:/path /mnt/test

# 7. iSCSIæŒ‚è½½é—®é¢˜
# æ£€æŸ¥iscsidæœåŠ¡
systemctl status iscsid
# å‘ç°target
iscsiadm -m discovery -t st -p <iscsi-server>
```

---

### èµ„æºç®¡ç†

**Q22: å¦‚ä½•é™åˆ¶Podèµ„æºä½¿ç”¨ï¼Ÿ**

**A**: èµ„æºé™åˆ¶æ–¹æ³•ï¼š

```yaml
# 1. Podçº§åˆ«é™åˆ¶
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: app
    image: myapp:v1
    resources:
      requests:  # è°ƒåº¦ä½¿ç”¨ï¼Œä¿è¯æœ€å°èµ„æº
        memory: "256Mi"
        cpu: "250m"
      limits:    # è¿è¡Œæ—¶é™åˆ¶ï¼Œè¶…è¿‡ä¼šè¢«æ€æ­»
        memory: "512Mi"
        cpu: "500m"

# 2. å‘½åç©ºé—´çº§åˆ«é™åˆ¶ (ResourceQuota)
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
    pods: "50"

# 3. é»˜è®¤èµ„æºé™åˆ¶ (LimitRange)
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limit
  namespace: dev
spec:
  limits:
  - default:      # é»˜è®¤limits
      cpu: 500m
      memory: 512Mi
    defaultRequest:  # é»˜è®¤requests
      cpu: 250m
      memory: 256Mi
    type: Container
```

---

**Q23: OOMKilledï¼ŒPodè¢«æ€æ­»ï¼Ÿ**

**A**: è§£å†³æ–¹æ³•ï¼š

```bash
# 1. æŸ¥çœ‹PodçŠ¶æ€
kubectl get pod <pod-name>
# STATUS: OOMKilled

# 2. æŸ¥çœ‹Podäº‹ä»¶
kubectl describe pod <pod-name>
# Reason: OOMKilled

# 3. æŸ¥çœ‹å®¹å™¨æ—¥å¿—
kubectl logs <pod-name> --previous

# 4. å¢åŠ å†…å­˜limits
spec:
  containers:
  - name: app
    resources:
      limits:
        memory: "2Gi"  # å¢åŠ å†…å­˜é™åˆ¶

# 5. ä¼˜åŒ–åº”ç”¨å†…å­˜ä½¿ç”¨
# - æ£€æŸ¥å†…å­˜æ³„æ¼
# - è°ƒæ•´JVMå †å¤§å° (Javaåº”ç”¨)
# - ä½¿ç”¨å†…å­˜åˆ†æå·¥å…·

# 6. ä½¿ç”¨VPAè‡ªåŠ¨è°ƒæ•´
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: myapp-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: myapp
  updatePolicy:
    updateMode: "Auto"
```

---

## ç½‘ç»œæ•…éšœæ’æŸ¥

### è¿é€šæ€§é—®é¢˜

**Q24: æ— æ³•pingé€šæŸä¸ªIPï¼Ÿ**

**A**: æ’æŸ¥æ­¥éª¤ï¼š

```bash
# 1. æ£€æŸ¥ç½‘ç»œæ¥å£
ip addr show
ip link show

# 2. æ£€æŸ¥è·¯ç”±
ip route show
# æ·»åŠ è·¯ç”±
ip route add 192.168.1.0/24 via 192.168.0.1

# 3. æ£€æŸ¥é˜²ç«å¢™
iptables -L -n
firewall-cmd --list-all

# 4. æ£€æŸ¥ICMPæ˜¯å¦è¢«ç¦ç”¨
# ä¸´æ—¶å…è®¸
iptables -A INPUT -p icmp -j ACCEPT
# æˆ–
firewall-cmd --add-protocol=icmp

# 5. ä½¿ç”¨tcpdumpæŠ“åŒ…
tcpdump -i eth0 icmp

# 6. æ£€æŸ¥å¯¹ç«¯ä¸»æœº
# ç¡®ä¿å¯¹ç«¯ä¸»æœºåœ¨çº¿
# ç¡®ä¿å¯¹ç«¯é˜²ç«å¢™å…è®¸ICMP
```

---

### DNSé—®é¢˜

**Q25: åŸŸåè§£æå¤±è´¥ï¼Ÿ**

**A**: æ’æŸ¥æ–¹æ³•ï¼š

```bash
# 1. æ£€æŸ¥DNSé…ç½®
cat /etc/resolv.conf

# 2. æµ‹è¯•DNSè§£æ
nslookup example.com
dig example.com
host example.com

# 3. æµ‹è¯•ç‰¹å®šDNSæœåŠ¡å™¨
nslookup example.com 8.8.8.8
dig @8.8.8.8 example.com

# 4. æ£€æŸ¥DNSæœåŠ¡å™¨è¿é€šæ€§
ping 8.8.8.8

# 5. Kubernetes DNSé—®é¢˜
# æ£€æŸ¥CoreDNS
kubectl get pods -n kube-system -l k8s-app=kube-dns
kubectl logs -n kube-system -l k8s-app=kube-dns

# æµ‹è¯•é›†ç¾¤DNS
kubectl run test --image=busybox --rm -it -- nslookup kubernetes.default

# 6. é…ç½®æ­£ç¡®çš„DNS
# Docker
cat > /etc/docker/daemon.json <<EOF
{
  "dns": ["8.8.8.8", "8.8.4.4"]
}
EOF

# Kubernetes Pod
spec:
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
    - 8.8.8.8
    searches:
    - default.svc.cluster.local
    - svc.cluster.local
```

---

### è´Ÿè½½å‡è¡¡é—®é¢˜

**Q26: Nginxè´Ÿè½½å‡è¡¡ä¸å‡åŒ€ï¼Ÿ**

**A**: ä¼˜åŒ–é…ç½®ï¼š

```nginx
upstream backend {
    # é»˜è®¤: è½®è¯¢
    server backend1:8080;
    server backend2:8080;
    
    # åŠ æƒè½®è¯¢
    server backend1:8080 weight=3;
    server backend2:8080 weight=1;
    
    # æœ€å°‘è¿æ¥
    least_conn;
    
    # IP Hash (ä¼šè¯ä¿æŒ)
    ip_hash;
    
    # å¥åº·æ£€æŸ¥
    server backend1:8080 max_fails=3 fail_timeout=30s;
    server backend2:8080 max_fails=3 fail_timeout=30s;
    
    # å¤‡ä»½æœåŠ¡å™¨
    server backend3:8080 backup;
    
    # Keepaliveè¿æ¥
    keepalive 32;
}

server {
    listen 80;
    
    location / {
        proxy_pass http://backend;
        
        # ä¿æŒå®¢æˆ·ç«¯ä¿¡æ¯
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # è¶…æ—¶è®¾ç½®
        proxy_connect_timeout 5s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }
}
```

---

## å­˜å‚¨æ•…éšœæ’æŸ¥

### å­˜å‚¨æŒ‚è½½é—®é¢˜

**Q27: NFSæŒ‚è½½å¤±è´¥ï¼Ÿ**

**A**: æ’æŸ¥æ­¥éª¤ï¼š

```bash
# 1. æ£€æŸ¥NFSæœåŠ¡å™¨
systemctl status nfs-server

# 2. æ£€æŸ¥å¯¼å‡ºé…ç½®
cat /etc/exports
exportfs -v

# 3. æµ‹è¯•NFSæœåŠ¡å™¨è¿é€šæ€§
ping <nfs-server>
showmount -e <nfs-server>

# 4. æ£€æŸ¥é˜²ç«å¢™
# NFSéœ€è¦å¼€æ”¾: 111, 2049, 20048ç«¯å£
firewall-cmd --permanent --add-service=nfs
firewall-cmd --permanent --add-service=mountd
firewall-cmd --permanent --add-service=rpc-bind
firewall-cmd --reload

# 5. æ‰‹åŠ¨æŒ‚è½½æµ‹è¯•
mount -t nfs <nfs-server>:/path /mnt/test
# å¦‚æœå¤±è´¥ï¼ŒæŸ¥çœ‹è¯¦ç»†é”™è¯¯
mount -t nfs -v <nfs-server>:/path /mnt/test

# 6. æ£€æŸ¥æƒé™
ls -ld /path/on/nfs-server
# ç¡®ä¿no_root_squashæˆ–æ­£ç¡®çš„ç”¨æˆ·æ˜ å°„

# 7. å¸¸è§é”™è¯¯:
# mount.nfs: access denied â†’ æ£€æŸ¥/etc/exportsçš„IPé™åˆ¶
# mount.nfs: Connection refused â†’ æ£€æŸ¥nfs-serveræœåŠ¡å’Œé˜²ç«å¢™
# mount.nfs: No such file or directory â†’ æ£€æŸ¥å¯¼å‡ºè·¯å¾„
```

---

### æ€§èƒ½é—®é¢˜

**Q28: ç£ç›˜IOæ€§èƒ½å·®ï¼Ÿ**

**A**: æ€§èƒ½ä¼˜åŒ–ï¼š

```bash
# 1. æµ‹è¯•ç£ç›˜æ€§èƒ½
# é¡ºåºå†™
dd if=/dev/zero of=/tmp/test bs=1M count=1024
# é¡ºåºè¯»
dd if=/tmp/test of=/dev/null bs=1M

# ä½¿ç”¨fioå…¨é¢æµ‹è¯•
fio --name=randwrite --ioengine=libaio --iodepth=16 \
    --rw=randwrite --bs=4k --direct=1 --size=1G \
    --numjobs=4 --runtime=60 --group_reporting

# 2. æ£€æŸ¥IOä½¿ç”¨æƒ…å†µ
iostat -x 1
iotop

# 3. ä¼˜åŒ–æ–‡ä»¶ç³»ç»Ÿ
# ä½¿ç”¨noatimeå‡å°‘å†™å…¥
mount -o remount,noatime /

# /etc/fstab
/dev/sda1  /data  ext4  noatime,nodiratime  0 2

# 4. è°ƒæ•´IOè°ƒåº¦å™¨
# æŸ¥çœ‹å½“å‰è°ƒåº¦å™¨
cat /sys/block/sda/queue/scheduler

# SSDä½¿ç”¨noopæˆ–none
echo noop > /sys/block/sda/queue/scheduler

# HDDä½¿ç”¨deadlineæˆ–cfq
echo deadline > /sys/block/sda/queue/scheduler

# 5. å¢åŠ ç¼“å­˜
# å¢åŠ page cache
sysctl -w vm.dirty_ratio=40
sysctl -w vm.dirty_background_ratio=10

# 6. ä½¿ç”¨RAIDæå‡æ€§èƒ½
# RAID0: æ€§èƒ½æœ€é«˜ï¼Œæ— å†—ä½™
# RAID10: æ€§èƒ½å’Œå†—ä½™å¹³è¡¡
```

---

## æ€§èƒ½ä¼˜åŒ–é—®é¢˜

### CPUä¼˜åŒ–

**Q29: CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Ÿ**

**A**: ä¼˜åŒ–æ–¹æ³•ï¼š

```bash
# 1. æŸ¥æ‰¾CPUå ç”¨é«˜çš„è¿›ç¨‹
top -o %CPU
htop

# 2. æŸ¥çœ‹è¿›ç¨‹è¯¦æƒ…
ps -eLf | grep <pid>  # æŸ¥çœ‹çº¿ç¨‹
pidstat -p <pid> 1    # CPUä½¿ç”¨è¯¦æƒ…

# 3. åˆ†æCPUä½¿ç”¨
# æŸ¥çœ‹ç³»ç»Ÿè°ƒç”¨
strace -p <pid> -c

# æŸ¥çœ‹å‡½æ•°è°ƒç”¨
perf top -p <pid>
perf record -p <pid> -g -- sleep 10
perf report

# 4. å®¹å™¨CPUé™åˆ¶
docker run --cpus=2 myapp     # é™åˆ¶2ä¸ªCPU
docker run --cpu-shares=512 myapp  # CPUæƒé‡

# Kubernetes CPUé™åˆ¶
resources:
  limits:
    cpu: 2000m

# 5. ä¼˜åŒ–åº”ç”¨
# - å‡å°‘è®¡ç®—å¯†é›†å‹æ“ä½œ
# - ä½¿ç”¨ç¼“å­˜
# - å¼‚æ­¥å¤„ç†
# - è´Ÿè½½å‡è¡¡
```

---

### å†…å­˜ä¼˜åŒ–

**Q30: å†…å­˜å ç”¨è¿‡é«˜ï¼Ÿ**

**A**: ä¼˜åŒ–æ–¹æ³•ï¼š

```bash
# 1. æŸ¥çœ‹å†…å­˜ä½¿ç”¨
free -h
vmstat 1

# 2. æŸ¥æ‰¾å†…å­˜å ç”¨é«˜çš„è¿›ç¨‹
top -o %MEM
ps aux --sort=-%mem | head

# 3. æŸ¥çœ‹è¿›ç¨‹å†…å­˜è¯¦æƒ…
pmap -x <pid>
cat /proc/<pid>/status | grep -i vm

# 4. åˆ†æå†…å­˜æ³„æ¼
valgrind --leak-check=full ./myapp

# 5. æ¸…ç†ç¼“å­˜
sync
echo 3 > /proc/sys/vm/drop_caches

# 6. è°ƒæ•´swapä½¿ç”¨
sysctl -w vm.swappiness=10  # å‡å°‘swapä½¿ç”¨

# 7. å®¹å™¨å†…å­˜é™åˆ¶
docker run -m 512m myapp

# Kuberneteså†…å­˜é™åˆ¶
resources:
  limits:
    memory: 2Gi

# 8. ä¼˜åŒ–åº”ç”¨
# - ä¿®å¤å†…å­˜æ³„æ¼
# - è°ƒæ•´JVMå †å¤§å°
# - ä½¿ç”¨å¯¹è±¡æ± 
```

---

### IOä¼˜åŒ–

**Q31: ç£ç›˜IOç“¶é¢ˆï¼Ÿ**

**A**: ä¼˜åŒ–æ–¹æ¡ˆï¼š

```bash
# 1. ç›‘æ§IO
iostat -x 1
iotop -o

# 2. æŸ¥æ‰¾IOå ç”¨é«˜çš„è¿›ç¨‹
iotop
pidstat -d 1

# 3. ä½¿ç”¨SSD
# - ç³»ç»Ÿç›˜ä½¿ç”¨SSD
# - æ•°æ®åº“ä½¿ç”¨SSD
# - æ—¥å¿—å¯ä»¥ä½¿ç”¨HDD

# 4. ä½¿ç”¨RAID
# RAID0: æœ€é«˜æ€§èƒ½
# RAID10: æ€§èƒ½å’Œå†—ä½™

# 5. æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–
# ext4ä¼˜åŒ–
tune2fs -o journal_data_writeback /dev/sda1

# xfsä¼˜åŒ–
mount -o noatime,nodiratime,logbufs=8 /dev/sda1 /data

# 6. æ•°æ®åº“ä¼˜åŒ–
# MySQL
[mysqld]
innodb_flush_log_at_trx_commit = 2
innodb_buffer_pool_size = 70% of RAM

# 7. Dockerå­˜å‚¨é©±åŠ¨
# ä½¿ç”¨overlay2
cat > /etc/docker/daemon.json <<EOF
{
  "storage-driver": "overlay2"
}
EOF
```

---

## å®‰å…¨ç›¸å…³é—®é¢˜

### é•œåƒå®‰å…¨

**Q32: å¦‚ä½•æ‰«æé•œåƒæ¼æ´ï¼Ÿ**

**A**: æ‰«ææ–¹æ³•ï¼š

```bash
# 1. ä½¿ç”¨Trivyæ‰«æ
trivy image nginx:latest

# è¯¦ç»†æ‰«æ
trivy image --severity HIGH,CRITICAL nginx:latest

# 2. ä½¿ç”¨Clairæ‰«æ
# éƒ¨ç½²ClairæœåŠ¡å™¨
docker run -p 5432:5432 -d --name clairdb postgres:11
docker run -p 6060-6061:6060-6061 -d --name clair \
  --link clairdb:postgres \
  quay.io/coreos/clair:latest

# ä½¿ç”¨clairctlæ‰«æ
clairctl analyze nginx:latest

# 3. ä½¿ç”¨Anchore
anchore-cli image add nginx:latest
anchore-cli image wait nginx:latest
anchore-cli image vuln nginx:latest all

# 4. Harborå†…ç½®æ‰«æ
# Harbor UIä¸­å¯ç”¨é•œåƒæ‰«æ

# 5. æœ€ä½³å®è·µ
# - ä½¿ç”¨æœ€å°åŒ–åŸºç¡€é•œåƒ (alpine, distroless)
# - å®šæœŸæ›´æ–°é•œåƒ
# - ä¸è¦åœ¨é•œåƒä¸­å­˜å‚¨æ•æ„Ÿä¿¡æ¯
# - ä½¿ç”¨å¤šé˜¶æ®µæ„å»º
```

---

### ç½‘ç»œå®‰å…¨

**Q33: å¦‚ä½•å®ç°Kubernetesç½‘ç»œéš”ç¦»ï¼Ÿ**

**A**: ç½‘ç»œç­–ç•¥é…ç½®ï¼š

```yaml
# 1. é»˜è®¤æ‹’ç»æ‰€æœ‰å…¥ç«™æµé‡
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress

# 2. åªå…è®¸ç‰¹å®šPodè®¿é—®
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-frontend
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8080

# 3. å…è®¸ç‰¹å®šå‘½åç©ºé—´è®¿é—®
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-namespace
spec:
  podSelector:
    matchLabels:
      app: api
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: production

# 4. é™åˆ¶å‡ºç«™æµé‡
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-external
spec:
  podSelector:
    matchLabels:
      app: secure-app
  policyTypes:
  - Egress
  egress:
  - to:
    - podSelector: {}  # åªå…è®¸åŒå‘½åç©ºé—´
```

---

### æƒé™ç®¡ç†

**Q34: å¦‚ä½•é…ç½®æœ€å°æƒé™RBACï¼Ÿ**

**A**: RBACé…ç½®ï¼š

```yaml
# 1. åˆ›å»ºServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: myapp-sa
  namespace: default

# 2. åˆ›å»ºRole (å‘½åç©ºé—´çº§åˆ«)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: default
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]

# 3. ç»‘å®šRoleåˆ°ServiceAccount
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: ServiceAccount
  name: myapp-sa
  namespace: default
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

# 4. åœ¨Podä¸­ä½¿ç”¨
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  serviceAccountName: myapp-sa
  containers:
  - name: app
    image: myapp:v1

# 5. ClusterRole (é›†ç¾¤çº§åˆ«)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: node-reader
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]
```

---

## æ—¥å¸¸è¿ç»´é—®é¢˜

### å¤‡ä»½æ¢å¤

**Q35: å¦‚ä½•å¤‡ä»½Kubernetesé›†ç¾¤ï¼Ÿ**

**A**: å¤‡ä»½æ–¹æ¡ˆï¼š

```bash
# 1. å¤‡ä»½etcd (æœ€é‡è¦)
ETCDCTL_API=3 etcdctl snapshot save /backup/etcd-snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# éªŒè¯å¤‡ä»½
ETCDCTL_API=3 etcdctl snapshot status /backup/etcd-snapshot.db -w table

# 2. æ¢å¤etcd
systemctl stop etcd
rm -rf /var/lib/etcd
ETCDCTL_API=3 etcdctl snapshot restore /backup/etcd-snapshot.db \
  --data-dir=/var/lib/etcd
systemctl start etcd

# 3. ä½¿ç”¨Veleroå¤‡ä»½é›†ç¾¤èµ„æº
# å®‰è£…Velero
velero install --provider aws \
  --plugins velero/velero-plugin-for-aws:v1.5.0 \
  --bucket velero-backups \
  --secret-file ./credentials-velero

# å¤‡ä»½æ•´ä¸ªé›†ç¾¤
velero backup create full-backup

# å¤‡ä»½ç‰¹å®šå‘½åç©ºé—´
velero backup create prod-backup --include-namespaces production

# æ¢å¤
velero restore create --from-backup full-backup

# 4. å¤‡ä»½è¯ä¹¦
tar czf /backup/k8s-certs.tar.gz /etc/kubernetes/pki/

# 5. å¤‡ä»½é…ç½®
kubectl get all --all-namespaces -o yaml > all-resources.yaml
```

---

### ç›‘æ§å‘Šè­¦

**Q36: å¦‚ä½•é…ç½®Prometheuså‘Šè­¦ï¼Ÿ**

**A**: å‘Šè­¦é…ç½®ï¼š

```yaml
# 1. Prometheuså‘Šè­¦è§„åˆ™
# alerts.yml
groups:
- name: node
  interval: 30s
  rules:
  # CPUä½¿ç”¨ç‡è¿‡é«˜
  - alert: HighCPUUsage
    expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage on {{ $labels.instance }}"
      description: "CPU usage is above 80% (current: {{ $value }}%)"
  
  # å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜
  - alert: HighMemoryUsage
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High memory usage on {{ $labels.instance }}"
      description: "Memory usage is above 85% (current: {{ $value }}%)"
  
  # ç£ç›˜ç©ºé—´ä¸è¶³
  - alert: DiskSpaceLow
    expr: (node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes{fstype!="tmpfs"}) * 100 < 15
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Disk space low on {{ $labels.instance }}"
      description: "Disk {{ $labels.mountpoint }} has less than 15% space left"

# 2. Alertmanageré…ç½®
# alertmanager.yml
global:
  resolve_timeout: 5m

route:
  group_by: ['alertname', 'cluster']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  routes:
  - match:
      severity: critical
    receiver: 'critical'
  - match:
      severity: warning
    receiver: 'warning'

receivers:
- name: 'default'
  webhook_configs:
  - url: 'http://alertmanager-webhook:5001/'

- name: 'critical'
  email_configs:
  - to: 'ops@example.com'
    from: 'alertmanager@example.com'
    smarthost: 'smtp.gmail.com:587'
    auth_username: 'alertmanager@example.com'
    auth_password: 'password'
  webhook_configs:
  - url: 'http://your-webhook-url'

- name: 'warning'
  webhook_configs:
  - url: 'http://your-webhook-url'
```

---

### æ—¥å¿—ç®¡ç†

**Q37: å¦‚ä½•æ”¶é›†å’Œåˆ†ææ—¥å¿—ï¼Ÿ**

**A**: æ—¥å¿—æ–¹æ¡ˆï¼š

```bash
# 1. Dockeræ—¥å¿—
# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker logs <container>
docker logs -f --tail 100 <container>

# é…ç½®æ—¥å¿—é©±åŠ¨
cat > /etc/docker/daemon.json <<EOF
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m",
    "max-file": "3"
  }
}
EOF

# 2. Kubernetesæ—¥å¿—
# æŸ¥çœ‹Podæ—¥å¿—
kubectl logs <pod-name>
kubectl logs <pod-name> -c <container-name>
kubectl logs -f <pod-name>

# æŸ¥çœ‹æ‰€æœ‰Podæ—¥å¿—
kubectl logs -l app=myapp

# 3. ä½¿ç”¨ELK Stack
# Filebeaté…ç½®
filebeat.inputs:
- type: container
  paths:
    - /var/log/containers/*.log

output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  index: "kubernetes-%{+yyyy.MM.dd}"

# 4. ä½¿ç”¨Loki
# Promtailé…ç½®
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
- job_name: kubernetes-pods
  kubernetes_sd_configs:
  - role: pod
  relabel_configs:
  - source_labels: [__meta_kubernetes_pod_name]
    target_label: pod

# 5. GrafanaæŸ¥è¯¢Loki
{namespace="production", pod=~"myapp-.*"}
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-10-19  
**ç»´æŠ¤çŠ¶æ€**: âœ… å®Œæˆ

---

> ğŸ’¡ **æç¤º**:
>
> - é‡åˆ°é—®é¢˜å…ˆæŸ¥çœ‹æ—¥å¿—å’Œäº‹ä»¶
> - ä½¿ç”¨å®˜æ–¹æ–‡æ¡£å’Œç¤¾åŒºèµ„æº
> - ä¿æŒç³»ç»Ÿå’Œè½¯ä»¶æ›´æ–°
> - å®šæœŸå¤‡ä»½é‡è¦æ•°æ®
> - åœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯è§£å†³æ–¹æ¡ˆ
