# 告警管理

> **返回**: [监控告警首页](README.md) | [运维管理首页](../README.md) | [部署指南首页](../../00_索引导航/README.md)

---

## 📋 目录

- [告警管理](#告警管理)
  - [📋 目录](#-目录)
  - [1. Alertmanager架构](#1-alertmanager架构)
    - [1.1 部署配置](#11-部署配置)
    - [1.2 Alertmanager配置](#12-alertmanager配置)
  - [2. 告警规则](#2-告警规则)
    - [2.1 基础设施告警](#21-基础设施告警)
    - [2.2 Kubernetes告警](#22-kubernetes告警)
    - [2.3 应用告警](#23-应用告警)
  - [3. 通知路由](#3-通知路由)
    - [3.1 分级路由](#31-分级路由)
    - [3.2 静默规则](#32-静默规则)
  - [4. 告警最佳实践](#4-告警最佳实践)
    - [4.1 告警设计原则](#41-告警设计原则)
    - [4.2 告警模板](#42-告警模板)
    - [4.3 Runbook编写](#43-runbook编写)
    - [4.4 告警收敛](#44-告警收敛)

---

## 1. Alertmanager架构

### 1.1 部署配置

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  replicas: 3
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.26.0
        args:
        - '--config.file=/etc/alertmanager/alertmanager.yml'
        - '--storage.path=/alertmanager'
        - '--cluster.listen-address=0.0.0.0:9094'
        - '--cluster.peer=alertmanager-0.alertmanager:9094'
        - '--cluster.peer=alertmanager-1.alertmanager:9094'
        - '--cluster.peer=alertmanager-2.alertmanager:9094'
        ports:
        - containerPort: 9093
        - containerPort: 9094
        volumeMounts:
        - name: config
          mountPath: /etc/alertmanager
        - name: data
          mountPath: /alertmanager
      volumes:
      - name: config
        configMap:
          name: alertmanager-config
      - name: data
        emptyDir: {}
```

### 1.2 Alertmanager配置

```yaml
global:
  resolve_timeout: 5m
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alertmanager@example.com'
  smtp_auth_username: 'your-email@gmail.com'
  smtp_auth_password: 'your-password'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  routes:
  - match:
      severity: critical
    receiver: 'critical-team'
    continue: true
  - match:
      severity: warning
    receiver: 'warning-team'

receivers:
- name: 'default'
  email_configs:
  - to: 'team@example.com'
    headers:
      Subject: '[Monitoring] {{ .GroupLabels.alertname }}'

- name: 'critical-team'
  email_configs:
  - to: 'oncall@example.com'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/xxx'
    channel: '#critical-alerts'
    title: '{{ .GroupLabels.alertname }}'
    text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
  webhook_configs:
  - url: 'http://pagerduty-webhook:8080/alert'

- name: 'warning-team'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/yyy'
    channel: '#warnings'

inhibit_rules:
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  equal: ['alertname', 'instance']
```

---

## 2. 告警规则

### 2.1 基础设施告警

```yaml
groups:
- name: infrastructure_alerts
  interval: 30s
  rules:
  # 节点状态
  - alert: NodeDown
    expr: up{job="node-exporter"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "节点 {{ $labels.instance }} 离线"
      description: "节点已离线超过5分钟"

  # CPU告警
  - alert: HighCPUUsage
    expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 90
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "{{ $labels.instance }} CPU使用率过高"
      description: "当前CPU使用率: {{ $value | humanize }}%"

  # 内存告警
  - alert: HighMemoryUsage
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "{{ $labels.instance }} 内存使用率过高"
      description: "当前内存使用率: {{ $value | humanize }}%"

  # 磁盘告警
  - alert: DiskSpaceLow
    expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "{{ $labels.instance }} 磁盘空间不足"
      description: "挂载点 {{ $labels.mountpoint }} 使用率: {{ $value | humanize }}%"

  - alert: DiskSpaceCritical
    expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 95
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "{{ $labels.instance }} 磁盘空间严重不足"
      description: "挂载点 {{ $labels.mountpoint }} 使用率: {{ $value | humanize }}%"
```

### 2.2 Kubernetes告警

```yaml
groups:
- name: kubernetes_alerts
  rules:
  # Pod重启
  - alert: PodRestarting
    expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} 频繁重启"
      description: "过去15分钟重启 {{ $value }} 次"

  # Pod状态异常
  - alert: PodNotReady
    expr: kube_pod_status_phase{phase!="Running",phase!="Succeeded"} > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} 状态异常"
      description: "当前状态: {{ $labels.phase }}"

  # Deployment副本数不足
  - alert: DeploymentReplicasMismatch
    expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} 副本数异常"
      description: "期望: {{ $labels.spec_replicas }}, 实际: {{ $labels.status_replicas_available }}"

  # HPA达到上限
  - alert: HPAMaxedOut
    expr: kube_horizontalpodautoscaler_status_current_replicas == kube_horizontalpodautoscaler_spec_max_replicas
    for: 15m
    labels:
      severity: warning
    annotations:
      summary: "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} 达到最大副本数"
      description: "当前副本数: {{ $value }}"
```

### 2.3 应用告警

```yaml
groups:
- name: application_alerts
  rules:
  # HTTP错误率
  - alert: HighErrorRate
    expr: |
      rate(http_requests_total{status=~"5.."}[5m]) / 
      rate(http_requests_total[5m]) * 100 > 5
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "服务 {{ $labels.service }} 错误率过高"
      description: "当前5xx错误率: {{ $value | humanize }}%"

  # HTTP延迟
  - alert: HighLatency
    expr: histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 1
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "服务 {{ $labels.service }} 延迟过高"
      description: "P99延迟: {{ $value | humanize }}s"

  # 数据库连接池
  - alert: DBConnectionPoolHigh
    expr: db_connection_pool_usage > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "数据库连接池使用率过高"
      description: "当前使用率: {{ $value }}%"
```

---

## 3. 通知路由

### 3.1 分级路由

```yaml
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  
  routes:
  # Critical告警
  - match:
      severity: critical
    receiver: 'pagerduty'
    group_wait: 0s
    repeat_interval: 5m
    routes:
    - match:
        team: platform
      receiver: 'platform-oncall'
    - match:
        team: application
      receiver: 'app-oncall'
  
  # Warning告警
  - match:
      severity: warning
    receiver: 'slack-warnings'
    repeat_interval: 4h
  
  # 特定服务告警
  - match_re:
      service: ^(user|order|payment)$
    receiver: 'business-team'
    repeat_interval: 1h
```

### 3.2 静默规则

```yaml
# 通过amtool命令静默
amtool silence add \
  alertname=HighCPUUsage \
  instance=node-1 \
  --comment="计划维护" \
  --duration=2h

# 通过API静默
curl -X POST http://alertmanager:9093/api/v2/silences \
  -H 'Content-Type: application/json' \
  -d '{
    "matchers": [{
      "name": "alertname",
      "value": "HighCPUUsage"
    }],
    "startsAt": "2024-01-01T00:00:00Z",
    "endsAt": "2024-01-01T02:00:00Z",
    "comment": "计划维护",
    "createdBy": "admin"
  }'
```

---

## 4. 告警最佳实践

### 4.1 告警设计原则

**5个原则**:

1. **可操作性**: 告警必须需要人工干预
2. **可诊断性**: 提供足够的上下文信息
3. **避免噪音**: 避免误报和重复告警
4. **分级合理**: 根据影响程度分级
5. **及时有效**: 在问题严重前告警

**SRE四大黄金信号**:

| 信号 | 指标示例 | 告警阈值 |
|-----|---------|---------|
| **延迟** | `http_request_duration_seconds` | P99 > 1s |
| **流量** | `http_requests_total` | QPS下降50% |
| **错误** | `http_requests_total{status=~"5.."}` | 错误率 > 5% |
| **饱和度** | `node_cpu_seconds_total` | CPU > 90% |

### 4.2 告警模板

**告警标题模板**:

```yaml
annotations:
  summary: |
    [{{ $labels.severity }}] {{ $labels.alertname }} on {{ $labels.instance }}
  description: |
    {{ $labels.alertname }} 触发
    
    集群: {{ $labels.cluster }}
    命名空间: {{ $labels.namespace }}
    实例: {{ $labels.instance }}
    
    当前值: {{ $value }}
    
    Runbook: https://wiki.example.com/runbooks/{{ $labels.alertname }}
```

### 4.3 Runbook编写

**告警处理手册**:

```markdown
# HighCPUUsage告警处理

## 严重程度
Warning

## 告警含义
节点CPU使用率持续超过90%

## 影响
可能导致应用响应变慢，影响用户体验

## 排查步骤
1. 查看top命令确认高CPU进程
    ```bash
    ssh {{ $labels.instance }}
    top -o %CPU
    ```

2. 检查是否有异常进程

    ```bash
    ps aux | sort -nrk 3,3 | head -10
    ```

3. 查看Kubernetes Pod资源使用

    ```bash
    kubectl top pod -A
    ```

## 解决方案

- 短期: 扩容Pod副本数或增加资源限制
- 长期: 优化应用代码或升级节点规格

```

### 4.4 告警收敛

**时间窗口聚合**:

```yaml
group_by: ['alertname', 'cluster', 'service']
group_wait: 30s        # 等待30s聚合同组告警
group_interval: 5m     # 每5分钟发送一次聚合告警
repeat_interval: 4h    # 4小时后重复发送
```

**告警抑制**:

```yaml
inhibit_rules:
# 节点离线时抑制该节点的所有其他告警
- source_match:
    alertname: 'NodeDown'
  target_match_re:
    alertname: '.*'
  equal: ['instance']

# Critical告警抑制Warning告警
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  equal: ['alertname', 'instance']
```

---

**更新时间**: 2025-10-19  
**文档版本**: v1.0  
**状态**: ✅ 完成
