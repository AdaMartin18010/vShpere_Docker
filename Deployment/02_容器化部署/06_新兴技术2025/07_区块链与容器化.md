# 区块链与容器化/虚拟化技术 (2025)

> **返回**: [新兴技术2025首页](README.md) | [容器化部署首页](../README.md)

---

## 📋 目录

- [区块链与容器化/虚拟化技术 (2025)](#区块链与容器化虚拟化技术-2025)
  - [📋 目录](#-目录)
  - [1. 概述](#1-概述)
    - [1.1 为什么容器化区块链？](#11-为什么容器化区块链)
    - [1.2 2025年技术趋势](#12-2025年技术趋势)
    - [1.3 技术栈对比](#13-技术栈对比)
  - [2. Hyperledger Fabric容器化](#2-hyperledger-fabric容器化)
    - [2.1 架构概述](#21-架构概述)
    - [2.2 Kubernetes部署](#22-kubernetes部署)
    - [2.3 Fabric Operator (2025)](#23-fabric-operator-2025)
    - [2.4 链码容器化](#24-链码容器化)
    - [2.5 生产配置示例](#25-生产配置示例)
  - [3. Ethereum节点容器化](#3-ethereum节点容器化)
    - [3.1 客户端选择](#31-客户端选择)
    - [3.2 Geth容器化部署](#32-geth容器化部署)
    - [3.3 Kubernetes上的Ethereum](#33-kubernetes上的ethereum)
    - [3.4 Ethereum Operator](#34-ethereum-operator)
    - [3.5 验证器节点高可用](#35-验证器节点高可用)
  - [4. Layer 2解决方案容器化](#4-layer-2解决方案容器化)
    - [4.1 Optimism/Arbitrum容器化](#41-optimismarbitrum容器化)
    - [4.2 zkSync Era容器化](#42-zksync-era容器化)
    - [4.3 Polygon CDK容器化](#43-polygon-cdk容器化)
    - [4.4 Base (Coinbase L2)](#44-base-coinbase-l2)
  - [5. 区块链即服务 (BaaS)](#5-区块链即服务-baas)
    - [5.1 Kubernetes Operators](#51-kubernetes-operators)
    - [5.2 Helm Charts生态](#52-helm-charts生态)
    - [5.3 多租户BaaS平台](#53-多租户baas平台)
  - [6. Web3基础设施容器化](#6-web3基础设施容器化)
    - [6.1 IPFS容器化](#61-ipfs容器化)
    - [6.2 The Graph节点](#62-the-graph节点)
    - [6.3 Chainlink节点](#63-chainlink节点)
    - [6.4 分布式RPC服务](#64-分布式rpc服务)
  - [7. 零知识证明 (ZK) 基础设施](#7-零知识证明-zk-基础设施)
    - [7.1 zkEVM容器化](#71-zkevm容器化)
    - [7.2 证明生成器容器化](#72-证明生成器容器化)
    - [7.3 GPU加速证明](#73-gpu加速证明)
  - [8. 存储和持久化](#8-存储和持久化)
    - [8.1 区块链数据存储](#81-区块链数据存储)
    - [8.2 快照和备份](#82-快照和备份)
    - [8.3 存储优化](#83-存储优化)
  - [9. 网络和通信](#9-网络和通信)
    - [9.1 P2P网络配置](#91-p2p网络配置)
    - [9.2 RPC/WebSocket服务](#92-rpcwebsocket服务)
    - [9.3 负载均衡](#93-负载均衡)
  - [10. 安全和隔离](#10-安全和隔离)
    - [10.1 私钥管理](#101-私钥管理)
    - [10.2 网络隔离](#102-网络隔离)
    - [10.3 TEE和机密计算](#103-tee和机密计算)
  - [11. 监控和可观测性](#11-监控和可观测性)
    - [11.1 Prometheus指标](#111-prometheus指标)
    - [11.2 链上监控](#112-链上监控)
    - [11.3 告警策略](#113-告警策略)
  - [12. 高可用性和灾难恢复](#12-高可用性和灾难恢复)
    - [12.1 多区域部署](#121-多区域部署)
    - [12.2 故障转移](#122-故障转移)
    - [12.3 灾难恢复](#123-灾难恢复)
  - [13. 性能优化](#13-性能优化)
    - [13.1 资源配置](#131-资源配置)
    - [13.2 网络优化](#132-网络优化)
    - [13.3 存储I/O优化](#133-存储io优化)
  - [14. 跨链技术容器化](#14-跨链技术容器化)
    - [14.1 Cosmos IBC Relayer](#141-cosmos-ibc-relayer)
    - [14.2 Polkadot Parachain](#142-polkadot-parachain)
    - [14.3 跨链桥容器化](#143-跨链桥容器化)
  - [15. 生产最佳实践](#15-生产最佳实践)
    - [15.1 部署清单](#151-部署清单)
    - [15.2 运维建议](#152-运维建议)
    - [15.3 成本优化](#153-成本优化)
    - [15.4 合规性](#154-合规性)
  - [16. 案例研究](#16-案例研究)
    - [16.1 企业联盟链 (Fabric)](#161-企业联盟链-fabric)
    - [16.2 公链RPC服务 (Ethereum)](#162-公链rpc服务-ethereum)
    - [16.3 Layer 2 Sequencer](#163-layer-2-sequencer)
  - [17. 工具和资源](#17-工具和资源)
    - [17.1 开源工具](#171-开源工具)
    - [17.2 商业平台](#172-商业平台)
    - [17.3 学习资源](#173-学习资源)
  - [总结](#总结)

---

## 1. 概述

### 1.1 为什么容器化区块链？

**核心优势**:

```yaml
可移植性:
  ✅ 统一的部署环境 (开发/测试/生产)
  ✅ 跨云平台 (AWS、Azure、GCP、私有云)
  ✅ 简化依赖管理 (操作系统、库、配置)

可扩展性:
  ✅ 水平扩展 (读副本、验证器节点)
  ✅ 自动伸缩 (基于负载)
  ✅ 多链/多网络部署 (主网、测试网)

运维效率:
  ✅ 标准化部署流程 (Helm、Operator)
  ✅ 自动化更新和回滚
  ✅ 集中监控和日志管理
  ✅ 降低基础设施成本 (资源共享、GPU池)

开发体验:
  ✅ 本地快速启动 (Docker Compose)
  ✅ 一致的开发/生产环境
  ✅ CI/CD集成 (自动化测试、部署)
```

### 1.2 2025年技术趋势

```yaml
容器化成熟度:
  ✅ Kubernetes成为区块链基础设施标配
  ✅ Operator模式普及 (Fabric Operator、Besu Operator)
  ✅ Helm Charts生态完善
  ✅ 服务网格集成 (Istio、Linkerd)

Layer 2爆发:
  ✅ Optimistic Rollup主网 (Optimism、Arbitrum)
  ✅ ZK Rollup产品化 (zkSync Era、Polygon zkEVM、Scroll)
  ✅ App-specific Rollup (OP Stack、Orbit)
  ✅ Validium/Volition混合方案

零知识证明 (ZK):
  ✅ zkEVM成熟 (Type 2-4兼容性)
  ✅ GPU加速证明生成 (NVIDIA A100/H100)
  ✅ 证明聚合 (Aggregation Layer)
  ✅ 递归证明 (Recursive SNARKs)

模块化区块链:
  ✅ 数据可用性层 (Celestia、EigenDA)
  ✅ 执行层分离 (Rollup、Validium)
  ✅ 共识层模块化 (Tendermint、HotStuff)
  ✅ 跨层通信标准化

Web3基础设施:
  ✅ RPC服务商业化 (Alchemy、Infura、QuickNode)
  ✅ 索引器标准化 (The Graph、Subsquid)
  ✅ 预言机网络 (Chainlink 2.0、Pyth)
  ✅ IPFS/Arweave永久存储

机密计算集成:
  ✅ TEE环境 (Intel SGX、AMD SEV)
  ✅ 机密容器 (Confidential Containers)
  ✅ 隐私计算 (MPC、FHE)
  ✅ zkML (零知识机器学习)
```

### 1.3 技术栈对比

| 维度 | Hyperledger Fabric | Ethereum (L1) | Layer 2 (Rollup) | Cosmos/Tendermint |
|------|-------------------|---------------|------------------|-------------------|
| **共识机制** | Raft/BFT | PoS (Beacon Chain) | L1继承安全性 | Tendermint BFT |
| **TPS** | 1,000-20,000 | 15-30 (升级后~100) | 2,000-10,000 | 1,000-10,000 |
| **最终性** | 秒级 | 12分钟 (2 epochs) | 秒级 (软确认) | 秒级 |
| **智能合约** | Chaincode (Go/Java/Node.js) | Solidity/Vyper | EVM兼容 | CosmWasm/EVM |
| **许可模型** | 许可链 (Permissioned) | 无许可 (Permissionless) | 无许可 | 可配置 |
| **容器化难度** | ⭐⭐⭐⭐ (复杂) | ⭐⭐⭐ (中等) | ⭐⭐⭐ (中等) | ⭐⭐ (简单) |
| **典型场景** | 企业联盟链 | 公链DApp | 扩容方案 | 应用链 |
| **存储需求** | 中 (GB-TB) | 大 (TB+, 全节点~10TB) | 中 (GB-TB) | 中 (GB-TB) |
| **K8s Operator** | ✅ (hlf-operator) | ✅ (besu-operator) | ⚠️ (社区方案) | ✅ (cosmos-operator) |

---

## 2. Hyperledger Fabric容器化

### 2.1 架构概述

```yaml
Hyperledger Fabric 2.5+ 组件:
  
  Orderer (排序服务):
    - 职责: 交易排序、区块生成
    - 共识: Raft (推荐) / BFT (实验)
    - 容器: fabric-orderer
    - 副本: 3-7 (奇数, 生产至少3)
  
  Peer (对等节点):
    - 职责: 账本维护、链码执行
    - 角色: Endorser、Committer、Anchor Peer
    - 容器: fabric-peer
    - 副本: 2+ (每组织)
  
  CA (证书颁发):
    - 职责: 证书管理、身份注册
    - 容器: fabric-ca
    - 副本: 2+ (主备)
  
  Chaincode (链码):
    - 职责: 智能合约逻辑
    - 运行时: Go、Java、Node.js
    - 容器: 动态创建 (per chaincode)
  
  CouchDB (状态数据库):
    - 职责: 世界状态存储、富查询
    - 容器: couchdb
    - 副本: 1 per Peer (可选)
```

**网络拓扑**:

```text
┌─────────────────────── Kubernetes Cluster ───────────────────────┐
│                                                                    │
│  ┌─── Organization 1 Namespace ───┐                               │
│  │                                 │                               │
│  │  ┌─────────┐      ┌─────────┐  │                               │
│  │  │ Peer 0  │      │ Peer 1  │  │                               │
│  │  │(Endorser)│◄────►│(Endorser)│  │                               │
│  │  └─────────┘      └─────────┘  │                               │
│  │       │                │        │                               │
│  │       ▼                ▼        │                               │
│  │  ┌─────────┐      ┌─────────┐  │                               │
│  │  │CouchDB 0│      │CouchDB 1│  │                               │
│  │  └─────────┘      └─────────┘  │                               │
│  │                                 │                               │
│  │  ┌─────────┐                   │                               │
│  │  │ CA Org1 │                   │                               │
│  │  └─────────┘                   │                               │
│  └─────────────────────────────────┘                               │
│                                                                    │
│  ┌─── Organization 2 Namespace ───┐                               │
│  │  (类似结构)                      │                               │
│  └─────────────────────────────────┘                               │
│                                                                    │
│  ┌─── Orderer Namespace ─────────┐                                │
│  │                               │                                │
│  │  ┌────────┐  ┌────────┐  ┌────────┐                           │
│  │  │Orderer0│◄─┤Orderer1│─►│Orderer2│  (Raft集群)               │
│  │  └────────┘  └────────┘  └────────┘                           │
│  │                               │                                │
│  └───────────────────────────────┘                                │
│                                                                    │
└────────────────────────────────────────────────────────────────────┘
```

### 2.2 Kubernetes部署

**Namespace隔离**:

```yaml
# fabric-namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: fabric-orderer
  labels:
    app: hyperledger-fabric
    component: orderer
---
apiVersion: v1
kind: Namespace
metadata:
  name: fabric-org1
  labels:
    app: hyperledger-fabric
    component: peer
    organization: org1
---
apiVersion: v1
kind: Namespace
metadata:
  name: fabric-org2
  labels:
    app: hyperledger-fabric
    component: peer
    organization: org2
```

**Orderer部署** (StatefulSet):

```yaml
# orderer-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: orderer
  namespace: fabric-orderer
spec:
  serviceName: orderer
  replicas: 3
  selector:
    matchLabels:
      app: orderer
  template:
    metadata:
      labels:
        app: orderer
    spec:
      initContainers:
      - name: init-orderer
        image: hyperledger/fabric-orderer:2.5.10
        command:
        - sh
        - -c
        - |
          # 初始化orderer配置
          if [ ! -f /var/hyperledger/orderer/orderer.genesis.block ]; then
            configtxgen -profile SampleMultiNodeEtcdRaft \
              -channelID system-channel \
              -outputBlock /var/hyperledger/orderer/orderer.genesis.block
          fi
        volumeMounts:
        - name: orderer-data
          mountPath: /var/hyperledger/orderer
        - name: config
          mountPath: /etc/hyperledger/configtx
      
      containers:
      - name: orderer
        image: hyperledger/fabric-orderer:2.5.10
        ports:
        - containerPort: 7050
          name: grpc
        - containerPort: 9443
          name: operations
        env:
        - name: ORDERER_GENERAL_LISTENADDRESS
          value: "0.0.0.0"
        - name: ORDERER_GENERAL_LISTENPORT
          value: "7050"
        - name: ORDERER_GENERAL_GENESISMETHOD
          value: "file"
        - name: ORDERER_GENERAL_GENESISFILE
          value: "/var/hyperledger/orderer/orderer.genesis.block"
        - name: ORDERER_GENERAL_LOCALMSPID
          value: "OrdererMSP"
        - name: ORDERER_GENERAL_LOCALMSPDIR
          value: "/var/hyperledger/orderer/msp"
        - name: ORDERER_GENERAL_TLS_ENABLED
          value: "true"
        - name: ORDERER_GENERAL_TLS_PRIVATEKEY
          value: "/var/hyperledger/orderer/tls/server.key"
        - name: ORDERER_GENERAL_TLS_CERTIFICATE
          value: "/var/hyperledger/orderer/tls/server.crt"
        - name: ORDERER_GENERAL_TLS_ROOTCAS
          value: "[/var/hyperledger/orderer/tls/ca.crt]"
        - name: ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE
          value: "/var/hyperledger/orderer/tls/server.crt"
        - name: ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY
          value: "/var/hyperledger/orderer/tls/server.key"
        - name: ORDERER_GENERAL_CLUSTER_ROOTCAS
          value: "[/var/hyperledger/orderer/tls/ca.crt]"
        - name: ORDERER_OPERATIONS_LISTENADDRESS
          value: "0.0.0.0:9443"
        - name: ORDERER_METRICS_PROVIDER
          value: "prometheus"
        
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        
        volumeMounts:
        - name: orderer-data
          mountPath: /var/hyperledger/orderer
        - name: tls-cert
          mountPath: /var/hyperledger/orderer/tls
        - name: msp
          mountPath: /var/hyperledger/orderer/msp
        
        livenessProbe:
          httpGet:
            path: /healthz
            port: 9443
          initialDelaySeconds: 30
          periodSeconds: 10
        
        readinessProbe:
          httpGet:
            path: /healthz
            port: 9443
          initialDelaySeconds: 10
          periodSeconds: 5
      
      volumes:
      - name: tls-cert
        secret:
          secretName: orderer-tls
      - name: msp
        secret:
          secretName: orderer-msp
      - name: config
        configMap:
          name: fabric-config
  
  volumeClaimTemplates:
  - metadata:
      name: orderer-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi

---
apiVersion: v1
kind: Service
metadata:
  name: orderer
  namespace: fabric-orderer
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - port: 7050
    name: grpc
  - port: 9443
    name: operations
  selector:
    app: orderer
```

**Peer部署**:

```yaml
# peer-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: peer
  namespace: fabric-org1
spec:
  serviceName: peer
  replicas: 2
  selector:
    matchLabels:
      app: peer
  template:
    metadata:
      labels:
        app: peer
    spec:
      containers:
      - name: peer
        image: hyperledger/fabric-peer:2.5.10
        ports:
        - containerPort: 7051
          name: grpc
        - containerPort: 7052
          name: events
        - containerPort: 9443
          name: operations
        env:
        - name: CORE_PEER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: CORE_PEER_ADDRESS
          value: "$(CORE_PEER_ID).peer.fabric-org1.svc.cluster.local:7051"
        - name: CORE_PEER_LISTENADDRESS
          value: "0.0.0.0:7051"
        - name: CORE_PEER_CHAINCODEADDRESS
          value: "$(CORE_PEER_ID).peer.fabric-org1.svc.cluster.local:7052"
        - name: CORE_PEER_CHAINCODELISTENADDRESS
          value: "0.0.0.0:7052"
        - name: CORE_PEER_GOSSIP_BOOTSTRAP
          value: "peer-0.peer.fabric-org1.svc.cluster.local:7051"
        - name: CORE_PEER_GOSSIP_EXTERNALENDPOINT
          value: "$(CORE_PEER_ID).peer.fabric-org1.svc.cluster.local:7051"
        - name: CORE_PEER_LOCALMSPID
          value: "Org1MSP"
        - name: CORE_PEER_TLS_ENABLED
          value: "true"
        - name: CORE_PEER_TLS_CERT_FILE
          value: "/etc/hyperledger/fabric/tls/server.crt"
        - name: CORE_PEER_TLS_KEY_FILE
          value: "/etc/hyperledger/fabric/tls/server.key"
        - name: CORE_PEER_TLS_ROOTCERT_FILE
          value: "/etc/hyperledger/fabric/tls/ca.crt"
        - name: CORE_LEDGER_STATE_STATEDATABASE
          value: "CouchDB"
        - name: CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS
          value: "localhost:5984"
        - name: CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME
          value: "admin"
        - name: CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD
          valueFrom:
            secretKeyRef:
              name: couchdb-secret
              key: password
        - name: CORE_OPERATIONS_LISTENADDRESS
          value: "0.0.0.0:9443"
        - name: CORE_METRICS_PROVIDER
          value: "prometheus"
        - name: FABRIC_LOGGING_SPEC
          value: "INFO"
        
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        
        volumeMounts:
        - name: peer-data
          mountPath: /var/hyperledger/production
        - name: tls-cert
          mountPath: /etc/hyperledger/fabric/tls
        - name: msp
          mountPath: /etc/hyperledger/fabric/msp
        - name: docker-sock
          mountPath: /var/run/docker.sock
        
        livenessProbe:
          httpGet:
            path: /healthz
            port: 9443
          initialDelaySeconds: 60
          periodSeconds: 10
        
        readinessProbe:
          httpGet:
            path: /healthz
            port: 9443
          initialDelaySeconds: 30
          periodSeconds: 5
      
      # Sidecar: CouchDB
      - name: couchdb
        image: couchdb:3.3.3
        ports:
        - containerPort: 5984
        env:
        - name: COUCHDB_USER
          value: "admin"
        - name: COUCHDB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: couchdb-secret
              key: password
        
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        
        volumeMounts:
        - name: couchdb-data
          mountPath: /opt/couchdb/data
        
        livenessProbe:
          httpGet:
            path: /_up
            port: 5984
          initialDelaySeconds: 60
          periodSeconds: 10
      
      volumes:
      - name: tls-cert
        secret:
          secretName: peer-tls
      - name: msp
        secret:
          secretName: peer-msp
      - name: docker-sock
        hostPath:
          path: /var/run/docker.sock
          type: Socket
  
  volumeClaimTemplates:
  - metadata:
      name: peer-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 200Gi
  - metadata:
      name: couchdb-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi

---
apiVersion: v1
kind: Service
metadata:
  name: peer
  namespace: fabric-org1
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - port: 7051
    name: grpc
  - port: 7052
    name: events
  - port: 9443
    name: operations
  selector:
    app: peer
```

### 2.3 Fabric Operator (2025)

**安装hlf-operator**:

```bash
# 添加Helm仓库
helm repo add hlf-operator https://hyperledger-labs.github.io/hlf-operator/
helm repo update

# 安装Operator
helm install hlf-operator hlf-operator/hlf-operator \
  --namespace hlf-operator \
  --create-namespace \
  --version 1.10.0

# 验证
kubectl get pods -n hlf-operator
```

**使用CRD部署Peer**:

```yaml
# fabricpeer-crd.yaml
apiVersion: hlf.hyperledger.org/v1alpha1
kind: FabricPeer
metadata:
  name: org1-peer0
  namespace: fabric-org1
spec:
  image: hyperledger/fabric-peer
  tag: 2.5.10
  
  mspID: Org1MSP
  
  replicas: 1
  
  resources:
    peer:
      requests:
        cpu: "1000m"
        memory: "2Gi"
      limits:
        cpu: "2000m"
        memory: "4Gi"
    couchdb:
      requests:
        cpu: "500m"
        memory: "1Gi"
      limits:
        cpu: "1000m"
        memory: "2Gi"
  
  storage:
    peer:
      size: "200Gi"
      storageClass: "fast-ssd"
    couchdb:
      size: "100Gi"
      storageClass: "fast-ssd"
  
  stateDb: CouchDB
  
  secret:
    enrollment:
      component:
        cahost: ca-org1.fabric-org1.svc.cluster.local
        caname: ca
        caport: 7054
        catls:
          cacert: LS0tLS1CRUdJTi... # CA证书
        enrollid: peer0
        enrollsecret: peer0pw
      tls:
        cahost: ca-org1.fabric-org1.svc.cluster.local
        caname: tlsca
        caport: 7054
        catls:
          cacert: LS0tLS1CRUdJTi... # TLS CA证书
        enrollid: peer0
        enrollsecret: peer0pw
  
  service:
    type: ClusterIP
  
  hostAliases: []
  
  externalEndpoint: peer0-org1.example.com:443
  
  gossipExternalEndpoint: peer0-org1.example.com:443
  
  istio:
    port: 443
```

**部署链码 (Chaincode)**:

```yaml
# fabricchaincode-crd.yaml
apiVersion: hlf.hyperledger.org/v1alpha1
kind: FabricChaincode
metadata:
  name: asset-transfer
  namespace: fabric-org1
spec:
  image: hyperledger/fabric-chaincode-node
  tag: 2.5.10
  
  packageID: asset-transfer:1.0
  
  version: "1.0"
  
  language: node
  
  peers:
  - name: org1-peer0
    namespace: fabric-org1
  
  channel: mychannel
  
  endorsementPolicy: "OR('Org1MSP.peer','Org2MSP.peer')"
  
  initRequired: true
  
  env:
  - name: CHAINCODE_CCID
    value: asset-transfer:1.0
  - name: CHAINCODE_ADDRESS
    value: "0.0.0.0:9999"
  
  resources:
    requests:
      cpu: "500m"
      memory: "512Mi"
    limits:
      cpu: "1000m"
      memory: "1Gi"
```

### 2.4 链码容器化

**External Chaincode模式** (推荐):

```yaml
# chaincode-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: asset-transfer-chaincode
  namespace: fabric-org1
spec:
  replicas: 2
  selector:
    matchLabels:
      app: asset-transfer-chaincode
  template:
    metadata:
      labels:
        app: asset-transfer-chaincode
    spec:
      containers:
      - name: chaincode
        image: myregistry/asset-transfer:1.0
        ports:
        - containerPort: 9999
        env:
        - name: CHAINCODE_SERVER_ADDRESS
          value: "0.0.0.0:9999"
        - name: CHAINCODE_ID
          value: "asset-transfer:1.0"
        - name: CHAINCODE_TLS_DISABLED
          value: "false"
        - name: CHAINCODE_TLS_KEY
          value: "/etc/hyperledger/fabric/tls/server.key"
        - name: CHAINCODE_TLS_CERT
          value: "/etc/hyperledger/fabric/tls/server.crt"
        - name: CHAINCODE_TLS_CLIENT_CACERT
          value: "/etc/hyperledger/fabric/tls/ca.crt"
        
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        
        volumeMounts:
        - name: tls-cert
          mountPath: /etc/hyperledger/fabric/tls
        
        livenessProbe:
          tcpSocket:
            port: 9999
          initialDelaySeconds: 30
          periodSeconds: 10
        
        readinessProbe:
          tcpSocket:
            port: 9999
          initialDelaySeconds: 10
          periodSeconds: 5
      
      volumes:
      - name: tls-cert
        secret:
          secretName: chaincode-tls

---
apiVersion: v1
kind: Service
metadata:
  name: asset-transfer-chaincode
  namespace: fabric-org1
spec:
  selector:
    app: asset-transfer-chaincode
  ports:
  - protocol: TCP
    port: 9999
    targetPort: 9999
```

**connection.json配置**:

```json
{
  "address": "asset-transfer-chaincode.fabric-org1.svc.cluster.local:9999",
  "dial_timeout": "10s",
  "tls_required": true,
  "client_auth_required": true,
  "client_key": "-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n",
  "client_cert": "-----BEGIN CERTIFICATE-----\n...\n-----END CERTIFICATE-----\n",
  "root_cert": "-----BEGIN CERTIFICATE-----\n...\n-----END CERTIFICATE-----\n"
}
```

### 2.5 生产配置示例

**资源配置**:

```yaml
# 生产环境推荐配置
Orderer:
  replicas: 3-7 (奇数)
  cpu: 2-4 cores
  memory: 4-8 GB
  storage: 100-500 GB SSD

Peer:
  replicas: 2-4 per organization
  cpu: 4-8 cores
  memory: 8-16 GB
  storage: 500 GB - 2 TB SSD
  
CouchDB:
  cpu: 2-4 cores
  memory: 4-8 GB
  storage: 200 GB - 1 TB SSD

CA:
  replicas: 2 (主备)
  cpu: 1-2 cores
  memory: 2-4 GB
  storage: 10-20 GB
```

**网络策略**:

```yaml
# networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: fabric-network-policy
  namespace: fabric-org1
spec:
  podSelector:
    matchLabels:
      app: peer
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # 允许来自同组织Peer的连接
  - from:
    - podSelector:
        matchLabels:
          app: peer
    ports:
    - protocol: TCP
      port: 7051
  # 允许来自Orderer的连接
  - from:
    - namespaceSelector:
        matchLabels:
          component: orderer
    ports:
    - protocol: TCP
      port: 7051
  # 允许来自其他组织Peer的连接 (跨Namespace)
  - from:
    - namespaceSelector:
        matchLabels:
          app: hyperledger-fabric
          component: peer
    ports:
    - protocol: TCP
      port: 7051
  # 允许Prometheus监控
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    - podSelector:
        matchLabels:
          app: prometheus
    ports:
    - protocol: TCP
      port: 9443
  
  egress:
  # 允许DNS
  - to:
    - namespaceSelector: {}
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  # 允许访问Orderer
  - to:
    - namespaceSelector:
        matchLabels:
          component: orderer
    ports:
    - protocol: TCP
      port: 7050
  # 允许访问CouchDB (同Pod)
  - to:
    - podSelector:
        matchLabels:
          app: peer
    ports:
    - protocol: TCP
      port: 5984
  # 允许访问其他组织Peer
  - to:
    - namespaceSelector:
        matchLabels:
          app: hyperledger-fabric
          component: peer
    ports:
    - protocol: TCP
      port: 7051
```

---

## 3. Ethereum节点容器化

### 3.1 客户端选择

| 客户端 | 语言 | 共识+执行 | 资源占用 | 同步速度 | 推荐场景 |
|--------|------|-----------|----------|----------|----------|
| **Geth** | Go | ✅ | 中 (8-16GB RAM) | 快 (~6h) | 通用、生产 ⭐ |
| **Besu** | Java | ✅ | 高 (16-32GB RAM) | 中 (~12h) | 企业、隐私 |
| **Nethermind** | C# | ✅ | 中 (8-16GB RAM) | 快 (~4h, 快照) | 归档、RPC ⭐ |
| **Erigon** | Go | ✅ | 低 (16GB RAM) | 快 (~2h, 优化) | 归档、开发 ⭐ |
| **Reth** | Rust | ❌ (开发中) | 低 (TBD) | 极快 (TBD) | 未来 🚀 |

**共识层客户端** (Post-Merge):

| 客户端 | 语言 | 资源占用 | 推荐场景 |
|--------|------|----------|----------|
| **Prysm** | Go | 中 (4-8GB) | 通用 ⭐ |
| **Lighthouse** | Rust | 低 (4-6GB) | 性能优先 ⭐ |
| **Teku** | Java | 高 (8-16GB) | 企业 |
| **Nimbus** | Nim | 低 (2-4GB) | 资源受限 ⭐ |
| **Lodestar** | TypeScript | 中 (4-8GB) | 开发 |

### 3.2 Geth容器化部署

**Docker Compose (本地开发)**:

```yaml
# docker-compose.yml
version: '3.8'

services:
  geth:
    image: ethereum/client-go:v1.14.8
    container_name: geth-mainnet
    restart: unless-stopped
    ports:
      - "8545:8545"   # HTTP RPC
      - "8546:8546"   # WebSocket
      - "30303:30303" # P2P (TCP)
      - "30303:30303/udp" # P2P (UDP)
      - "8551:8551"   # Engine API (共识层)
    volumes:
      - geth-data:/root/.ethereum
      - ./jwt.hex:/root/jwt.hex:ro
    command:
      - --mainnet
      - --http
      - --http.addr=0.0.0.0
      - --http.vhosts=*
      - --http.corsdomain=*
      - --http.api=eth,net,web3,txpool
      - --ws
      - --ws.addr=0.0.0.0
      - --ws.origins=*
      - --ws.api=eth,net,web3,txpool
      - --authrpc.addr=0.0.0.0
      - --authrpc.port=8551
      - --authrpc.vhosts=*
      - --authrpc.jwtsecret=/root/jwt.hex
      - --syncmode=snap
      - --cache=8192
      - --maxpeers=50
      - --metrics
      - --metrics.addr=0.0.0.0
      - --metrics.port=6060
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "10"
  
  lighthouse:
    image: sigp/lighthouse:v5.3.0
    container_name: lighthouse-mainnet
    restart: unless-stopped
    ports:
      - "5052:5052"   # HTTP API
      - "9000:9000"   # P2P (TCP)
      - "9000:9000/udp" # P2P (UDP)
    volumes:
      - lighthouse-data:/root/.lighthouse
      - ./jwt.hex:/root/jwt.hex:ro
    command:
      - lighthouse
      - bn
      - --network=mainnet
      - --datadir=/root/.lighthouse
      - --http
      - --http-address=0.0.0.0
      - --http-port=5052
      - --execution-endpoint=http://geth:8551
      - --execution-jwt=/root/jwt.hex
      - --checkpoint-sync-url=https://beaconstate.ethstaker.cc
      - --metrics
      - --metrics-address=0.0.0.0
      - --metrics-port=5054
    depends_on:
      - geth
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "10"

volumes:
  geth-data:
  lighthouse-data:
```

**生成JWT密钥**:

```bash
# 生成JWT密钥 (执行层和共识层共享)
openssl rand -hex 32 > jwt.hex

# 启动
docker-compose up -d

# 查看日志
docker-compose logs -f

# 检查同步状态
docker exec geth-mainnet geth attach --exec 'eth.syncing'
docker exec lighthouse-mainnet lighthouse bn --network mainnet --datadir /root/.lighthouse http get http://localhost:5052/eth/v1/node/syncing
```

### 3.3 Kubernetes上的Ethereum

**Namespace**:

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ethereum-mainnet
  labels:
    name: ethereum-mainnet
```

**JWT Secret**:

```bash
# 生成JWT密钥
openssl rand -hex 32 > jwt.hex

# 创建Secret
kubectl create secret generic jwt-secret \
  --from-file=jwt.hex=jwt.hex \
  --namespace ethereum-mainnet
```

**Geth StatefulSet**:

```yaml
# geth-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: geth
  namespace: ethereum-mainnet
spec:
  serviceName: geth
  replicas: 1
  selector:
    matchLabels:
      app: geth
  template:
    metadata:
      labels:
        app: geth
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "6060"
        prometheus.io/path: "/debug/metrics/prometheus"
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
        runAsNonRoot: true
      
      initContainers:
      - name: init-datadir
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          mkdir -p /data/geth
          chown -R 1000:1000 /data
        volumeMounts:
        - name: geth-data
          mountPath: /data
        securityContext:
          runAsUser: 0
      
      containers:
      - name: geth
        image: ethereum/client-go:v1.14.8
        ports:
        - containerPort: 8545
          name: http-rpc
        - containerPort: 8546
          name: ws-rpc
        - containerPort: 30303
          name: p2p-tcp
          protocol: TCP
        - containerPort: 30303
          name: p2p-udp
          protocol: UDP
        - containerPort: 8551
          name: engine-api
        - containerPort: 6060
          name: metrics
        
        command:
        - geth
        - --mainnet
        - --datadir=/data/geth
        - --http
        - --http.addr=0.0.0.0
        - --http.port=8545
        - --http.vhosts=*
        - --http.corsdomain=*
        - --http.api=eth,net,web3,txpool
        - --ws
        - --ws.addr=0.0.0.0
        - --ws.port=8546
        - --ws.origins=*
        - --ws.api=eth,net,web3,txpool
        - --authrpc.addr=0.0.0.0
        - --authrpc.port=8551
        - --authrpc.vhosts=*
        - --authrpc.jwtsecret=/secrets/jwt.hex
        - --syncmode=snap
        - --cache=8192
        - --maxpeers=50
        - --metrics
        - --metrics.addr=0.0.0.0
        - --metrics.port=6060
        - --pprof
        - --pprof.addr=0.0.0.0
        - --pprof.port=6061
        
        resources:
          requests:
            memory: "16Gi"
            cpu: "4000m"
          limits:
            memory: "32Gi"
            cpu: "8000m"
        
        volumeMounts:
        - name: geth-data
          mountPath: /data
        - name: jwt-secret
          mountPath: /secrets
          readOnly: true
        
        livenessProbe:
          httpGet:
            path: /
            port: 8545
            httpHeaders:
            - name: Content-Type
              value: application/json
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - |
              geth attach --exec 'eth.syncing' http://localhost:8545 | grep -q false
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 10
      
      volumes:
      - name: jwt-secret
        secret:
          secretName: jwt-secret
  
  volumeClaimTemplates:
  - metadata:
      name: geth-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 2Ti  # 主网归档节点需要10+ TB

---
apiVersion: v1
kind: Service
metadata:
  name: geth
  namespace: ethereum-mainnet
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "6060"
spec:
  selector:
    app: geth
  ports:
  - name: http-rpc
    port: 8545
    targetPort: 8545
  - name: ws-rpc
    port: 8546
    targetPort: 8546
  - name: p2p-tcp
    port: 30303
    targetPort: 30303
    protocol: TCP
  - name: p2p-udp
    port: 30303
    targetPort: 30303
    protocol: UDP
  - name: engine-api
    port: 8551
    targetPort: 8551
  - name: metrics
    port: 6060
    targetPort: 6060
  type: LoadBalancer  # 或 NodePort, ClusterIP
```

**Lighthouse StatefulSet**:

```yaml
# lighthouse-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: lighthouse
  namespace: ethereum-mainnet
spec:
  serviceName: lighthouse
  replicas: 1
  selector:
    matchLabels:
      app: lighthouse
  template:
    metadata:
      labels:
        app: lighthouse
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "5054"
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
        runAsNonRoot: true
      
      initContainers:
      - name: init-datadir
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          mkdir -p /data/lighthouse
          chown -R 1000:1000 /data
        volumeMounts:
        - name: lighthouse-data
          mountPath: /data
        securityContext:
          runAsUser: 0
      
      containers:
      - name: lighthouse
        image: sigp/lighthouse:v5.3.0
        ports:
        - containerPort: 5052
          name: http-api
        - containerPort: 9000
          name: p2p-tcp
          protocol: TCP
        - containerPort: 9000
          name: p2p-udp
          protocol: UDP
        - containerPort: 5054
          name: metrics
        
        command:
        - lighthouse
        - bn
        - --network=mainnet
        - --datadir=/data/lighthouse
        - --http
        - --http-address=0.0.0.0
        - --http-port=5052
        - --execution-endpoint=http://geth.ethereum-mainnet.svc.cluster.local:8551
        - --execution-jwt=/secrets/jwt.hex
        - --checkpoint-sync-url=https://beaconstate.ethstaker.cc
        - --metrics
        - --metrics-address=0.0.0.0
        - --metrics-port=5054
        - --target-peers=50
        
        resources:
          requests:
            memory: "8Gi"
            cpu: "2000m"
          limits:
            memory: "16Gi"
            cpu: "4000m"
        
        volumeMounts:
        - name: lighthouse-data
          mountPath: /data
        - name: jwt-secret
          mountPath: /secrets
          readOnly: true
        
        livenessProbe:
          httpGet:
            path: /eth/v1/node/health
            port: 5052
          initialDelaySeconds: 60
          periodSeconds: 30
        
        readinessProbe:
          httpGet:
            path: /eth/v1/node/health
            port: 5052
          initialDelaySeconds: 30
          periodSeconds: 10
      
      volumes:
      - name: jwt-secret
        secret:
          secretName: jwt-secret
  
  volumeClaimTemplates:
  - metadata:
      name: lighthouse-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 500Gi

---
apiVersion: v1
kind: Service
metadata:
  name: lighthouse
  namespace: ethereum-mainnet
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "5054"
spec:
  selector:
    app: lighthouse
  ports:
  - name: http-api
    port: 5052
    targetPort: 5052
  - name: p2p-tcp
    port: 9000
    targetPort: 9000
    protocol: TCP
  - name: p2p-udp
    port: 9000
    targetPort: 9000
    protocol: UDP
  - name: metrics
    port: 5054
    targetPort: 5054
  type: LoadBalancer
```

### 3.4 Ethereum Operator

**使用Besu Operator**:

```bash
# 安装Operator
kubectl apply -f https://raw.githubusercontent.com/Consensys/besu-kubernetes/main/deploy/besu-operator.yaml

# 验证
kubectl get pods -n besu-operator-system
```

**创建Besu网络**:

```yaml
# besu-network.yaml
apiVersion: besu.k8s.io/v1alpha1
kind: BesuNetwork
metadata:
  name: besu-mainnet
  namespace: ethereum-mainnet
spec:
  network: mainnet
  
  nodes:
  - name: validator-1
    type: validator
    replicas: 1
    resources:
      requests:
        cpu: "4"
        memory: "16Gi"
      limits:
        cpu: "8"
        memory: "32Gi"
    storage:
      size: 2Ti
      storageClass: fast-ssd
    p2p:
      enabled: true
      port: 30303
    rpc:
      enabled: true
      port: 8545
      apis:
      - eth
      - net
      - web3
    ws:
      enabled: true
      port: 8546
    metrics:
      enabled: true
      port: 9545
  
  - name: rpc-1
    type: rpc
    replicas: 2
    resources:
      requests:
        cpu: "4"
        memory: "16Gi"
      limits:
        cpu: "8"
        memory: "32Gi"
    storage:
      size: 2Ti
      storageClass: fast-ssd
    rpc:
      enabled: true
      port: 8545
      apis:
      - eth
      - net
      - web3
      - txpool
    ws:
      enabled: true
      port: 8546
  
  monitoring:
    enabled: true
    prometheus:
      enabled: true
      namespace: monitoring
```

### 3.5 验证器节点高可用

**多区域部署**:

```yaml
# validator-deployment.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: eth-validator
  namespace: ethereum-mainnet
spec:
  serviceName: eth-validator
  replicas: 3  # 主+备份
  podManagementPolicy: Parallel
  
  selector:
    matchLabels:
      app: eth-validator
  
  template:
    metadata:
      labels:
        app: eth-validator
    spec:
      # 反亲和性: 不同可用区
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: eth-validator
            topologyKey: topology.kubernetes.io/zone
      
      containers:
      - name: validator
        image: sigp/lighthouse:v5.3.0
        command:
        - lighthouse
        - vc
        - --network=mainnet
        - --datadir=/data
        - --beacon-nodes=http://lighthouse.ethereum-mainnet.svc.cluster.local:5052
        - --graffiti="K8s Validator"
        - --suggested-fee-recipient=0xYourFeeRecipient
        - --metrics
        - --metrics-address=0.0.0.0
        - --metrics-port=5064
        
        volumeMounts:
        - name: validator-data
          mountPath: /data
        - name: validator-keys
          mountPath: /keys
          readOnly: true
        
        resources:
          requests:
            memory: "4Gi"
            cpu: "1000m"
          limits:
            memory: "8Gi"
            cpu: "2000m"
  
  volumeClaimTemplates:
  - metadata:
      name: validator-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 50Gi
```

**自动切换 (Slashing保护)**:

```yaml
# slashing-protection配置
# 使用DVT (分布式验证器技术) 如SSV Network或Obol Network
apiVersion: v1
kind: ConfigMap
metadata:
  name: slashing-protection-config
  namespace: ethereum-mainnet
data:
  config.yaml: |
    # Web3Signer配置 (统一密钥管理)
    slashing-protection-db-url: postgresql://user:pass@postgres:5432/slashing
    slashing-protection-enabled: true
    
    # 或使用Dirk (分布式密钥管理)
    distributed-validator-enabled: true
    threshold: 2  # 3节点中2个签名
```

---

## 4. Layer 2解决方案容器化

### 4.1 Optimism/Arbitrum容器化

**Optimism架构**:

```text
┌──────────────── Optimism Stack (OP Stack) ────────────────┐
│                                                            │
│  ┌─────────────┐      ┌─────────────┐      ┌──────────┐  │
│  │  op-node    │◄────►│ op-geth     │◄────►│ op-batch │  │
│  │ (Rollup节点)│      │ (执行引擎)   │      │ (批处理器) │  │
│  └─────────────┘      └─────────────┘      └──────────┘  │
│         ▲                                         │       │
│         │                                         ▼       │
│         │                                  ┌──────────┐   │
│         └──────────────────────────────────┤ op-propose│   │
│                                            │ (提议器)  │   │
│                                            └──────────┘   │
│                                                  │        │
└──────────────────────────────────────────────────│────────┘
                                                  ▼
                                        Ethereum L1 (主网)
```

**Docker Compose部署** (Optimism Sepolia测试网):

```yaml
# optimism-docker-compose.yml
version: '3.8'

services:
  l1-geth:
    image: ethereum/client-go:v1.14.8
    container_name: l1-geth
    ports:
      - "8545:8545"
    volumes:
      - l1-data:/data
    command:
      - --sepolia
      - --datadir=/data
      - --http
      - --http.addr=0.0.0.0
      - --http.api=eth,net,web3
      - --ws
      - --ws.addr=0.0.0.0
  
  op-geth:
    image: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth:v1.101411.2
    container_name: op-geth
    ports:
      - "9545:8545"
      - "9546:8546"
      - "8551:8551"
    volumes:
      - op-geth-data:/data
      - ./jwt.hex:/jwt.hex:ro
    command:
      - --datadir=/data
      - --http
      - --http.addr=0.0.0.0
      - --http.port=8545
      - --http.api=eth,net,web3,debug
      - --ws
      - --ws.addr=0.0.0.0
      - --ws.port=8546
      - --authrpc.addr=0.0.0.0
      - --authrpc.port=8551
      - --authrpc.jwtsecret=/jwt.hex
      - --syncmode=full
      - --gcmode=archive
      - --rollup.sequencerhttp=https://sepolia-sequencer.optimism.io
    depends_on:
      - l1-geth
  
  op-node:
    image: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-node:v1.8.0
    container_name: op-node
    ports:
      - "9003:9003"
      - "7545:8545"
    volumes:
      - op-node-data:/data
      - ./jwt.hex:/jwt.hex:ro
      - ./rollup.json:/rollup.json:ro
    command:
      - op-node
      - --l1=http://l1-geth:8545
      - --l2=http://op-geth:8551
      - --l2.jwt-secret=/jwt.hex
      - --rollup.config=/rollup.json
      - --rpc.addr=0.0.0.0
      - --rpc.port=8545
      - --p2p.listen.ip=0.0.0.0
      - --p2p.listen.tcp=9003
      - --p2p.listen.udp=9003
      - --p2p.bootnodes=enr:-J64QBwRIWAco... # Sepolia bootnodes
      - --metrics.enabled
      - --metrics.addr=0.0.0.0
      - --metrics.port=7300
    depends_on:
      - l1-geth
      - op-geth

volumes:
  l1-data:
  op-geth-data:
  op-node-data:
```

**Kubernetes部署** (Optimism):

```yaml
# op-geth-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: op-geth
  namespace: optimism-sepolia
spec:
  serviceName: op-geth
  replicas: 1
  selector:
    matchLabels:
      app: op-geth
  template:
    metadata:
      labels:
        app: op-geth
    spec:
      containers:
      - name: op-geth
        image: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth:v1.101411.2
        ports:
        - containerPort: 8545
          name: rpc
        - containerPort: 8546
          name: ws
        - containerPort: 8551
          name: engine
        command:
        - geth
        - --datadir=/data
        - --http
        - --http.addr=0.0.0.0
        - --http.port=8545
        - --http.vhosts=*
        - --http.corsdomain=*
        - --http.api=eth,net,web3,debug,txpool
        - --ws
        - --ws.addr=0.0.0.0
        - --ws.port=8546
        - --ws.origins=*
        - --ws.api=eth,net,web3
        - --authrpc.addr=0.0.0.0
        - --authrpc.port=8551
        - --authrpc.vhosts=*
        - --authrpc.jwtsecret=/secrets/jwt.hex
        - --syncmode=full
        - --gcmode=archive
        - --rollup.sequencerhttp=https://sepolia-sequencer.optimism.io
        - --cache=4096
        - --maxpeers=50
        - --metrics
        - --metrics.addr=0.0.0.0
        - --metrics.port=6060
        
        resources:
          requests:
            memory: "8Gi"
            cpu: "2000m"
          limits:
            memory: "16Gi"
            cpu: "4000m"
        
        volumeMounts:
        - name: op-geth-data
          mountPath: /data
        - name: jwt-secret
          mountPath: /secrets
  
  volumeClaimTemplates:
  - metadata:
      name: op-geth-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 500Gi

---
# op-node-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: op-node
  namespace: optimism-sepolia
spec:
  replicas: 1
  selector:
    matchLabels:
      app: op-node
  template:
    metadata:
      labels:
        app: op-node
    spec:
      containers:
      - name: op-node
        image: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-node:v1.8.0
        ports:
        - containerPort: 8545
          name: rpc
        - containerPort: 9003
          name: p2p-tcp
          protocol: TCP
        - containerPort: 9003
          name: p2p-udp
          protocol: UDP
        - containerPort: 7300
          name: metrics
        
        command:
        - op-node
        - --l1=http://l1-geth.ethereum-sepolia.svc.cluster.local:8545
        - --l2=http://op-geth.optimism-sepolia.svc.cluster.local:8551
        - --l2.jwt-secret=/secrets/jwt.hex
        - --rollup.config=/config/rollup.json
        - --rpc.addr=0.0.0.0
        - --rpc.port=8545
        - --p2p.listen.ip=0.0.0.0
        - --p2p.listen.tcp=9003
        - --p2p.listen.udp=9003
        - --p2p.bootnodes=enr:-J64QBwRIWAco...
        - --metrics.enabled
        - --metrics.addr=0.0.0.0
        - --metrics.port=7300
        - --pprof.enabled
        - --pprof.addr=0.0.0.0
        - --pprof.port=6060
        
        resources:
          requests:
            memory: "4Gi"
            cpu: "1000m"
          limits:
            memory: "8Gi"
            cpu: "2000m"
        
        volumeMounts:
        - name: jwt-secret
          mountPath: /secrets
        - name: rollup-config
          mountPath: /config
      
      volumes:
      - name: jwt-secret
        secret:
          secretName: jwt-secret
      - name: rollup-config
        configMap:
          name: op-rollup-config
```

### 4.2 zkSync Era容器化

**zkSync Era架构**:

```text
┌──────────────── zkSync Era ────────────────┐
│                                            │
│  ┌────────────┐      ┌──────────────────┐ │
│  │   Server   │◄────►│   Prover (GPU)   │ │
│  │(API+Sequen)│      │  (证明生成器)     │ │
│  └────────────┘      └──────────────────┘ │
│         │                      │          │
│         ▼                      ▼          │
│  ┌────────────┐      ┌──────────────────┐ │
│  │ PostgreSQL │      │ Object Storage   │ │
│  │  (State)   │      │  (Proof/Witness) │ │
│  └────────────┘      └──────────────────┘ │
│                                            │
└────────────────────────────────────────────┘
```

**Docker Compose部署**:

```yaml
# zksync-docker-compose.yml
version: '3.8'

services:
  postgres:
    image: postgres:16
    container_name: zksync-postgres
    environment:
      POSTGRES_USER: zksync
      POSTGRES_PASSWORD: zksync
      POSTGRES_DB: zksync
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
  
  server:
    image: matterlabs/server-v2:latest2.0-v24.24.0
    container_name: zksync-server
    ports:
      - "3050:3050"  # JSON-RPC
      - "3051:3051"  # WebSocket
    environment:
      DATABASE_URL: postgres://zksync:zksync@postgres:5432/zksync
      ETH_CLIENT_WEB3_URL: https://sepolia.infura.io/v3/YOUR_KEY
      ZKSYNC_HOME: /usr/src/zksync
      RUST_LOG: info
    depends_on:
      - postgres
    volumes:
      - ./config:/config:ro
      - server-data:/data
    command:
      - zksync_server
      - --config=/config/server.toml
  
  prover:
    image: matterlabs/prover-v2:latest2.0-v24.24.0
    container_name: zksync-prover
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
    environment:
      DATABASE_URL: postgres://zksync:zksync@postgres:5432/zksync
      RUST_LOG: info
      CRS_FILE: /crs/setup_2^26.key
    depends_on:
      - postgres
      - server
    volumes:
      - ./crs:/crs:ro
      - prover-data:/data
    command:
      - zksync_prover

volumes:
  postgres-data:
  server-data:
  prover-data:
```

**Kubernetes部署** (zkSync Era Prover with GPU):

```yaml
# zksync-prover-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zksync-prover
  namespace: zksync-era
spec:
  replicas: 2  # GPU节点数量
  selector:
    matchLabels:
      app: zksync-prover
  template:
    metadata:
      labels:
        app: zksync-prover
    spec:
      # GPU节点调度
      nodeSelector:
        nvidia.com/gpu: "true"
      
      containers:
      - name: prover
        image: matterlabs/prover-v2:latest2.0-v24.24.0
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: zksync-db-secret
              key: url
        - name: RUST_LOG
          value: "info"
        - name: CRS_FILE
          value: "/crs/setup_2^26.key"
        - name: PROVER_TYPE
          value: "gpu"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        
        resources:
          requests:
            memory: "32Gi"
            cpu: "8000m"
            nvidia.com/gpu: 1  # 1个GPU
          limits:
            memory: "64Gi"
            cpu: "16000m"
            nvidia.com/gpu: 1
        
        volumeMounts:
        - name: crs
          mountPath: /crs
          readOnly: true
        - name: prover-data
          mountPath: /data
      
      volumes:
      - name: crs
        persistentVolumeClaim:
          claimName: crs-pvc
      - name: prover-data
        emptyDir: {}
```

### 4.3 Polygon CDK容器化

**Polygon CDK架构** (Chain Development Kit):

```yaml
# Polygon CDK组件:
components:
  zkevm-node:
    - Sequencer (排序器)
    - Aggregator (聚合器)
    - RPC (JSON-RPC服务)
    - Synchronizer (同步器)
  
  zkevm-prover:
    - Executor (执行器)
    - Prover (证明生成器, GPU)
  
  zkevm-bridge:
    - Bridge Service (跨链桥服务)
  
  databases:
    - State DB (PostgreSQL)
    - Pool DB (PostgreSQL)
```

**Docker Compose**:

```yaml
# polygon-cdk-docker-compose.yml
version: '3.8'

services:
  zkevm-state-db:
    image: postgres:16
    container_name: zkevm-state-db
    environment:
      POSTGRES_USER: state_user
      POSTGRES_PASSWORD: state_password
      POSTGRES_DB: state_db
    volumes:
      - state-db-data:/var/lib/postgresql/data
  
  zkevm-pool-db:
    image: postgres:16
    container_name: zkevm-pool-db
    environment:
      POSTGRES_USER: pool_user
      POSTGRES_PASSWORD: pool_password
      POSTGRES_DB: pool_db
    volumes:
      - pool-db-data:/var/lib/postgresql/data
  
  zkevm-prover:
    image: hermeznetwork/zkevm-prover:v6.0.0
    container_name: zkevm-prover
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
    volumes:
      - ./config/prover:/app/config:ro
    ports:
      - "50051:50051"
      - "50052:50052"
    command: /app/zkProver -c /app/config/config.json
  
  zkevm-node:
    image: hermeznetwork/zkevm-node:v0.6.7
    container_name: zkevm-node
    ports:
      - "8545:8545"  # RPC
      - "9091:9091"  # Prometheus
    environment:
      ZKEVM_NODE_ETHERMAN_URL: "https://sepolia.infura.io/v3/YOUR_KEY"
      ZKEVM_NODE_STATEDB_HOST: zkevm-state-db
      ZKEVM_NODE_POOL_DB_HOST: zkevm-pool-db
    volumes:
      - ./config/node:/app/config:ro
    depends_on:
      - zkevm-state-db
      - zkevm-pool-db
      - zkevm-prover
    command:
      - run
      - --network=custom
      - --custom-network-file=/app/config/genesis.json
      - --cfg=/app/config/node.toml
      - --components=rpc,synchronizer,sequencer,aggregator

volumes:
  state-db-data:
  pool-db-data:
```

### 4.4 Base (Coinbase L2)

**Base架构** (基于OP Stack):

Base是Coinbase基于OP Stack构建的L2,部署方式与Optimism类似:

```yaml
# base-node-values.yaml (Helm)
image:
  repository: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth
  tag: v1.101411.2

network: base-mainnet

l1:
  rpcUrl: https://mainnet.infura.io/v3/YOUR_KEY
  wsUrl: wss://mainnet.infura.io/ws/v3/YOUR_KEY

sequencer:
  http: https://mainnet-sequencer.base.org

resources:
  geth:
    requests:
      memory: "16Gi"
      cpu: "4000m"
    limits:
      memory: "32Gi"
      cpu: "8000m"
  node:
    requests:
      memory: "8Gi"
      cpu: "2000m"
    limits:
      memory: "16Gi"
      cpu: "4000m"

storage:
  geth:
    size: 1Ti
    storageClass: fast-ssd
  node:
    size: 100Gi
    storageClass: fast-ssd

metrics:
  enabled: true
  serviceMonitor:
    enabled: true

ingress:
  enabled: true
  className: nginx
  hosts:
  - host: base-rpc.example.com
    paths:
    - path: /
      pathType: Prefix
      backend:
        service:
          name: base-geth
          port:
            number: 8545
```

**Helm部署**:

```bash
# 添加仓库 (示例,实际需要官方仓库)
helm repo add base https://base.org/charts
helm repo update

# 部署
helm install base-mainnet base/op-stack \
  --namespace base-mainnet \
  --create-namespace \
  -f base-node-values.yaml

# 验证
kubectl get pods -n base-mainnet
kubectl logs -f base-mainnet-op-geth-0 -n base-mainnet
```

---

## 5. 区块链即服务 (BaaS)

### 5.1 Kubernetes Operators

**常用Operators**:

| Operator | 区块链 | GitHub | 功能 |
|----------|--------|--------|------|
| **hlf-operator** | Hyperledger Fabric | hyperledger-labs/hlf-operator | Peer/Orderer/CA/Chaincode |
| **besu-operator** | Hyperledger Besu | ConsenSys/besu-kubernetes | Besu节点管理 |
| **quorum-operator** | Quorum | ConsenSys/quorum-kubernetes | Quorum网络 |
| **substrate-operator** | Substrate/Polkadot | paritytech/substrate-operator | Substrate链 |
| **cosmos-operator** | Cosmos SDK | strangelove-ventures/cosmos-operator | Cosmos链 |

**Fabric Operator部署示例** (完整流程):

```bash
# 1. 安装Operator
helm install hlf-operator hlf-operator/hlf-operator \
  --namespace hlf-operator \
  --create-namespace

# 2. 部署CA
kubectl apply -f - <<EOF
apiVersion: hlf.hyperledger.org/v1alpha1
kind: FabricCA
metadata:
  name: ca-org1
  namespace: fabric-org1
spec:
  hosts:
  - ca-org1
  subject:
    cn: ca
    country: US
    locality: San Francisco
    organization: Org1
    organizationalUnit: Org1
    stateOrProvince: California
  tlsCA:
    subject:
      cn: tlsca
      country: US
      locality: San Francisco
      organization: Org1
      organizationalUnit: Org1
      stateOrProvince: California
  ca:
    name: ca
    cfg:
      identities:
        allowremove: false
      affiliations:
        allowremove: false
  service:
    type: ClusterIP
  image: hyperledger/fabric-ca
  version: 1.5.12
  storage:
    size: 10Gi
    storageClass: standard
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
EOF

# 3. 注册用户
kubectl hlf ca register --name=ca-org1 --namespace=fabric-org1 \
  --user=peer0 --secret=peer0pw --type=peer \
  --enroll-id=enroll --enroll-secret=enrollpw \
  --mspid=Org1MSP

# 4. 部署Peer (使用前面的FabricPeer CRD)
kubectl apply -f fabricpeer-crd.yaml

# 5. 创建通道
kubectl hlf channel generate --output=mychannel.block --name=mychannel \
  --organizations Org1MSP --ordererOrganizations OrdererMSP

kubectl hlf channel create --name=mychannel \
  --orderer-name=orderer --orderer-namespace=fabric-orderer

# 6. Peer加入通道
kubectl hlf channel join --name=mychannel \
  --peer-name=org1-peer0 --peer-namespace=fabric-org1

# 7. 安装链码 (使用FabricChaincode CRD)
kubectl apply -f fabricchaincode-crd.yaml
```

### 5.2 Helm Charts生态

**社区Helm Charts**:

```yaml
# 示例: Ethereum节点Helm部署
# 添加仓库
helm repo add ethereum-helm-charts https://skylenet.github.io/ethereum-helm-charts
helm repo update

# 安装Geth
helm install geth-mainnet ethereum-helm-charts/geth \
  --namespace ethereum \
  --create-namespace \
  -f - <<EOF
image:
  repository: ethereum/client-go
  tag: v1.14.8

syncMode: snap
gcMode: full
cache: 8192

resources:
  requests:
    memory: 16Gi
    cpu: 4
  limits:
    memory: 32Gi
    cpu: 8

persistence:
  enabled: true
  size: 2Ti
  storageClass: fast-ssd

service:
  type: LoadBalancer
  rpc:
    enabled: true
    port: 8545
  ws:
    enabled: true
    port: 8546

metrics:
  enabled: true
  serviceMonitor:
    enabled: true

ingress:
  enabled: true
  className: nginx
  hosts:
  - host: eth-rpc.example.com
    paths:
    - /
EOF
```

### 5.3 多租户BaaS平台

**架构设计**:

```text
┌──────────────────── BaaS平台 ────────────────────┐
│                                                  │
│  ┌──────────────── 控制平面 ─────────────────┐   │
│  │  - Web UI                                 │   │
│  │  - API Gateway                            │   │
│  │  - 用户管理/认证                           │   │
│  │  - 计费系统                                │   │
│  └───────────────────────────────────────────┘   │
│                       │                          │
│         ┌─────────────┼─────────────┐           │
│         ▼             ▼             ▼           │
│  ┌────────────┐ ┌────────────┐ ┌────────────┐  │
│  │ Tenant A   │ │ Tenant B   │ │ Tenant C   │  │
│  │ Namespace  │ │ Namespace  │ │ Namespace  │  │
│  │            │ │            │ │            │  │
│  │ ┌────────┐ │ │ ┌────────┐ │ │ ┌────────┐ │  │
│  │ │Fabric  │ │ │ │Ethereum│ │ │ │Polygon │ │  │
│  │ │Network │ │ │ │Node    │ │ │ │CDK     │ │  │
│  │ └────────┘ │ │ └────────┘ │ │ └────────┘ │  │
│  └────────────┘ └────────────┘ └────────────┘  │
│                                                  │
└──────────────────────────────────────────────────┘
```

**Namespace隔离 + ResourceQuota**:

```yaml
# tenant-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-alice
  labels:
    tenant: alice
    plan: premium
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-alice-quota
  namespace: tenant-alice
spec:
  hard:
    requests.cpu: "20"
    requests.memory: "64Gi"
    requests.storage: "2Ti"
    persistentvolumeclaims: "10"
    pods: "50"
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tenant-alice-isolation
  namespace: tenant-alice
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # 仅允许来自Ingress Controller的流量
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
  egress:
  # 允许DNS
  - to:
    - namespaceSelector: {}
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  # 允许出站到L1网络 (外部)
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443
  # 禁止访问其他租户
```

**API Gateway (Kong)**:

```yaml
# kong-route.yaml
apiVersion: configuration.konghq.com/v1
kind: KongIngress
metadata:
  name: tenant-alice-rpc
  namespace: tenant-alice
route:
  methods:
  - POST
  strip_path: false
  preserve_host: true
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tenant-alice-ethereum-rpc
  namespace: tenant-alice
  annotations:
    konghq.com/plugins: rate-limiting-alice,key-auth-alice
    konghq.com/override: tenant-alice-rpc
spec:
  ingressClassName: kong
  rules:
  - host: alice.baas-platform.com
    http:
      paths:
      - path: /eth/rpc
        pathType: Prefix
        backend:
          service:
            name: geth-rpc
            port:
              number: 8545
---
# 速率限制插件
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: rate-limiting-alice
  namespace: tenant-alice
config:
  minute: 1000  # Premium plan: 1000 req/min
  policy: local
plugin: rate-limiting
---
# API Key认证插件
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: key-auth-alice
  namespace: tenant-alice
plugin: key-auth
```

**计费系统集成** (Prometheus + Custom Metrics):

```yaml
# servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: tenant-alice-billing
  namespace: tenant-alice
spec:
  selector:
    matchLabels:
      app: geth
  endpoints:
  - port: metrics
    interval: 30s
    path: /debug/metrics/prometheus
---
# PrometheusRule: 计费告警
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: tenant-alice-billing-alert
  namespace: tenant-alice
spec:
  groups:
  - name: billing
    interval: 5m
    rules:
    # RPC请求计数
    - record: tenant:rpc_requests_total:rate5m
      expr: rate(rpc_requests_total{tenant="alice"}[5m])
    
    # 计算成本 (每1000请求 = $0.01)
    - record: tenant:cost_per_hour:usd
      expr: |
        (
          sum(rate(rpc_requests_total{tenant="alice"}[1h])) * 3600 / 1000 * 0.01
        ) + (
          sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate{namespace="tenant-alice"}) * 0.05
        ) + (
          sum(container_memory_working_set_bytes{namespace="tenant-alice"}) / 1024 / 1024 / 1024 * 0.01
        )
    
    # 超额告警
    - alert: BillingLimitExceeded
      expr: tenant:cost_per_hour:usd{tenant="alice"} > 10
      for: 1h
      annotations:
        summary: "Tenant alice exceeded hourly billing limit"
        description: "Current hourly cost: ${{ $value | humanize }}"
```

---

## 6. Web3基础设施容器化

### 6.1 IPFS容器化

**IPFS Cluster部署**:

```yaml
# ipfs-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ipfs
  namespace: web3
spec:
  serviceName: ipfs
  replicas: 3
  selector:
    matchLabels:
      app: ipfs
  template:
    metadata:
      labels:
        app: ipfs
    spec:
      initContainers:
      - name: init-ipfs
        image: ipfs/kubo:v0.30.0
        command:
        - sh
        - -c
        - |
          if [ ! -f /data/ipfs/config ]; then
            ipfs init --profile=server
          fi
        volumeMounts:
        - name: ipfs-data
          mountPath: /data/ipfs
      
      containers:
      - name: ipfs
        image: ipfs/kubo:v0.30.0
        ports:
        - containerPort: 4001
          name: swarm
        - containerPort: 5001
          name: api
        - containerPort: 8080
          name: gateway
        
        env:
        - name: IPFS_PATH
          value: /data/ipfs
        
        command:
        - ipfs
        - daemon
        - --migrate=true
        - --enable-gc
        - --routing=dhtclient
        
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        
        volumeMounts:
        - name: ipfs-data
          mountPath: /data/ipfs
        
        livenessProbe:
          httpGet:
            path: /api/v0/id
            port: 5001
          initialDelaySeconds: 60
          periodSeconds: 30
        
        readinessProbe:
          httpGet:
            path: /api/v0/id
            port: 5001
          initialDelaySeconds: 30
          periodSeconds: 10
  
  volumeClaimTemplates:
  - metadata:
      name: ipfs-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 500Gi
```

**IPFS Cluster (集群模式)**:

```yaml
# ipfs-cluster-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ipfs-cluster
  namespace: web3
spec:
  serviceName: ipfs-cluster
  replicas: 3
  selector:
    matchLabels:
      app: ipfs-cluster
  template:
    metadata:
      labels:
        app: ipfs-cluster
    spec:
      containers:
      - name: cluster
        image: ipfs/ipfs-cluster:v1.1.1
        ports:
        - containerPort: 9094
          name: api
        - containerPort: 9095
          name: proxy
        - containerPort: 9096
          name: cluster
        
        env:
        - name: CLUSTER_SECRET
          valueFrom:
            secretKeyRef:
              name: ipfs-cluster-secret
              key: secret
        - name: IPFS_API
          value: "/dns4/ipfs-0.ipfs.web3.svc.cluster.local/tcp/5001"
        
        volumeMounts:
        - name: cluster-data
          mountPath: /data/ipfs-cluster
        
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
  
  volumeClaimTemplates:
  - metadata:
      name: cluster-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi
```

### 6.2 The Graph节点

**The Graph Indexer部署**:

```yaml
# graph-node-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: graph-node
  namespace: web3
spec:
  serviceName: graph-node
  replicas: 2
  selector:
    matchLabels:
      app: graph-node
  template:
    metadata:
      labels:
        app: graph-node
    spec:
      containers:
      - name: graph-node
        image: graphprotocol/graph-node:v0.35.1
        ports:
        - containerPort: 8000
          name: http-rpc
        - containerPort: 8001
          name: ws-rpc
        - containerPort: 8020
          name: admin
        - containerPort: 8030
          name: index-status
        - containerPort: 8040
          name: metrics
        
        env:
        - name: postgres_host
          value: "postgres.web3.svc.cluster.local"
        - name: postgres_user
          value: "graph"
        - name: postgres_pass
          valueFrom:
            secretKeyRef:
              name: graph-db-secret
              key: password
        - name: postgres_db
          value: "graph"
        - name: ipfs
          value: "ipfs-0.ipfs.web3.svc.cluster.local:5001"
        - name: ethereum
          value: "mainnet:https://eth-mainnet.g.alchemy.com/v2/YOUR_KEY"
        - name: GRAPH_LOG
          value: "info"
        - name: GRAPH_ALLOW_NON_DETERMINISTIC_IPFS
          value: "true"
        - name: ETHEREUM_REORG_THRESHOLD
          value: "50"
        - name: ETHEREUM_ANCESTOR_COUNT
          value: "50"
        
        resources:
          requests:
            memory: "8Gi"
            cpu: "4000m"
          limits:
            memory: "16Gi"
            cpu: "8000m"
        
        livenessProbe:
          httpGet:
            path: /
            port: 8030
          initialDelaySeconds: 120
          periodSeconds: 30
        
        readinessProbe:
          httpGet:
            path: /
            port: 8030
          initialDelaySeconds: 60
          periodSeconds: 10

---
apiVersion: v1
kind: Service
metadata:
  name: graph-node
  namespace: web3
spec:
  selector:
    app: graph-node
  ports:
  - name: http-rpc
    port: 8000
  - name: ws-rpc
    port: 8001
  - name: admin
    port: 8020
  - name: index-status
    port: 8030
  - name: metrics
    port: 8040
  type: LoadBalancer
```

### 6.3 Chainlink节点

**Chainlink Node部署**:

```yaml
# chainlink-node-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: chainlink-node
  namespace: web3
spec:
  replicas: 1  # 单实例 (密钥管理)
  selector:
    matchLabels:
      app: chainlink-node
  template:
    metadata:
      labels:
        app: chainlink-node
    spec:
      securityContext:
        fsGroup: 14933
        runAsUser: 14933
        runAsNonRoot: true
      
      containers:
      - name: chainlink
        image: smartcontract/chainlink:2.16.0
        ports:
        - containerPort: 6688
          name: http
        
        env:
        - name: ROOT
          value: "/chainlink"
        - name: LOG_LEVEL
          value: "info"
        - name: ETH_CHAIN_ID
          value: "1"  # Mainnet
        - name: CHAINLINK_TLS_PORT
          value: "0"
        - name: SECURE_COOKIES
          value: "false"
        - name: ALLOW_ORIGINS
          value: "*"
        - name: ETH_URL
          value: "wss://eth-mainnet.g.alchemy.com/v2/YOUR_KEY"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: chainlink-db-secret
              key: url
        - name: LINK_CONTRACT_ADDRESS
          value: "0x514910771AF9Ca656af840dff83E8264EcF986CA"
        
        volumeMounts:
        - name: chainlink-data
          mountPath: /chainlink
        - name: chainlink-secrets
          mountPath: /secrets
          readOnly: true
        
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        
        livenessProbe:
          httpGet:
            path: /health
            port: 6688
          initialDelaySeconds: 60
          periodSeconds: 30
        
        readinessProbe:
          httpGet:
            path: /health
            port: 6688
          initialDelaySeconds: 30
          periodSeconds: 10
      
      volumes:
      - name: chainlink-data
        persistentVolumeClaim:
          claimName: chainlink-data-pvc
      - name: chainlink-secrets
        secret:
          secretName: chainlink-node-secret
```

### 6.4 分布式RPC服务

**Nginx负载均衡 (多Geth节点)**:

```yaml
# nginx-rpc-lb-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-rpc-config
  namespace: ethereum-mainnet
data:
  nginx.conf: |
    events {
        worker_connections 10000;
    }
    
    http {
        upstream geth_rpc {
            least_conn;
            server geth-0.geth.ethereum-mainnet.svc.cluster.local:8545 max_fails=3 fail_timeout=30s;
            server geth-1.geth.ethereum-mainnet.svc.cluster.local:8545 max_fails=3 fail_timeout=30s;
            server geth-2.geth.ethereum-mainnet.svc.cluster.local:8545 max_fails=3 fail_timeout=30s;
        }
        
        upstream geth_ws {
            least_conn;
            server geth-0.geth.ethereum-mainnet.svc.cluster.local:8546 max_fails=3 fail_timeout=30s;
            server geth-1.geth.ethereum-mainnet.svc.cluster.local:8546 max_fails=3 fail_timeout=30s;
            server geth-2.geth.ethereum-mainnet.svc.cluster.local:8546 max_fails=3 fail_timeout=30s;
        }
        
        # 速率限制
        limit_req_zone $binary_remote_addr zone=rpc_limit:10m rate=100r/s;
        limit_req_zone $http_x_api_key zone=api_key_limit:10m rate=1000r/s;
        
        # 健康检查
        server {
            listen 8090;
            location /health {
                access_log off;
                return 200 "healthy\n";
            }
        }
        
        # RPC代理
        server {
            listen 80;
            server_name _;
            
            # 速率限制
            limit_req zone=rpc_limit burst=200 nodelay;
            limit_req zone=api_key_limit burst=2000 nodelay;
            
            # JSON-RPC
            location / {
                proxy_pass http://geth_rpc;
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                
                # 超时设置
                proxy_connect_timeout 60s;
                proxy_send_timeout 120s;
                proxy_read_timeout 120s;
                
                # 缓存设置 (eth_chainId, net_version等静态方法)
                proxy_cache rpc_cache;
                proxy_cache_key "$request_body";
                proxy_cache_methods POST;
                proxy_cache_valid 200 5m;
                proxy_cache_bypass $http_cache_control;
            }
            
            # WebSocket
            location /ws {
                proxy_pass http://geth_ws;
                proxy_http_version 1.1;
                proxy_set_header Upgrade $http_upgrade;
                proxy_set_header Connection "upgrade";
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
                
                # WebSocket超时
                proxy_read_timeout 3600s;
                proxy_send_timeout 3600s;
            }
        }
        
        proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=rpc_cache:100m max_size=1g inactive=10m use_temp_path=off;
    }

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-rpc-lb
  namespace: ethereum-mainnet
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-rpc-lb
  template:
    metadata:
      labels:
        app: nginx-rpc-lb
    spec:
      containers:
      - name: nginx
        image: nginx:1.27-alpine
        ports:
        - containerPort: 80
          name: http
        - containerPort: 8090
          name: health
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8090
          initialDelaySeconds: 10
          periodSeconds: 10
      volumes:
      - name: nginx-config
        configMap:
          name: nginx-rpc-config

---
apiVersion: v1
kind: Service
metadata:
  name: rpc-loadbalancer
  namespace: ethereum-mainnet
spec:
  selector:
    app: nginx-rpc-lb
  ports:
  - port: 80
    targetPort: 80
  type: LoadBalancer
```

---

## 7. 零知识证明 (ZK) 基础设施

### 7.1 zkEVM容器化

**Polygon zkEVM Prover配置**:

```yaml
# zkevm-prover-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zkevm-prover
  namespace: polygon-zkevm
spec:
  serviceName: zkevm-prover
  replicas: 4  # GPU节点数量
  selector:
    matchLabels:
      app: zkevm-prover
  template:
    metadata:
      labels:
        app: zkevm-prover
    spec:
      nodeSelector:
        nvidia.com/gpu: "true"
        gpu-type: "a100"  # 指定A100 GPU
      
      containers:
      - name: prover
        image: hermeznetwork/zkevm-prover:v6.0.0
        ports:
        - containerPort: 50051
          name: executor
        - containerPort: 50052
          name: aggregator
        - containerPort: 50061
          name: metrics
        
        env:
        - name: PROVER_SERVER_PORT
          value: "50051"
        - name: PROVER_DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: zkevm-db-secret
              key: url
        - name: PROVER_CRS_PATH
          value: "/crs"
        - name: USE_GPU
          value: "true"
        - name: GPU_COUNT
          value: "1"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        
        resources:
          requests:
            memory: "64Gi"
            cpu: "16000m"
            nvidia.com/gpu: 1
          limits:
            memory: "128Gi"
            cpu: "32000m"
            nvidia.com/gpu: 1
        
        volumeMounts:
        - name: crs
          mountPath: /crs
          readOnly: true
        - name: prover-data
          mountPath: /data
        
        livenessProbe:
          tcpSocket:
            port: 50051
          initialDelaySeconds: 60
          periodSeconds: 30
      
      volumes:
      - name: crs
        persistentVolumeClaim:
          claimName: zkevm-crs-pvc
  
  volumeClaimTemplates:
  - metadata:
      name: prover-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 500Gi
```

### 7.2 证明生成器容器化

**通用ZK证明服务**:

```yaml
# zk-proof-service-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zk-proof-service
  namespace: zk-infra
spec:
  replicas: 2
  selector:
    matchLabels:
      app: zk-proof-service
  template:
    metadata:
      labels:
        app: zk-proof-service
    spec:
      nodeSelector:
        nvidia.com/gpu: "true"
      
      containers:
      - name: proof-generator
        image: myregistry/zk-proof-generator:latest
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 50051
          name: grpc
        
        env:
        - name: ZK_SCHEME
          value: "PLONK"  # PLONK, Groth16, STARK
        - name: GPU_ENABLED
          value: "true"
        - name: MAX_CONCURRENT_PROOFS
          value: "4"
        - name: REDIS_URL
          value: "redis://redis.zk-infra.svc.cluster.local:6379"
        
        resources:
          requests:
            memory: "32Gi"
            cpu: "8000m"
            nvidia.com/gpu: 1
          limits:
            memory: "64Gi"
            cpu: "16000m"
            nvidia.com/gpu: 1
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10

---
# HPA (基于自定义指标)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: zk-proof-service-hpa
  namespace: zk-infra
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: zk-proof-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  # 基于GPU利用率
  - type: Pods
    pods:
      metric:
        name: DCGM_FI_DEV_GPU_UTIL
      target:
        type: AverageValue
        averageValue: "80"
  # 基于队列长度
  - type: External
    external:
      metric:
        name: redis_queue_length
        selector:
          matchLabels:
            queue: "proof-requests"
      target:
        type: AverageValue
        averageValue: "50"
```

### 7.3 GPU加速证明

**NVIDIA GPU Operator配置**:

```bash
# 安装GPU Operator
helm install gpu-operator nvidia/gpu-operator \
  --namespace gpu-operator \
  --create-namespace \
  --set driver.enabled=true \
  --set toolkit.enabled=true \
  --set dcgm.enabled=true \
  --set dcgmExporter.enabled=true \
  --set gfd.enabled=true \
  --set migManager.enabled=true \
  --set operator.defaultRuntime=containerd

# 验证
kubectl get nodes -l nvidia.com/gpu=true
kubectl describe node <gpu-node>
```

**MIG (Multi-Instance GPU) 配置**:

```yaml
# mig-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mig-parted-config
  namespace: gpu-operator
data:
  config.yaml: |
    version: v1
    mig-configs:
      # 7个1g.10gb实例 (适合小型证明任务)
      all-1g.10gb:
        - devices: all
          mig-enabled: true
          mig-devices:
            "1g.10gb": 7
      
      # 混合配置 (大+小任务)
      mixed:
        - devices: all
          mig-enabled: true
          mig-devices:
            "3g.40gb": 2
            "1g.10gb": 1
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-operator-config
  namespace: gpu-operator
data:
  mig.strategy: "mixed"
  mig.config: "all-1g.10gb"
```

**资源请求 (MIG实例)**:

```yaml
resources:
  requests:
    nvidia.com/mig-1g.10gb: 1  # 1个MIG实例
  limits:
    nvidia.com/mig-1g.10gb: 1
```

---

## 8. 存储和持久化

### 8.1 区块链数据存储

**存储需求对比**:

| 区块链 | 模式 | 存储大小 | IOPS需求 | 推荐存储 |
|--------|------|----------|----------|----------|
| Ethereum (Mainnet) | Full Node | ~1TB | 高 (>10K) | NVMe SSD |
| Ethereum (Mainnet) | Archive Node | ~14TB | 极高 (>50K) | NVMe SSD RAID0 |
| Ethereum (Testnet) | Full Node | ~300GB | 中 (~5K) | SSD |
| Bitcoin | Full Node | ~600GB | 中 (~5K) | SSD |
| Hyperledger Fabric | Peer+CouchDB | 500GB-2TB | 中-高 | SSD |
| Polygon zkEVM | Sequencer | 1-2TB | 高 (>10K) | NVMe SSD |
| IPFS | Gateway | 500GB-5TB | 中 (~5K) | HDD/SSD混合 |

**StorageClass配置**:

```yaml
# fast-ssd-storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs  # 或 pd.csi.storage.gke.io (GCP)
parameters:
  type: gp3  # AWS: gp3, io2; GCP: pd-ssd, pd-extreme; Azure: Premium_LRS
  iopsPerGB: "50"
  throughput: "1000"
  fsType: ext4
  encrypted: "true"
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Retain  # 生产环境使用Retain

---
# nvme-ssd-storageclass.yaml (高性能)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nvme-ssd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io2  # AWS Provisioned IOPS SSD
  iopsPerGB: "500"  # 最高64000 IOPS
  throughput: "4000"  # 4000 MiB/s
  fsType: ext4
  encrypted: "true"
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Retain
```

**Local PV (本地NVMe)**:

```yaml
# local-storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-nvme
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Retain

---
# local-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-node1-nvme0
spec:
  capacity:
    storage: 3800Gi  # 4TB NVMe
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-nvme
  local:
    path: /mnt/nvme0n1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1
```

### 8.2 快照和备份

**VolumeSnapshot (CSI)**:

```yaml
# volumesnapshot-class.yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: ebs-snapshot-class
driver: ebs.csi.aws.com
deletionPolicy: Retain
parameters:
  tagSpecification_1: "Name=blockchain-snapshot"
  tagSpecification_2: "Environment=production"

---
# geth-snapshot.yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: geth-snapshot-20251020
  namespace: ethereum-mainnet
spec:
  volumeSnapshotClassName: ebs-snapshot-class
  source:
    persistentVolumeClaimName: geth-data-geth-0
```

**CronJob自动备份**:

```yaml
# geth-backup-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: geth-backup
  namespace: ethereum-mainnet
spec:
  schedule: "0 2 * * *"  # 每天凌晨2点
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 7
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: snapshot-creator
          containers:
          - name: backup
            image: bitnami/kubectl:1.31
            command:
            - sh
            - -c
            - |
              # 创建快照
              SNAPSHOT_NAME="geth-snapshot-$(date +%Y%m%d-%H%M%S)"
              cat <<EOF | kubectl apply -f -
              apiVersion: snapshot.storage.k8s.io/v1
              kind: VolumeSnapshot
              metadata:
                name: ${SNAPSHOT_NAME}
                namespace: ethereum-mainnet
              spec:
                volumeSnapshotClassName: ebs-snapshot-class
                source:
                  persistentVolumeClaimName: geth-data-geth-0
              EOF
              
              # 等待快照完成
              kubectl wait --for=jsonpath='{.status.readyToUse}'=true \
                volumesnapshot/${SNAPSHOT_NAME} \
                --namespace=ethereum-mainnet \
                --timeout=3600s
              
              # 清理旧快照 (保留7天)
              kubectl get volumesnapshot -n ethereum-mainnet \
                --sort-by=.metadata.creationTimestamp \
                -o name | head -n -7 | xargs -r kubectl delete -n ethereum-mainnet
          
          restartPolicy: OnFailure
```

### 8.3 存储优化

**Geth存储优化**:

```yaml
# 启用Ancient数据分离
command:
- geth
- --datadir=/data/geth
- --datadir.ancient=/data/ancient  # 分离古老数据 (>90天)
- --cache=8192
- --snapshot
- --txlookuplimit=0  # 禁用交易索引 (节省~400GB)

# 定期裁剪
- --state.scheme=path  # 使用Path-based storage (2.5x更快)

# 多卷挂载
volumeMounts:
- name: geth-data
  mountPath: /data/geth
- name: geth-ancient
  mountPath: /data/ancient  # 可使用更便宜的HDD
```

**定期裁剪 (Pruning)**:

```yaml
# geth-prune-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: geth-prune-$(date +%s)
  namespace: ethereum-mainnet
spec:
  template:
    spec:
      containers:
      - name: prune
        image: ethereum/client-go:v1.14.8
        command:
        - sh
        - -c
        - |
          # 停止Geth
          kubectl scale statefulset geth --replicas=0 -n ethereum-mainnet
          
          # 等待Pod终止
          sleep 60
          
          # 执行裁剪
          geth snapshot prune-state --datadir=/data/geth
          
          # 重启Geth
          kubectl scale statefulset geth --replicas=1 -n ethereum-mainnet
        
        volumeMounts:
        - name: geth-data
          mountPath: /data/geth
      
      restartPolicy: Never
      volumes:
      - name: geth-data
        persistentVolumeClaim:
          claimName: geth-data-geth-0
```

---

## 9. 网络和通信

### 9.1 P2P网络配置

**NodePort暴露P2P端口**:

```yaml
# geth-p2p-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: geth-p2p
  namespace: ethereum-mainnet
spec:
  type: NodePort
  selector:
    app: geth
  ports:
  - name: p2p-tcp
    port: 30303
    targetPort: 30303
    nodePort: 30303  # 固定NodePort
    protocol: TCP
  - name: p2p-udp
    port: 30303
    targetPort: 30303
    nodePort: 30303
    protocol: UDP
  externalTrafficPolicy: Local  # 保留源IP

---
# 或使用LoadBalancer (带静态IP)
apiVersion: v1
kind: Service
metadata:
  name: geth-p2p-lb
  namespace: ethereum-mainnet
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"  # AWS NLB
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "ip"
    service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
    service.beta.kubernetes.io/aws-load-balancer-eip-allocations: "eipalloc-xxxxx"  # 静态EIP
spec:
  type: LoadBalancer
  selector:
    app: geth
  ports:
  - name: p2p-tcp
    port: 30303
    targetPort: 30303
    protocol: TCP
  - name: p2p-udp
    port: 30303
    targetPort: 30303
    protocol: UDP
  externalTrafficPolicy: Local
```

**NAT配置 (Geth)**:

```yaml
command:
- geth
- --nat=extip:<STATIC_PUBLIC_IP>  # 或 --nat=any
- --netrestrict=10.0.0.0/8  # 限制P2P到VPC内
```

### 9.2 RPC/WebSocket服务

**Ingress (HTTPS + 认证)**:

```yaml
# geth-rpc-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: geth-rpc
  namespace: ethereum-mainnet
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: rpc-basic-auth
    nginx.ingress.kubernetes.io/auth-realm: "Ethereum RPC"
    nginx.ingress.kubernetes.io/rate-limit: "100"  # 100 req/s per IP
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: "100m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "120"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "120"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - eth-rpc.example.com
    secretName: eth-rpc-tls
  rules:
  - host: eth-rpc.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: rpc-loadbalancer
            port:
              number: 80
      - path: /ws
        pathType: Prefix
        backend:
          service:
            name: geth
            port:
              number: 8546

---
# Basic Auth Secret
apiVersion: v1
kind: Secret
metadata:
  name: rpc-basic-auth
  namespace: ethereum-mainnet
type: Opaque
data:
  auth: dXNlcjokYXByMSR... # htpasswd生成
```

### 9.3 负载均衡

**Envoy (L7负载均衡 + gRPC)**:

```yaml
# envoy-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: envoy-config
  namespace: ethereum-mainnet
data:
  envoy.yaml: |
    admin:
      address:
        socket_address:
          address: 0.0.0.0
          port_value: 9901
    
    static_resources:
      listeners:
      - name: rpc_listener
        address:
          socket_address:
            address: 0.0.0.0
            port_value: 8545
        filter_chains:
        - filters:
          - name: envoy.filters.network.http_connection_manager
            typed_config:
              "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
              stat_prefix: ingress_http
              codec_type: AUTO
              route_config:
                name: local_route
                virtual_hosts:
                - name: backend
                  domains: ["*"]
                  routes:
                  - match:
                      prefix: "/"
                    route:
                      cluster: geth_cluster
                      timeout: 120s
                      retry_policy:
                        retry_on: "5xx"
                        num_retries: 3
              http_filters:
              - name: envoy.filters.http.router
                typed_config:
                  "@type": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router
      
      clusters:
      - name: geth_cluster
        connect_timeout: 5s
        type: STRICT_DNS
        lb_policy: LEAST_REQUEST
        health_checks:
        - timeout: 5s
          interval: 10s
          unhealthy_threshold: 3
          healthy_threshold: 2
          http_health_check:
            path: "/"
            request_headers_to_add:
            - header:
                key: "Content-Type"
                value: "application/json"
        load_assignment:
          cluster_name: geth_cluster
          endpoints:
          - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: geth-0.geth.ethereum-mainnet.svc.cluster.local
                    port_value: 8545
            - endpoint:
                address:
                  socket_address:
                    address: geth-1.geth.ethereum-mainnet.svc.cluster.local
                    port_value: 8545
            - endpoint:
                address:
                  socket_address:
                    address: geth-2.geth.ethereum-mainnet.svc.cluster.local
                    port_value: 8545
```

---

## 10. 安全和隔离

### 10.1 私钥管理

**HashiCorp Vault集成**:

```yaml
# vault-injector-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: geth-with-vault
  namespace: ethereum-mainnet
spec:
  replicas: 1
  selector:
    matchLabels:
      app: geth-with-vault
  template:
    metadata:
      labels:
        app: geth-with-vault
      annotations:
        vault.hashicorp.com/agent-inject: "true"
        vault.hashicorp.com/role: "geth"
        vault.hashicorp.com/agent-inject-secret-account-key: "secret/ethereum/account"
        vault.hashicorp.com/agent-inject-template-account-key: |
          {{- with secret "secret/ethereum/account" -}}
          {{ .Data.data.private_key }}
          {{- end -}}
    spec:
      serviceAccountName: geth
      containers:
      - name: geth
        image: ethereum/client-go:v1.14.8
        command:
        - sh
        - -c
        - |
          # 从Vault注入的密钥导入账户
          geth account import --datadir=/data /vault/secrets/account-key
          
          # 启动Geth
          geth --datadir=/data ...
        volumeMounts:
        - name: geth-data
          mountPath: /data
```

**Kubernetes Secrets (加密)**:

```bash
# 启用etcd加密
cat <<EOF > /etc/kubernetes/encryption-config.yaml
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: $(head -c 32 /dev/urandom | base64)
    - identity: {}
EOF

# 更新apiserver配置
--encryption-provider-config=/etc/kubernetes/encryption-config.yaml
```

**External Secrets Operator**:

```yaml
# external-secret.yaml
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: vault-backend
  namespace: ethereum-mainnet
spec:
  provider:
    vault:
      server: "https://vault.example.com"
      path: "secret"
      version: "v2"
      auth:
        kubernetes:
          mountPath: "kubernetes"
          role: "geth"
---
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: geth-account
  namespace: ethereum-mainnet
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: vault-backend
    kind: SecretStore
  target:
    name: geth-account-secret
    creationPolicy: Owner
  data:
  - secretKey: private_key
    remoteRef:
      key: ethereum/account
      property: private_key
```

### 10.2 网络隔离

**NetworkPolicy (严格隔离)**:

```yaml
# blockchain-network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: geth-network-policy
  namespace: ethereum-mainnet
spec:
  podSelector:
    matchLabels:
      app: geth
  policyTypes:
  - Ingress
  - Egress
  
  ingress:
  # 仅允许RPC LB访问8545
  - from:
    - podSelector:
        matchLabels:
          app: nginx-rpc-lb
    ports:
    - protocol: TCP
      port: 8545
  
  # 仅允许Prometheus访问metrics
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
      podSelector:
        matchLabels:
          app: prometheus
    ports:
    - protocol: TCP
      port: 6060
  
  # 允许P2P (其他Geth节点 + 外部)
  - from:
    - podSelector:
        matchLabels:
          app: geth
    - namespaceSelector: {}  # 外部P2P流量
    ports:
    - protocol: TCP
      port: 30303
    - protocol: UDP
      port: 30303
  
  egress:
  # 允许DNS
  - to:
    - namespaceSelector: {}
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  
  # 允许P2P出站
  - to:
    - podSelector:
        matchLabels:
          app: geth
    ports:
    - protocol: TCP
      port: 30303
    - protocol: UDP
      port: 30303
  
  # 允许外部P2P连接
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 30303
    - protocol: UDP
      port: 30303
  
  # 允许访问L1 RPC (Layer 2节点)
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443
```

**Cilium NetworkPolicy (L7 + DNS)**:

```yaml
# cilium-network-policy.yaml
apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: geth-l7-policy
  namespace: ethereum-mainnet
spec:
  endpointSelector:
    matchLabels:
      app: geth
  
  egress:
  # 允许访问特定外部RPC (如Alchemy, Infura)
  - toFQDNs:
    - matchName: "eth-mainnet.g.alchemy.com"
    - matchPattern: "*.infura.io"
    toPorts:
    - ports:
      - port: "443"
        protocol: TCP
  
  # 允许P2P
  - toEntities:
    - world
    toPorts:
    - ports:
      - port: "30303"
        protocol: TCP
      - port: "30303"
        protocol: UDP
```

### 10.3 TEE和机密计算

**Intel SGX (Confidential Containers)**:

```yaml
# sgx-enabled-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: confidential-validator
  namespace: ethereum-mainnet
spec:
  replicas: 1
  selector:
    matchLabels:
      app: confidential-validator
  template:
    metadata:
      labels:
        app: confidential-validator
    spec:
      runtimeClassName: kata-cc  # Confidential Containers runtime
      nodeSelector:
        intel.feature.node.kubernetes.io/sgx: "true"
      
      containers:
      - name: validator
        image: myregistry/confidential-validator:latest
        resources:
          limits:
            sgx.intel.com/epc: "512Mi"  # SGX EPC内存
        
        env:
        - name: SGX_MODE
          value: "HW"  # 硬件模式
        - name: PRIVATE_KEY_SEALED
          value: "true"  # SGX密封私钥
        
        volumeMounts:
        - name: sgx-enclave
          mountPath: /dev/sgx
      
      volumes:
      - name: sgx-enclave
        hostPath:
          path: /dev/sgx
          type: CharDevice
```

**AMD SEV-SNP**:

```yaml
# sev-enabled-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sev-validator
  namespace: ethereum-mainnet
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sev-validator
  template:
    metadata:
      labels:
        app: sev-validator
    spec:
      runtimeClassName: kata-cc-sev
      nodeSelector:
        amd.feature.node.kubernetes.io/sev: "true"
      
      containers:
      - name: validator
        image: myregistry/validator:latest
        securityContext:
          privileged: false
          runAsNonRoot: true
          capabilities:
            drop:
            - ALL
```

---

## 11. 监控和可观测性

### 11.1 Prometheus指标

**Geth Metrics**:

```yaml
# servicemonitor-geth.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: geth
  namespace: ethereum-mainnet
spec:
  selector:
    matchLabels:
      app: geth
  endpoints:
  - port: metrics
    interval: 30s
    path: /debug/metrics/prometheus
```

**关键指标**:

```yaml
# prometheusrule-geth.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: geth-alerts
  namespace: ethereum-mainnet
spec:
  groups:
  - name: geth
    interval: 30s
    rules:
    # 同步状态
    - alert: GethNotSynced
      expr: eth_sync_known_highest_block - eth_sync_current_block > 100
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "Geth node {{ $labels.instance }} is not fully synced"
        description: "Block difference: {{ $value }}"
    
    # Peer数量
    - alert: GethLowPeerCount
      expr: p2p_peers < 10
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Geth node {{ $labels.instance }} has low peer count"
        description: "Current peers: {{ $value }}"
    
    # 磁盘空间
    - alert: GethDiskSpaceLow
      expr: |
        (
          node_filesystem_avail_bytes{mountpoint="/data"} 
          / node_filesystem_size_bytes{mountpoint="/data"}
        ) < 0.1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Geth node {{ $labels.instance }} disk space low"
        description: "Free space: {{ $value | humanizePercentage }}"
    
    # 区块时间延迟
    - alert: GethBlockTimeLag
      expr: time() - eth_latest_block_timestamp > 300
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Geth node {{ $labels.instance }} block time lag"
        description: "Lag: {{ $value }}s"
```

### 11.2 链上监控

**自定义Exporter**:

```python
# blockchain-exporter.py
from prometheus_client import start_http_server, Gauge
from web3 import Web3
import time

# 定义指标
latest_block = Gauge('ethereum_latest_block', 'Latest block number')
gas_price = Gauge('ethereum_gas_price_gwei', 'Current gas price in Gwei')
peer_count = Gauge('ethereum_peer_count', 'Number of connected peers')
syncing = Gauge('ethereum_syncing', 'Is node syncing (1=yes, 0=no)')

def collect_metrics(w3):
    # 最新区块
    latest_block.set(w3.eth.block_number)
    
    # Gas价格
    gas_price.set(w3.eth.gas_price / 1e9)  # 转换为Gwei
    
    # Peer数量
    peer_count.set(w3.net.peer_count)
    
    # 同步状态
    syncing_status = w3.eth.syncing
    syncing.set(1 if syncing_status else 0)

if __name__ == '__main__':
    w3 = Web3(Web3.HTTPProvider('http://geth:8545'))
    start_http_server(8000)
    
    while True:
        collect_metrics(w3)
        time.sleep(15)
```

**Deployment**:

```yaml
# blockchain-exporter-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blockchain-exporter
  namespace: ethereum-mainnet
spec:
  replicas: 1
  selector:
    matchLabels:
      app: blockchain-exporter
  template:
    metadata:
      labels:
        app: blockchain-exporter
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
    spec:
      containers:
      - name: exporter
        image: myregistry/blockchain-exporter:latest
        ports:
        - containerPort: 8000
          name: metrics
        env:
        - name: RPC_URL
          value: "http://geth.ethereum-mainnet.svc.cluster.local:8545"
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
```

### 11.3 告警策略

**PagerDuty集成**:

```yaml
# alertmanager-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-config
  namespace: monitoring
type: Opaque
stringData:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
    
    route:
      receiver: 'default'
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      routes:
      # 区块链关键告警
      - match:
          severity: critical
          namespace: ethereum-mainnet
        receiver: 'pagerduty-critical'
        continue: true
      
      # 区块链警告
      - match:
          severity: warning
          namespace: ethereum-mainnet
        receiver: 'slack-blockchain'
    
    receivers:
    - name: 'default'
      slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        channel: '#alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    
    - name: 'pagerduty-critical'
      pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}'
    
    - name: 'slack-blockchain'
      slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        channel: '#blockchain-ops'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
```

---

## 12. 高可用性和灾难恢复

### 12.1 多区域部署

**跨区域StatefulSet**:

```yaml
# multi-zone-geth.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: geth
  namespace: ethereum-mainnet
spec:
  serviceName: geth
  replicas: 3
  selector:
    matchLabels:
      app: geth
  template:
    metadata:
      labels:
        app: geth
    spec:
      # Pod反亲和性: 不同可用区
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: geth
            topologyKey: topology.kubernetes.io/zone
      
      # 优先调度到不同区域
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: geth
      
      containers:
      - name: geth
        image: ethereum/client-go:v1.14.8
        ...
  
  volumeClaimTemplates:
  - metadata:
      name: geth-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd-zone-aware  # 区域感知存储
      resources:
        requests:
          storage: 2Ti
```

### 12.2 故障转移

**PDB (Pod Disruption Budget)**:

```yaml
# pdb-geth.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: geth-pdb
  namespace: ethereum-mainnet
spec:
  minAvailable: 2  # 至少保持2个Pod运行
  selector:
    matchLabels:
      app: geth
```

**自动故障切换**:

```yaml
# geth-with-readiness.yaml
readinessProbe:
  exec:
    command:
    - sh
    - -c
    - |
      # 检查同步状态
      SYNCING=$(geth attach --exec 'eth.syncing' http://localhost:8545)
      if [ "$SYNCING" = "false" ]; then
        exit 0
      else
        exit 1
      fi
  initialDelaySeconds: 120
  periodSeconds: 30
  failureThreshold: 3
```

### 12.3 灾难恢复

**跨区域备份**:

```yaml
# cross-region-backup-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: geth-cross-region-backup
  namespace: ethereum-mainnet
spec:
  schedule: "0 0 * * 0"  # 每周日
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: backup-sa
          containers:
          - name: backup
            image: amazon/aws-cli:latest
            command:
            - sh
            - -c
            - |
              # 创建快照
              SNAPSHOT_ID=$(aws ec2 create-snapshot \
                --volume-id vol-xxxxx \
                --description "Geth backup $(date +%Y%m%d)" \
                --tag-specifications 'ResourceType=snapshot,Tags=[{Key=Type,Value=Blockchain}]' \
                --query 'SnapshotId' \
                --output text)
              
              # 复制到灾备区域
              aws ec2 copy-snapshot \
                --source-region us-east-1 \
                --source-snapshot-id $SNAPSHOT_ID \
                --destination-region us-west-2 \
                --description "DR backup $(date +%Y%m%d)"
              
              # 清理旧快照 (保留4周)
              aws ec2 describe-snapshots \
                --owner-ids self \
                --filters "Name=tag:Type,Values=Blockchain" \
                --query 'Snapshots[?StartTime<=`'$(date -d '4 weeks ago' --iso-8601)'`].SnapshotId' \
                --output text | xargs -n 1 aws ec2 delete-snapshot --snapshot-id
          
          restartPolicy: OnFailure
```

---

## 13. 性能优化

### 13.1 资源配置

**生产级配置示例**:

```yaml
# Ethereum Full Node (RPC服务)
resources:
  requests:
    memory: "16Gi"
    cpu: "4000m"
  limits:
    memory: "32Gi"
    cpu: "8000m"

command:
- geth
- --cache=8192  # 8GB内存缓存
- --maxpeers=50
- --syncmode=snap

---
# Hyperledger Fabric Peer (企业级)
resources:
  requests:
    memory: "8Gi"
    cpu: "4000m"
  limits:
    memory: "16Gi"
    cpu: "8000m"

---
# zkEVM Prover (GPU)
resources:
  requests:
    memory: "64Gi"
    cpu: "16000m"
    nvidia.com/gpu: 1  # A100 80GB
  limits:
    memory: "128Gi"
    cpu: "32000m"
    nvidia.com/gpu: 1
```

### 13.2 网络优化

**CNI优化 (Cilium)**:

```yaml
# cilium-values.yaml
kubeProxyReplacement: "strict"  # 完全替代kube-proxy (eBPF)
enableIPv4Masquerade: true
enableBPFMasquerade: true

# eBPF主机路由
enableHostReachableServices: true
hostServices:
  enabled: true
  protocols: tcp,udp

# MTU优化
mtu: 9000  # Jumbo frames (如果网络支持)

# 带宽管理器
bandwidthManager: true
```

### 13.3 存储I/O优化

**NVMe优化**:

```bash
# 调整文件系统参数
mkfs.ext4 -E lazy_itable_init=0,lazy_journal_init=0,stride=128,stripe_width=128 /dev/nvme0n1

# 挂载选项
mount -o noatime,nodiratime,discard,data=writeback /dev/nvme0n1 /mnt/blockchain

# Kubernetes StorageClass
parameters:
  type: io2
  iopsPerGB: "500"
  throughput: "4000"
  fsType: ext4
  csi.storage.k8s.io/fstype: ext4
  blockExpress: "true"  # 启用EBS Block Express
```

**I/O调度器优化**:

```yaml
# node-tuning-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-tuning
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: node-tuning
  template:
    metadata:
      labels:
        app: node-tuning
    spec:
      hostPID: true
      hostNetwork: true
      nodeSelector:
        blockchain-node: "true"
      containers:
      - name: tuning
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          # 设置I/O调度器为none (NVMe)
          echo none > /sys/block/nvme0n1/queue/scheduler
          
          # 增加I/O队列深度
          echo 1024 > /sys/block/nvme0n1/queue/nr_requests
          
          # 禁用读前预读 (减少随机I/O延迟)
          echo 0 > /sys/block/nvme0n1/queue/read_ahead_kb
          
          # 持续运行
          sleep infinity
        securityContext:
          privileged: true
```

---

## 14. 跨链技术容器化

### 14.1 Cosmos IBC Relayer

**Hermes Relayer部署**:

```yaml
# hermes-relayer-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hermes-relayer
  namespace: cosmos
spec:
  replicas: 2
  selector:
    matchLabels:
      app: hermes-relayer
  template:
    metadata:
      labels:
        app: hermes-relayer
    spec:
      containers:
      - name: hermes
        image: informalsystems/hermes:v1.9.0
        command:
        - hermes
        - start
        env:
        - name: RUST_LOG
          value: "info"
        volumeMounts:
        - name: config
          mountPath: /root/.hermes
          readOnly: true
        - name: keys
          mountPath: /keys
          readOnly: true
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
      volumes:
      - name: config
        configMap:
          name: hermes-config
      - name: keys
        secret:
          secretName: hermes-keys
```

**配置**:

```toml
# hermes-config.toml
[global]
log_level = 'info'

[mode.clients]
enabled = true
refresh = true
misbehaviour = true

[mode.connections]
enabled = true

[mode.channels]
enabled = true

[mode.packets]
enabled = true
clear_interval = 100
clear_on_start = true

[[chains]]
id = 'cosmoshub-4'
rpc_addr = 'http://cosmos-rpc:26657'
grpc_addr = 'http://cosmos-grpc:9090'
websocket_addr = 'ws://cosmos-rpc:26657/websocket'
rpc_timeout = '10s'
account_prefix = 'cosmos'
key_name = 'relayer'
store_prefix = 'ibc'
gas_price = { price = 0.025, denom = 'uatom' }

[[chains]]
id = 'osmosis-1'
rpc_addr = 'http://osmosis-rpc:26657'
grpc_addr = 'http://osmosis-grpc:9090'
websocket_addr = 'ws://osmosis-rpc:26657/websocket'
rpc_timeout = '10s'
account_prefix = 'osmo'
key_name = 'relayer'
store_prefix = 'ibc'
gas_price = { price = 0.025, denom = 'uosmo' }
```

### 14.2 Polkadot Parachain

**Substrate节点容器化**:

```yaml
# parachain-node-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: parachain-collator
  namespace: polkadot
spec:
  serviceName: parachain-collator
  replicas: 2
  selector:
    matchLabels:
      app: parachain-collator
  template:
    metadata:
      labels:
        app: parachain-collator
    spec:
      containers:
      - name: collator
        image: myregistry/parachain-collator:latest
        ports:
        - containerPort: 30333
          name: p2p
        - containerPort: 9933
          name: rpc
        - containerPort: 9944
          name: ws
        command:
        - ./parachain-collator
        - --collator
        - --base-path=/data
        - --chain=parachain-spec.json
        - --port=30333
        - --rpc-port=9933
        - --ws-port=9944
        - --rpc-cors=all
        - --unsafe-rpc-external
        - --unsafe-ws-external
        - --
        - --chain=polkadot
        - --port=30334
        - --rpc-port=9934
        resources:
          requests:
            memory: "8Gi"
            cpu: "4000m"
          limits:
            memory: "16Gi"
            cpu: "8000m"
        volumeMounts:
        - name: parachain-data
          mountPath: /data
        - name: chain-spec
          mountPath: /chain-spec.json
          subPath: chain-spec.json
      volumes:
      - name: chain-spec
        configMap:
          name: parachain-chain-spec
  volumeClaimTemplates:
  - metadata:
      name: parachain-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 500Gi
```

### 14.3 跨链桥容器化

**通用跨链桥架构**:

```yaml
# crosschain-bridge-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: crosschain-bridge
  namespace: bridge
spec:
  replicas: 3
  selector:
    matchLabels:
      app: crosschain-bridge
  template:
    metadata:
      labels:
        app: crosschain-bridge
    spec:
      containers:
      # Source Chain监听器
      - name: source-listener
        image: myregistry/bridge-listener:latest
        env:
        - name: CHAIN_TYPE
          value: "ethereum"
        - name: RPC_URL
          value: "http://geth.ethereum-mainnet.svc.cluster.local:8545"
        - name: BRIDGE_CONTRACT
          value: "0x..."
        - name: REDIS_URL
          value: "redis://redis.bridge.svc.cluster.local:6379"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
      
      # Target Chain执行器
      - name: target-executor
        image: myregistry/bridge-executor:latest
        env:
        - name: CHAIN_TYPE
          value: "binance-smart-chain"
        - name: RPC_URL
          value: "http://bsc-rpc.bsc.svc.cluster.local:8545"
        - name: BRIDGE_CONTRACT
          value: "0x..."
        - name: REDIS_URL
          value: "redis://redis.bridge.svc.cluster.local:6379"
        - name: PRIVATE_KEY_SECRET
          valueFrom:
            secretKeyRef:
              name: bridge-executor-key
              key: private_key
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
```

---

## 15. 生产最佳实践

### 15.1 部署清单

**生产环境检查清单**:

```yaml
安全:
  ✅ 私钥使用Vault/Secrets管理
  ✅ NetworkPolicy启用并配置
  ✅ RBAC最小权限原则
  ✅ 容器镜像签名验证 (Cosign)
  ✅ 漏洞扫描 (Trivy)
  ✅ 启用etcd加密
  ✅ TLS证书管理 (cert-manager)
  ✅ Admission Controller (OPA/Kyverno)

高可用:
  ✅ 多副本部署 (>=3)
  ✅ 跨可用区分布
  ✅ PodDisruptionBudget配置
  ✅ 健康检查完善
  ✅ 自动故障转移
  ✅ 备份策略 (日/周/月)

性能:
  ✅ 资源requests/limits合理
  ✅ 存储类型匹配 (NVMe SSD)
  ✅ 网络优化 (Cilium eBPF)
  ✅ 节点亲和性配置
  ✅ HPA/VPA配置

监控:
  ✅ Prometheus指标采集
  ✅ Grafana Dashboard
  ✅ 告警规则配置
  ✅ 日志聚合 (Loki/ELK)
  ✅ 链上指标监控
  ✅ PagerDuty/OpsGenie集成

合规:
  ✅ 审计日志启用
  ✅ 数据加密 (传输+静态)
  ✅ 访问控制记录
  ✅ 定期安全审计
  ✅ 灾难恢复测试
```

### 15.2 运维建议

**日常运维**:

```yaml
每日检查:
  - 节点同步状态
  - Peer连接数
  - 区块高度与主网对比
  - 磁盘使用率
  - 关键告警

每周任务:
  - 全量备份验证
  - 性能报告生成
  - 安全补丁评估
  - 资源使用趋势分析

每月任务:
  - 灾难恢复演练
  - 容量规划评估
  - 成本优化分析
  - 软件版本升级计划
```

**升级策略**:

```yaml
# rolling-update-strategy.yaml
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0  # 从partition开始更新
      maxUnavailable: 1  # 最多1个Pod不可用

# 金丝雀升级
kubectl set image statefulset/geth geth=ethereum/client-go:v1.14.9 --record
kubectl rollout status statefulset/geth
kubectl rollout undo statefulset/geth  # 回滚
```

### 15.3 成本优化

**Spot实例使用**:

```yaml
# node-pool-spot.yaml
apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: blockchain-spot
spec:
  requirements:
  - key: "karpenter.sh/capacity-type"
    operator: In
    values: ["spot"]
  - key: "node.kubernetes.io/instance-type"
    operator: In
    values: ["c6i.4xlarge", "c6i.8xlarge"]
  
  limits:
    resources:
      cpu: 1000
      memory: 2000Gi
  
  ttlSecondsAfterEmpty: 30
  ttlSecondsUntilExpired: 2592000  # 30天
  
  labels:
    blockchain-workload: "non-critical"
```

**自动扩缩容**:

```yaml
# vpa-geth.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: geth-vpa
  namespace: ethereum-mainnet
spec:
  targetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: geth
  updatePolicy:
    updateMode: "Auto"  # 自动调整
  resourcePolicy:
    containerPolicies:
    - containerName: geth
      minAllowed:
        memory: "8Gi"
        cpu: "2000m"
      maxAllowed:
        memory: "64Gi"
        cpu: "16000m"
```

### 15.4 合规性

**审计日志**:

```yaml
# audit-policy.yaml
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
# 记录区块链相关操作
- level: RequestResponse
  namespaces: ["ethereum-mainnet", "hyperledger-fabric"]
  verbs: ["create", "update", "patch", "delete"]
  resources:
  - group: ""
    resources: ["secrets", "configmaps"]
  - group: "apps"
    resources: ["statefulsets", "deployments"]

# 记录密钥访问
- level: Metadata
  namespaces: ["ethereum-mainnet"]
  verbs: ["get"]
  resources:
  - group: ""
    resources: ["secrets"]
  omitStages:
  - RequestReceived
```

**数据留存**:

```yaml
# 日志留存策略
Loki:
  retention_period: 90d  # 90天

Prometheus:
  retention: 30d  # 30天

Backup:
  daily: 7d    # 每日备份保留7天
  weekly: 30d  # 每周备份保留30天
  monthly: 365d  # 每月备份保留1年
```

---

## 16. 案例研究

### 16.1 企业联盟链 (Fabric)

**场景**: 供应链金融平台

```yaml
架构:
  - 4个组织 (银行、核心企业、供应商、物流)
  - 每组织2 Peer节点 (主备)
  - 3个Orderer节点 (Raft)
  - 2个CA节点 (每组织)

部署:
  - Kubernetes集群: 3 Master + 12 Worker
  - 存储: Rook-Ceph (3副本)
  - 网络: Calico + NetworkPolicy严格隔离
  - 监控: Prometheus + Grafana + PagerDuty

成果:
  - TPS: ~1,200
  - 延迟: <3s (含共识)
  - 可用性: 99.95%
  - 成本: 降低40% (vs 传统虚拟机)
```

### 16.2 公链RPC服务 (Ethereum)

**场景**: 去中心化应用RPC服务商

```yaml
架构:
  - 10个Full Node (读负载均衡)
  - 1个Archive Node (历史查询)
  - Nginx LB + Redis缓存
  - Envoy sidecar (gRPC支持)

部署:
  - EKS (AWS Kubernetes)
  - 存储: EBS io2 (64000 IOPS)
  - 网络: Cilium eBPF
  - CDN: CloudFront (静态方法缓存)

规模:
  - QPS: ~50,000
  - P99延迟: <100ms
  - 月成本: ~$15,000
  - 缓存命中率: 85%
```

### 16.3 Layer 2 Sequencer

**场景**: Optimistic Rollup Sequencer

```yaml
架构:
  - op-geth (执行引擎)
  - op-node (Rollup节点)
  - op-batcher (批处理器)
  - op-proposer (状态提议器)

部署:
  - GKE (Google Kubernetes Engine)
  - 存储: Persistent Disk Extreme (120000 IOPS)
  - 网络: VPC Native + Shared VPC
  - 灾备: Multi-region (us-central1 + us-east1)

性能:
  - TPS: ~4,000
  - Gas费用: 主网的1/50
  - 批次提交频率: 每5分钟
  - L1数据可用性成本: ~$2,000/天
```

---

## 17. 工具和资源

### 17.1 开源工具

**Kubernetes Operators**:

```yaml
Hyperledger Fabric:
  - hlf-operator: https://github.com/hyperledger-labs/hlf-operator
  - fabric-operator: https://github.com/hyperledger/fabric-operator

Ethereum:
  - besu-operator: https://github.com/ConsenSys/besu-kubernetes
  - ethereum-helm-charts: https://github.com/skylenet/ethereum-helm-charts

Cosmos:
  - cosmos-operator: https://github.com/strangelove-ventures/cosmos-operator

Polkadot:
  - substrate-operator: https://github.com/paritytech/substrate-operator
```

**监控工具**:

```yaml
链上监控:
  - Blockscout: 区块浏览器
  - The Graph: 索引器
  - Etherscan API: 以太坊数据

基础设施监控:
  - Prometheus: 指标采集
  - Grafana: 可视化
  - Loki: 日志聚合
  - Jaeger: 分布式追踪
```

**安全工具**:

```yaml
密钥管理:
  - HashiCorp Vault
  - AWS Secrets Manager
  - Azure Key Vault
  - GCP Secret Manager

审计:
  - Falco: 运行时安全
  - OPA: 策略执行
  - Trivy: 镜像扫描
  - Cosign: 镜像签名
```

### 17.2 商业平台

**BaaS (Blockchain as a Service)**:

```yaml
云厂商:
  - AWS Managed Blockchain (Hyperledger Fabric, Ethereum)
  - Azure Blockchain Service (Quorum)
  - Google Cloud Blockchain Node Engine (Ethereum)
  - IBM Blockchain Platform (Hyperledger Fabric)
  - Oracle Blockchain Platform (Hyperledger Fabric)

专业平台:
  - Alchemy: Ethereum RPC/WebSocket
  - Infura: Multi-chain RPC
  - QuickNode: 全链支持
  - Ankr: 去中心化RPC
```

**企业解决方案**:

```yaml
Hyperledger Fabric:
  - IBM Blockchain Platform
  - Oracle Blockchain Platform
  - SAP Blockchain

Ethereum:
  - ConsenSys Quorum
  - Chainstack
  - Kaleido
```

### 17.3 学习资源

**官方文档**:

```yaml
Hyperledger Fabric:
  - https://hyperledger-fabric.readthedocs.io/
  - https://github.com/hyperledger/fabric

Ethereum:
  - https://ethereum.org/en/developers/docs/
  - https://geth.ethereum.org/docs/
  - https://github.com/ethereum/go-ethereum

Layer 2:
  - Optimism: https://community.optimism.io/docs/
  - Arbitrum: https://docs.arbitrum.io/
  - zkSync: https://docs.zksync.io/
  - Polygon zkEVM: https://docs.polygon.technology/zkEVM/

Kubernetes:
  - https://kubernetes.io/docs/
  - https://operatorhub.io/
```

**社区**:

```yaml
论坛:
  - Hyperledger Discord
  - Ethereum StackExchange
  - Reddit: r/ethdev, r/ethereum
  - Kubernetes Slack

会议:
  - Hyperledger Global Forum
  - Ethereum Devcon
  - KubeCon + CloudNativeCon
```

---

## 总结

**区块链容器化的价值**:

```yaml
技术价值:
  ✅ 标准化部署流程 (Docker/Helm/Operator)
  ✅ 自动化运维 (自愈、扩容、升级)
  ✅ 资源弹性 (按需扩缩、成本优化)
  ✅ 多云可移植 (避免厂商锁定)
  ✅ 微服务架构 (组件解耦、独立升级)

业务价值:
  ✅ 缩短上市时间 (快速部署测试网/主网)
  ✅ 降低运维成本 (人力、基础设施)
  ✅ 提高可靠性 (HA、灾备、监控)
  ✅ 增强安全性 (隔离、加密、审计)
  ✅ 支持规模化 (从试点到生产)
```

**2025年趋势**:

```yaml
Layer 2主流化:
  - Optimistic Rollup成熟 (Optimism、Arbitrum)
  - ZK Rollup产品化 (zkSync Era、Polygon zkEVM)
  - 应用链爆发 (OP Stack、Orbit、Polygon CDK)

模块化区块链:
  - 数据可用性层 (Celestia、EigenDA)
  - 共识层与执行层解耦
  - Rollup-as-a-Service

GPU加速:
  - 零知识证明生成
  - 机密计算 (TEE + GPU)
  - zkML (零知识机器学习)

跨链互操作:
  - IBC成熟 (Cosmos生态)
  - 跨链桥安全性提升
  - 通用消息传递协议

企业采用:
  - 联盟链与公链融合
  - 隐私计算集成 (MPC、FHE)
  - 合规框架完善
```

**关键建议**:

```yaml
选择区块链平台:
  - 联盟链: Hyperledger Fabric (成熟稳定)
  - 公链: Ethereum (生态最大)
  - 高性能: Solana、Sui (低延迟)
  - 应用链: Cosmos SDK (灵活可定制)

容器化策略:
  - 开发/测试: Docker Compose (快速迭代)
  - 生产: Kubernetes + Operator (企业级)
  - 混合云: Helm Charts (跨平台)

安全优先:
  - 私钥管理: Vault/HSM (绝不泄露)
  - 网络隔离: NetworkPolicy (零信任)
  - 镜像安全: 签名+扫描 (供应链安全)

监控完善:
  - 基础设施: Prometheus + Grafana
  - 链上数据: 自定义Exporter
  - 告警集成: PagerDuty/OpsGenie
```

**下一步行动**:

```yaml
入门:
  1. Docker Compose部署测试网 (Geth/Fabric)
  2. 学习Kubernetes基础 (Pod、Service、StatefulSet)
  3. 尝试Helm部署单节点

进阶:
  1. Kubernetes上部署多节点集群
  2. 实施监控和告警
  3. 配置NetworkPolicy和RBAC
  4. 测试灾难恢复流程

生产:
  1. 评估Operator (hlf-operator/besu-operator)
  2. 设计多区域高可用架构
  3. 实施自动化备份和恢复
  4. 建立运维runbook和on-call流程
```

---

**更新时间**: 2025-10-20  
**文档版本**: v1.0  
**状态**: ✅ **完成**

---

**🌟 区块链与容器化技术的结合,正在重塑Web3基础设施的未来！🌟**-
