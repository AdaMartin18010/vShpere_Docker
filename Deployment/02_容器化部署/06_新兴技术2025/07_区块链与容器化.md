# åŒºå—é“¾ä¸å®¹å™¨åŒ–/è™šæ‹ŸåŒ–æŠ€æœ¯ (2025)

> **è¿”å›**: [æ–°å…´æŠ€æœ¯2025é¦–é¡µ](README.md) | [å®¹å™¨åŒ–éƒ¨ç½²é¦–é¡µ](../README.md)

---

## ğŸ“‹ ç›®å½•

- [åŒºå—é“¾ä¸å®¹å™¨åŒ–/è™šæ‹ŸåŒ–æŠ€æœ¯ (2025)](#åŒºå—é“¾ä¸å®¹å™¨åŒ–è™šæ‹ŸåŒ–æŠ€æœ¯-2025)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
    - [1.1 ä¸ºä»€ä¹ˆå®¹å™¨åŒ–åŒºå—é“¾ï¼Ÿ](#11-ä¸ºä»€ä¹ˆå®¹å™¨åŒ–åŒºå—é“¾)
    - [1.2 2025å¹´æŠ€æœ¯è¶‹åŠ¿](#12-2025å¹´æŠ€æœ¯è¶‹åŠ¿)
    - [1.3 æŠ€æœ¯æ ˆå¯¹æ¯”](#13-æŠ€æœ¯æ ˆå¯¹æ¯”)
  - [2. Hyperledger Fabricå®¹å™¨åŒ–](#2-hyperledger-fabricå®¹å™¨åŒ–)
    - [2.1 æ¶æ„æ¦‚è¿°](#21-æ¶æ„æ¦‚è¿°)
    - [2.2 Kuberneteséƒ¨ç½²](#22-kuberneteséƒ¨ç½²)
    - [2.3 Fabric Operator (2025)](#23-fabric-operator-2025)
    - [2.4 é“¾ç å®¹å™¨åŒ–](#24-é“¾ç å®¹å™¨åŒ–)
    - [2.5 ç”Ÿäº§é…ç½®ç¤ºä¾‹](#25-ç”Ÿäº§é…ç½®ç¤ºä¾‹)
  - [3. EthereumèŠ‚ç‚¹å®¹å™¨åŒ–](#3-ethereumèŠ‚ç‚¹å®¹å™¨åŒ–)
    - [3.1 å®¢æˆ·ç«¯é€‰æ‹©](#31-å®¢æˆ·ç«¯é€‰æ‹©)
    - [3.2 Gethå®¹å™¨åŒ–éƒ¨ç½²](#32-gethå®¹å™¨åŒ–éƒ¨ç½²)
    - [3.3 Kubernetesä¸Šçš„Ethereum](#33-kubernetesä¸Šçš„ethereum)
    - [3.4 Ethereum Operator](#34-ethereum-operator)
    - [3.5 éªŒè¯å™¨èŠ‚ç‚¹é«˜å¯ç”¨](#35-éªŒè¯å™¨èŠ‚ç‚¹é«˜å¯ç”¨)
  - [4. Layer 2è§£å†³æ–¹æ¡ˆå®¹å™¨åŒ–](#4-layer-2è§£å†³æ–¹æ¡ˆå®¹å™¨åŒ–)
    - [4.1 Optimism/Arbitrumå®¹å™¨åŒ–](#41-optimismarbitrumå®¹å™¨åŒ–)
    - [4.2 zkSync Eraå®¹å™¨åŒ–](#42-zksync-eraå®¹å™¨åŒ–)
    - [4.3 Polygon CDKå®¹å™¨åŒ–](#43-polygon-cdkå®¹å™¨åŒ–)
    - [4.4 Base (Coinbase L2)](#44-base-coinbase-l2)
  - [5. åŒºå—é“¾å³æœåŠ¡ (BaaS)](#5-åŒºå—é“¾å³æœåŠ¡-baas)
    - [5.1 Kubernetes Operators](#51-kubernetes-operators)
    - [5.2 Helm Chartsç”Ÿæ€](#52-helm-chartsç”Ÿæ€)
    - [5.3 å¤šç§Ÿæˆ·BaaSå¹³å°](#53-å¤šç§Ÿæˆ·baaså¹³å°)
  - [6. Web3åŸºç¡€è®¾æ–½å®¹å™¨åŒ–](#6-web3åŸºç¡€è®¾æ–½å®¹å™¨åŒ–)
    - [6.1 IPFSå®¹å™¨åŒ–](#61-ipfså®¹å™¨åŒ–)
    - [6.2 The GraphèŠ‚ç‚¹](#62-the-graphèŠ‚ç‚¹)
    - [6.3 ChainlinkèŠ‚ç‚¹](#63-chainlinkèŠ‚ç‚¹)
    - [6.4 åˆ†å¸ƒå¼RPCæœåŠ¡](#64-åˆ†å¸ƒå¼rpcæœåŠ¡)
  - [7. é›¶çŸ¥è¯†è¯æ˜ (ZK) åŸºç¡€è®¾æ–½](#7-é›¶çŸ¥è¯†è¯æ˜-zk-åŸºç¡€è®¾æ–½)
    - [7.1 zkEVMå®¹å™¨åŒ–](#71-zkevmå®¹å™¨åŒ–)
    - [7.2 è¯æ˜ç”Ÿæˆå™¨å®¹å™¨åŒ–](#72-è¯æ˜ç”Ÿæˆå™¨å®¹å™¨åŒ–)
    - [7.3 GPUåŠ é€Ÿè¯æ˜](#73-gpuåŠ é€Ÿè¯æ˜)
  - [8. å­˜å‚¨å’ŒæŒä¹…åŒ–](#8-å­˜å‚¨å’ŒæŒä¹…åŒ–)
    - [8.1 åŒºå—é“¾æ•°æ®å­˜å‚¨](#81-åŒºå—é“¾æ•°æ®å­˜å‚¨)
    - [8.2 å¿«ç…§å’Œå¤‡ä»½](#82-å¿«ç…§å’Œå¤‡ä»½)
    - [8.3 å­˜å‚¨ä¼˜åŒ–](#83-å­˜å‚¨ä¼˜åŒ–)
  - [9. ç½‘ç»œå’Œé€šä¿¡](#9-ç½‘ç»œå’Œé€šä¿¡)
    - [9.1 P2Pç½‘ç»œé…ç½®](#91-p2pç½‘ç»œé…ç½®)
    - [9.2 RPC/WebSocketæœåŠ¡](#92-rpcwebsocketæœåŠ¡)
    - [9.3 è´Ÿè½½å‡è¡¡](#93-è´Ÿè½½å‡è¡¡)
  - [10. å®‰å…¨å’Œéš”ç¦»](#10-å®‰å…¨å’Œéš”ç¦»)
    - [10.1 ç§é’¥ç®¡ç†](#101-ç§é’¥ç®¡ç†)
    - [10.2 ç½‘ç»œéš”ç¦»](#102-ç½‘ç»œéš”ç¦»)
    - [10.3 TEEå’Œæœºå¯†è®¡ç®—](#103-teeå’Œæœºå¯†è®¡ç®—)
  - [11. ç›‘æ§å’Œå¯è§‚æµ‹æ€§](#11-ç›‘æ§å’Œå¯è§‚æµ‹æ€§)
    - [11.1 PrometheusæŒ‡æ ‡](#111-prometheusæŒ‡æ ‡)
    - [11.2 é“¾ä¸Šç›‘æ§](#112-é“¾ä¸Šç›‘æ§)
    - [11.3 å‘Šè­¦ç­–ç•¥](#113-å‘Šè­¦ç­–ç•¥)
  - [12. é«˜å¯ç”¨æ€§å’Œç¾éš¾æ¢å¤](#12-é«˜å¯ç”¨æ€§å’Œç¾éš¾æ¢å¤)
    - [12.1 å¤šåŒºåŸŸéƒ¨ç½²](#121-å¤šåŒºåŸŸéƒ¨ç½²)
    - [12.2 æ•…éšœè½¬ç§»](#122-æ•…éšœè½¬ç§»)
    - [12.3 ç¾éš¾æ¢å¤](#123-ç¾éš¾æ¢å¤)
  - [13. æ€§èƒ½ä¼˜åŒ–](#13-æ€§èƒ½ä¼˜åŒ–)
    - [13.1 èµ„æºé…ç½®](#131-èµ„æºé…ç½®)
    - [13.2 ç½‘ç»œä¼˜åŒ–](#132-ç½‘ç»œä¼˜åŒ–)
    - [13.3 å­˜å‚¨I/Oä¼˜åŒ–](#133-å­˜å‚¨ioä¼˜åŒ–)
  - [14. è·¨é“¾æŠ€æœ¯å®¹å™¨åŒ–](#14-è·¨é“¾æŠ€æœ¯å®¹å™¨åŒ–)
    - [14.1 Cosmos IBC Relayer](#141-cosmos-ibc-relayer)
    - [14.2 Polkadot Parachain](#142-polkadot-parachain)
    - [14.3 è·¨é“¾æ¡¥å®¹å™¨åŒ–](#143-è·¨é“¾æ¡¥å®¹å™¨åŒ–)
  - [15. ç”Ÿäº§æœ€ä½³å®è·µ](#15-ç”Ÿäº§æœ€ä½³å®è·µ)
    - [15.1 éƒ¨ç½²æ¸…å•](#151-éƒ¨ç½²æ¸…å•)
    - [15.2 è¿ç»´å»ºè®®](#152-è¿ç»´å»ºè®®)
    - [15.3 æˆæœ¬ä¼˜åŒ–](#153-æˆæœ¬ä¼˜åŒ–)
    - [15.4 åˆè§„æ€§](#154-åˆè§„æ€§)
  - [16. æ¡ˆä¾‹ç ”ç©¶](#16-æ¡ˆä¾‹ç ”ç©¶)
    - [16.1 ä¼ä¸šè”ç›Ÿé“¾ (Fabric)](#161-ä¼ä¸šè”ç›Ÿé“¾-fabric)
    - [16.2 å…¬é“¾RPCæœåŠ¡ (Ethereum)](#162-å…¬é“¾rpcæœåŠ¡-ethereum)
    - [16.3 Layer 2 Sequencer](#163-layer-2-sequencer)
  - [17. å·¥å…·å’Œèµ„æº](#17-å·¥å…·å’Œèµ„æº)
    - [17.1 å¼€æºå·¥å…·](#171-å¼€æºå·¥å…·)
    - [17.2 å•†ä¸šå¹³å°](#172-å•†ä¸šå¹³å°)
    - [17.3 å­¦ä¹ èµ„æº](#173-å­¦ä¹ èµ„æº)
  - [æ€»ç»“](#æ€»ç»“)

---

## 1. æ¦‚è¿°

### 1.1 ä¸ºä»€ä¹ˆå®¹å™¨åŒ–åŒºå—é“¾ï¼Ÿ

**æ ¸å¿ƒä¼˜åŠ¿**:

```yaml
å¯ç§»æ¤æ€§:
  âœ… ç»Ÿä¸€çš„éƒ¨ç½²ç¯å¢ƒ (å¼€å‘/æµ‹è¯•/ç”Ÿäº§)
  âœ… è·¨äº‘å¹³å° (AWSã€Azureã€GCPã€ç§æœ‰äº‘)
  âœ… ç®€åŒ–ä¾èµ–ç®¡ç† (æ“ä½œç³»ç»Ÿã€åº“ã€é…ç½®)

å¯æ‰©å±•æ€§:
  âœ… æ°´å¹³æ‰©å±• (è¯»å‰¯æœ¬ã€éªŒè¯å™¨èŠ‚ç‚¹)
  âœ… è‡ªåŠ¨ä¼¸ç¼© (åŸºäºè´Ÿè½½)
  âœ… å¤šé“¾/å¤šç½‘ç»œéƒ¨ç½² (ä¸»ç½‘ã€æµ‹è¯•ç½‘)

è¿ç»´æ•ˆç‡:
  âœ… æ ‡å‡†åŒ–éƒ¨ç½²æµç¨‹ (Helmã€Operator)
  âœ… è‡ªåŠ¨åŒ–æ›´æ–°å’Œå›æ»š
  âœ… é›†ä¸­ç›‘æ§å’Œæ—¥å¿—ç®¡ç†
  âœ… é™ä½åŸºç¡€è®¾æ–½æˆæœ¬ (èµ„æºå…±äº«ã€GPUæ± )

å¼€å‘ä½“éªŒ:
  âœ… æœ¬åœ°å¿«é€Ÿå¯åŠ¨ (Docker Compose)
  âœ… ä¸€è‡´çš„å¼€å‘/ç”Ÿäº§ç¯å¢ƒ
  âœ… CI/CDé›†æˆ (è‡ªåŠ¨åŒ–æµ‹è¯•ã€éƒ¨ç½²)
```

### 1.2 2025å¹´æŠ€æœ¯è¶‹åŠ¿

```yaml
å®¹å™¨åŒ–æˆç†Ÿåº¦:
  âœ… Kubernetesæˆä¸ºåŒºå—é“¾åŸºç¡€è®¾æ–½æ ‡é…
  âœ… Operatoræ¨¡å¼æ™®åŠ (Fabric Operatorã€Besu Operator)
  âœ… Helm Chartsç”Ÿæ€å®Œå–„
  âœ… æœåŠ¡ç½‘æ ¼é›†æˆ (Istioã€Linkerd)

Layer 2çˆ†å‘:
  âœ… Optimistic Rollupä¸»ç½‘ (Optimismã€Arbitrum)
  âœ… ZK Rollupäº§å“åŒ– (zkSync Eraã€Polygon zkEVMã€Scroll)
  âœ… App-specific Rollup (OP Stackã€Orbit)
  âœ… Validium/Volitionæ··åˆæ–¹æ¡ˆ

é›¶çŸ¥è¯†è¯æ˜ (ZK):
  âœ… zkEVMæˆç†Ÿ (Type 2-4å…¼å®¹æ€§)
  âœ… GPUåŠ é€Ÿè¯æ˜ç”Ÿæˆ (NVIDIA A100/H100)
  âœ… è¯æ˜èšåˆ (Aggregation Layer)
  âœ… é€’å½’è¯æ˜ (Recursive SNARKs)

æ¨¡å—åŒ–åŒºå—é“¾:
  âœ… æ•°æ®å¯ç”¨æ€§å±‚ (Celestiaã€EigenDA)
  âœ… æ‰§è¡Œå±‚åˆ†ç¦» (Rollupã€Validium)
  âœ… å…±è¯†å±‚æ¨¡å—åŒ– (Tendermintã€HotStuff)
  âœ… è·¨å±‚é€šä¿¡æ ‡å‡†åŒ–

Web3åŸºç¡€è®¾æ–½:
  âœ… RPCæœåŠ¡å•†ä¸šåŒ– (Alchemyã€Infuraã€QuickNode)
  âœ… ç´¢å¼•å™¨æ ‡å‡†åŒ– (The Graphã€Subsquid)
  âœ… é¢„è¨€æœºç½‘ç»œ (Chainlink 2.0ã€Pyth)
  âœ… IPFS/Arweaveæ°¸ä¹…å­˜å‚¨

æœºå¯†è®¡ç®—é›†æˆ:
  âœ… TEEç¯å¢ƒ (Intel SGXã€AMD SEV)
  âœ… æœºå¯†å®¹å™¨ (Confidential Containers)
  âœ… éšç§è®¡ç®— (MPCã€FHE)
  âœ… zkML (é›¶çŸ¥è¯†æœºå™¨å­¦ä¹ )
```

### 1.3 æŠ€æœ¯æ ˆå¯¹æ¯”

| ç»´åº¦ | Hyperledger Fabric | Ethereum (L1) | Layer 2 (Rollup) | Cosmos/Tendermint |
|------|-------------------|---------------|------------------|-------------------|
| **å…±è¯†æœºåˆ¶** | Raft/BFT | PoS (Beacon Chain) | L1ç»§æ‰¿å®‰å…¨æ€§ | Tendermint BFT |
| **TPS** | 1,000-20,000 | 15-30 (å‡çº§å~100) | 2,000-10,000 | 1,000-10,000 |
| **æœ€ç»ˆæ€§** | ç§’çº§ | 12åˆ†é’Ÿ (2 epochs) | ç§’çº§ (è½¯ç¡®è®¤) | ç§’çº§ |
| **æ™ºèƒ½åˆçº¦** | Chaincode (Go/Java/Node.js) | Solidity/Vyper | EVMå…¼å®¹ | CosmWasm/EVM |
| **è®¸å¯æ¨¡å‹** | è®¸å¯é“¾ (Permissioned) | æ— è®¸å¯ (Permissionless) | æ— è®¸å¯ | å¯é…ç½® |
| **å®¹å™¨åŒ–éš¾åº¦** | â­â­â­â­ (å¤æ‚) | â­â­â­ (ä¸­ç­‰) | â­â­â­ (ä¸­ç­‰) | â­â­ (ç®€å•) |
| **å…¸å‹åœºæ™¯** | ä¼ä¸šè”ç›Ÿé“¾ | å…¬é“¾DApp | æ‰©å®¹æ–¹æ¡ˆ | åº”ç”¨é“¾ |
| **å­˜å‚¨éœ€æ±‚** | ä¸­ (GB-TB) | å¤§ (TB+, å…¨èŠ‚ç‚¹~10TB) | ä¸­ (GB-TB) | ä¸­ (GB-TB) |
| **K8s Operator** | âœ… (hlf-operator) | âœ… (besu-operator) | âš ï¸ (ç¤¾åŒºæ–¹æ¡ˆ) | âœ… (cosmos-operator) |

---

## 2. Hyperledger Fabricå®¹å™¨åŒ–

### 2.1 æ¶æ„æ¦‚è¿°

```yaml
Hyperledger Fabric 2.5+ ç»„ä»¶:
  
  Orderer (æ’åºæœåŠ¡):
    - èŒè´£: äº¤æ˜“æ’åºã€åŒºå—ç”Ÿæˆ
    - å…±è¯†: Raft (æ¨è) / BFT (å®éªŒ)
    - å®¹å™¨: fabric-orderer
    - å‰¯æœ¬: 3-7 (å¥‡æ•°, ç”Ÿäº§è‡³å°‘3)
  
  Peer (å¯¹ç­‰èŠ‚ç‚¹):
    - èŒè´£: è´¦æœ¬ç»´æŠ¤ã€é“¾ç æ‰§è¡Œ
    - è§’è‰²: Endorserã€Committerã€Anchor Peer
    - å®¹å™¨: fabric-peer
    - å‰¯æœ¬: 2+ (æ¯ç»„ç»‡)
  
  CA (è¯ä¹¦é¢å‘):
    - èŒè´£: è¯ä¹¦ç®¡ç†ã€èº«ä»½æ³¨å†Œ
    - å®¹å™¨: fabric-ca
    - å‰¯æœ¬: 2+ (ä¸»å¤‡)
  
  Chaincode (é“¾ç ):
    - èŒè´£: æ™ºèƒ½åˆçº¦é€»è¾‘
    - è¿è¡Œæ—¶: Goã€Javaã€Node.js
    - å®¹å™¨: åŠ¨æ€åˆ›å»º (per chaincode)
  
  CouchDB (çŠ¶æ€æ•°æ®åº“):
    - èŒè´£: ä¸–ç•ŒçŠ¶æ€å­˜å‚¨ã€å¯ŒæŸ¥è¯¢
    - å®¹å™¨: couchdb
    - å‰¯æœ¬: 1 per Peer (å¯é€‰)
```

**ç½‘ç»œæ‹“æ‰‘**:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Kubernetes Cluster â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€ Organization 1 Namespace â”€â”€â”€â”                               â”‚
â”‚  â”‚                                 â”‚                               â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                               â”‚
â”‚  â”‚  â”‚ Peer 0  â”‚      â”‚ Peer 1  â”‚  â”‚                               â”‚
â”‚  â”‚  â”‚(Endorser)â”‚â—„â”€â”€â”€â”€â–ºâ”‚(Endorser)â”‚  â”‚                               â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                               â”‚
â”‚  â”‚       â”‚                â”‚        â”‚                               â”‚
â”‚  â”‚       â–¼                â–¼        â”‚                               â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                               â”‚
â”‚  â”‚  â”‚CouchDB 0â”‚      â”‚CouchDB 1â”‚  â”‚                               â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                               â”‚
â”‚  â”‚                                 â”‚                               â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚                               â”‚
â”‚  â”‚  â”‚ CA Org1 â”‚                   â”‚                               â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€ Organization 2 Namespace â”€â”€â”€â”                               â”‚
â”‚  â”‚  (ç±»ä¼¼ç»“æ„)                      â”‚                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€ Orderer Namespace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚                               â”‚                                â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚  â”‚Orderer0â”‚â—„â”€â”¤Orderer1â”‚â”€â–ºâ”‚Orderer2â”‚  (Rafté›†ç¾¤)               â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚  â”‚                               â”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Kuberneteséƒ¨ç½²

**Namespaceéš”ç¦»**:

```yaml
# fabric-namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: fabric-orderer
  labels:
    app: hyperledger-fabric
    component: orderer
---
apiVersion: v1
kind: Namespace
metadata:
  name: fabric-org1
  labels:
    app: hyperledger-fabric
    component: peer
    organization: org1
---
apiVersion: v1
kind: Namespace
metadata:
  name: fabric-org2
  labels:
    app: hyperledger-fabric
    component: peer
    organization: org2
```

**Ordereréƒ¨ç½²** (StatefulSet):

```yaml
# orderer-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: orderer
  namespace: fabric-orderer
spec:
  serviceName: orderer
  replicas: 3
  selector:
    matchLabels:
      app: orderer
  template:
    metadata:
      labels:
        app: orderer
    spec:
      initContainers:
      - name: init-orderer
        image: hyperledger/fabric-orderer:2.5.10
        command:
        - sh
        - -c
        - |
          # åˆå§‹åŒ–ordereré…ç½®
          if [ ! -f /var/hyperledger/orderer/orderer.genesis.block ]; then
            configtxgen -profile SampleMultiNodeEtcdRaft \
              -channelID system-channel \
              -outputBlock /var/hyperledger/orderer/orderer.genesis.block
          fi
        volumeMounts:
        - name: orderer-data
          mountPath: /var/hyperledger/orderer
        - name: config
          mountPath: /etc/hyperledger/configtx
      
      containers:
      - name: orderer
        image: hyperledger/fabric-orderer:2.5.10
        ports:
        - containerPort: 7050
          name: grpc
        - containerPort: 9443
          name: operations
        env:
        - name: ORDERER_GENERAL_LISTENADDRESS
          value: "0.0.0.0"
        - name: ORDERER_GENERAL_LISTENPORT
          value: "7050"
        - name: ORDERER_GENERAL_GENESISMETHOD
          value: "file"
        - name: ORDERER_GENERAL_GENESISFILE
          value: "/var/hyperledger/orderer/orderer.genesis.block"
        - name: ORDERER_GENERAL_LOCALMSPID
          value: "OrdererMSP"
        - name: ORDERER_GENERAL_LOCALMSPDIR
          value: "/var/hyperledger/orderer/msp"
        - name: ORDERER_GENERAL_TLS_ENABLED
          value: "true"
        - name: ORDERER_GENERAL_TLS_PRIVATEKEY
          value: "/var/hyperledger/orderer/tls/server.key"
        - name: ORDERER_GENERAL_TLS_CERTIFICATE
          value: "/var/hyperledger/orderer/tls/server.crt"
        - name: ORDERER_GENERAL_TLS_ROOTCAS
          value: "[/var/hyperledger/orderer/tls/ca.crt]"
        - name: ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE
          value: "/var/hyperledger/orderer/tls/server.crt"
        - name: ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY
          value: "/var/hyperledger/orderer/tls/server.key"
        - name: ORDERER_GENERAL_CLUSTER_ROOTCAS
          value: "[/var/hyperledger/orderer/tls/ca.crt]"
        - name: ORDERER_OPERATIONS_LISTENADDRESS
          value: "0.0.0.0:9443"
        - name: ORDERER_METRICS_PROVIDER
          value: "prometheus"
        
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        
        volumeMounts:
        - name: orderer-data
          mountPath: /var/hyperledger/orderer
        - name: tls-cert
          mountPath: /var/hyperledger/orderer/tls
        - name: msp
          mountPath: /var/hyperledger/orderer/msp
        
        livenessProbe:
          httpGet:
            path: /healthz
            port: 9443
          initialDelaySeconds: 30
          periodSeconds: 10
        
        readinessProbe:
          httpGet:
            path: /healthz
            port: 9443
          initialDelaySeconds: 10
          periodSeconds: 5
      
      volumes:
      - name: tls-cert
        secret:
          secretName: orderer-tls
      - name: msp
        secret:
          secretName: orderer-msp
      - name: config
        configMap:
          name: fabric-config
  
  volumeClaimTemplates:
  - metadata:
      name: orderer-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi

---
apiVersion: v1
kind: Service
metadata:
  name: orderer
  namespace: fabric-orderer
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - port: 7050
    name: grpc
  - port: 9443
    name: operations
  selector:
    app: orderer
```

**Peeréƒ¨ç½²**:

```yaml
# peer-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: peer
  namespace: fabric-org1
spec:
  serviceName: peer
  replicas: 2
  selector:
    matchLabels:
      app: peer
  template:
    metadata:
      labels:
        app: peer
    spec:
      containers:
      - name: peer
        image: hyperledger/fabric-peer:2.5.10
        ports:
        - containerPort: 7051
          name: grpc
        - containerPort: 7052
          name: events
        - containerPort: 9443
          name: operations
        env:
        - name: CORE_PEER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: CORE_PEER_ADDRESS
          value: "$(CORE_PEER_ID).peer.fabric-org1.svc.cluster.local:7051"
        - name: CORE_PEER_LISTENADDRESS
          value: "0.0.0.0:7051"
        - name: CORE_PEER_CHAINCODEADDRESS
          value: "$(CORE_PEER_ID).peer.fabric-org1.svc.cluster.local:7052"
        - name: CORE_PEER_CHAINCODELISTENADDRESS
          value: "0.0.0.0:7052"
        - name: CORE_PEER_GOSSIP_BOOTSTRAP
          value: "peer-0.peer.fabric-org1.svc.cluster.local:7051"
        - name: CORE_PEER_GOSSIP_EXTERNALENDPOINT
          value: "$(CORE_PEER_ID).peer.fabric-org1.svc.cluster.local:7051"
        - name: CORE_PEER_LOCALMSPID
          value: "Org1MSP"
        - name: CORE_PEER_TLS_ENABLED
          value: "true"
        - name: CORE_PEER_TLS_CERT_FILE
          value: "/etc/hyperledger/fabric/tls/server.crt"
        - name: CORE_PEER_TLS_KEY_FILE
          value: "/etc/hyperledger/fabric/tls/server.key"
        - name: CORE_PEER_TLS_ROOTCERT_FILE
          value: "/etc/hyperledger/fabric/tls/ca.crt"
        - name: CORE_LEDGER_STATE_STATEDATABASE
          value: "CouchDB"
        - name: CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS
          value: "localhost:5984"
        - name: CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME
          value: "admin"
        - name: CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD
          valueFrom:
            secretKeyRef:
              name: couchdb-secret
              key: password
        - name: CORE_OPERATIONS_LISTENADDRESS
          value: "0.0.0.0:9443"
        - name: CORE_METRICS_PROVIDER
          value: "prometheus"
        - name: FABRIC_LOGGING_SPEC
          value: "INFO"
        
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        
        volumeMounts:
        - name: peer-data
          mountPath: /var/hyperledger/production
        - name: tls-cert
          mountPath: /etc/hyperledger/fabric/tls
        - name: msp
          mountPath: /etc/hyperledger/fabric/msp
        - name: docker-sock
          mountPath: /var/run/docker.sock
        
        livenessProbe:
          httpGet:
            path: /healthz
            port: 9443
          initialDelaySeconds: 60
          periodSeconds: 10
        
        readinessProbe:
          httpGet:
            path: /healthz
            port: 9443
          initialDelaySeconds: 30
          periodSeconds: 5
      
      # Sidecar: CouchDB
      - name: couchdb
        image: couchdb:3.3.3
        ports:
        - containerPort: 5984
        env:
        - name: COUCHDB_USER
          value: "admin"
        - name: COUCHDB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: couchdb-secret
              key: password
        
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        
        volumeMounts:
        - name: couchdb-data
          mountPath: /opt/couchdb/data
        
        livenessProbe:
          httpGet:
            path: /_up
            port: 5984
          initialDelaySeconds: 60
          periodSeconds: 10
      
      volumes:
      - name: tls-cert
        secret:
          secretName: peer-tls
      - name: msp
        secret:
          secretName: peer-msp
      - name: docker-sock
        hostPath:
          path: /var/run/docker.sock
          type: Socket
  
  volumeClaimTemplates:
  - metadata:
      name: peer-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 200Gi
  - metadata:
      name: couchdb-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi

---
apiVersion: v1
kind: Service
metadata:
  name: peer
  namespace: fabric-org1
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - port: 7051
    name: grpc
  - port: 7052
    name: events
  - port: 9443
    name: operations
  selector:
    app: peer
```

### 2.3 Fabric Operator (2025)

**å®‰è£…hlf-operator**:

```bash
# æ·»åŠ Helmä»“åº“
helm repo add hlf-operator https://hyperledger-labs.github.io/hlf-operator/
helm repo update

# å®‰è£…Operator
helm install hlf-operator hlf-operator/hlf-operator \
  --namespace hlf-operator \
  --create-namespace \
  --version 1.10.0

# éªŒè¯
kubectl get pods -n hlf-operator
```

**ä½¿ç”¨CRDéƒ¨ç½²Peer**:

```yaml
# fabricpeer-crd.yaml
apiVersion: hlf.hyperledger.org/v1alpha1
kind: FabricPeer
metadata:
  name: org1-peer0
  namespace: fabric-org1
spec:
  image: hyperledger/fabric-peer
  tag: 2.5.10
  
  mspID: Org1MSP
  
  replicas: 1
  
  resources:
    peer:
      requests:
        cpu: "1000m"
        memory: "2Gi"
      limits:
        cpu: "2000m"
        memory: "4Gi"
    couchdb:
      requests:
        cpu: "500m"
        memory: "1Gi"
      limits:
        cpu: "1000m"
        memory: "2Gi"
  
  storage:
    peer:
      size: "200Gi"
      storageClass: "fast-ssd"
    couchdb:
      size: "100Gi"
      storageClass: "fast-ssd"
  
  stateDb: CouchDB
  
  secret:
    enrollment:
      component:
        cahost: ca-org1.fabric-org1.svc.cluster.local
        caname: ca
        caport: 7054
        catls:
          cacert: LS0tLS1CRUdJTi... # CAè¯ä¹¦
        enrollid: peer0
        enrollsecret: peer0pw
      tls:
        cahost: ca-org1.fabric-org1.svc.cluster.local
        caname: tlsca
        caport: 7054
        catls:
          cacert: LS0tLS1CRUdJTi... # TLS CAè¯ä¹¦
        enrollid: peer0
        enrollsecret: peer0pw
  
  service:
    type: ClusterIP
  
  hostAliases: []
  
  externalEndpoint: peer0-org1.example.com:443
  
  gossipExternalEndpoint: peer0-org1.example.com:443
  
  istio:
    port: 443
```

**éƒ¨ç½²é“¾ç  (Chaincode)**:

```yaml
# fabricchaincode-crd.yaml
apiVersion: hlf.hyperledger.org/v1alpha1
kind: FabricChaincode
metadata:
  name: asset-transfer
  namespace: fabric-org1
spec:
  image: hyperledger/fabric-chaincode-node
  tag: 2.5.10
  
  packageID: asset-transfer:1.0
  
  version: "1.0"
  
  language: node
  
  peers:
  - name: org1-peer0
    namespace: fabric-org1
  
  channel: mychannel
  
  endorsementPolicy: "OR('Org1MSP.peer','Org2MSP.peer')"
  
  initRequired: true
  
  env:
  - name: CHAINCODE_CCID
    value: asset-transfer:1.0
  - name: CHAINCODE_ADDRESS
    value: "0.0.0.0:9999"
  
  resources:
    requests:
      cpu: "500m"
      memory: "512Mi"
    limits:
      cpu: "1000m"
      memory: "1Gi"
```

### 2.4 é“¾ç å®¹å™¨åŒ–

**External Chaincodeæ¨¡å¼** (æ¨è):

```yaml
# chaincode-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: asset-transfer-chaincode
  namespace: fabric-org1
spec:
  replicas: 2
  selector:
    matchLabels:
      app: asset-transfer-chaincode
  template:
    metadata:
      labels:
        app: asset-transfer-chaincode
    spec:
      containers:
      - name: chaincode
        image: myregistry/asset-transfer:1.0
        ports:
        - containerPort: 9999
        env:
        - name: CHAINCODE_SERVER_ADDRESS
          value: "0.0.0.0:9999"
        - name: CHAINCODE_ID
          value: "asset-transfer:1.0"
        - name: CHAINCODE_TLS_DISABLED
          value: "false"
        - name: CHAINCODE_TLS_KEY
          value: "/etc/hyperledger/fabric/tls/server.key"
        - name: CHAINCODE_TLS_CERT
          value: "/etc/hyperledger/fabric/tls/server.crt"
        - name: CHAINCODE_TLS_CLIENT_CACERT
          value: "/etc/hyperledger/fabric/tls/ca.crt"
        
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        
        volumeMounts:
        - name: tls-cert
          mountPath: /etc/hyperledger/fabric/tls
        
        livenessProbe:
          tcpSocket:
            port: 9999
          initialDelaySeconds: 30
          periodSeconds: 10
        
        readinessProbe:
          tcpSocket:
            port: 9999
          initialDelaySeconds: 10
          periodSeconds: 5
      
      volumes:
      - name: tls-cert
        secret:
          secretName: chaincode-tls

---
apiVersion: v1
kind: Service
metadata:
  name: asset-transfer-chaincode
  namespace: fabric-org1
spec:
  selector:
    app: asset-transfer-chaincode
  ports:
  - protocol: TCP
    port: 9999
    targetPort: 9999
```

**connection.jsoné…ç½®**:

```json
{
  "address": "asset-transfer-chaincode.fabric-org1.svc.cluster.local:9999",
  "dial_timeout": "10s",
  "tls_required": true,
  "client_auth_required": true,
  "client_key": "-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n",
  "client_cert": "-----BEGIN CERTIFICATE-----\n...\n-----END CERTIFICATE-----\n",
  "root_cert": "-----BEGIN CERTIFICATE-----\n...\n-----END CERTIFICATE-----\n"
}
```

### 2.5 ç”Ÿäº§é…ç½®ç¤ºä¾‹

**èµ„æºé…ç½®**:

```yaml
# ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®
Orderer:
  replicas: 3-7 (å¥‡æ•°)
  cpu: 2-4 cores
  memory: 4-8 GB
  storage: 100-500 GB SSD

Peer:
  replicas: 2-4 per organization
  cpu: 4-8 cores
  memory: 8-16 GB
  storage: 500 GB - 2 TB SSD
  
CouchDB:
  cpu: 2-4 cores
  memory: 4-8 GB
  storage: 200 GB - 1 TB SSD

CA:
  replicas: 2 (ä¸»å¤‡)
  cpu: 1-2 cores
  memory: 2-4 GB
  storage: 10-20 GB
```

**ç½‘ç»œç­–ç•¥**:

```yaml
# networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: fabric-network-policy
  namespace: fabric-org1
spec:
  podSelector:
    matchLabels:
      app: peer
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # å…è®¸æ¥è‡ªåŒç»„ç»‡Peerçš„è¿æ¥
  - from:
    - podSelector:
        matchLabels:
          app: peer
    ports:
    - protocol: TCP
      port: 7051
  # å…è®¸æ¥è‡ªOrdererçš„è¿æ¥
  - from:
    - namespaceSelector:
        matchLabels:
          component: orderer
    ports:
    - protocol: TCP
      port: 7051
  # å…è®¸æ¥è‡ªå…¶ä»–ç»„ç»‡Peerçš„è¿æ¥ (è·¨Namespace)
  - from:
    - namespaceSelector:
        matchLabels:
          app: hyperledger-fabric
          component: peer
    ports:
    - protocol: TCP
      port: 7051
  # å…è®¸Prometheusç›‘æ§
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    - podSelector:
        matchLabels:
          app: prometheus
    ports:
    - protocol: TCP
      port: 9443
  
  egress:
  # å…è®¸DNS
  - to:
    - namespaceSelector: {}
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  # å…è®¸è®¿é—®Orderer
  - to:
    - namespaceSelector:
        matchLabels:
          component: orderer
    ports:
    - protocol: TCP
      port: 7050
  # å…è®¸è®¿é—®CouchDB (åŒPod)
  - to:
    - podSelector:
        matchLabels:
          app: peer
    ports:
    - protocol: TCP
      port: 5984
  # å…è®¸è®¿é—®å…¶ä»–ç»„ç»‡Peer
  - to:
    - namespaceSelector:
        matchLabels:
          app: hyperledger-fabric
          component: peer
    ports:
    - protocol: TCP
      port: 7051
```

---

## 3. EthereumèŠ‚ç‚¹å®¹å™¨åŒ–

### 3.1 å®¢æˆ·ç«¯é€‰æ‹©

| å®¢æˆ·ç«¯ | è¯­è¨€ | å…±è¯†+æ‰§è¡Œ | èµ„æºå ç”¨ | åŒæ­¥é€Ÿåº¦ | æ¨èåœºæ™¯ |
|--------|------|-----------|----------|----------|----------|
| **Geth** | Go | âœ… | ä¸­ (8-16GB RAM) | å¿« (~6h) | é€šç”¨ã€ç”Ÿäº§ â­ |
| **Besu** | Java | âœ… | é«˜ (16-32GB RAM) | ä¸­ (~12h) | ä¼ä¸šã€éšç§ |
| **Nethermind** | C# | âœ… | ä¸­ (8-16GB RAM) | å¿« (~4h, å¿«ç…§) | å½’æ¡£ã€RPC â­ |
| **Erigon** | Go | âœ… | ä½ (16GB RAM) | å¿« (~2h, ä¼˜åŒ–) | å½’æ¡£ã€å¼€å‘ â­ |
| **Reth** | Rust | âŒ (å¼€å‘ä¸­) | ä½ (TBD) | æå¿« (TBD) | æœªæ¥ ğŸš€ |

**å…±è¯†å±‚å®¢æˆ·ç«¯** (Post-Merge):

| å®¢æˆ·ç«¯ | è¯­è¨€ | èµ„æºå ç”¨ | æ¨èåœºæ™¯ |
|--------|------|----------|----------|
| **Prysm** | Go | ä¸­ (4-8GB) | é€šç”¨ â­ |
| **Lighthouse** | Rust | ä½ (4-6GB) | æ€§èƒ½ä¼˜å…ˆ â­ |
| **Teku** | Java | é«˜ (8-16GB) | ä¼ä¸š |
| **Nimbus** | Nim | ä½ (2-4GB) | èµ„æºå—é™ â­ |
| **Lodestar** | TypeScript | ä¸­ (4-8GB) | å¼€å‘ |

### 3.2 Gethå®¹å™¨åŒ–éƒ¨ç½²

**Docker Compose (æœ¬åœ°å¼€å‘)**:

```yaml
# docker-compose.yml
version: '3.8'

services:
  geth:
    image: ethereum/client-go:v1.14.8
    container_name: geth-mainnet
    restart: unless-stopped
    ports:
      - "8545:8545"   # HTTP RPC
      - "8546:8546"   # WebSocket
      - "30303:30303" # P2P (TCP)
      - "30303:30303/udp" # P2P (UDP)
      - "8551:8551"   # Engine API (å…±è¯†å±‚)
    volumes:
      - geth-data:/root/.ethereum
      - ./jwt.hex:/root/jwt.hex:ro
    command:
      - --mainnet
      - --http
      - --http.addr=0.0.0.0
      - --http.vhosts=*
      - --http.corsdomain=*
      - --http.api=eth,net,web3,txpool
      - --ws
      - --ws.addr=0.0.0.0
      - --ws.origins=*
      - --ws.api=eth,net,web3,txpool
      - --authrpc.addr=0.0.0.0
      - --authrpc.port=8551
      - --authrpc.vhosts=*
      - --authrpc.jwtsecret=/root/jwt.hex
      - --syncmode=snap
      - --cache=8192
      - --maxpeers=50
      - --metrics
      - --metrics.addr=0.0.0.0
      - --metrics.port=6060
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "10"
  
  lighthouse:
    image: sigp/lighthouse:v5.3.0
    container_name: lighthouse-mainnet
    restart: unless-stopped
    ports:
      - "5052:5052"   # HTTP API
      - "9000:9000"   # P2P (TCP)
      - "9000:9000/udp" # P2P (UDP)
    volumes:
      - lighthouse-data:/root/.lighthouse
      - ./jwt.hex:/root/jwt.hex:ro
    command:
      - lighthouse
      - bn
      - --network=mainnet
      - --datadir=/root/.lighthouse
      - --http
      - --http-address=0.0.0.0
      - --http-port=5052
      - --execution-endpoint=http://geth:8551
      - --execution-jwt=/root/jwt.hex
      - --checkpoint-sync-url=https://beaconstate.ethstaker.cc
      - --metrics
      - --metrics-address=0.0.0.0
      - --metrics-port=5054
    depends_on:
      - geth
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "10"

volumes:
  geth-data:
  lighthouse-data:
```

**ç”ŸæˆJWTå¯†é’¥**:

```bash
# ç”ŸæˆJWTå¯†é’¥ (æ‰§è¡Œå±‚å’Œå…±è¯†å±‚å…±äº«)
openssl rand -hex 32 > jwt.hex

# å¯åŠ¨
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# æ£€æŸ¥åŒæ­¥çŠ¶æ€
docker exec geth-mainnet geth attach --exec 'eth.syncing'
docker exec lighthouse-mainnet lighthouse bn --network mainnet --datadir /root/.lighthouse http get http://localhost:5052/eth/v1/node/syncing
```

### 3.3 Kubernetesä¸Šçš„Ethereum

**Namespace**:

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ethereum-mainnet
  labels:
    name: ethereum-mainnet
```

**JWT Secret**:

```bash
# ç”ŸæˆJWTå¯†é’¥
openssl rand -hex 32 > jwt.hex

# åˆ›å»ºSecret
kubectl create secret generic jwt-secret \
  --from-file=jwt.hex=jwt.hex \
  --namespace ethereum-mainnet
```

**Geth StatefulSet**:

```yaml
# geth-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: geth
  namespace: ethereum-mainnet
spec:
  serviceName: geth
  replicas: 1
  selector:
    matchLabels:
      app: geth
  template:
    metadata:
      labels:
        app: geth
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "6060"
        prometheus.io/path: "/debug/metrics/prometheus"
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
        runAsNonRoot: true
      
      initContainers:
      - name: init-datadir
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          mkdir -p /data/geth
          chown -R 1000:1000 /data
        volumeMounts:
        - name: geth-data
          mountPath: /data
        securityContext:
          runAsUser: 0
      
      containers:
      - name: geth
        image: ethereum/client-go:v1.14.8
        ports:
        - containerPort: 8545
          name: http-rpc
        - containerPort: 8546
          name: ws-rpc
        - containerPort: 30303
          name: p2p-tcp
          protocol: TCP
        - containerPort: 30303
          name: p2p-udp
          protocol: UDP
        - containerPort: 8551
          name: engine-api
        - containerPort: 6060
          name: metrics
        
        command:
        - geth
        - --mainnet
        - --datadir=/data/geth
        - --http
        - --http.addr=0.0.0.0
        - --http.port=8545
        - --http.vhosts=*
        - --http.corsdomain=*
        - --http.api=eth,net,web3,txpool
        - --ws
        - --ws.addr=0.0.0.0
        - --ws.port=8546
        - --ws.origins=*
        - --ws.api=eth,net,web3,txpool
        - --authrpc.addr=0.0.0.0
        - --authrpc.port=8551
        - --authrpc.vhosts=*
        - --authrpc.jwtsecret=/secrets/jwt.hex
        - --syncmode=snap
        - --cache=8192
        - --maxpeers=50
        - --metrics
        - --metrics.addr=0.0.0.0
        - --metrics.port=6060
        - --pprof
        - --pprof.addr=0.0.0.0
        - --pprof.port=6061
        
        resources:
          requests:
            memory: "16Gi"
            cpu: "4000m"
          limits:
            memory: "32Gi"
            cpu: "8000m"
        
        volumeMounts:
        - name: geth-data
          mountPath: /data
        - name: jwt-secret
          mountPath: /secrets
          readOnly: true
        
        livenessProbe:
          httpGet:
            path: /
            port: 8545
            httpHeaders:
            - name: Content-Type
              value: application/json
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - |
              geth attach --exec 'eth.syncing' http://localhost:8545 | grep -q false
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 10
      
      volumes:
      - name: jwt-secret
        secret:
          secretName: jwt-secret
  
  volumeClaimTemplates:
  - metadata:
      name: geth-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 2Ti  # ä¸»ç½‘å½’æ¡£èŠ‚ç‚¹éœ€è¦10+ TB

---
apiVersion: v1
kind: Service
metadata:
  name: geth
  namespace: ethereum-mainnet
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "6060"
spec:
  selector:
    app: geth
  ports:
  - name: http-rpc
    port: 8545
    targetPort: 8545
  - name: ws-rpc
    port: 8546
    targetPort: 8546
  - name: p2p-tcp
    port: 30303
    targetPort: 30303
    protocol: TCP
  - name: p2p-udp
    port: 30303
    targetPort: 30303
    protocol: UDP
  - name: engine-api
    port: 8551
    targetPort: 8551
  - name: metrics
    port: 6060
    targetPort: 6060
  type: LoadBalancer  # æˆ– NodePort, ClusterIP
```

**Lighthouse StatefulSet**:

```yaml
# lighthouse-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: lighthouse
  namespace: ethereum-mainnet
spec:
  serviceName: lighthouse
  replicas: 1
  selector:
    matchLabels:
      app: lighthouse
  template:
    metadata:
      labels:
        app: lighthouse
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "5054"
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
        runAsNonRoot: true
      
      initContainers:
      - name: init-datadir
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          mkdir -p /data/lighthouse
          chown -R 1000:1000 /data
        volumeMounts:
        - name: lighthouse-data
          mountPath: /data
        securityContext:
          runAsUser: 0
      
      containers:
      - name: lighthouse
        image: sigp/lighthouse:v5.3.0
        ports:
        - containerPort: 5052
          name: http-api
        - containerPort: 9000
          name: p2p-tcp
          protocol: TCP
        - containerPort: 9000
          name: p2p-udp
          protocol: UDP
        - containerPort: 5054
          name: metrics
        
        command:
        - lighthouse
        - bn
        - --network=mainnet
        - --datadir=/data/lighthouse
        - --http
        - --http-address=0.0.0.0
        - --http-port=5052
        - --execution-endpoint=http://geth.ethereum-mainnet.svc.cluster.local:8551
        - --execution-jwt=/secrets/jwt.hex
        - --checkpoint-sync-url=https://beaconstate.ethstaker.cc
        - --metrics
        - --metrics-address=0.0.0.0
        - --metrics-port=5054
        - --target-peers=50
        
        resources:
          requests:
            memory: "8Gi"
            cpu: "2000m"
          limits:
            memory: "16Gi"
            cpu: "4000m"
        
        volumeMounts:
        - name: lighthouse-data
          mountPath: /data
        - name: jwt-secret
          mountPath: /secrets
          readOnly: true
        
        livenessProbe:
          httpGet:
            path: /eth/v1/node/health
            port: 5052
          initialDelaySeconds: 60
          periodSeconds: 30
        
        readinessProbe:
          httpGet:
            path: /eth/v1/node/health
            port: 5052
          initialDelaySeconds: 30
          periodSeconds: 10
      
      volumes:
      - name: jwt-secret
        secret:
          secretName: jwt-secret
  
  volumeClaimTemplates:
  - metadata:
      name: lighthouse-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 500Gi

---
apiVersion: v1
kind: Service
metadata:
  name: lighthouse
  namespace: ethereum-mainnet
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "5054"
spec:
  selector:
    app: lighthouse
  ports:
  - name: http-api
    port: 5052
    targetPort: 5052
  - name: p2p-tcp
    port: 9000
    targetPort: 9000
    protocol: TCP
  - name: p2p-udp
    port: 9000
    targetPort: 9000
    protocol: UDP
  - name: metrics
    port: 5054
    targetPort: 5054
  type: LoadBalancer
```

### 3.4 Ethereum Operator

**ä½¿ç”¨Besu Operator**:

```bash
# å®‰è£…Operator
kubectl apply -f https://raw.githubusercontent.com/Consensys/besu-kubernetes/main/deploy/besu-operator.yaml

# éªŒè¯
kubectl get pods -n besu-operator-system
```

**åˆ›å»ºBesuç½‘ç»œ**:

```yaml
# besu-network.yaml
apiVersion: besu.k8s.io/v1alpha1
kind: BesuNetwork
metadata:
  name: besu-mainnet
  namespace: ethereum-mainnet
spec:
  network: mainnet
  
  nodes:
  - name: validator-1
    type: validator
    replicas: 1
    resources:
      requests:
        cpu: "4"
        memory: "16Gi"
      limits:
        cpu: "8"
        memory: "32Gi"
    storage:
      size: 2Ti
      storageClass: fast-ssd
    p2p:
      enabled: true
      port: 30303
    rpc:
      enabled: true
      port: 8545
      apis:
      - eth
      - net
      - web3
    ws:
      enabled: true
      port: 8546
    metrics:
      enabled: true
      port: 9545
  
  - name: rpc-1
    type: rpc
    replicas: 2
    resources:
      requests:
        cpu: "4"
        memory: "16Gi"
      limits:
        cpu: "8"
        memory: "32Gi"
    storage:
      size: 2Ti
      storageClass: fast-ssd
    rpc:
      enabled: true
      port: 8545
      apis:
      - eth
      - net
      - web3
      - txpool
    ws:
      enabled: true
      port: 8546
  
  monitoring:
    enabled: true
    prometheus:
      enabled: true
      namespace: monitoring
```

### 3.5 éªŒè¯å™¨èŠ‚ç‚¹é«˜å¯ç”¨

**å¤šåŒºåŸŸéƒ¨ç½²**:

```yaml
# validator-deployment.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: eth-validator
  namespace: ethereum-mainnet
spec:
  serviceName: eth-validator
  replicas: 3  # ä¸»+å¤‡ä»½
  podManagementPolicy: Parallel
  
  selector:
    matchLabels:
      app: eth-validator
  
  template:
    metadata:
      labels:
        app: eth-validator
    spec:
      # åäº²å’Œæ€§: ä¸åŒå¯ç”¨åŒº
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: eth-validator
            topologyKey: topology.kubernetes.io/zone
      
      containers:
      - name: validator
        image: sigp/lighthouse:v5.3.0
        command:
        - lighthouse
        - vc
        - --network=mainnet
        - --datadir=/data
        - --beacon-nodes=http://lighthouse.ethereum-mainnet.svc.cluster.local:5052
        - --graffiti="K8s Validator"
        - --suggested-fee-recipient=0xYourFeeRecipient
        - --metrics
        - --metrics-address=0.0.0.0
        - --metrics-port=5064
        
        volumeMounts:
        - name: validator-data
          mountPath: /data
        - name: validator-keys
          mountPath: /keys
          readOnly: true
        
        resources:
          requests:
            memory: "4Gi"
            cpu: "1000m"
          limits:
            memory: "8Gi"
            cpu: "2000m"
  
  volumeClaimTemplates:
  - metadata:
      name: validator-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 50Gi
```

**è‡ªåŠ¨åˆ‡æ¢ (Slashingä¿æŠ¤)**:

```yaml
# slashing-protectioné…ç½®
# ä½¿ç”¨DVT (åˆ†å¸ƒå¼éªŒè¯å™¨æŠ€æœ¯) å¦‚SSV Networkæˆ–Obol Network
apiVersion: v1
kind: ConfigMap
metadata:
  name: slashing-protection-config
  namespace: ethereum-mainnet
data:
  config.yaml: |
    # Web3Signeré…ç½® (ç»Ÿä¸€å¯†é’¥ç®¡ç†)
    slashing-protection-db-url: postgresql://user:pass@postgres:5432/slashing
    slashing-protection-enabled: true
    
    # æˆ–ä½¿ç”¨Dirk (åˆ†å¸ƒå¼å¯†é’¥ç®¡ç†)
    distributed-validator-enabled: true
    threshold: 2  # 3èŠ‚ç‚¹ä¸­2ä¸ªç­¾å
```

---

## 4. Layer 2è§£å†³æ–¹æ¡ˆå®¹å™¨åŒ–

### 4.1 Optimism/Arbitrumå®¹å™¨åŒ–

**Optimismæ¶æ„**:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Optimism Stack (OP Stack) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  op-node    â”‚â—„â”€â”€â”€â”€â–ºâ”‚ op-geth     â”‚â—„â”€â”€â”€â”€â–ºâ”‚ op-batch â”‚  â”‚
â”‚  â”‚ (RollupèŠ‚ç‚¹)â”‚      â”‚ (æ‰§è¡Œå¼•æ“)   â”‚      â”‚ (æ‰¹å¤„ç†å™¨) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â–²                                         â”‚       â”‚
â”‚         â”‚                                         â–¼       â”‚
â”‚         â”‚                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ op-proposeâ”‚   â”‚
â”‚                                            â”‚ (æè®®å™¨)  â”‚   â”‚
â”‚                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                  â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â–¼
                                        Ethereum L1 (ä¸»ç½‘)
```

**Docker Composeéƒ¨ç½²** (Optimism Sepoliaæµ‹è¯•ç½‘):

```yaml
# optimism-docker-compose.yml
version: '3.8'

services:
  l1-geth:
    image: ethereum/client-go:v1.14.8
    container_name: l1-geth
    ports:
      - "8545:8545"
    volumes:
      - l1-data:/data
    command:
      - --sepolia
      - --datadir=/data
      - --http
      - --http.addr=0.0.0.0
      - --http.api=eth,net,web3
      - --ws
      - --ws.addr=0.0.0.0
  
  op-geth:
    image: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth:v1.101411.2
    container_name: op-geth
    ports:
      - "9545:8545"
      - "9546:8546"
      - "8551:8551"
    volumes:
      - op-geth-data:/data
      - ./jwt.hex:/jwt.hex:ro
    command:
      - --datadir=/data
      - --http
      - --http.addr=0.0.0.0
      - --http.port=8545
      - --http.api=eth,net,web3,debug
      - --ws
      - --ws.addr=0.0.0.0
      - --ws.port=8546
      - --authrpc.addr=0.0.0.0
      - --authrpc.port=8551
      - --authrpc.jwtsecret=/jwt.hex
      - --syncmode=full
      - --gcmode=archive
      - --rollup.sequencerhttp=https://sepolia-sequencer.optimism.io
    depends_on:
      - l1-geth
  
  op-node:
    image: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-node:v1.8.0
    container_name: op-node
    ports:
      - "9003:9003"
      - "7545:8545"
    volumes:
      - op-node-data:/data
      - ./jwt.hex:/jwt.hex:ro
      - ./rollup.json:/rollup.json:ro
    command:
      - op-node
      - --l1=http://l1-geth:8545
      - --l2=http://op-geth:8551
      - --l2.jwt-secret=/jwt.hex
      - --rollup.config=/rollup.json
      - --rpc.addr=0.0.0.0
      - --rpc.port=8545
      - --p2p.listen.ip=0.0.0.0
      - --p2p.listen.tcp=9003
      - --p2p.listen.udp=9003
      - --p2p.bootnodes=enr:-J64QBwRIWAco... # Sepolia bootnodes
      - --metrics.enabled
      - --metrics.addr=0.0.0.0
      - --metrics.port=7300
    depends_on:
      - l1-geth
      - op-geth

volumes:
  l1-data:
  op-geth-data:
  op-node-data:
```

**Kuberneteséƒ¨ç½²** (Optimism):

```yaml
# op-geth-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: op-geth
  namespace: optimism-sepolia
spec:
  serviceName: op-geth
  replicas: 1
  selector:
    matchLabels:
      app: op-geth
  template:
    metadata:
      labels:
        app: op-geth
    spec:
      containers:
      - name: op-geth
        image: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth:v1.101411.2
        ports:
        - containerPort: 8545
          name: rpc
        - containerPort: 8546
          name: ws
        - containerPort: 8551
          name: engine
        command:
        - geth
        - --datadir=/data
        - --http
        - --http.addr=0.0.0.0
        - --http.port=8545
        - --http.vhosts=*
        - --http.corsdomain=*
        - --http.api=eth,net,web3,debug,txpool
        - --ws
        - --ws.addr=0.0.0.0
        - --ws.port=8546
        - --ws.origins=*
        - --ws.api=eth,net,web3
        - --authrpc.addr=0.0.0.0
        - --authrpc.port=8551
        - --authrpc.vhosts=*
        - --authrpc.jwtsecret=/secrets/jwt.hex
        - --syncmode=full
        - --gcmode=archive
        - --rollup.sequencerhttp=https://sepolia-sequencer.optimism.io
        - --cache=4096
        - --maxpeers=50
        - --metrics
        - --metrics.addr=0.0.0.0
        - --metrics.port=6060
        
        resources:
          requests:
            memory: "8Gi"
            cpu: "2000m"
          limits:
            memory: "16Gi"
            cpu: "4000m"
        
        volumeMounts:
        - name: op-geth-data
          mountPath: /data
        - name: jwt-secret
          mountPath: /secrets
  
  volumeClaimTemplates:
  - metadata:
      name: op-geth-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 500Gi

---
# op-node-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: op-node
  namespace: optimism-sepolia
spec:
  replicas: 1
  selector:
    matchLabels:
      app: op-node
  template:
    metadata:
      labels:
        app: op-node
    spec:
      containers:
      - name: op-node
        image: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-node:v1.8.0
        ports:
        - containerPort: 8545
          name: rpc
        - containerPort: 9003
          name: p2p-tcp
          protocol: TCP
        - containerPort: 9003
          name: p2p-udp
          protocol: UDP
        - containerPort: 7300
          name: metrics
        
        command:
        - op-node
        - --l1=http://l1-geth.ethereum-sepolia.svc.cluster.local:8545
        - --l2=http://op-geth.optimism-sepolia.svc.cluster.local:8551
        - --l2.jwt-secret=/secrets/jwt.hex
        - --rollup.config=/config/rollup.json
        - --rpc.addr=0.0.0.0
        - --rpc.port=8545
        - --p2p.listen.ip=0.0.0.0
        - --p2p.listen.tcp=9003
        - --p2p.listen.udp=9003
        - --p2p.bootnodes=enr:-J64QBwRIWAco...
        - --metrics.enabled
        - --metrics.addr=0.0.0.0
        - --metrics.port=7300
        - --pprof.enabled
        - --pprof.addr=0.0.0.0
        - --pprof.port=6060
        
        resources:
          requests:
            memory: "4Gi"
            cpu: "1000m"
          limits:
            memory: "8Gi"
            cpu: "2000m"
        
        volumeMounts:
        - name: jwt-secret
          mountPath: /secrets
        - name: rollup-config
          mountPath: /config
      
      volumes:
      - name: jwt-secret
        secret:
          secretName: jwt-secret
      - name: rollup-config
        configMap:
          name: op-rollup-config
```

### 4.2 zkSync Eraå®¹å™¨åŒ–

**zkSync Eraæ¶æ„**:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ zkSync Era â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Server   â”‚â—„â”€â”€â”€â”€â–ºâ”‚   Prover (GPU)   â”‚ â”‚
â”‚  â”‚(API+Sequen)â”‚      â”‚  (è¯æ˜ç”Ÿæˆå™¨)     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                      â”‚          â”‚
â”‚         â–¼                      â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ PostgreSQL â”‚      â”‚ Object Storage   â”‚ â”‚
â”‚  â”‚  (State)   â”‚      â”‚  (Proof/Witness) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Docker Composeéƒ¨ç½²**:

```yaml
# zksync-docker-compose.yml
version: '3.8'

services:
  postgres:
    image: postgres:16
    container_name: zksync-postgres
    environment:
      POSTGRES_USER: zksync
      POSTGRES_PASSWORD: zksync
      POSTGRES_DB: zksync
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
  
  server:
    image: matterlabs/server-v2:latest2.0-v24.24.0
    container_name: zksync-server
    ports:
      - "3050:3050"  # JSON-RPC
      - "3051:3051"  # WebSocket
    environment:
      DATABASE_URL: postgres://zksync:zksync@postgres:5432/zksync
      ETH_CLIENT_WEB3_URL: https://sepolia.infura.io/v3/YOUR_KEY
      ZKSYNC_HOME: /usr/src/zksync
      RUST_LOG: info
    depends_on:
      - postgres
    volumes:
      - ./config:/config:ro
      - server-data:/data
    command:
      - zksync_server
      - --config=/config/server.toml
  
  prover:
    image: matterlabs/prover-v2:latest2.0-v24.24.0
    container_name: zksync-prover
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
    environment:
      DATABASE_URL: postgres://zksync:zksync@postgres:5432/zksync
      RUST_LOG: info
      CRS_FILE: /crs/setup_2^26.key
    depends_on:
      - postgres
      - server
    volumes:
      - ./crs:/crs:ro
      - prover-data:/data
    command:
      - zksync_prover

volumes:
  postgres-data:
  server-data:
  prover-data:
```

**Kuberneteséƒ¨ç½²** (zkSync Era Prover with GPU):

```yaml
# zksync-prover-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zksync-prover
  namespace: zksync-era
spec:
  replicas: 2  # GPUèŠ‚ç‚¹æ•°é‡
  selector:
    matchLabels:
      app: zksync-prover
  template:
    metadata:
      labels:
        app: zksync-prover
    spec:
      # GPUèŠ‚ç‚¹è°ƒåº¦
      nodeSelector:
        nvidia.com/gpu: "true"
      
      containers:
      - name: prover
        image: matterlabs/prover-v2:latest2.0-v24.24.0
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: zksync-db-secret
              key: url
        - name: RUST_LOG
          value: "info"
        - name: CRS_FILE
          value: "/crs/setup_2^26.key"
        - name: PROVER_TYPE
          value: "gpu"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        
        resources:
          requests:
            memory: "32Gi"
            cpu: "8000m"
            nvidia.com/gpu: 1  # 1ä¸ªGPU
          limits:
            memory: "64Gi"
            cpu: "16000m"
            nvidia.com/gpu: 1
        
        volumeMounts:
        - name: crs
          mountPath: /crs
          readOnly: true
        - name: prover-data
          mountPath: /data
      
      volumes:
      - name: crs
        persistentVolumeClaim:
          claimName: crs-pvc
      - name: prover-data
        emptyDir: {}
```

### 4.3 Polygon CDKå®¹å™¨åŒ–

**Polygon CDKæ¶æ„** (Chain Development Kit):

```yaml
# Polygon CDKç»„ä»¶:
components:
  zkevm-node:
    - Sequencer (æ’åºå™¨)
    - Aggregator (èšåˆå™¨)
    - RPC (JSON-RPCæœåŠ¡)
    - Synchronizer (åŒæ­¥å™¨)
  
  zkevm-prover:
    - Executor (æ‰§è¡Œå™¨)
    - Prover (è¯æ˜ç”Ÿæˆå™¨, GPU)
  
  zkevm-bridge:
    - Bridge Service (è·¨é“¾æ¡¥æœåŠ¡)
  
  databases:
    - State DB (PostgreSQL)
    - Pool DB (PostgreSQL)
```

**Docker Compose**:

```yaml
# polygon-cdk-docker-compose.yml
version: '3.8'

services:
  zkevm-state-db:
    image: postgres:16
    container_name: zkevm-state-db
    environment:
      POSTGRES_USER: state_user
      POSTGRES_PASSWORD: state_password
      POSTGRES_DB: state_db
    volumes:
      - state-db-data:/var/lib/postgresql/data
  
  zkevm-pool-db:
    image: postgres:16
    container_name: zkevm-pool-db
    environment:
      POSTGRES_USER: pool_user
      POSTGRES_PASSWORD: pool_password
      POSTGRES_DB: pool_db
    volumes:
      - pool-db-data:/var/lib/postgresql/data
  
  zkevm-prover:
    image: hermeznetwork/zkevm-prover:v6.0.0
    container_name: zkevm-prover
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
    volumes:
      - ./config/prover:/app/config:ro
    ports:
      - "50051:50051"
      - "50052:50052"
    command: /app/zkProver -c /app/config/config.json
  
  zkevm-node:
    image: hermeznetwork/zkevm-node:v0.6.7
    container_name: zkevm-node
    ports:
      - "8545:8545"  # RPC
      - "9091:9091"  # Prometheus
    environment:
      ZKEVM_NODE_ETHERMAN_URL: "https://sepolia.infura.io/v3/YOUR_KEY"
      ZKEVM_NODE_STATEDB_HOST: zkevm-state-db
      ZKEVM_NODE_POOL_DB_HOST: zkevm-pool-db
    volumes:
      - ./config/node:/app/config:ro
    depends_on:
      - zkevm-state-db
      - zkevm-pool-db
      - zkevm-prover
    command:
      - run
      - --network=custom
      - --custom-network-file=/app/config/genesis.json
      - --cfg=/app/config/node.toml
      - --components=rpc,synchronizer,sequencer,aggregator

volumes:
  state-db-data:
  pool-db-data:
```

### 4.4 Base (Coinbase L2)

**Baseæ¶æ„** (åŸºäºOP Stack):

Baseæ˜¯CoinbaseåŸºäºOP Stackæ„å»ºçš„L2,éƒ¨ç½²æ–¹å¼ä¸Optimismç±»ä¼¼:

```yaml
# base-node-values.yaml (Helm)
image:
  repository: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth
  tag: v1.101411.2

network: base-mainnet

l1:
  rpcUrl: https://mainnet.infura.io/v3/YOUR_KEY
  wsUrl: wss://mainnet.infura.io/ws/v3/YOUR_KEY

sequencer:
  http: https://mainnet-sequencer.base.org

resources:
  geth:
    requests:
      memory: "16Gi"
      cpu: "4000m"
    limits:
      memory: "32Gi"
      cpu: "8000m"
  node:
    requests:
      memory: "8Gi"
      cpu: "2000m"
    limits:
      memory: "16Gi"
      cpu: "4000m"

storage:
  geth:
    size: 1Ti
    storageClass: fast-ssd
  node:
    size: 100Gi
    storageClass: fast-ssd

metrics:
  enabled: true
  serviceMonitor:
    enabled: true

ingress:
  enabled: true
  className: nginx
  hosts:
  - host: base-rpc.example.com
    paths:
    - path: /
      pathType: Prefix
      backend:
        service:
          name: base-geth
          port:
            number: 8545
```

**Helméƒ¨ç½²**:

```bash
# æ·»åŠ ä»“åº“ (ç¤ºä¾‹,å®é™…éœ€è¦å®˜æ–¹ä»“åº“)
helm repo add base https://base.org/charts
helm repo update

# éƒ¨ç½²
helm install base-mainnet base/op-stack \
  --namespace base-mainnet \
  --create-namespace \
  -f base-node-values.yaml

# éªŒè¯
kubectl get pods -n base-mainnet
kubectl logs -f base-mainnet-op-geth-0 -n base-mainnet
```

---

## 5. åŒºå—é“¾å³æœåŠ¡ (BaaS)

### 5.1 Kubernetes Operators

**å¸¸ç”¨Operators**:

| Operator | åŒºå—é“¾ | GitHub | åŠŸèƒ½ |
|----------|--------|--------|------|
| **hlf-operator** | Hyperledger Fabric | hyperledger-labs/hlf-operator | Peer/Orderer/CA/Chaincode |
| **besu-operator** | Hyperledger Besu | ConsenSys/besu-kubernetes | BesuèŠ‚ç‚¹ç®¡ç† |
| **quorum-operator** | Quorum | ConsenSys/quorum-kubernetes | Quorumç½‘ç»œ |
| **substrate-operator** | Substrate/Polkadot | paritytech/substrate-operator | Substrateé“¾ |
| **cosmos-operator** | Cosmos SDK | strangelove-ventures/cosmos-operator | Cosmosé“¾ |

**Fabric Operatoréƒ¨ç½²ç¤ºä¾‹** (å®Œæ•´æµç¨‹):

```bash
# 1. å®‰è£…Operator
helm install hlf-operator hlf-operator/hlf-operator \
  --namespace hlf-operator \
  --create-namespace

# 2. éƒ¨ç½²CA
kubectl apply -f - <<EOF
apiVersion: hlf.hyperledger.org/v1alpha1
kind: FabricCA
metadata:
  name: ca-org1
  namespace: fabric-org1
spec:
  hosts:
  - ca-org1
  subject:
    cn: ca
    country: US
    locality: San Francisco
    organization: Org1
    organizationalUnit: Org1
    stateOrProvince: California
  tlsCA:
    subject:
      cn: tlsca
      country: US
      locality: San Francisco
      organization: Org1
      organizationalUnit: Org1
      stateOrProvince: California
  ca:
    name: ca
    cfg:
      identities:
        allowremove: false
      affiliations:
        allowremove: false
  service:
    type: ClusterIP
  image: hyperledger/fabric-ca
  version: 1.5.12
  storage:
    size: 10Gi
    storageClass: standard
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
EOF

# 3. æ³¨å†Œç”¨æˆ·
kubectl hlf ca register --name=ca-org1 --namespace=fabric-org1 \
  --user=peer0 --secret=peer0pw --type=peer \
  --enroll-id=enroll --enroll-secret=enrollpw \
  --mspid=Org1MSP

# 4. éƒ¨ç½²Peer (ä½¿ç”¨å‰é¢çš„FabricPeer CRD)
kubectl apply -f fabricpeer-crd.yaml

# 5. åˆ›å»ºé€šé“
kubectl hlf channel generate --output=mychannel.block --name=mychannel \
  --organizations Org1MSP --ordererOrganizations OrdererMSP

kubectl hlf channel create --name=mychannel \
  --orderer-name=orderer --orderer-namespace=fabric-orderer

# 6. PeeråŠ å…¥é€šé“
kubectl hlf channel join --name=mychannel \
  --peer-name=org1-peer0 --peer-namespace=fabric-org1

# 7. å®‰è£…é“¾ç  (ä½¿ç”¨FabricChaincode CRD)
kubectl apply -f fabricchaincode-crd.yaml
```

### 5.2 Helm Chartsç”Ÿæ€

**ç¤¾åŒºHelm Charts**:

```yaml
# ç¤ºä¾‹: EthereumèŠ‚ç‚¹Helméƒ¨ç½²
# æ·»åŠ ä»“åº“
helm repo add ethereum-helm-charts https://skylenet.github.io/ethereum-helm-charts
helm repo update

# å®‰è£…Geth
helm install geth-mainnet ethereum-helm-charts/geth \
  --namespace ethereum \
  --create-namespace \
  -f - <<EOF
image:
  repository: ethereum/client-go
  tag: v1.14.8

syncMode: snap
gcMode: full
cache: 8192

resources:
  requests:
    memory: 16Gi
    cpu: 4
  limits:
    memory: 32Gi
    cpu: 8

persistence:
  enabled: true
  size: 2Ti
  storageClass: fast-ssd

service:
  type: LoadBalancer
  rpc:
    enabled: true
    port: 8545
  ws:
    enabled: true
    port: 8546

metrics:
  enabled: true
  serviceMonitor:
    enabled: true

ingress:
  enabled: true
  className: nginx
  hosts:
  - host: eth-rpc.example.com
    paths:
    - /
EOF
```

### 5.3 å¤šç§Ÿæˆ·BaaSå¹³å°

**æ¶æ„è®¾è®¡**:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ BaaSå¹³å° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ§åˆ¶å¹³é¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  - Web UI                                 â”‚   â”‚
â”‚  â”‚  - API Gateway                            â”‚   â”‚
â”‚  â”‚  - ç”¨æˆ·ç®¡ç†/è®¤è¯                           â”‚   â”‚
â”‚  â”‚  - è®¡è´¹ç³»ç»Ÿ                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                       â”‚                          â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚         â–¼             â–¼             â–¼           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Tenant A   â”‚ â”‚ Tenant B   â”‚ â”‚ Tenant C   â”‚  â”‚
â”‚  â”‚ Namespace  â”‚ â”‚ Namespace  â”‚ â”‚ Namespace  â”‚  â”‚
â”‚  â”‚            â”‚ â”‚            â”‚ â”‚            â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚ â”‚Fabric  â”‚ â”‚ â”‚ â”‚Ethereumâ”‚ â”‚ â”‚ â”‚Polygon â”‚ â”‚  â”‚
â”‚  â”‚ â”‚Network â”‚ â”‚ â”‚ â”‚Node    â”‚ â”‚ â”‚ â”‚CDK     â”‚ â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Namespaceéš”ç¦» + ResourceQuota**:

```yaml
# tenant-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-alice
  labels:
    tenant: alice
    plan: premium
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-alice-quota
  namespace: tenant-alice
spec:
  hard:
    requests.cpu: "20"
    requests.memory: "64Gi"
    requests.storage: "2Ti"
    persistentvolumeclaims: "10"
    pods: "50"
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tenant-alice-isolation
  namespace: tenant-alice
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # ä»…å…è®¸æ¥è‡ªIngress Controllerçš„æµé‡
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
  egress:
  # å…è®¸DNS
  - to:
    - namespaceSelector: {}
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  # å…è®¸å‡ºç«™åˆ°L1ç½‘ç»œ (å¤–éƒ¨)
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443
  # ç¦æ­¢è®¿é—®å…¶ä»–ç§Ÿæˆ·
```

**API Gateway (Kong)**:

```yaml
# kong-route.yaml
apiVersion: configuration.konghq.com/v1
kind: KongIngress
metadata:
  name: tenant-alice-rpc
  namespace: tenant-alice
route:
  methods:
  - POST
  strip_path: false
  preserve_host: true
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tenant-alice-ethereum-rpc
  namespace: tenant-alice
  annotations:
    konghq.com/plugins: rate-limiting-alice,key-auth-alice
    konghq.com/override: tenant-alice-rpc
spec:
  ingressClassName: kong
  rules:
  - host: alice.baas-platform.com
    http:
      paths:
      - path: /eth/rpc
        pathType: Prefix
        backend:
          service:
            name: geth-rpc
            port:
              number: 8545
---
# é€Ÿç‡é™åˆ¶æ’ä»¶
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: rate-limiting-alice
  namespace: tenant-alice
config:
  minute: 1000  # Premium plan: 1000 req/min
  policy: local
plugin: rate-limiting
---
# API Keyè®¤è¯æ’ä»¶
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: key-auth-alice
  namespace: tenant-alice
plugin: key-auth
```

**è®¡è´¹ç³»ç»Ÿé›†æˆ** (Prometheus + Custom Metrics):

```yaml
# servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: tenant-alice-billing
  namespace: tenant-alice
spec:
  selector:
    matchLabels:
      app: geth
  endpoints:
  - port: metrics
    interval: 30s
    path: /debug/metrics/prometheus
---
# PrometheusRule: è®¡è´¹å‘Šè­¦
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: tenant-alice-billing-alert
  namespace: tenant-alice
spec:
  groups:
  - name: billing
    interval: 5m
    rules:
    # RPCè¯·æ±‚è®¡æ•°
    - record: tenant:rpc_requests_total:rate5m
      expr: rate(rpc_requests_total{tenant="alice"}[5m])
    
    # è®¡ç®—æˆæœ¬ (æ¯1000è¯·æ±‚ = $0.01)
    - record: tenant:cost_per_hour:usd
      expr: |
        (
          sum(rate(rpc_requests_total{tenant="alice"}[1h])) * 3600 / 1000 * 0.01
        ) + (
          sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate{namespace="tenant-alice"}) * 0.05
        ) + (
          sum(container_memory_working_set_bytes{namespace="tenant-alice"}) / 1024 / 1024 / 1024 * 0.01
        )
    
    # è¶…é¢å‘Šè­¦
    - alert: BillingLimitExceeded
      expr: tenant:cost_per_hour:usd{tenant="alice"} > 10
      for: 1h
      annotations:
        summary: "Tenant alice exceeded hourly billing limit"
        description: "Current hourly cost: ${{ $value | humanize }}"
```

---

## 6. Web3åŸºç¡€è®¾æ–½å®¹å™¨åŒ–

### 6.1 IPFSå®¹å™¨åŒ–

**IPFS Clusteréƒ¨ç½²**:

```yaml
# ipfs-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ipfs
  namespace: web3
spec:
  serviceName: ipfs
  replicas: 3
  selector:
    matchLabels:
      app: ipfs
  template:
    metadata:
      labels:
        app: ipfs
    spec:
      initContainers:
      - name: init-ipfs
        image: ipfs/kubo:v0.30.0
        command:
        - sh
        - -c
        - |
          if [ ! -f /data/ipfs/config ]; then
            ipfs init --profile=server
          fi
        volumeMounts:
        - name: ipfs-data
          mountPath: /data/ipfs
      
      containers:
      - name: ipfs
        image: ipfs/kubo:v0.30.0
        ports:
        - containerPort: 4001
          name: swarm
        - containerPort: 5001
          name: api
        - containerPort: 8080
          name: gateway
        
        env:
        - name: IPFS_PATH
          value: /data/ipfs
        
        command:
        - ipfs
        - daemon
        - --migrate=true
        - --enable-gc
        - --routing=dhtclient
        
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        
        volumeMounts:
        - name: ipfs-data
          mountPath: /data/ipfs
        
        livenessProbe:
          httpGet:
            path: /api/v0/id
            port: 5001
          initialDelaySeconds: 60
          periodSeconds: 30
        
        readinessProbe:
          httpGet:
            path: /api/v0/id
            port: 5001
          initialDelaySeconds: 30
          periodSeconds: 10
  
  volumeClaimTemplates:
  - metadata:
      name: ipfs-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 500Gi
```

**IPFS Cluster (é›†ç¾¤æ¨¡å¼)**:

```yaml
# ipfs-cluster-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ipfs-cluster
  namespace: web3
spec:
  serviceName: ipfs-cluster
  replicas: 3
  selector:
    matchLabels:
      app: ipfs-cluster
  template:
    metadata:
      labels:
        app: ipfs-cluster
    spec:
      containers:
      - name: cluster
        image: ipfs/ipfs-cluster:v1.1.1
        ports:
        - containerPort: 9094
          name: api
        - containerPort: 9095
          name: proxy
        - containerPort: 9096
          name: cluster
        
        env:
        - name: CLUSTER_SECRET
          valueFrom:
            secretKeyRef:
              name: ipfs-cluster-secret
              key: secret
        - name: IPFS_API
          value: "/dns4/ipfs-0.ipfs.web3.svc.cluster.local/tcp/5001"
        
        volumeMounts:
        - name: cluster-data
          mountPath: /data/ipfs-cluster
        
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
  
  volumeClaimTemplates:
  - metadata:
      name: cluster-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi
```

### 6.2 The GraphèŠ‚ç‚¹

**The Graph Indexeréƒ¨ç½²**:

```yaml
# graph-node-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: graph-node
  namespace: web3
spec:
  serviceName: graph-node
  replicas: 2
  selector:
    matchLabels:
      app: graph-node
  template:
    metadata:
      labels:
        app: graph-node
    spec:
      containers:
      - name: graph-node
        image: graphprotocol/graph-node:v0.35.1
        ports:
        - containerPort: 8000
          name: http-rpc
        - containerPort: 8001
          name: ws-rpc
        - containerPort: 8020
          name: admin
        - containerPort: 8030
          name: index-status
        - containerPort: 8040
          name: metrics
        
        env:
        - name: postgres_host
          value: "postgres.web3.svc.cluster.local"
        - name: postgres_user
          value: "graph"
        - name: postgres_pass
          valueFrom:
            secretKeyRef:
              name: graph-db-secret
              key: password
        - name: postgres_db
          value: "graph"
        - name: ipfs
          value: "ipfs-0.ipfs.web3.svc.cluster.local:5001"
        - name: ethereum
          value: "mainnet:https://eth-mainnet.g.alchemy.com/v2/YOUR_KEY"
        - name: GRAPH_LOG
          value: "info"
        - name: GRAPH_ALLOW_NON_DETERMINISTIC_IPFS
          value: "true"
        - name: ETHEREUM_REORG_THRESHOLD
          value: "50"
        - name: ETHEREUM_ANCESTOR_COUNT
          value: "50"
        
        resources:
          requests:
            memory: "8Gi"
            cpu: "4000m"
          limits:
            memory: "16Gi"
            cpu: "8000m"
        
        livenessProbe:
          httpGet:
            path: /
            port: 8030
          initialDelaySeconds: 120
          periodSeconds: 30
        
        readinessProbe:
          httpGet:
            path: /
            port: 8030
          initialDelaySeconds: 60
          periodSeconds: 10

---
apiVersion: v1
kind: Service
metadata:
  name: graph-node
  namespace: web3
spec:
  selector:
    app: graph-node
  ports:
  - name: http-rpc
    port: 8000
  - name: ws-rpc
    port: 8001
  - name: admin
    port: 8020
  - name: index-status
    port: 8030
  - name: metrics
    port: 8040
  type: LoadBalancer
```

### 6.3 ChainlinkèŠ‚ç‚¹

**Chainlink Nodeéƒ¨ç½²**:

```yaml
# chainlink-node-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: chainlink-node
  namespace: web3
spec:
  replicas: 1  # å•å®ä¾‹ (å¯†é’¥ç®¡ç†)
  selector:
    matchLabels:
      app: chainlink-node
  template:
    metadata:
      labels:
        app: chainlink-node
    spec:
      securityContext:
        fsGroup: 14933
        runAsUser: 14933
        runAsNonRoot: true
      
      containers:
      - name: chainlink
        image: smartcontract/chainlink:2.16.0
        ports:
        - containerPort: 6688
          name: http
        
        env:
        - name: ROOT
          value: "/chainlink"
        - name: LOG_LEVEL
          value: "info"
        - name: ETH_CHAIN_ID
          value: "1"  # Mainnet
        - name: CHAINLINK_TLS_PORT
          value: "0"
        - name: SECURE_COOKIES
          value: "false"
        - name: ALLOW_ORIGINS
          value: "*"
        - name: ETH_URL
          value: "wss://eth-mainnet.g.alchemy.com/v2/YOUR_KEY"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: chainlink-db-secret
              key: url
        - name: LINK_CONTRACT_ADDRESS
          value: "0x514910771AF9Ca656af840dff83E8264EcF986CA"
        
        volumeMounts:
        - name: chainlink-data
          mountPath: /chainlink
        - name: chainlink-secrets
          mountPath: /secrets
          readOnly: true
        
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        
        livenessProbe:
          httpGet:
            path: /health
            port: 6688
          initialDelaySeconds: 60
          periodSeconds: 30
        
        readinessProbe:
          httpGet:
            path: /health
            port: 6688
          initialDelaySeconds: 30
          periodSeconds: 10
      
      volumes:
      - name: chainlink-data
        persistentVolumeClaim:
          claimName: chainlink-data-pvc
      - name: chainlink-secrets
        secret:
          secretName: chainlink-node-secret
```

### 6.4 åˆ†å¸ƒå¼RPCæœåŠ¡

**Nginxè´Ÿè½½å‡è¡¡ (å¤šGethèŠ‚ç‚¹)**:

```yaml
# nginx-rpc-lb-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-rpc-config
  namespace: ethereum-mainnet
data:
  nginx.conf: |
    events {
        worker_connections 10000;
    }
    
    http {
        upstream geth_rpc {
            least_conn;
            server geth-0.geth.ethereum-mainnet.svc.cluster.local:8545 max_fails=3 fail_timeout=30s;
            server geth-1.geth.ethereum-mainnet.svc.cluster.local:8545 max_fails=3 fail_timeout=30s;
            server geth-2.geth.ethereum-mainnet.svc.cluster.local:8545 max_fails=3 fail_timeout=30s;
        }
        
        upstream geth_ws {
            least_conn;
            server geth-0.geth.ethereum-mainnet.svc.cluster.local:8546 max_fails=3 fail_timeout=30s;
            server geth-1.geth.ethereum-mainnet.svc.cluster.local:8546 max_fails=3 fail_timeout=30s;
            server geth-2.geth.ethereum-mainnet.svc.cluster.local:8546 max_fails=3 fail_timeout=30s;
        }
        
        # é€Ÿç‡é™åˆ¶
        limit_req_zone $binary_remote_addr zone=rpc_limit:10m rate=100r/s;
        limit_req_zone $http_x_api_key zone=api_key_limit:10m rate=1000r/s;
        
        # å¥åº·æ£€æŸ¥
        server {
            listen 8090;
            location /health {
                access_log off;
                return 200 "healthy\n";
            }
        }
        
        # RPCä»£ç†
        server {
            listen 80;
            server_name _;
            
            # é€Ÿç‡é™åˆ¶
            limit_req zone=rpc_limit burst=200 nodelay;
            limit_req zone=api_key_limit burst=2000 nodelay;
            
            # JSON-RPC
            location / {
                proxy_pass http://geth_rpc;
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                
                # è¶…æ—¶è®¾ç½®
                proxy_connect_timeout 60s;
                proxy_send_timeout 120s;
                proxy_read_timeout 120s;
                
                # ç¼“å­˜è®¾ç½® (eth_chainId, net_versionç­‰é™æ€æ–¹æ³•)
                proxy_cache rpc_cache;
                proxy_cache_key "$request_body";
                proxy_cache_methods POST;
                proxy_cache_valid 200 5m;
                proxy_cache_bypass $http_cache_control;
            }
            
            # WebSocket
            location /ws {
                proxy_pass http://geth_ws;
                proxy_http_version 1.1;
                proxy_set_header Upgrade $http_upgrade;
                proxy_set_header Connection "upgrade";
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
                
                # WebSocketè¶…æ—¶
                proxy_read_timeout 3600s;
                proxy_send_timeout 3600s;
            }
        }
        
        proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=rpc_cache:100m max_size=1g inactive=10m use_temp_path=off;
    }

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-rpc-lb
  namespace: ethereum-mainnet
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-rpc-lb
  template:
    metadata:
      labels:
        app: nginx-rpc-lb
    spec:
      containers:
      - name: nginx
        image: nginx:1.27-alpine
        ports:
        - containerPort: 80
          name: http
        - containerPort: 8090
          name: health
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8090
          initialDelaySeconds: 10
          periodSeconds: 10
      volumes:
      - name: nginx-config
        configMap:
          name: nginx-rpc-config

---
apiVersion: v1
kind: Service
metadata:
  name: rpc-loadbalancer
  namespace: ethereum-mainnet
spec:
  selector:
    app: nginx-rpc-lb
  ports:
  - port: 80
    targetPort: 80
  type: LoadBalancer
```

---

## 7. é›¶çŸ¥è¯†è¯æ˜ (ZK) åŸºç¡€è®¾æ–½

### 7.1 zkEVMå®¹å™¨åŒ–

**Polygon zkEVM Proveré…ç½®**:

```yaml
# zkevm-prover-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zkevm-prover
  namespace: polygon-zkevm
spec:
  serviceName: zkevm-prover
  replicas: 4  # GPUèŠ‚ç‚¹æ•°é‡
  selector:
    matchLabels:
      app: zkevm-prover
  template:
    metadata:
      labels:
        app: zkevm-prover
    spec:
      nodeSelector:
        nvidia.com/gpu: "true"
        gpu-type: "a100"  # æŒ‡å®šA100 GPU
      
      containers:
      - name: prover
        image: hermeznetwork/zkevm-prover:v6.0.0
        ports:
        - containerPort: 50051
          name: executor
        - containerPort: 50052
          name: aggregator
        - containerPort: 50061
          name: metrics
        
        env:
        - name: PROVER_SERVER_PORT
          value: "50051"
        - name: PROVER_DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: zkevm-db-secret
              key: url
        - name: PROVER_CRS_PATH
          value: "/crs"
        - name: USE_GPU
          value: "true"
        - name: GPU_COUNT
          value: "1"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        
        resources:
          requests:
            memory: "64Gi"
            cpu: "16000m"
            nvidia.com/gpu: 1
          limits:
            memory: "128Gi"
            cpu: "32000m"
            nvidia.com/gpu: 1
        
        volumeMounts:
        - name: crs
          mountPath: /crs
          readOnly: true
        - name: prover-data
          mountPath: /data
        
        livenessProbe:
          tcpSocket:
            port: 50051
          initialDelaySeconds: 60
          periodSeconds: 30
      
      volumes:
      - name: crs
        persistentVolumeClaim:
          claimName: zkevm-crs-pvc
  
  volumeClaimTemplates:
  - metadata:
      name: prover-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 500Gi
```

### 7.2 è¯æ˜ç”Ÿæˆå™¨å®¹å™¨åŒ–

**é€šç”¨ZKè¯æ˜æœåŠ¡**:

```yaml
# zk-proof-service-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zk-proof-service
  namespace: zk-infra
spec:
  replicas: 2
  selector:
    matchLabels:
      app: zk-proof-service
  template:
    metadata:
      labels:
        app: zk-proof-service
    spec:
      nodeSelector:
        nvidia.com/gpu: "true"
      
      containers:
      - name: proof-generator
        image: myregistry/zk-proof-generator:latest
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 50051
          name: grpc
        
        env:
        - name: ZK_SCHEME
          value: "PLONK"  # PLONK, Groth16, STARK
        - name: GPU_ENABLED
          value: "true"
        - name: MAX_CONCURRENT_PROOFS
          value: "4"
        - name: REDIS_URL
          value: "redis://redis.zk-infra.svc.cluster.local:6379"
        
        resources:
          requests:
            memory: "32Gi"
            cpu: "8000m"
            nvidia.com/gpu: 1
          limits:
            memory: "64Gi"
            cpu: "16000m"
            nvidia.com/gpu: 1
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10

---
# HPA (åŸºäºè‡ªå®šä¹‰æŒ‡æ ‡)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: zk-proof-service-hpa
  namespace: zk-infra
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: zk-proof-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  # åŸºäºGPUåˆ©ç”¨ç‡
  - type: Pods
    pods:
      metric:
        name: DCGM_FI_DEV_GPU_UTIL
      target:
        type: AverageValue
        averageValue: "80"
  # åŸºäºé˜Ÿåˆ—é•¿åº¦
  - type: External
    external:
      metric:
        name: redis_queue_length
        selector:
          matchLabels:
            queue: "proof-requests"
      target:
        type: AverageValue
        averageValue: "50"
```

### 7.3 GPUåŠ é€Ÿè¯æ˜

**NVIDIA GPU Operatoré…ç½®**:

```bash
# å®‰è£…GPU Operator
helm install gpu-operator nvidia/gpu-operator \
  --namespace gpu-operator \
  --create-namespace \
  --set driver.enabled=true \
  --set toolkit.enabled=true \
  --set dcgm.enabled=true \
  --set dcgmExporter.enabled=true \
  --set gfd.enabled=true \
  --set migManager.enabled=true \
  --set operator.defaultRuntime=containerd

# éªŒè¯
kubectl get nodes -l nvidia.com/gpu=true
kubectl describe node <gpu-node>
```

**MIG (Multi-Instance GPU) é…ç½®**:

```yaml
# mig-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mig-parted-config
  namespace: gpu-operator
data:
  config.yaml: |
    version: v1
    mig-configs:
      # 7ä¸ª1g.10gbå®ä¾‹ (é€‚åˆå°å‹è¯æ˜ä»»åŠ¡)
      all-1g.10gb:
        - devices: all
          mig-enabled: true
          mig-devices:
            "1g.10gb": 7
      
      # æ··åˆé…ç½® (å¤§+å°ä»»åŠ¡)
      mixed:
        - devices: all
          mig-enabled: true
          mig-devices:
            "3g.40gb": 2
            "1g.10gb": 1
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-operator-config
  namespace: gpu-operator
data:
  mig.strategy: "mixed"
  mig.config: "all-1g.10gb"
```

**èµ„æºè¯·æ±‚ (MIGå®ä¾‹)**:

```yaml
resources:
  requests:
    nvidia.com/mig-1g.10gb: 1  # 1ä¸ªMIGå®ä¾‹
  limits:
    nvidia.com/mig-1g.10gb: 1
```

---

## 8. å­˜å‚¨å’ŒæŒä¹…åŒ–

### 8.1 åŒºå—é“¾æ•°æ®å­˜å‚¨

**å­˜å‚¨éœ€æ±‚å¯¹æ¯”**:

| åŒºå—é“¾ | æ¨¡å¼ | å­˜å‚¨å¤§å° | IOPSéœ€æ±‚ | æ¨èå­˜å‚¨ |
|--------|------|----------|----------|----------|
| Ethereum (Mainnet) | Full Node | ~1TB | é«˜ (>10K) | NVMe SSD |
| Ethereum (Mainnet) | Archive Node | ~14TB | æé«˜ (>50K) | NVMe SSD RAID0 |
| Ethereum (Testnet) | Full Node | ~300GB | ä¸­ (~5K) | SSD |
| Bitcoin | Full Node | ~600GB | ä¸­ (~5K) | SSD |
| Hyperledger Fabric | Peer+CouchDB | 500GB-2TB | ä¸­-é«˜ | SSD |
| Polygon zkEVM | Sequencer | 1-2TB | é«˜ (>10K) | NVMe SSD |
| IPFS | Gateway | 500GB-5TB | ä¸­ (~5K) | HDD/SSDæ··åˆ |

**StorageClassé…ç½®**:

```yaml
# fast-ssd-storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs  # æˆ– pd.csi.storage.gke.io (GCP)
parameters:
  type: gp3  # AWS: gp3, io2; GCP: pd-ssd, pd-extreme; Azure: Premium_LRS
  iopsPerGB: "50"
  throughput: "1000"
  fsType: ext4
  encrypted: "true"
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Retain  # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨Retain

---
# nvme-ssd-storageclass.yaml (é«˜æ€§èƒ½)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nvme-ssd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io2  # AWS Provisioned IOPS SSD
  iopsPerGB: "500"  # æœ€é«˜64000 IOPS
  throughput: "4000"  # 4000 MiB/s
  fsType: ext4
  encrypted: "true"
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Retain
```

**Local PV (æœ¬åœ°NVMe)**:

```yaml
# local-storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-nvme
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Retain

---
# local-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-node1-nvme0
spec:
  capacity:
    storage: 3800Gi  # 4TB NVMe
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-nvme
  local:
    path: /mnt/nvme0n1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1
```

### 8.2 å¿«ç…§å’Œå¤‡ä»½

**VolumeSnapshot (CSI)**:

```yaml
# volumesnapshot-class.yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: ebs-snapshot-class
driver: ebs.csi.aws.com
deletionPolicy: Retain
parameters:
  tagSpecification_1: "Name=blockchain-snapshot"
  tagSpecification_2: "Environment=production"

---
# geth-snapshot.yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: geth-snapshot-20251020
  namespace: ethereum-mainnet
spec:
  volumeSnapshotClassName: ebs-snapshot-class
  source:
    persistentVolumeClaimName: geth-data-geth-0
```

**CronJobè‡ªåŠ¨å¤‡ä»½**:

```yaml
# geth-backup-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: geth-backup
  namespace: ethereum-mainnet
spec:
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨2ç‚¹
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 7
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: snapshot-creator
          containers:
          - name: backup
            image: bitnami/kubectl:1.31
            command:
            - sh
            - -c
            - |
              # åˆ›å»ºå¿«ç…§
              SNAPSHOT_NAME="geth-snapshot-$(date +%Y%m%d-%H%M%S)"
              cat <<EOF | kubectl apply -f -
              apiVersion: snapshot.storage.k8s.io/v1
              kind: VolumeSnapshot
              metadata:
                name: ${SNAPSHOT_NAME}
                namespace: ethereum-mainnet
              spec:
                volumeSnapshotClassName: ebs-snapshot-class
                source:
                  persistentVolumeClaimName: geth-data-geth-0
              EOF
              
              # ç­‰å¾…å¿«ç…§å®Œæˆ
              kubectl wait --for=jsonpath='{.status.readyToUse}'=true \
                volumesnapshot/${SNAPSHOT_NAME} \
                --namespace=ethereum-mainnet \
                --timeout=3600s
              
              # æ¸…ç†æ—§å¿«ç…§ (ä¿ç•™7å¤©)
              kubectl get volumesnapshot -n ethereum-mainnet \
                --sort-by=.metadata.creationTimestamp \
                -o name | head -n -7 | xargs -r kubectl delete -n ethereum-mainnet
          
          restartPolicy: OnFailure
```

### 8.3 å­˜å‚¨ä¼˜åŒ–

**Gethå­˜å‚¨ä¼˜åŒ–**:

```yaml
# å¯ç”¨Ancientæ•°æ®åˆ†ç¦»
command:
- geth
- --datadir=/data/geth
- --datadir.ancient=/data/ancient  # åˆ†ç¦»å¤è€æ•°æ® (>90å¤©)
- --cache=8192
- --snapshot
- --txlookuplimit=0  # ç¦ç”¨äº¤æ˜“ç´¢å¼• (èŠ‚çœ~400GB)

# å®šæœŸè£å‰ª
- --state.scheme=path  # ä½¿ç”¨Path-based storage (2.5xæ›´å¿«)

# å¤šå·æŒ‚è½½
volumeMounts:
- name: geth-data
  mountPath: /data/geth
- name: geth-ancient
  mountPath: /data/ancient  # å¯ä½¿ç”¨æ›´ä¾¿å®œçš„HDD
```

**å®šæœŸè£å‰ª (Pruning)**:

```yaml
# geth-prune-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: geth-prune-$(date +%s)
  namespace: ethereum-mainnet
spec:
  template:
    spec:
      containers:
      - name: prune
        image: ethereum/client-go:v1.14.8
        command:
        - sh
        - -c
        - |
          # åœæ­¢Geth
          kubectl scale statefulset geth --replicas=0 -n ethereum-mainnet
          
          # ç­‰å¾…Podç»ˆæ­¢
          sleep 60
          
          # æ‰§è¡Œè£å‰ª
          geth snapshot prune-state --datadir=/data/geth
          
          # é‡å¯Geth
          kubectl scale statefulset geth --replicas=1 -n ethereum-mainnet
        
        volumeMounts:
        - name: geth-data
          mountPath: /data/geth
      
      restartPolicy: Never
      volumes:
      - name: geth-data
        persistentVolumeClaim:
          claimName: geth-data-geth-0
```

---

## 9. ç½‘ç»œå’Œé€šä¿¡

### 9.1 P2Pç½‘ç»œé…ç½®

**NodePortæš´éœ²P2Pç«¯å£**:

```yaml
# geth-p2p-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: geth-p2p
  namespace: ethereum-mainnet
spec:
  type: NodePort
  selector:
    app: geth
  ports:
  - name: p2p-tcp
    port: 30303
    targetPort: 30303
    nodePort: 30303  # å›ºå®šNodePort
    protocol: TCP
  - name: p2p-udp
    port: 30303
    targetPort: 30303
    nodePort: 30303
    protocol: UDP
  externalTrafficPolicy: Local  # ä¿ç•™æºIP

---
# æˆ–ä½¿ç”¨LoadBalancer (å¸¦é™æ€IP)
apiVersion: v1
kind: Service
metadata:
  name: geth-p2p-lb
  namespace: ethereum-mainnet
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"  # AWS NLB
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "ip"
    service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
    service.beta.kubernetes.io/aws-load-balancer-eip-allocations: "eipalloc-xxxxx"  # é™æ€EIP
spec:
  type: LoadBalancer
  selector:
    app: geth
  ports:
  - name: p2p-tcp
    port: 30303
    targetPort: 30303
    protocol: TCP
  - name: p2p-udp
    port: 30303
    targetPort: 30303
    protocol: UDP
  externalTrafficPolicy: Local
```

**NATé…ç½® (Geth)**:

```yaml
command:
- geth
- --nat=extip:<STATIC_PUBLIC_IP>  # æˆ– --nat=any
- --netrestrict=10.0.0.0/8  # é™åˆ¶P2Påˆ°VPCå†…
```

### 9.2 RPC/WebSocketæœåŠ¡

**Ingress (HTTPS + è®¤è¯)**:

```yaml
# geth-rpc-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: geth-rpc
  namespace: ethereum-mainnet
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: rpc-basic-auth
    nginx.ingress.kubernetes.io/auth-realm: "Ethereum RPC"
    nginx.ingress.kubernetes.io/rate-limit: "100"  # 100 req/s per IP
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: "100m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "120"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "120"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - eth-rpc.example.com
    secretName: eth-rpc-tls
  rules:
  - host: eth-rpc.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: rpc-loadbalancer
            port:
              number: 80
      - path: /ws
        pathType: Prefix
        backend:
          service:
            name: geth
            port:
              number: 8546

---
# Basic Auth Secret
apiVersion: v1
kind: Secret
metadata:
  name: rpc-basic-auth
  namespace: ethereum-mainnet
type: Opaque
data:
  auth: dXNlcjokYXByMSR... # htpasswdç”Ÿæˆ
```

### 9.3 è´Ÿè½½å‡è¡¡

**Envoy (L7è´Ÿè½½å‡è¡¡ + gRPC)**:

```yaml
# envoy-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: envoy-config
  namespace: ethereum-mainnet
data:
  envoy.yaml: |
    admin:
      address:
        socket_address:
          address: 0.0.0.0
          port_value: 9901
    
    static_resources:
      listeners:
      - name: rpc_listener
        address:
          socket_address:
            address: 0.0.0.0
            port_value: 8545
        filter_chains:
        - filters:
          - name: envoy.filters.network.http_connection_manager
            typed_config:
              "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
              stat_prefix: ingress_http
              codec_type: AUTO
              route_config:
                name: local_route
                virtual_hosts:
                - name: backend
                  domains: ["*"]
                  routes:
                  - match:
                      prefix: "/"
                    route:
                      cluster: geth_cluster
                      timeout: 120s
                      retry_policy:
                        retry_on: "5xx"
                        num_retries: 3
              http_filters:
              - name: envoy.filters.http.router
                typed_config:
                  "@type": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router
      
      clusters:
      - name: geth_cluster
        connect_timeout: 5s
        type: STRICT_DNS
        lb_policy: LEAST_REQUEST
        health_checks:
        - timeout: 5s
          interval: 10s
          unhealthy_threshold: 3
          healthy_threshold: 2
          http_health_check:
            path: "/"
            request_headers_to_add:
            - header:
                key: "Content-Type"
                value: "application/json"
        load_assignment:
          cluster_name: geth_cluster
          endpoints:
          - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: geth-0.geth.ethereum-mainnet.svc.cluster.local
                    port_value: 8545
            - endpoint:
                address:
                  socket_address:
                    address: geth-1.geth.ethereum-mainnet.svc.cluster.local
                    port_value: 8545
            - endpoint:
                address:
                  socket_address:
                    address: geth-2.geth.ethereum-mainnet.svc.cluster.local
                    port_value: 8545
```

---

## 10. å®‰å…¨å’Œéš”ç¦»

### 10.1 ç§é’¥ç®¡ç†

**HashiCorp Vaulté›†æˆ**:

```yaml
# vault-injector-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: geth-with-vault
  namespace: ethereum-mainnet
spec:
  replicas: 1
  selector:
    matchLabels:
      app: geth-with-vault
  template:
    metadata:
      labels:
        app: geth-with-vault
      annotations:
        vault.hashicorp.com/agent-inject: "true"
        vault.hashicorp.com/role: "geth"
        vault.hashicorp.com/agent-inject-secret-account-key: "secret/ethereum/account"
        vault.hashicorp.com/agent-inject-template-account-key: |
          {{- with secret "secret/ethereum/account" -}}
          {{ .Data.data.private_key }}
          {{- end -}}
    spec:
      serviceAccountName: geth
      containers:
      - name: geth
        image: ethereum/client-go:v1.14.8
        command:
        - sh
        - -c
        - |
          # ä»Vaultæ³¨å…¥çš„å¯†é’¥å¯¼å…¥è´¦æˆ·
          geth account import --datadir=/data /vault/secrets/account-key
          
          # å¯åŠ¨Geth
          geth --datadir=/data ...
        volumeMounts:
        - name: geth-data
          mountPath: /data
```

**Kubernetes Secrets (åŠ å¯†)**:

```bash
# å¯ç”¨etcdåŠ å¯†
cat <<EOF > /etc/kubernetes/encryption-config.yaml
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: $(head -c 32 /dev/urandom | base64)
    - identity: {}
EOF

# æ›´æ–°apiserveré…ç½®
--encryption-provider-config=/etc/kubernetes/encryption-config.yaml
```

**External Secrets Operator**:

```yaml
# external-secret.yaml
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: vault-backend
  namespace: ethereum-mainnet
spec:
  provider:
    vault:
      server: "https://vault.example.com"
      path: "secret"
      version: "v2"
      auth:
        kubernetes:
          mountPath: "kubernetes"
          role: "geth"
---
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: geth-account
  namespace: ethereum-mainnet
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: vault-backend
    kind: SecretStore
  target:
    name: geth-account-secret
    creationPolicy: Owner
  data:
  - secretKey: private_key
    remoteRef:
      key: ethereum/account
      property: private_key
```

### 10.2 ç½‘ç»œéš”ç¦»

**NetworkPolicy (ä¸¥æ ¼éš”ç¦»)**:

```yaml
# blockchain-network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: geth-network-policy
  namespace: ethereum-mainnet
spec:
  podSelector:
    matchLabels:
      app: geth
  policyTypes:
  - Ingress
  - Egress
  
  ingress:
  # ä»…å…è®¸RPC LBè®¿é—®8545
  - from:
    - podSelector:
        matchLabels:
          app: nginx-rpc-lb
    ports:
    - protocol: TCP
      port: 8545
  
  # ä»…å…è®¸Prometheusè®¿é—®metrics
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
      podSelector:
        matchLabels:
          app: prometheus
    ports:
    - protocol: TCP
      port: 6060
  
  # å…è®¸P2P (å…¶ä»–GethèŠ‚ç‚¹ + å¤–éƒ¨)
  - from:
    - podSelector:
        matchLabels:
          app: geth
    - namespaceSelector: {}  # å¤–éƒ¨P2Pæµé‡
    ports:
    - protocol: TCP
      port: 30303
    - protocol: UDP
      port: 30303
  
  egress:
  # å…è®¸DNS
  - to:
    - namespaceSelector: {}
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
  
  # å…è®¸P2På‡ºç«™
  - to:
    - podSelector:
        matchLabels:
          app: geth
    ports:
    - protocol: TCP
      port: 30303
    - protocol: UDP
      port: 30303
  
  # å…è®¸å¤–éƒ¨P2Pè¿æ¥
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 30303
    - protocol: UDP
      port: 30303
  
  # å…è®¸è®¿é—®L1 RPC (Layer 2èŠ‚ç‚¹)
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443
```

**Cilium NetworkPolicy (L7 + DNS)**:

```yaml
# cilium-network-policy.yaml
apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: geth-l7-policy
  namespace: ethereum-mainnet
spec:
  endpointSelector:
    matchLabels:
      app: geth
  
  egress:
  # å…è®¸è®¿é—®ç‰¹å®šå¤–éƒ¨RPC (å¦‚Alchemy, Infura)
  - toFQDNs:
    - matchName: "eth-mainnet.g.alchemy.com"
    - matchPattern: "*.infura.io"
    toPorts:
    - ports:
      - port: "443"
        protocol: TCP
  
  # å…è®¸P2P
  - toEntities:
    - world
    toPorts:
    - ports:
      - port: "30303"
        protocol: TCP
      - port: "30303"
        protocol: UDP
```

### 10.3 TEEå’Œæœºå¯†è®¡ç®—

**Intel SGX (Confidential Containers)**:

```yaml
# sgx-enabled-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: confidential-validator
  namespace: ethereum-mainnet
spec:
  replicas: 1
  selector:
    matchLabels:
      app: confidential-validator
  template:
    metadata:
      labels:
        app: confidential-validator
    spec:
      runtimeClassName: kata-cc  # Confidential Containers runtime
      nodeSelector:
        intel.feature.node.kubernetes.io/sgx: "true"
      
      containers:
      - name: validator
        image: myregistry/confidential-validator:latest
        resources:
          limits:
            sgx.intel.com/epc: "512Mi"  # SGX EPCå†…å­˜
        
        env:
        - name: SGX_MODE
          value: "HW"  # ç¡¬ä»¶æ¨¡å¼
        - name: PRIVATE_KEY_SEALED
          value: "true"  # SGXå¯†å°ç§é’¥
        
        volumeMounts:
        - name: sgx-enclave
          mountPath: /dev/sgx
      
      volumes:
      - name: sgx-enclave
        hostPath:
          path: /dev/sgx
          type: CharDevice
```

**AMD SEV-SNP**:

```yaml
# sev-enabled-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sev-validator
  namespace: ethereum-mainnet
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sev-validator
  template:
    metadata:
      labels:
        app: sev-validator
    spec:
      runtimeClassName: kata-cc-sev
      nodeSelector:
        amd.feature.node.kubernetes.io/sev: "true"
      
      containers:
      - name: validator
        image: myregistry/validator:latest
        securityContext:
          privileged: false
          runAsNonRoot: true
          capabilities:
            drop:
            - ALL
```

---

## 11. ç›‘æ§å’Œå¯è§‚æµ‹æ€§

### 11.1 PrometheusæŒ‡æ ‡

**Geth Metrics**:

```yaml
# servicemonitor-geth.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: geth
  namespace: ethereum-mainnet
spec:
  selector:
    matchLabels:
      app: geth
  endpoints:
  - port: metrics
    interval: 30s
    path: /debug/metrics/prometheus
```

**å…³é”®æŒ‡æ ‡**:

```yaml
# prometheusrule-geth.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: geth-alerts
  namespace: ethereum-mainnet
spec:
  groups:
  - name: geth
    interval: 30s
    rules:
    # åŒæ­¥çŠ¶æ€
    - alert: GethNotSynced
      expr: eth_sync_known_highest_block - eth_sync_current_block > 100
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "Geth node {{ $labels.instance }} is not fully synced"
        description: "Block difference: {{ $value }}"
    
    # Peeræ•°é‡
    - alert: GethLowPeerCount
      expr: p2p_peers < 10
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Geth node {{ $labels.instance }} has low peer count"
        description: "Current peers: {{ $value }}"
    
    # ç£ç›˜ç©ºé—´
    - alert: GethDiskSpaceLow
      expr: |
        (
          node_filesystem_avail_bytes{mountpoint="/data"} 
          / node_filesystem_size_bytes{mountpoint="/data"}
        ) < 0.1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Geth node {{ $labels.instance }} disk space low"
        description: "Free space: {{ $value | humanizePercentage }}"
    
    # åŒºå—æ—¶é—´å»¶è¿Ÿ
    - alert: GethBlockTimeLag
      expr: time() - eth_latest_block_timestamp > 300
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Geth node {{ $labels.instance }} block time lag"
        description: "Lag: {{ $value }}s"
```

### 11.2 é“¾ä¸Šç›‘æ§

**è‡ªå®šä¹‰Exporter**:

```python
# blockchain-exporter.py
from prometheus_client import start_http_server, Gauge
from web3 import Web3
import time

# å®šä¹‰æŒ‡æ ‡
latest_block = Gauge('ethereum_latest_block', 'Latest block number')
gas_price = Gauge('ethereum_gas_price_gwei', 'Current gas price in Gwei')
peer_count = Gauge('ethereum_peer_count', 'Number of connected peers')
syncing = Gauge('ethereum_syncing', 'Is node syncing (1=yes, 0=no)')

def collect_metrics(w3):
    # æœ€æ–°åŒºå—
    latest_block.set(w3.eth.block_number)
    
    # Gasä»·æ ¼
    gas_price.set(w3.eth.gas_price / 1e9)  # è½¬æ¢ä¸ºGwei
    
    # Peeræ•°é‡
    peer_count.set(w3.net.peer_count)
    
    # åŒæ­¥çŠ¶æ€
    syncing_status = w3.eth.syncing
    syncing.set(1 if syncing_status else 0)

if __name__ == '__main__':
    w3 = Web3(Web3.HTTPProvider('http://geth:8545'))
    start_http_server(8000)
    
    while True:
        collect_metrics(w3)
        time.sleep(15)
```

**Deployment**:

```yaml
# blockchain-exporter-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blockchain-exporter
  namespace: ethereum-mainnet
spec:
  replicas: 1
  selector:
    matchLabels:
      app: blockchain-exporter
  template:
    metadata:
      labels:
        app: blockchain-exporter
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
    spec:
      containers:
      - name: exporter
        image: myregistry/blockchain-exporter:latest
        ports:
        - containerPort: 8000
          name: metrics
        env:
        - name: RPC_URL
          value: "http://geth.ethereum-mainnet.svc.cluster.local:8545"
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
```

### 11.3 å‘Šè­¦ç­–ç•¥

**PagerDutyé›†æˆ**:

```yaml
# alertmanager-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-config
  namespace: monitoring
type: Opaque
stringData:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
    
    route:
      receiver: 'default'
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      routes:
      # åŒºå—é“¾å…³é”®å‘Šè­¦
      - match:
          severity: critical
          namespace: ethereum-mainnet
        receiver: 'pagerduty-critical'
        continue: true
      
      # åŒºå—é“¾è­¦å‘Š
      - match:
          severity: warning
          namespace: ethereum-mainnet
        receiver: 'slack-blockchain'
    
    receivers:
    - name: 'default'
      slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        channel: '#alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    
    - name: 'pagerduty-critical'
      pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}'
    
    - name: 'slack-blockchain'
      slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        channel: '#blockchain-ops'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
```

---

## 12. é«˜å¯ç”¨æ€§å’Œç¾éš¾æ¢å¤

### 12.1 å¤šåŒºåŸŸéƒ¨ç½²

**è·¨åŒºåŸŸStatefulSet**:

```yaml
# multi-zone-geth.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: geth
  namespace: ethereum-mainnet
spec:
  serviceName: geth
  replicas: 3
  selector:
    matchLabels:
      app: geth
  template:
    metadata:
      labels:
        app: geth
    spec:
      # Podåäº²å’Œæ€§: ä¸åŒå¯ç”¨åŒº
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: geth
            topologyKey: topology.kubernetes.io/zone
      
      # ä¼˜å…ˆè°ƒåº¦åˆ°ä¸åŒåŒºåŸŸ
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: geth
      
      containers:
      - name: geth
        image: ethereum/client-go:v1.14.8
        ...
  
  volumeClaimTemplates:
  - metadata:
      name: geth-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd-zone-aware  # åŒºåŸŸæ„ŸçŸ¥å­˜å‚¨
      resources:
        requests:
          storage: 2Ti
```

### 12.2 æ•…éšœè½¬ç§»

**PDB (Pod Disruption Budget)**:

```yaml
# pdb-geth.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: geth-pdb
  namespace: ethereum-mainnet
spec:
  minAvailable: 2  # è‡³å°‘ä¿æŒ2ä¸ªPodè¿è¡Œ
  selector:
    matchLabels:
      app: geth
```

**è‡ªåŠ¨æ•…éšœåˆ‡æ¢**:

```yaml
# geth-with-readiness.yaml
readinessProbe:
  exec:
    command:
    - sh
    - -c
    - |
      # æ£€æŸ¥åŒæ­¥çŠ¶æ€
      SYNCING=$(geth attach --exec 'eth.syncing' http://localhost:8545)
      if [ "$SYNCING" = "false" ]; then
        exit 0
      else
        exit 1
      fi
  initialDelaySeconds: 120
  periodSeconds: 30
  failureThreshold: 3
```

### 12.3 ç¾éš¾æ¢å¤

**è·¨åŒºåŸŸå¤‡ä»½**:

```yaml
# cross-region-backup-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: geth-cross-region-backup
  namespace: ethereum-mainnet
spec:
  schedule: "0 0 * * 0"  # æ¯å‘¨æ—¥
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: backup-sa
          containers:
          - name: backup
            image: amazon/aws-cli:latest
            command:
            - sh
            - -c
            - |
              # åˆ›å»ºå¿«ç…§
              SNAPSHOT_ID=$(aws ec2 create-snapshot \
                --volume-id vol-xxxxx \
                --description "Geth backup $(date +%Y%m%d)" \
                --tag-specifications 'ResourceType=snapshot,Tags=[{Key=Type,Value=Blockchain}]' \
                --query 'SnapshotId' \
                --output text)
              
              # å¤åˆ¶åˆ°ç¾å¤‡åŒºåŸŸ
              aws ec2 copy-snapshot \
                --source-region us-east-1 \
                --source-snapshot-id $SNAPSHOT_ID \
                --destination-region us-west-2 \
                --description "DR backup $(date +%Y%m%d)"
              
              # æ¸…ç†æ—§å¿«ç…§ (ä¿ç•™4å‘¨)
              aws ec2 describe-snapshots \
                --owner-ids self \
                --filters "Name=tag:Type,Values=Blockchain" \
                --query 'Snapshots[?StartTime<=`'$(date -d '4 weeks ago' --iso-8601)'`].SnapshotId' \
                --output text | xargs -n 1 aws ec2 delete-snapshot --snapshot-id
          
          restartPolicy: OnFailure
```

---

## 13. æ€§èƒ½ä¼˜åŒ–

### 13.1 èµ„æºé…ç½®

**ç”Ÿäº§çº§é…ç½®ç¤ºä¾‹**:

```yaml
# Ethereum Full Node (RPCæœåŠ¡)
resources:
  requests:
    memory: "16Gi"
    cpu: "4000m"
  limits:
    memory: "32Gi"
    cpu: "8000m"

command:
- geth
- --cache=8192  # 8GBå†…å­˜ç¼“å­˜
- --maxpeers=50
- --syncmode=snap

---
# Hyperledger Fabric Peer (ä¼ä¸šçº§)
resources:
  requests:
    memory: "8Gi"
    cpu: "4000m"
  limits:
    memory: "16Gi"
    cpu: "8000m"

---
# zkEVM Prover (GPU)
resources:
  requests:
    memory: "64Gi"
    cpu: "16000m"
    nvidia.com/gpu: 1  # A100 80GB
  limits:
    memory: "128Gi"
    cpu: "32000m"
    nvidia.com/gpu: 1
```

### 13.2 ç½‘ç»œä¼˜åŒ–

**CNIä¼˜åŒ– (Cilium)**:

```yaml
# cilium-values.yaml
kubeProxyReplacement: "strict"  # å®Œå…¨æ›¿ä»£kube-proxy (eBPF)
enableIPv4Masquerade: true
enableBPFMasquerade: true

# eBPFä¸»æœºè·¯ç”±
enableHostReachableServices: true
hostServices:
  enabled: true
  protocols: tcp,udp

# MTUä¼˜åŒ–
mtu: 9000  # Jumbo frames (å¦‚æœç½‘ç»œæ”¯æŒ)

# å¸¦å®½ç®¡ç†å™¨
bandwidthManager: true
```

### 13.3 å­˜å‚¨I/Oä¼˜åŒ–

**NVMeä¼˜åŒ–**:

```bash
# è°ƒæ•´æ–‡ä»¶ç³»ç»Ÿå‚æ•°
mkfs.ext4 -E lazy_itable_init=0,lazy_journal_init=0,stride=128,stripe_width=128 /dev/nvme0n1

# æŒ‚è½½é€‰é¡¹
mount -o noatime,nodiratime,discard,data=writeback /dev/nvme0n1 /mnt/blockchain

# Kubernetes StorageClass
parameters:
  type: io2
  iopsPerGB: "500"
  throughput: "4000"
  fsType: ext4
  csi.storage.k8s.io/fstype: ext4
  blockExpress: "true"  # å¯ç”¨EBS Block Express
```

**I/Oè°ƒåº¦å™¨ä¼˜åŒ–**:

```yaml
# node-tuning-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-tuning
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: node-tuning
  template:
    metadata:
      labels:
        app: node-tuning
    spec:
      hostPID: true
      hostNetwork: true
      nodeSelector:
        blockchain-node: "true"
      containers:
      - name: tuning
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          # è®¾ç½®I/Oè°ƒåº¦å™¨ä¸ºnone (NVMe)
          echo none > /sys/block/nvme0n1/queue/scheduler
          
          # å¢åŠ I/Oé˜Ÿåˆ—æ·±åº¦
          echo 1024 > /sys/block/nvme0n1/queue/nr_requests
          
          # ç¦ç”¨è¯»å‰é¢„è¯» (å‡å°‘éšæœºI/Oå»¶è¿Ÿ)
          echo 0 > /sys/block/nvme0n1/queue/read_ahead_kb
          
          # æŒç»­è¿è¡Œ
          sleep infinity
        securityContext:
          privileged: true
```

---

## 14. è·¨é“¾æŠ€æœ¯å®¹å™¨åŒ–

### 14.1 Cosmos IBC Relayer

**Hermes Relayeréƒ¨ç½²**:

```yaml
# hermes-relayer-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hermes-relayer
  namespace: cosmos
spec:
  replicas: 2
  selector:
    matchLabels:
      app: hermes-relayer
  template:
    metadata:
      labels:
        app: hermes-relayer
    spec:
      containers:
      - name: hermes
        image: informalsystems/hermes:v1.9.0
        command:
        - hermes
        - start
        env:
        - name: RUST_LOG
          value: "info"
        volumeMounts:
        - name: config
          mountPath: /root/.hermes
          readOnly: true
        - name: keys
          mountPath: /keys
          readOnly: true
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
      volumes:
      - name: config
        configMap:
          name: hermes-config
      - name: keys
        secret:
          secretName: hermes-keys
```

**é…ç½®**:

```toml
# hermes-config.toml
[global]
log_level = 'info'

[mode.clients]
enabled = true
refresh = true
misbehaviour = true

[mode.connections]
enabled = true

[mode.channels]
enabled = true

[mode.packets]
enabled = true
clear_interval = 100
clear_on_start = true

[[chains]]
id = 'cosmoshub-4'
rpc_addr = 'http://cosmos-rpc:26657'
grpc_addr = 'http://cosmos-grpc:9090'
websocket_addr = 'ws://cosmos-rpc:26657/websocket'
rpc_timeout = '10s'
account_prefix = 'cosmos'
key_name = 'relayer'
store_prefix = 'ibc'
gas_price = { price = 0.025, denom = 'uatom' }

[[chains]]
id = 'osmosis-1'
rpc_addr = 'http://osmosis-rpc:26657'
grpc_addr = 'http://osmosis-grpc:9090'
websocket_addr = 'ws://osmosis-rpc:26657/websocket'
rpc_timeout = '10s'
account_prefix = 'osmo'
key_name = 'relayer'
store_prefix = 'ibc'
gas_price = { price = 0.025, denom = 'uosmo' }
```

### 14.2 Polkadot Parachain

**SubstrateèŠ‚ç‚¹å®¹å™¨åŒ–**:

```yaml
# parachain-node-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: parachain-collator
  namespace: polkadot
spec:
  serviceName: parachain-collator
  replicas: 2
  selector:
    matchLabels:
      app: parachain-collator
  template:
    metadata:
      labels:
        app: parachain-collator
    spec:
      containers:
      - name: collator
        image: myregistry/parachain-collator:latest
        ports:
        - containerPort: 30333
          name: p2p
        - containerPort: 9933
          name: rpc
        - containerPort: 9944
          name: ws
        command:
        - ./parachain-collator
        - --collator
        - --base-path=/data
        - --chain=parachain-spec.json
        - --port=30333
        - --rpc-port=9933
        - --ws-port=9944
        - --rpc-cors=all
        - --unsafe-rpc-external
        - --unsafe-ws-external
        - --
        - --chain=polkadot
        - --port=30334
        - --rpc-port=9934
        resources:
          requests:
            memory: "8Gi"
            cpu: "4000m"
          limits:
            memory: "16Gi"
            cpu: "8000m"
        volumeMounts:
        - name: parachain-data
          mountPath: /data
        - name: chain-spec
          mountPath: /chain-spec.json
          subPath: chain-spec.json
      volumes:
      - name: chain-spec
        configMap:
          name: parachain-chain-spec
  volumeClaimTemplates:
  - metadata:
      name: parachain-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 500Gi
```

### 14.3 è·¨é“¾æ¡¥å®¹å™¨åŒ–

**é€šç”¨è·¨é“¾æ¡¥æ¶æ„**:

```yaml
# crosschain-bridge-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: crosschain-bridge
  namespace: bridge
spec:
  replicas: 3
  selector:
    matchLabels:
      app: crosschain-bridge
  template:
    metadata:
      labels:
        app: crosschain-bridge
    spec:
      containers:
      # Source Chainç›‘å¬å™¨
      - name: source-listener
        image: myregistry/bridge-listener:latest
        env:
        - name: CHAIN_TYPE
          value: "ethereum"
        - name: RPC_URL
          value: "http://geth.ethereum-mainnet.svc.cluster.local:8545"
        - name: BRIDGE_CONTRACT
          value: "0x..."
        - name: REDIS_URL
          value: "redis://redis.bridge.svc.cluster.local:6379"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
      
      # Target Chainæ‰§è¡Œå™¨
      - name: target-executor
        image: myregistry/bridge-executor:latest
        env:
        - name: CHAIN_TYPE
          value: "binance-smart-chain"
        - name: RPC_URL
          value: "http://bsc-rpc.bsc.svc.cluster.local:8545"
        - name: BRIDGE_CONTRACT
          value: "0x..."
        - name: REDIS_URL
          value: "redis://redis.bridge.svc.cluster.local:6379"
        - name: PRIVATE_KEY_SECRET
          valueFrom:
            secretKeyRef:
              name: bridge-executor-key
              key: private_key
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
```

---

## 15. ç”Ÿäº§æœ€ä½³å®è·µ

### 15.1 éƒ¨ç½²æ¸…å•

**ç”Ÿäº§ç¯å¢ƒæ£€æŸ¥æ¸…å•**:

```yaml
å®‰å…¨:
  âœ… ç§é’¥ä½¿ç”¨Vault/Secretsç®¡ç†
  âœ… NetworkPolicyå¯ç”¨å¹¶é…ç½®
  âœ… RBACæœ€å°æƒé™åŸåˆ™
  âœ… å®¹å™¨é•œåƒç­¾åéªŒè¯ (Cosign)
  âœ… æ¼æ´æ‰«æ (Trivy)
  âœ… å¯ç”¨etcdåŠ å¯†
  âœ… TLSè¯ä¹¦ç®¡ç† (cert-manager)
  âœ… Admission Controller (OPA/Kyverno)

é«˜å¯ç”¨:
  âœ… å¤šå‰¯æœ¬éƒ¨ç½² (>=3)
  âœ… è·¨å¯ç”¨åŒºåˆ†å¸ƒ
  âœ… PodDisruptionBudgeté…ç½®
  âœ… å¥åº·æ£€æŸ¥å®Œå–„
  âœ… è‡ªåŠ¨æ•…éšœè½¬ç§»
  âœ… å¤‡ä»½ç­–ç•¥ (æ—¥/å‘¨/æœˆ)

æ€§èƒ½:
  âœ… èµ„æºrequests/limitsåˆç†
  âœ… å­˜å‚¨ç±»å‹åŒ¹é… (NVMe SSD)
  âœ… ç½‘ç»œä¼˜åŒ– (Cilium eBPF)
  âœ… èŠ‚ç‚¹äº²å’Œæ€§é…ç½®
  âœ… HPA/VPAé…ç½®

ç›‘æ§:
  âœ… PrometheusæŒ‡æ ‡é‡‡é›†
  âœ… Grafana Dashboard
  âœ… å‘Šè­¦è§„åˆ™é…ç½®
  âœ… æ—¥å¿—èšåˆ (Loki/ELK)
  âœ… é“¾ä¸ŠæŒ‡æ ‡ç›‘æ§
  âœ… PagerDuty/OpsGenieé›†æˆ

åˆè§„:
  âœ… å®¡è®¡æ—¥å¿—å¯ç”¨
  âœ… æ•°æ®åŠ å¯† (ä¼ è¾“+é™æ€)
  âœ… è®¿é—®æ§åˆ¶è®°å½•
  âœ… å®šæœŸå®‰å…¨å®¡è®¡
  âœ… ç¾éš¾æ¢å¤æµ‹è¯•
```

### 15.2 è¿ç»´å»ºè®®

**æ—¥å¸¸è¿ç»´**:

```yaml
æ¯æ—¥æ£€æŸ¥:
  - èŠ‚ç‚¹åŒæ­¥çŠ¶æ€
  - Peerè¿æ¥æ•°
  - åŒºå—é«˜åº¦ä¸ä¸»ç½‘å¯¹æ¯”
  - ç£ç›˜ä½¿ç”¨ç‡
  - å…³é”®å‘Šè­¦

æ¯å‘¨ä»»åŠ¡:
  - å…¨é‡å¤‡ä»½éªŒè¯
  - æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ
  - å®‰å…¨è¡¥ä¸è¯„ä¼°
  - èµ„æºä½¿ç”¨è¶‹åŠ¿åˆ†æ

æ¯æœˆä»»åŠ¡:
  - ç¾éš¾æ¢å¤æ¼”ç»ƒ
  - å®¹é‡è§„åˆ’è¯„ä¼°
  - æˆæœ¬ä¼˜åŒ–åˆ†æ
  - è½¯ä»¶ç‰ˆæœ¬å‡çº§è®¡åˆ’
```

**å‡çº§ç­–ç•¥**:

```yaml
# rolling-update-strategy.yaml
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0  # ä»partitionå¼€å§‹æ›´æ–°
      maxUnavailable: 1  # æœ€å¤š1ä¸ªPodä¸å¯ç”¨

# é‡‘ä¸é›€å‡çº§
kubectl set image statefulset/geth geth=ethereum/client-go:v1.14.9 --record
kubectl rollout status statefulset/geth
kubectl rollout undo statefulset/geth  # å›æ»š
```

### 15.3 æˆæœ¬ä¼˜åŒ–

**Spotå®ä¾‹ä½¿ç”¨**:

```yaml
# node-pool-spot.yaml
apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: blockchain-spot
spec:
  requirements:
  - key: "karpenter.sh/capacity-type"
    operator: In
    values: ["spot"]
  - key: "node.kubernetes.io/instance-type"
    operator: In
    values: ["c6i.4xlarge", "c6i.8xlarge"]
  
  limits:
    resources:
      cpu: 1000
      memory: 2000Gi
  
  ttlSecondsAfterEmpty: 30
  ttlSecondsUntilExpired: 2592000  # 30å¤©
  
  labels:
    blockchain-workload: "non-critical"
```

**è‡ªåŠ¨æ‰©ç¼©å®¹**:

```yaml
# vpa-geth.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: geth-vpa
  namespace: ethereum-mainnet
spec:
  targetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: geth
  updatePolicy:
    updateMode: "Auto"  # è‡ªåŠ¨è°ƒæ•´
  resourcePolicy:
    containerPolicies:
    - containerName: geth
      minAllowed:
        memory: "8Gi"
        cpu: "2000m"
      maxAllowed:
        memory: "64Gi"
        cpu: "16000m"
```

### 15.4 åˆè§„æ€§

**å®¡è®¡æ—¥å¿—**:

```yaml
# audit-policy.yaml
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
# è®°å½•åŒºå—é“¾ç›¸å…³æ“ä½œ
- level: RequestResponse
  namespaces: ["ethereum-mainnet", "hyperledger-fabric"]
  verbs: ["create", "update", "patch", "delete"]
  resources:
  - group: ""
    resources: ["secrets", "configmaps"]
  - group: "apps"
    resources: ["statefulsets", "deployments"]

# è®°å½•å¯†é’¥è®¿é—®
- level: Metadata
  namespaces: ["ethereum-mainnet"]
  verbs: ["get"]
  resources:
  - group: ""
    resources: ["secrets"]
  omitStages:
  - RequestReceived
```

**æ•°æ®ç•™å­˜**:

```yaml
# æ—¥å¿—ç•™å­˜ç­–ç•¥
Loki:
  retention_period: 90d  # 90å¤©

Prometheus:
  retention: 30d  # 30å¤©

Backup:
  daily: 7d    # æ¯æ—¥å¤‡ä»½ä¿ç•™7å¤©
  weekly: 30d  # æ¯å‘¨å¤‡ä»½ä¿ç•™30å¤©
  monthly: 365d  # æ¯æœˆå¤‡ä»½ä¿ç•™1å¹´
```

---

## 16. æ¡ˆä¾‹ç ”ç©¶

### 16.1 ä¼ä¸šè”ç›Ÿé“¾ (Fabric)

**åœºæ™¯**: ä¾›åº”é“¾é‡‘èå¹³å°

```yaml
æ¶æ„:
  - 4ä¸ªç»„ç»‡ (é“¶è¡Œã€æ ¸å¿ƒä¼ä¸šã€ä¾›åº”å•†ã€ç‰©æµ)
  - æ¯ç»„ç»‡2 PeerèŠ‚ç‚¹ (ä¸»å¤‡)
  - 3ä¸ªOrdererèŠ‚ç‚¹ (Raft)
  - 2ä¸ªCAèŠ‚ç‚¹ (æ¯ç»„ç»‡)

éƒ¨ç½²:
  - Kubernetesé›†ç¾¤: 3 Master + 12 Worker
  - å­˜å‚¨: Rook-Ceph (3å‰¯æœ¬)
  - ç½‘ç»œ: Calico + NetworkPolicyä¸¥æ ¼éš”ç¦»
  - ç›‘æ§: Prometheus + Grafana + PagerDuty

æˆæœ:
  - TPS: ~1,200
  - å»¶è¿Ÿ: <3s (å«å…±è¯†)
  - å¯ç”¨æ€§: 99.95%
  - æˆæœ¬: é™ä½40% (vs ä¼ ç»Ÿè™šæ‹Ÿæœº)
```

### 16.2 å…¬é“¾RPCæœåŠ¡ (Ethereum)

**åœºæ™¯**: å»ä¸­å¿ƒåŒ–åº”ç”¨RPCæœåŠ¡å•†

```yaml
æ¶æ„:
  - 10ä¸ªFull Node (è¯»è´Ÿè½½å‡è¡¡)
  - 1ä¸ªArchive Node (å†å²æŸ¥è¯¢)
  - Nginx LB + Redisç¼“å­˜
  - Envoy sidecar (gRPCæ”¯æŒ)

éƒ¨ç½²:
  - EKS (AWS Kubernetes)
  - å­˜å‚¨: EBS io2 (64000 IOPS)
  - ç½‘ç»œ: Cilium eBPF
  - CDN: CloudFront (é™æ€æ–¹æ³•ç¼“å­˜)

è§„æ¨¡:
  - QPS: ~50,000
  - P99å»¶è¿Ÿ: <100ms
  - æœˆæˆæœ¬: ~$15,000
  - ç¼“å­˜å‘½ä¸­ç‡: 85%
```

### 16.3 Layer 2 Sequencer

**åœºæ™¯**: Optimistic Rollup Sequencer

```yaml
æ¶æ„:
  - op-geth (æ‰§è¡Œå¼•æ“)
  - op-node (RollupèŠ‚ç‚¹)
  - op-batcher (æ‰¹å¤„ç†å™¨)
  - op-proposer (çŠ¶æ€æè®®å™¨)

éƒ¨ç½²:
  - GKE (Google Kubernetes Engine)
  - å­˜å‚¨: Persistent Disk Extreme (120000 IOPS)
  - ç½‘ç»œ: VPC Native + Shared VPC
  - ç¾å¤‡: Multi-region (us-central1 + us-east1)

æ€§èƒ½:
  - TPS: ~4,000
  - Gasè´¹ç”¨: ä¸»ç½‘çš„1/50
  - æ‰¹æ¬¡æäº¤é¢‘ç‡: æ¯5åˆ†é’Ÿ
  - L1æ•°æ®å¯ç”¨æ€§æˆæœ¬: ~$2,000/å¤©
```

---

## 17. å·¥å…·å’Œèµ„æº

### 17.1 å¼€æºå·¥å…·

**Kubernetes Operators**:

```yaml
Hyperledger Fabric:
  - hlf-operator: https://github.com/hyperledger-labs/hlf-operator
  - fabric-operator: https://github.com/hyperledger/fabric-operator

Ethereum:
  - besu-operator: https://github.com/ConsenSys/besu-kubernetes
  - ethereum-helm-charts: https://github.com/skylenet/ethereum-helm-charts

Cosmos:
  - cosmos-operator: https://github.com/strangelove-ventures/cosmos-operator

Polkadot:
  - substrate-operator: https://github.com/paritytech/substrate-operator
```

**ç›‘æ§å·¥å…·**:

```yaml
é“¾ä¸Šç›‘æ§:
  - Blockscout: åŒºå—æµè§ˆå™¨
  - The Graph: ç´¢å¼•å™¨
  - Etherscan API: ä»¥å¤ªåŠæ•°æ®

åŸºç¡€è®¾æ–½ç›‘æ§:
  - Prometheus: æŒ‡æ ‡é‡‡é›†
  - Grafana: å¯è§†åŒ–
  - Loki: æ—¥å¿—èšåˆ
  - Jaeger: åˆ†å¸ƒå¼è¿½è¸ª
```

**å®‰å…¨å·¥å…·**:

```yaml
å¯†é’¥ç®¡ç†:
  - HashiCorp Vault
  - AWS Secrets Manager
  - Azure Key Vault
  - GCP Secret Manager

å®¡è®¡:
  - Falco: è¿è¡Œæ—¶å®‰å…¨
  - OPA: ç­–ç•¥æ‰§è¡Œ
  - Trivy: é•œåƒæ‰«æ
  - Cosign: é•œåƒç­¾å
```

### 17.2 å•†ä¸šå¹³å°

**BaaS (Blockchain as a Service)**:

```yaml
äº‘å‚å•†:
  - AWS Managed Blockchain (Hyperledger Fabric, Ethereum)
  - Azure Blockchain Service (Quorum)
  - Google Cloud Blockchain Node Engine (Ethereum)
  - IBM Blockchain Platform (Hyperledger Fabric)
  - Oracle Blockchain Platform (Hyperledger Fabric)

ä¸“ä¸šå¹³å°:
  - Alchemy: Ethereum RPC/WebSocket
  - Infura: Multi-chain RPC
  - QuickNode: å…¨é“¾æ”¯æŒ
  - Ankr: å»ä¸­å¿ƒåŒ–RPC
```

**ä¼ä¸šè§£å†³æ–¹æ¡ˆ**:

```yaml
Hyperledger Fabric:
  - IBM Blockchain Platform
  - Oracle Blockchain Platform
  - SAP Blockchain

Ethereum:
  - ConsenSys Quorum
  - Chainstack
  - Kaleido
```

### 17.3 å­¦ä¹ èµ„æº

**å®˜æ–¹æ–‡æ¡£**:

```yaml
Hyperledger Fabric:
  - https://hyperledger-fabric.readthedocs.io/
  - https://github.com/hyperledger/fabric

Ethereum:
  - https://ethereum.org/en/developers/docs/
  - https://geth.ethereum.org/docs/
  - https://github.com/ethereum/go-ethereum

Layer 2:
  - Optimism: https://community.optimism.io/docs/
  - Arbitrum: https://docs.arbitrum.io/
  - zkSync: https://docs.zksync.io/
  - Polygon zkEVM: https://docs.polygon.technology/zkEVM/

Kubernetes:
  - https://kubernetes.io/docs/
  - https://operatorhub.io/
```

**ç¤¾åŒº**:

```yaml
è®ºå›:
  - Hyperledger Discord
  - Ethereum StackExchange
  - Reddit: r/ethdev, r/ethereum
  - Kubernetes Slack

ä¼šè®®:
  - Hyperledger Global Forum
  - Ethereum Devcon
  - KubeCon + CloudNativeCon
```

---

## æ€»ç»“

**åŒºå—é“¾å®¹å™¨åŒ–çš„ä»·å€¼**:

```yaml
æŠ€æœ¯ä»·å€¼:
  âœ… æ ‡å‡†åŒ–éƒ¨ç½²æµç¨‹ (Docker/Helm/Operator)
  âœ… è‡ªåŠ¨åŒ–è¿ç»´ (è‡ªæ„ˆã€æ‰©å®¹ã€å‡çº§)
  âœ… èµ„æºå¼¹æ€§ (æŒ‰éœ€æ‰©ç¼©ã€æˆæœ¬ä¼˜åŒ–)
  âœ… å¤šäº‘å¯ç§»æ¤ (é¿å…å‚å•†é”å®š)
  âœ… å¾®æœåŠ¡æ¶æ„ (ç»„ä»¶è§£è€¦ã€ç‹¬ç«‹å‡çº§)

ä¸šåŠ¡ä»·å€¼:
  âœ… ç¼©çŸ­ä¸Šå¸‚æ—¶é—´ (å¿«é€Ÿéƒ¨ç½²æµ‹è¯•ç½‘/ä¸»ç½‘)
  âœ… é™ä½è¿ç»´æˆæœ¬ (äººåŠ›ã€åŸºç¡€è®¾æ–½)
  âœ… æé«˜å¯é æ€§ (HAã€ç¾å¤‡ã€ç›‘æ§)
  âœ… å¢å¼ºå®‰å…¨æ€§ (éš”ç¦»ã€åŠ å¯†ã€å®¡è®¡)
  âœ… æ”¯æŒè§„æ¨¡åŒ– (ä»è¯•ç‚¹åˆ°ç”Ÿäº§)
```

**2025å¹´è¶‹åŠ¿**:

```yaml
Layer 2ä¸»æµåŒ–:
  - Optimistic Rollupæˆç†Ÿ (Optimismã€Arbitrum)
  - ZK Rollupäº§å“åŒ– (zkSync Eraã€Polygon zkEVM)
  - åº”ç”¨é“¾çˆ†å‘ (OP Stackã€Orbitã€Polygon CDK)

æ¨¡å—åŒ–åŒºå—é“¾:
  - æ•°æ®å¯ç”¨æ€§å±‚ (Celestiaã€EigenDA)
  - å…±è¯†å±‚ä¸æ‰§è¡Œå±‚è§£è€¦
  - Rollup-as-a-Service

GPUåŠ é€Ÿ:
  - é›¶çŸ¥è¯†è¯æ˜ç”Ÿæˆ
  - æœºå¯†è®¡ç®— (TEE + GPU)
  - zkML (é›¶çŸ¥è¯†æœºå™¨å­¦ä¹ )

è·¨é“¾äº’æ“ä½œ:
  - IBCæˆç†Ÿ (Cosmosç”Ÿæ€)
  - è·¨é“¾æ¡¥å®‰å…¨æ€§æå‡
  - é€šç”¨æ¶ˆæ¯ä¼ é€’åè®®

ä¼ä¸šé‡‡ç”¨:
  - è”ç›Ÿé“¾ä¸å…¬é“¾èåˆ
  - éšç§è®¡ç®—é›†æˆ (MPCã€FHE)
  - åˆè§„æ¡†æ¶å®Œå–„
```

**å…³é”®å»ºè®®**:

```yaml
é€‰æ‹©åŒºå—é“¾å¹³å°:
  - è”ç›Ÿé“¾: Hyperledger Fabric (æˆç†Ÿç¨³å®š)
  - å…¬é“¾: Ethereum (ç”Ÿæ€æœ€å¤§)
  - é«˜æ€§èƒ½: Solanaã€Sui (ä½å»¶è¿Ÿ)
  - åº”ç”¨é“¾: Cosmos SDK (çµæ´»å¯å®šåˆ¶)

å®¹å™¨åŒ–ç­–ç•¥:
  - å¼€å‘/æµ‹è¯•: Docker Compose (å¿«é€Ÿè¿­ä»£)
  - ç”Ÿäº§: Kubernetes + Operator (ä¼ä¸šçº§)
  - æ··åˆäº‘: Helm Charts (è·¨å¹³å°)

å®‰å…¨ä¼˜å…ˆ:
  - ç§é’¥ç®¡ç†: Vault/HSM (ç»ä¸æ³„éœ²)
  - ç½‘ç»œéš”ç¦»: NetworkPolicy (é›¶ä¿¡ä»»)
  - é•œåƒå®‰å…¨: ç­¾å+æ‰«æ (ä¾›åº”é“¾å®‰å…¨)

ç›‘æ§å®Œå–„:
  - åŸºç¡€è®¾æ–½: Prometheus + Grafana
  - é“¾ä¸Šæ•°æ®: è‡ªå®šä¹‰Exporter
  - å‘Šè­¦é›†æˆ: PagerDuty/OpsGenie
```

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**:

```yaml
å…¥é—¨:
  1. Docker Composeéƒ¨ç½²æµ‹è¯•ç½‘ (Geth/Fabric)
  2. å­¦ä¹ KubernetesåŸºç¡€ (Podã€Serviceã€StatefulSet)
  3. å°è¯•Helméƒ¨ç½²å•èŠ‚ç‚¹

è¿›é˜¶:
  1. Kubernetesä¸Šéƒ¨ç½²å¤šèŠ‚ç‚¹é›†ç¾¤
  2. å®æ–½ç›‘æ§å’Œå‘Šè­¦
  3. é…ç½®NetworkPolicyå’ŒRBAC
  4. æµ‹è¯•ç¾éš¾æ¢å¤æµç¨‹

ç”Ÿäº§:
  1. è¯„ä¼°Operator (hlf-operator/besu-operator)
  2. è®¾è®¡å¤šåŒºåŸŸé«˜å¯ç”¨æ¶æ„
  3. å®æ–½è‡ªåŠ¨åŒ–å¤‡ä»½å’Œæ¢å¤
  4. å»ºç«‹è¿ç»´runbookå’Œon-callæµç¨‹
```

---

**æ›´æ–°æ—¶é—´**: 2025-10-20  
**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**çŠ¶æ€**: âœ… **å®Œæˆ**

---

**ğŸŒŸ åŒºå—é“¾ä¸å®¹å™¨åŒ–æŠ€æœ¯çš„ç»“åˆ,æ­£åœ¨é‡å¡‘Web3åŸºç¡€è®¾æ–½çš„æœªæ¥ï¼ğŸŒŸ**-
