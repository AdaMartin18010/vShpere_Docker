# Rook-Cephå­˜å‚¨

> **è¿”å›**: [å®¹å™¨å­˜å‚¨ç›®å½•](README.md) | [å®¹å™¨åŒ–éƒ¨ç½²é¦–é¡µ](../README.md) | [éƒ¨ç½²æŒ‡å—é¦–é¡µ](../../00_ç´¢å¼•å¯¼èˆª/README.md)

---

## ğŸ“‹ ç›®å½•

- [Rook-Cephå­˜å‚¨](#rook-cephå­˜å‚¨)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. Rook-Cephç®€ä»‹](#1-rook-cephç®€ä»‹)
  - [2. Cephæ¶æ„](#2-cephæ¶æ„)
  - [3. Rookæ¶æ„](#3-rookæ¶æ„)
  - [4. Rook-Cephéƒ¨ç½²](#4-rook-cephéƒ¨ç½²)
    - [å‰ç½®è¦æ±‚](#å‰ç½®è¦æ±‚)
    - [å®‰è£…æ­¥éª¤](#å®‰è£…æ­¥éª¤)
  - [5. RBDå—å­˜å‚¨](#5-rbdå—å­˜å‚¨)
  - [6. CephFSæ–‡ä»¶å­˜å‚¨](#6-cephfsæ–‡ä»¶å­˜å‚¨)
  - [7. RGWå¯¹è±¡å­˜å‚¨](#7-rgwå¯¹è±¡å­˜å‚¨)
  - [8. å­˜å‚¨ç®¡ç†](#8-å­˜å‚¨ç®¡ç†)
  - [9. ç›‘æ§ä¸è¿ç»´](#9-ç›‘æ§ä¸è¿ç»´)
  - [10. æœ€ä½³å®è·µ](#10-æœ€ä½³å®è·µ)
  - [ç›¸å…³æ–‡æ¡£](#ç›¸å…³æ–‡æ¡£)

---

## 1. Rook-Cephç®€ä»‹

```yaml
Rook_Ceph_Overview:
  Rook:
    å®šä¹‰:
      - CNCFæ¯•ä¸šé¡¹ç›®
      - äº‘åŸç”Ÿå­˜å‚¨ç¼–æ’å™¨
      - Kubernetes Operator
      - è‡ªåŠ¨åŒ–å­˜å‚¨ç®¡ç†
    
    æ”¯æŒå­˜å‚¨:
      - Ceph (ä¸»è¦)
      - NFS
      - Cassandra (å·²åºŸå¼ƒ)
      - EdgeFS (å·²åºŸå¼ƒ)
  
  Ceph:
    å®šä¹‰:
      - å¼€æºåˆ†å¸ƒå¼å­˜å‚¨
      - ç»Ÿä¸€å­˜å‚¨å¹³å°
      - é«˜å¯ç”¨é«˜æ‰©å±•
      - ä¼ä¸šçº§åŠŸèƒ½
    
    å­˜å‚¨ç±»å‹:
      - RBD: å—å­˜å‚¨
      - CephFS: æ–‡ä»¶å­˜å‚¨
      - RGW: å¯¹è±¡å­˜å‚¨
  
  Rook-Cephä¼˜åŠ¿:
    - äº‘åŸç”Ÿè®¾è®¡
    - Operatorè‡ªåŠ¨åŒ–
    - ç®€åŒ–éƒ¨ç½²
    - è‡ªæˆ‘ä¿®å¤
    - æ»šåŠ¨å‡çº§
    - å¤šé›†ç¾¤ç®¡ç†
  
  é€‚ç”¨åœºæ™¯:
    - ç”Ÿäº§ç¯å¢ƒå­˜å‚¨
    - å¤šPodå…±äº«å­˜å‚¨
    - å¯¹è±¡å­˜å‚¨éœ€æ±‚
    - ä¼ä¸šçº§åº”ç”¨
    - å¤§è§„æ¨¡é›†ç¾¤
```

---

## 2. Cephæ¶æ„

```yaml
Ceph_Architecture:
  æ ¸å¿ƒç»„ä»¶:
    Monitor (MON):
      ä½œç”¨: é›†ç¾¤ç›‘æ§
      åŠŸèƒ½:
        - ç»´æŠ¤é›†ç¾¤çŠ¶æ€
        - ç®¡ç†é›†ç¾¤æ‹“æ‰‘
        - è®¤è¯æˆæƒ
        - æ•…éšœæ£€æµ‹
      æ¨èæ•°é‡: 3ä¸ªæˆ–5ä¸ª (å¥‡æ•°)
    
    OSD (Object Storage Daemon):
      ä½œç”¨: å­˜å‚¨æ•°æ®
      åŠŸèƒ½:
        - å­˜å‚¨å¯¹è±¡
        - æ•°æ®å¤åˆ¶
        - æ•°æ®æ¢å¤
        - æ•°æ®å¹³è¡¡
      æ¨è: æ¯å—ç£ç›˜ä¸€ä¸ªOSD
    
    Manager (MGR):
      ä½œç”¨: ç®¡ç†æœåŠ¡
      åŠŸèƒ½:
        - ç›‘æ§æŒ‡æ ‡
        - Dashboard
        - REST API
        - æ¨¡å—ç®¡ç†
      æ¨èæ•°é‡: 2ä¸ª (ä¸»å¤‡)
    
    MDS (Metadata Server):
      ä½œç”¨: CephFSå…ƒæ•°æ®
      åŠŸèƒ½:
        - æ–‡ä»¶ç³»ç»Ÿå…ƒæ•°æ®
        - ç›®å½•ç»“æ„
        - æƒé™ç®¡ç†
      éœ€æ±‚: ä»…CephFSéœ€è¦
      æ¨èæ•°é‡: 2ä¸ª (ä¸»å¤‡)
    
    RGW (RADOS Gateway):
      ä½œç”¨: å¯¹è±¡å­˜å‚¨ç½‘å…³
      åŠŸèƒ½:
        - S3 API
        - Swift API
        - å¯¹è±¡å­˜å‚¨
        - å¤šç§Ÿæˆ·
      éœ€æ±‚: ä»…å¯¹è±¡å­˜å‚¨éœ€è¦
  
  æ•°æ®ç»„ç»‡:
    Object:
      - æœ€å°å­˜å‚¨å•å…ƒ
      - é»˜è®¤4MB
      - åŒ…å«æ•°æ®å’Œå…ƒæ•°æ®
    
    PG (Placement Group):
      - å¯¹è±¡é€»è¾‘åˆ†ç»„
      - CRUSHç®—æ³•æ˜ å°„
      - æ•°æ®åˆ†å¸ƒ
      - æ¨è: 100-200 PG/OSD
    
    Pool:
      - PGé›†åˆ
      - å­˜å‚¨ç­–ç•¥
      - å‰¯æœ¬æ•°/çº åˆ ç 
      - è®¿é—®æ§åˆ¶
  
  CRUSHç®—æ³•:
    - Controlled Replication Under Scalable Hashing
    - ä¼ªéšæœºæ•°æ®åˆ†å¸ƒ
    - æ— éœ€ä¸­å¿ƒå…ƒæ•°æ®
    - æ‹“æ‰‘æ„ŸçŸ¥
    - æ•…éšœåŸŸéš”ç¦»
```

**Cephæ¶æ„å›¾**:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Client Applications               â”‚
â”‚     (RBD / CephFS / RGW)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
        â”‚   librados  â”‚  (Cephå®¢æˆ·ç«¯åº“)
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            RADOS (å¯é è‡ªä¸»åˆ†å¸ƒå¼å¯¹è±¡å­˜å‚¨)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”
â”‚ MON   â”‚  â”‚  MGR  â”‚  â”‚  MDS  â”‚
â”‚ (ç›‘æ§) â”‚  â”‚(ç®¡ç†) â”‚  â”‚(å…ƒæ•°æ®)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”
â”‚ OSD-1 â”‚  â”‚ OSD-2 â”‚  â”‚ OSD-3 â”‚
â”‚ Disk1 â”‚  â”‚ Disk2 â”‚  â”‚ Disk3 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Rookæ¶æ„

```yaml
Rook_Architecture:
  ç»„ä»¶:
    Rook Operator:
      - æ ¸å¿ƒæ§åˆ¶å™¨
      - ç›‘å¬CRD
      - ç®¡ç†Cephé›†ç¾¤
      - è‡ªåŠ¨åŒ–è¿ç»´
    
    Rook Agent:
      - èŠ‚ç‚¹ä»£ç† (å·²åºŸå¼ƒ)
      - ä½¿ç”¨CSIæ›¿ä»£
    
    Rook Discover:
      - ç£ç›˜å‘ç°
      - èŠ‚ç‚¹æ ‡è®°
      - è‡ªåŠ¨åŒ–å‡†å¤‡
    
    CSI Driver:
      - RBD CSI
      - CephFS CSI
      - å·ç®¡ç†
      - åŠ¨æ€ä¾›åº”
  
  CRDèµ„æº:
    CephCluster:
      - å®šä¹‰Cephé›†ç¾¤
      - é…ç½®MON/OSD/MGR
      - é›†ç¾¤çº§åˆ«è®¾ç½®
    
    CephBlockPool:
      - å®šä¹‰å­˜å‚¨æ± 
      - RBDä½¿ç”¨
      - å‰¯æœ¬é…ç½®
    
    CephFilesystem:
      - å®šä¹‰æ–‡ä»¶ç³»ç»Ÿ
      - CephFSä½¿ç”¨
      - MDSé…ç½®
    
    CephObjectStore:
      - å®šä¹‰å¯¹è±¡å­˜å‚¨
      - RGWé…ç½®
      - S3 API
    
    CephNFS:
      - NFSæœåŠ¡
      - Ganeshaé…ç½®
```

---

## 4. Rook-Cephéƒ¨ç½²

### å‰ç½®è¦æ±‚

```yaml
Prerequisites:
  ç¡¬ä»¶è¦æ±‚:
    èŠ‚ç‚¹æ•°é‡: >=3 (ç”Ÿäº§ç¯å¢ƒ)
    CPU: æ¯OSD 1æ ¸
    å†…å­˜: æ¯OSD 2-4GB
    å­˜å‚¨:
      - åŸå§‹å—è®¾å¤‡ (æ¨è)
      - æœªæ ¼å¼åŒ–
      - æœªæŒ‚è½½
      - GPTåˆ†åŒºæ¸…é™¤
  
  è½¯ä»¶è¦æ±‚:
    Kubernetes: >=1.21
    å†…æ ¸: >=4.17 (RBD), >=4.17 (CephFS)
    å·¥å…·: lvm2
```

### å®‰è£…æ­¥éª¤

```bash
# ========================================
# 1. éƒ¨ç½²Rook Operator
# ========================================

# å…‹éš†Rookä»“åº“
git clone --single-branch --branch v1.12.0 https://github.com/rook/rook.git
cd rook/deploy/examples

# éƒ¨ç½²CRDå’ŒOperator
kubectl create -f crds.yaml
kubectl create -f common.yaml
kubectl create -f operator.yaml

# éªŒè¯Operator
kubectl get pods -n rook-ceph

# ========================================
# 2. åˆ›å»ºCephé›†ç¾¤
# ========================================

# ç¼–è¾‘cluster.yaml (æ ¹æ®éœ€è¦)
cat <<EOF > cluster.yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v17.2.6
    allowUnsupported: false
  
  dataDirHostPath: /var/lib/rook
  
  mon:
    count: 3
    allowMultiplePerNode: false
  
  mgr:
    count: 2
    allowMultiplePerNode: false
  
  dashboard:
    enabled: true
    ssl: true
  
  monitoring:
    enabled: true
  
  storage:
    useAllNodes: true
    useAllDevices: true
    # æˆ–æŒ‡å®šè®¾å¤‡
    # config:
    #   osdsPerDevice: "1"
  
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
EOF

# éƒ¨ç½²é›†ç¾¤
kubectl create -f cluster.yaml

# æŸ¥çœ‹é›†ç¾¤çŠ¶æ€
kubectl get cephcluster -n rook-ceph
kubectl get pods -n rook-ceph -w

# ========================================
# 3. å®‰è£…Rookå·¥å…·ç®±
# ========================================

kubectl create -f toolbox.yaml

# è¿›å…¥å·¥å…·ç®±
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash

# æŸ¥çœ‹CephçŠ¶æ€
ceph status
ceph osd status
ceph df
ceph health detail

# ========================================
# 4. è®¿é—®Dashboard
# ========================================

# è·å–Dashboardåœ°å€
kubectl get svc -n rook-ceph | grep dashboard

# ç«¯å£è½¬å‘
kubectl port-forward -n rook-ceph svc/rook-ceph-mgr-dashboard 8443:8443

# è·å–adminå¯†ç 
kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode

# æµè§ˆå™¨è®¿é—®: https://localhost:8443
# ç”¨æˆ·å: admin
```

---

## 5. RBDå—å­˜å‚¨

```yaml
# ========================================
# 1. åˆ›å»ºBlock Pool
# ========================================
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  failureDomain: host
  replicated:
    size: 3  # å‰¯æœ¬æ•°
    requireSafeReplicaSize: true
  deviceClass: ssd  # å¯é€‰: hdd/ssd/nvme

---
# ========================================
# 2. åˆ›å»ºStorageClass
# ========================================
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: replicapool
  imageFormat: "2"
  imageFeatures: layering
  
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  
  csi.storage.k8s.io/fstype: ext4
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate

---
# ========================================
# 3. åˆ›å»ºPVC
# ========================================
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: rook-ceph-block

---
# ========================================
# 4. ä½¿ç”¨PVC
# ========================================
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: rbd-pvc
```

**RBDç®¡ç†å‘½ä»¤**:

```bash
# åœ¨å·¥å…·ç®±ä¸­æ‰§è¡Œ
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash

# æŸ¥çœ‹RBDæ± 
ceph osd pool ls

# æŸ¥çœ‹RBDé•œåƒ
rbd ls -p replicapool

# æŸ¥çœ‹é•œåƒè¯¦æƒ…
rbd info replicapool/<image-name>

# æŸ¥çœ‹é•œåƒä½¿ç”¨
rbd du -p replicapool

# åˆ é™¤é•œåƒ
rbd rm replicapool/<image-name>
```

---

## 6. CephFSæ–‡ä»¶å­˜å‚¨

```yaml
# ========================================
# 1. åˆ›å»ºæ–‡ä»¶ç³»ç»Ÿ
# ========================================
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: myfs
  namespace: rook-ceph
spec:
  metadataPool:
    replicated:
      size: 3
  dataPools:
  - name: replicated
    replicated:
      size: 3
  
  preserveFilesystemOnDelete: false
  
  metadataServer:
    activeCount: 1
    activeStandby: true

---
# ========================================
# 2. åˆ›å»ºStorageClass
# ========================================
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs
provisioner: rook-ceph.cephfs.csi.ceph.com
parameters:
  clusterID: rook-ceph
  fsName: myfs
  pool: myfs-replicated
  
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate

---
# ========================================
# 3. åˆ›å»ºRWX PVC
# ========================================
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cephfs-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: rook-cephfs

---
# ========================================
# 4. å¤šPodå…±äº«
# ========================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: nginx
        image: nginx
        volumeMounts:
        - name: shared-data
          mountPath: /usr/share/nginx/html
      volumes:
      - name: shared-data
        persistentVolumeClaim:
          claimName: cephfs-pvc
```

---

## 7. RGWå¯¹è±¡å­˜å‚¨

```yaml
# ========================================
# 1. åˆ›å»ºå¯¹è±¡å­˜å‚¨
# ========================================
apiVersion: ceph.rook.io/v1
kind: CephObjectStore
metadata:
  name: my-store
  namespace: rook-ceph
spec:
  metadataPool:
    replicated:
      size: 3
  dataPool:
    replicated:
      size: 3
  preservePoolsOnDelete: false
  
  gateway:
    port: 80
    instances: 2
    priorityClassName: system-cluster-critical

---
# ========================================
# 2. åˆ›å»ºStorageClass
# ========================================
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-bucket
provisioner: rook-ceph.ceph.rook.io/bucket
reclaimPolicy: Delete
parameters:
  objectStoreName: my-store
  objectStoreNamespace: rook-ceph

---
# ========================================
# 3. åˆ›å»ºObject Bucket Claim
# ========================================
apiVersion: objectbucket.io/v1alpha1
kind: ObjectBucketClaim
metadata:
  name: my-bucket
spec:
  generateBucketName: my-bucket
  storageClassName: rook-ceph-bucket

---
# ========================================
# 4. è®¿é—®å¯¹è±¡å­˜å‚¨
# ========================================
# è·å–è®¿é—®å‡­è¯
kubectl get secret my-bucket -o yaml

# ä½¿ç”¨S3 APIè®¿é—®
# Endpoint: http://rook-ceph-rgw-my-store.rook-ceph:80
# Access Key: ä»secretè·å–
# Secret Key: ä»secretè·å–
```

**S3å®¢æˆ·ç«¯ä½¿ç”¨**:

```bash
# å®‰è£…AWS CLI
pip install awscli

# é…ç½®
export AWS_ACCESS_KEY_ID=<access-key>
export AWS_SECRET_ACCESS_KEY=<secret-key>
export AWS_ENDPOINT=http://rook-ceph-rgw-my-store.rook-ceph:80

# åˆ—å‡ºbucket
aws s3 ls --endpoint-url $AWS_ENDPOINT

# ä¸Šä¼ æ–‡ä»¶
aws s3 cp file.txt s3://my-bucket/ --endpoint-url $AWS_ENDPOINT

# ä¸‹è½½æ–‡ä»¶
aws s3 cp s3://my-bucket/file.txt . --endpoint-url $AWS_ENDPOINT
```

---

## 8. å­˜å‚¨ç®¡ç†

```bash
# ========================================
# OSDç®¡ç†
# ========================================

# æŸ¥çœ‹OSD
ceph osd tree
ceph osd status

# OSD out (ç»´æŠ¤å‰)
ceph osd out osd.0

# OSD in
ceph osd in osd.0

# ç§»é™¤OSD
ceph osd crush remove osd.0
ceph auth del osd.0
ceph osd rm osd.0

# ========================================
# å­˜å‚¨æ± ç®¡ç†
# ========================================

# æŸ¥çœ‹æ± 
ceph osd pool ls detail

# åˆ›å»ºæ± 
ceph osd pool create mypool 128 128

# è®¾ç½®å‰¯æœ¬æ•°
ceph osd pool set mypool size 3

# è®¾ç½®PGæ•°
ceph osd pool set mypool pg_num 256

# åˆ é™¤æ± 
ceph osd pool delete mypool mypool --yes-i-really-really-mean-it

# ========================================
# PGç®¡ç†
# ========================================

# æŸ¥çœ‹PG
ceph pg stat
ceph pg dump

# æŸ¥çœ‹PGçŠ¶æ€
ceph pg <pgid> query

# ä¿®å¤PG
ceph pg repair <pgid>

# ========================================
# é›†ç¾¤ç»´æŠ¤
# ========================================

# æŸ¥çœ‹é›†ç¾¤å¥åº·
ceph health detail

# æŸ¥çœ‹é›†ç¾¤çŠ¶æ€
ceph status

# æŸ¥çœ‹IOç»Ÿè®¡
ceph osd perf

# æŸ¥çœ‹ç©ºé—´ä½¿ç”¨
ceph df

#åœæ­¢è‡ªåŠ¨å¹³è¡¡ (ç»´æŠ¤æ—¶)
ceph osd set noout
ceph osd set norebalance

# æ¢å¤
ceph osd unset noout
ceph osd unset norebalance
```

---

## 9. ç›‘æ§ä¸è¿ç»´

```yaml
# ========================================
# Prometheusç›‘æ§
# ========================================
# Rookè‡ªåŠ¨éƒ¨ç½²Ceph exporter

# æŸ¥çœ‹Service Monitor
kubectl get servicemonitor -n rook-ceph

# Grafana Dashboard
# å¯¼å…¥Ceph Dashboard: ID 2842

# ========================================
# å‘Šè­¦è§„åˆ™
# ========================================
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ceph-alerts
  namespace: rook-ceph
spec:
  groups:
  - name: ceph
    rules:
    - alert: CephHealthWarning
      expr: ceph_health_status{job="rook-ceph-mgr"} == 1
      for: 5m
      annotations:
        summary: "Ceph health warning"
    
    - alert: CephOSDDown
      expr: ceph_osd_up < 1
      for: 5m
      annotations:
        summary: "Ceph OSD down"
```

---

## 10. æœ€ä½³å®è·µ

```yaml
Best_Practices:
  éƒ¨ç½²è§„åˆ’:
    âœ… è‡³å°‘3ä¸ªèŠ‚ç‚¹
    âœ… ä½¿ç”¨SSDä½œä¸ºOSD
    âœ… ç‹¬ç«‹çš„ç½‘ç»œ (10GbE+)
    âœ… é¢„ç•™è¶³å¤Ÿç©ºé—´
  
  é…ç½®ä¼˜åŒ–:
    âœ… MONæ•°é‡: 3æˆ–5ä¸ª (å¥‡æ•°)
    âœ… OSDå‰¯æœ¬æ•°: 3 (ç”Ÿäº§)
    âœ… PGæ•°é‡: 100-200/OSD
    âœ… å¯ç”¨Dashboard
  
  æ€§èƒ½ä¼˜åŒ–:
    âœ… ä½¿ç”¨NVMe SSD
    âœ… åˆ†ç¦»MONå’ŒOSD
    âœ… ä¼˜åŒ–PGæ•°é‡
    âœ… å¯ç”¨BlueStore
  
  é«˜å¯ç”¨:
    âœ… å¤šå‰¯æœ¬
    âœ… è·¨ä¸»æœº/æœºæ¶
    âœ… æ•…éšœåŸŸéš”ç¦»
    âœ… å®šæœŸå¤‡ä»½
  
  ç›‘æ§:
    âœ… Prometheus + Grafana
    âœ… Ceph Dashboard
    âœ… å‘Šè­¦è§„åˆ™
    âœ… å®¹é‡è§„åˆ’
  
  è¿ç»´:
    âœ… å®šæœŸå·¡æ£€
    âœ… å‡çº§è®¡åˆ’
    âœ… æ•…éšœæ¼”ç»ƒ
    âœ… æ–‡æ¡£ç»´æŠ¤
```

---

## ç›¸å…³æ–‡æ¡£

- [CSIå­˜å‚¨æ¦‚è¿°](01_CSIå­˜å‚¨æ¦‚è¿°.md)
- [Longhornå­˜å‚¨](03_Longhornå­˜å‚¨.md)
- [StorageClassæœ€ä½³å®è·µ](04_StorageClassæœ€ä½³å®è·µ.md)
- [Kuberneteså­˜å‚¨æ•…éšœæ’æŸ¥](../02_Kuberneteséƒ¨ç½²/05_æ•…éšœæ’æŸ¥.md#4-å­˜å‚¨æ•…éšœæ’æŸ¥)

---

**æ›´æ–°æ—¶é—´**: 2025-10-19  
**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª
