# 2025年云原生存储新特性

> **返回**: [容器存储目录](README.md) | [容器化部署首页](../README.md) | [部署指南首页](../../00_索引导航/README.md)

---

## 📋 目录

- [2025年云原生存储新特性](#2025年云原生存储新特性)
  - [📋 目录](#-目录)
  - [1. 2025年存储技术概览](#1-2025年存储技术概览)
    - [核心技术趋势](#核心技术趋势)
    - [技术选型矩阵](#技术选型矩阵)
  - [2. Rook 1.13+ 新特性](#2-rook-113-新特性)
    - [核心增强](#核心增强)
    - [Rook 1.13 部署示例](#rook-113-部署示例)
    - [RBD 加密卷示例 (2025新特性)](#rbd-加密卷示例-2025新特性)
  - [3. Longhorn 1.6+ 新特性](#3-longhorn-16-新特性)
    - [核心增强](#核心增强-1)
    - [Longhorn 1.6 部署](#longhorn-16-部署)
    - [V2数据引擎配置 (2025实验性功能)](#v2数据引擎配置-2025实验性功能)
    - [Longhorn 备份到S3 (2025最佳实践)](#longhorn-备份到s3-2025最佳实践)
  - [4. OpenEBS 4.0+ 云原生存储](#4-openebs-40-云原生存储)
    - [核心架构](#核心架构)
    - [OpenEBS Mayastor 2.0 部署](#openebs-mayastor-20-部署)
    - [Mayastor 性能优化配置](#mayastor-性能优化配置)
  - [5. CSI 1.9+ 新规范](#5-csi-19-新规范)
    - [核心新增功能](#核心新增功能)
    - [VolumeGroupSnapshot 示例 (2025新特性)](#volumegroupsnapshot-示例-2025新特性)
    - [卷健康检查 (CSI 1.9 GA)](#卷健康检查-csi-19-ga)
  - [6. 存储性能优化 2025](#6-存储性能优化-2025)
    - [NVMe优化](#nvme优化)
    - [性能调优脚本](#性能调优脚本)
    - [GPU Direct Storage (GDS) 配置](#gpu-direct-storage-gds-配置)
  - [7. 存储安全增强](#7-存储安全增强)
    - [加密卷实现](#加密卷实现)
    - [KMS集成示例 (Vault)](#kms集成示例-vault)
  - [8. 多集群存储联邦](#8-多集群存储联邦)
    - [跨集群存储复制](#跨集群存储复制)
    - [Rook-Ceph 跨集群镜像](#rook-ceph-跨集群镜像)
    - [Longhorn 跨集群备份](#longhorn-跨集群备份)
  - [9. 存储成本优化 (FinOps)](#9-存储成本优化-finops)
    - [成本分析](#成本分析)
    - [Kubecost 存储监控部署](#kubecost-存储监控部署)
    - [自动清理未使用存储](#自动清理未使用存储)
  - [10. 生产环境最佳实践 2025](#10-生产环境最佳实践-2025)
    - [架构设计清单](#架构设计清单)
    - [生产环境部署检查清单](#生产环境部署检查清单)
  - [相关文档](#相关文档)
    - [本模块文档](#本模块文档)
    - [相关模块](#相关模块)
    - [外部资源](#外部资源)

---

## 1. 2025年存储技术概览

### 核心技术趋势

```yaml
2025_Storage_Trends:
  技术演进:
    容器原生存储:
      - Rook 1.13+ (CNCF毕业项目)
      - Longhorn 1.6+ (轻量级云原生)
      - OpenEBS 4.0+ (CAS架构)
      - Portworx 3.0+ (企业级)
    
    CSI规范升级:
      - CSI 1.9+ (最新规范)
      - 卷组快照 (VolumeGroupSnapshot)
      - 跨命名空间卷克隆
      - 卷健康检查 GA
      - 卷修改 (VolumeModification)
    
    性能突破:
      - NVMe over Fabrics (NVMe-oF)
      - SPDK用户态驱动
      - io_uring异步I/O
      - eBPF存储监控
      - RDMA/RoCE高速网络
    
    安全增强:
      - 加密卷 (LUKS2)
      - 密钥管理 (KMS集成)
      - 存储访问控制 (RBAC)
      - 数据不可变性 (WORM)
      - 供应链安全 (镜像签名)

  新兴特性:
    AI_ML存储优化:
      - GPU Direct Storage (GDS)
      - 高吞吐数据管道
      - 数据集缓存
      - 分布式训练存储
    
    边缘存储:
      - K3s + Longhorn
      - 轻量级CSI驱动
      - 断网自治
      - 数据同步
    
    FinOps成本优化:
      - 存储使用分析
      - 自动数据分层
      - 冷热数据分离
      - 成本可视化

  技术对比_2025:
    | 存储方案 | 版本 | 架构 | 适用场景 | 性能 | 复杂度 |
    |---------|------|------|---------|------|--------|
    | Rook-Ceph | 1.13+ | 分布式 | 大规模生产 | ⭐⭐⭐⭐ | 高 |
    | Longhorn | 1.6+ | 超融合 | 中小规模 | ⭐⭐⭐⭐ | 中 |
    | OpenEBS | 4.0+ | CAS | 云原生应用 | ⭐⭐⭐⭐⭐ | 中 |
    | Portworx | 3.0+ | 企业级 | 关键业务 | ⭐⭐⭐⭐⭐ | 高 |
```

### 技术选型矩阵

```yaml
Storage_Selection_Matrix_2025:
  场景一_大规模生产环境:
    推荐: Rook-Ceph 1.13+
    原因:
      - 成熟稳定，CNCF毕业
      - 支持块/文件/对象存储
      - 自动化运维
      - 企业级功能完善
    规模: 100+ 节点，PB级存储
  
  场景二_中小规模云原生:
    推荐: Longhorn 1.6+
    原因:
      - 轻量级，易部署
      - UI友好，运维简单
      - 内置备份恢复
      - K3s官方推荐
    规模: 10-100 节点，TB级存储
  
  场景三_高性能应用:
    推荐: OpenEBS 4.0+ (Mayastor)
    原因:
      - NVMe性能优化
      - 用户态驱动 (SPDK)
      - 低延迟 (<100μs)
      - CAS架构
    规模: 性能敏感应用
  
  场景四_边缘计算:
    推荐: Longhorn 1.6+ / Local PV
    原因:
      - 资源占用小
      - 离线自治
      - 轻量级
      - K3s集成
    规模: 边缘节点，单机/小集群
  
  场景五_AI_ML工作负载:
    推荐: OpenEBS + NFS / WekaFS
    原因:
      - 高吞吐量
      - GPU Direct Storage
      - 数据集共享
      - 分布式训练支持
    规模: GPU集群
```

---

## 2. Rook 1.13+ 新特性

### 核心增强

```yaml
Rook_1.13_Features:
  新增功能:
    Ceph_Quincy_17.2+:
      - Crimson OSD (下一代OSD)
      - RBD镜像加密
      - CephFS快照镜像
      - RGW多站点增强
      - 性能提升20-30%
    
    Operator增强:
      - 多集群管理改进
      - 自动化升级策略
      - 健康检查增强
      - 故障自愈改进
      - 资源使用优化
    
    CSI驱动升级:
      - CSI 1.9+支持
      - 卷克隆性能提升
      - 快照恢复加速
      - RWX卷性能优化
      - 加密卷支持
    
    监控可观测性:
      - Prometheus指标增强
      - Grafana仪表板更新
      - Ceph Dashboard集成
      - 告警规则优化
      - 性能分析工具

  性能优化:
    BlueStore优化:
      - RocksDB调优
      - 压缩算法改进
      - 缓存策略优化
      - 内存使用降低
    
    网络性能:
      - RDMA支持增强
      - 多网络接口
      - 流量分离
      - QoS控制
```

### Rook 1.13 部署示例

```yaml
# rook-ceph-cluster-1.13.yaml - 2025年配置
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v17.2.7  # Ceph Quincy
    allowUnsupported: false
  
  dataDirHostPath: /var/lib/rook
  
  # 2025新增：Crimson OSD支持
  experimental:
    crimsonOSD: false  # 实验性功能，高性能OSD
  
  mon:
    count: 3
    allowMultiplePerNode: false
    volumeClaimTemplate:
      spec:
        storageClassName: local-storage
        resources:
          requests:
            storage: 10Gi
  
  mgr:
    count: 2
    modules:
      - name: pg_autoscaler  # 自动PG调整
        enabled: true
      - name: dashboard
        enabled: true
      - name: prometheus
        enabled: true
      - name: telemetry
        enabled: false
  
  # 2025增强：多网络配置
  network:
    provider: host
    connections:
      encryption:
        enabled: true  # 网络加密
      compression:
        enabled: false
    # 分离公共网络和集群网络
    addressRanges:
      public:
        - 10.0.0.0/16
      cluster:
        - 10.1.0.0/16
  
  storage:
    useAllNodes: false
    useAllDevices: false
    nodes:
      - name: "node1"
        devices:
          - name: "nvme0n1"
            config:
              # 2025优化：BlueStore配置
              osdsPerDevice: "1"
              encryptedDevice: "true"  # 设备加密
              metadataDevice: "nvme0n2"  # 独立元数据设备
      - name: "node2"
        devices:
          - name: "nvme0n1"
      - name: "node3"
        devices:
          - name: "nvme0n1"
  
  # 2025新增：资源请求和限制
  resources:
    mon:
      requests:
        cpu: "1000m"
        memory: "2Gi"
      limits:
        memory: "4Gi"
    osd:
      requests:
        cpu: "2000m"
        memory: "4Gi"
      limits:
        memory: "8Gi"
    mgr:
      requests:
        cpu: "1000m"
        memory: "2Gi"
      limits:
        memory: "4Gi"
  
  # 健康检查增强
  healthCheck:
    daemonHealth:
      mon:
        interval: 45s
        timeout: 10s
      osd:
        interval: 60s
        timeout: 10s
      status:
        interval: 60s
  
  # 2025新增：自动化升级策略
  upgradeOSDRequiresHealthyPGs: true
  
  # 2025新增：存储类配置
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: role
              operator: In
              values:
              - storage-node
      tolerations:
      - key: storage-node
        operator: Exists
```

### RBD 加密卷示例 (2025新特性)

```yaml
# rbd-encrypted-storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block-encrypted
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: replicapool
  imageFeatures: layering
  
  # 2025新增：LUKS2加密支持
  encrypted: "true"
  encryptionType: "luks2"
  
  # KMS集成 (Vault/AWS KMS/Azure Key Vault)
  encryptionKMSID: "vault"
  
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  
  csi.storage.k8s.io/fstype: ext4

allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate

---
# KMS Secret配置 (Vault示例)
apiVersion: v1
kind: Secret
metadata:
  name: ceph-csi-kms-config
  namespace: rook-ceph
stringData:
  config.json: |-
    {
      "vault": {
        "VAULT_ADDR": "https://vault.example.com:8200",
        "VAULT_BACKEND_PATH": "rook",
        "VAULT_CACERT": "/etc/vault/ca.crt",
        "VAULT_AUTH_METHOD": "kubernetes",
        "VAULT_AUTH_PATH": "auth/kubernetes/login",
        "VAULT_ROLE": "ceph-csi"
      }
    }
```

---

## 3. Longhorn 1.6+ 新特性

### 核心增强

```yaml
Longhorn_1.6_Features:
  重大新增:
    V2数据引擎:
      - 基于SPDK用户态驱动
      - 性能提升2-3倍
      - 延迟降低50%
      - NVMe原生支持
      - 实验性功能 (1.6开始)
    
    快照增强:
      - 增量快照
      - 快照压缩
      - 快照加密
      - 跨集群快照复制
      - 自动快照清理
    
    备份恢复:
      - S3兼容存储增强
      - 增量备份优化
      - 备份加密
      - 灾难恢复演练
      - 跨区域备份
    
    高可用性:
      - 多副本优化
      - 自动化故障转移
      - 数据重建加速
      - 节点驱逐策略
      - 网络隔离恢复
    
    监控可观测性:
      - Prometheus指标完善
      - Grafana仪表板
      - 卷性能分析
      - 实时告警
      - 审计日志

  性能优化:
    V2引擎特性:
      - SPDK用户态I/O
      - 零拷贝数据路径
      - 批量I/O处理
      - NVMe队列优化
      - CPU亲和性
    
    网络优化:
      - RDMA支持 (实验性)
      - 多路径I/O
      - 网络压缩
      - 流量控制
```

### Longhorn 1.6 部署

```bash
#!/bin/bash
# longhorn-1.6-install.sh - 2025年安装脚本

set -e

echo "=== Longhorn 1.6+ 安装 (2025) ==="

# 1. 前置检查
echo "1. 检查前置条件..."
kubectl get nodes

# 检查依赖
for node in $(kubectl get nodes -o jsonpath='{.items[*].metadata.name}'); do
  echo "检查节点: $node"
  kubectl exec -n kube-system $(kubectl get pods -n kube-system -l component=kube-proxy --field-selector spec.nodeName=$node -o jsonpath='{.items[0].metadata.name}') -- \
    sh -c "command -v iscsiadm && command -v nfs-common || echo '需要安装 open-iscsi nfs-common'"
done

# 2. 添加Longhorn Helm仓库
echo "2. 添加Helm仓库..."
helm repo add longhorn https://charts.longhorn.io
helm repo update

# 3. 创建命名空间
kubectl create namespace longhorn-system --dry-run=client -o yaml | kubectl apply -f -

# 4. 安装Longhorn 1.6+ (2025配置)
echo "3. 安装Longhorn 1.6+..."
helm install longhorn longhorn/longhorn \
  --namespace longhorn-system \
  --version "1.6.0" \
  --set defaultSettings.backupTarget="s3://longhorn-backup@us-east-1/" \
  --set defaultSettings.backupTargetCredentialSecret="aws-secret" \
  --set defaultSettings.defaultDataPath="/var/lib/longhorn/" \
  --set defaultSettings.defaultReplicaCount="3" \
  --set defaultSettings.guaranteedInstanceManagerCPU="10" \
  --set defaultSettings.createDefaultDiskLabeledNodes="true" \
  --set defaultSettings.storageMinimalAvailablePercentage="10" \
  --set defaultSettings.storageOverProvisioningPercentage="200" \
  --set defaultSettings.upgradeChecker="false" \
  --set defaultSettings.nodeDownPodDeletionPolicy="delete-both-statefulset-and-deployment-pod" \
  --set defaultSettings.autoSalvage="true" \
  --set defaultSettings.autoDeletePodWhenVolumeDetachedUnexpectedly="true" \
  --set defaultSettings.disableSchedulingOnCordonedNode="true" \
  --set defaultSettings.replicaSoftAntiAffinity="false" \
  --set defaultSettings.systemManagedComponentsNodeSelector="node-role.kubernetes.io/storage:true" \
  --set defaultSettings.v2DataEngine="true" \
  --set persistence.defaultClass="true" \
  --set persistence.defaultClassReplicaCount="3" \
  --set csi.kubeletRootDir="/var/lib/kubelet" \
  --set defaultSettings.snapshotMaxCount="25" \
  --set defaultSettings.backupCompressionMethod="lz4" \
  --set defaultSettings.backupConcurrentLimit="5" \
  --set defaultSettings.restoreConcurrentLimit="5"

# 5. 等待部署完成
echo "4. 等待Longhorn部署完成..."
kubectl -n longhorn-system rollout status deployment/longhorn-driver-deployer
kubectl -n longhorn-system rollout status daemonset/longhorn-manager

# 6. 验证安装
echo "5. 验证安装..."
kubectl -n longhorn-system get pods
kubectl -n longhorn-system get storageclass

# 7. 访问UI
echo "6. Longhorn UI访问方式:"
echo "   kubectl -n longhorn-system port-forward svc/longhorn-frontend 8080:80"
echo "   浏览器访问: http://localhost:8080"

echo ""
echo "=== Longhorn 1.6安装完成 ==="
```

### V2数据引擎配置 (2025实验性功能)

```yaml
# longhorn-v2-engine-storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-v2-fast
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "30"
  fromBackup: ""
  fsType: "ext4"
  
  # 2025新增：V2数据引擎 (SPDK)
  dataEngine: "v2"
  
  # V2引擎参数
  diskSelector: "ssd,nvme"  # 仅使用NVMe设备
  nodeSelector: "storage-node"
  
  # 性能优化
  unmapMarkSnapChainRemoved: "enabled"
  disableRevisionCounter: "false"
  
  # 快照配置
  recurringJobSelector: '[
    {
      "name": "snapshot-daily",
      "isGroup": false
    }
  ]'

---
# RecurringJob for automated snapshots
apiVersion: longhorn.io/v1beta2
kind: RecurringJob
metadata:
  name: snapshot-daily
  namespace: longhorn-system
spec:
  cron: "0 2 * * *"  # 每天凌晨2点
  task: "snapshot"
  groups:
    - default
  retain: 7  # 保留7天
  concurrency: 5
  labels:
    type: snapshot
```

### Longhorn 备份到S3 (2025最佳实践)

```yaml
# longhorn-backup-s3-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: aws-secret
  namespace: longhorn-system
type: Opaque
stringData:
  AWS_ACCESS_KEY_ID: "YOUR_ACCESS_KEY"
  AWS_SECRET_ACCESS_KEY: "YOUR_SECRET_KEY"
  AWS_ENDPOINTS: "https://s3.us-east-1.amazonaws.com"
  # 或使用MinIO/其他S3兼容存储
  # AWS_ENDPOINTS: "https://minio.example.com"

---
# BackupTarget配置
apiVersion: longhorn.io/v1beta2
kind: Setting
metadata:
  name: backup-target
  namespace: longhorn-system
value: "s3://longhorn-backups@us-east-1/"

---
apiVersion: longhorn.io/v1beta2
kind: Setting
metadata:
  name: backup-target-credential-secret
  namespace: longhorn-system
value: "aws-secret"

---
# 2025新增：备份策略
apiVersion: longhorn.io/v1beta2
kind: RecurringJob
metadata:
  name: backup-daily
  namespace: longhorn-system
spec:
  cron: "0 3 * * *"  # 每天凌晨3点
  task: "backup"
  groups:
    - default
  retain: 30  # 保留30天
  concurrency: 3
  labels:
    type: backup
    encrypted: "true"  # 加密备份

---
# 灾难恢复演练Job
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dr-drill
  namespace: longhorn-system
spec:
  schedule: "0 0 1 * *"  # 每月1号
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: longhorn-dr
          containers:
          - name: dr-drill
            image: longhornio/longhorn-manager:v1.6.0
            command: ["/bin/sh"]
            args:
              - -c
              - |
                # 验证备份可用性
                longhorn-manager backup list --backup-target s3://longhorn-backups@us-east-1/
                # 测试恢复 (到测试命名空间)
                # ...
          restartPolicy: OnFailure
```

---

## 4. OpenEBS 4.0+ 云原生存储

### 核心架构

```yaml
OpenEBS_4.0_Architecture:
  数据引擎:
    Mayastor_2.0+:
      - NVMe-oF原生
      - SPDK用户态驱动
      - 超低延迟 (<100μs)
      - 高IOPS (>1M)
      - 适用: 高性能应用、数据库
    
    cStor:
      - ZFS存储
      - 快照克隆
      - 数据完整性
      - 适用: 通用应用
    
    Jiva:
      - 轻量级
      - 易部署
      - 适用: 小规模
    
    Local_PV:
      - 本地存储
      - 最高性能
      - 适用: StatefulSet

  CAS架构:
    Container_Attached_Storage:
      优势:
        - 容器粒度存储
        - 微服务化
        - 自动化运维
        - 云原生设计
      
      组件:
        - Control Plane: Kubernetes原生
        - Data Plane: 每个卷独立容器
        - 存储策略: CRD定义

  2025新特性:
    Mayastor_2.0:
      - NVMe-oF TCP/RDMA
      - io_uring异步I/O
      - 快照增强
      - 副本重建优化
      - HA拓扑感知
    
    监控可观测性:
      - Prometheus导出器
      - Grafana仪表板
      - 实时性能分析
      - 容量规划
```

### OpenEBS Mayastor 2.0 部署

```bash
#!/bin/bash
# openebs-mayastor-2.0-install.sh

set -e

echo "=== OpenEBS Mayastor 2.0 安装 (2025) ==="

# 1. 前置检查
echo "1. 检查前置条件..."

# 检查内核版本 (需要 >= 5.10)
uname -r

# 检查Hugepages配置
echo "检查Hugepages..."
grep HugePages /proc/meminfo

# 如果未配置，设置Hugepages (每个存储节点)
for node in $(kubectl get nodes -l openebs.io/data-plane=true -o jsonpath='{.items[*].metadata.name}'); do
  echo "配置节点: $node"
  kubectl label node $node openebs.io/engine=mayastor --overwrite
  
  # 通过DaemonSet配置Hugepages
  kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: mayastor-hugepages-setup
  namespace: openebs
spec:
  selector:
    matchLabels:
      app: mayastor-hugepages
  template:
    metadata:
      labels:
        app: mayastor-hugepages
    spec:
      nodeSelector:
        openebs.io/engine: mayastor
      hostPID: true
      hostNetwork: true
      containers:
      - name: setup
        image: alpine:3.18
        command:
          - sh
          - -c
          - |
            echo 1024 > /proc/sys/vm/nr_hugepages
            sleep infinity
        securityContext:
          privileged: true
EOF
done

# 2. 安装OpenEBS Operator
echo "2. 安装OpenEBS Operator..."
kubectl create namespace openebs --dry-run=client -o yaml | kubectl apply -f -

helm repo add openebs https://openebs.github.io/charts
helm repo update

# 安装openebs-operator
helm install openebs openebs/openebs \
  --namespace openebs \
  --set engines.replicated.mayastor.enabled=true \
  --set mayastor.csi.node.kubeletDir="/var/lib/kubelet" \
  --set mayastor.etcd.replicaCount=3

# 3. 创建Mayastor存储池
echo "3. 创建Mayastor DiskPool..."

# 为每个存储节点创建DiskPool
kubectl apply -f - <<EOF
apiVersion: openebs.io/v1beta2
kind: DiskPool
metadata:
  name: pool-on-node1
  namespace: openebs
spec:
  node: node1  # 存储节点名称
  disks:
    - /dev/nvme0n1  # NVMe设备路径
---
apiVersion: openebs.io/v1beta2
kind: DiskPool
metadata:
  name: pool-on-node2
  namespace: openebs
spec:
  node: node2
  disks:
    - /dev/nvme0n1
---
apiVersion: openebs.io/v1beta2
kind: DiskPool
metadata:
  name: pool-on-node3
  namespace: openebs
spec:
  node: node3
  disks:
    - /dev/nvme0n1
EOF

# 4. 验证DiskPool
echo "4. 验证DiskPool状态..."
kubectl -n openebs get diskpool

# 5. 创建StorageClass
echo "5. 创建高性能StorageClass..."
kubectl apply -f - <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: mayastor-nvme-3replica
parameters:
  protocol: nvmf
  repl: "3"  # 3副本
  ioTimeout: "60"
  # 2025新增：拓扑感知
  thin: "false"
provisioner: io.openebs.csi-mayastor
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
EOF

# 6. 性能测试
echo "6. 运行性能测试..."
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mayastor-test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: mayastor-nvme-3replica
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: fio-test
spec:
  containers:
  - name: fio
    image: ljishen/fio:latest
    command:
      - sh
      - -c
      - |
        fio --name=randwrite --ioengine=libaio --iodepth=32 --rw=randwrite \\
            --bs=4k --direct=1 --size=1G --numjobs=4 --runtime=60 \\
            --group_reporting --filename=/data/test
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: mayastor-test-pvc
EOF

echo "=== OpenEBS Mayastor 2.0 安装完成 ==="
echo "查看DiskPool: kubectl -n openebs get diskpool"
echo "查看StorageClass: kubectl get sc"
```

### Mayastor 性能优化配置

```yaml
# mayastor-performance-tuning.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mayastor-tuning
  namespace: openebs
data:
  # I/O路径优化
  io-engine.yaml: |
    # io_uring队列深度
    io_uring_queue_depth: 512
    
    # CPU亲和性
    core_mask: "0x0F"  # 使用CPU 0-3
    
    # 内存
    mem_size: 4096  # MB
    hugepage_size: 2048  # KB
    
    # NVMe-oF配置
    nvmf:
      max_queue_depth: 128
      num_io_queues: 4
      tcp_nodelay: true

---
# DaemonSet应用性能调优
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: mayastor-io-engine-tuned
  namespace: openebs
spec:
  selector:
    matchLabels:
      app: mayastor-io-engine
  template:
    metadata:
      labels:
        app: mayastor-io-engine
    spec:
      nodeSelector:
        openebs.io/engine: mayastor
      hostNetwork: true
      containers:
      - name: io-engine
        image: openebs/mayastor-io-engine:v2.0.0
        env:
          - name: MAYASTOR_HPA_SIZE
            value: "4096Mi"
          - name: MAYASTOR_CORE_MASK
            value: "0x0F"
          - name: MAYASTOR_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
        resources:
          requests:
            cpu: "4"
            memory: "8Gi"
            hugepages-2Mi: "4Gi"
          limits:
            cpu: "8"
            memory: "16Gi"
            hugepages-2Mi: "4Gi"
        securityContext:
          privileged: true
        volumeMounts:
          - name: device
            mountPath: /dev
          - name: hugepages
            mountPath: /dev/hugepages
      volumes:
        - name: device
          hostPath:
            path: /dev
        - name: hugepages
          emptyDir:
            medium: HugePages
```

---

## 5. CSI 1.9+ 新规范

### 核心新增功能

```yaml
CSI_1.9_Features:
  卷组快照:
    VolumeGroupSnapshot:
      用途: 一致性快照多个卷
      场景: 数据库集群、应用一致性备份
      支持: Rook 1.13+, Longhorn 1.6+
      
      示例:
        - 一次性快照多个PV
        - 保证数据一致性
        - 简化备份流程
  
  跨命名空间卷克隆:
    CrossNamespaceVolumeClone:
      用途: 跨命名空间克隆卷
      场景: 环境复制、数据共享
      限制: 需要RBAC授权
  
  卷健康检查_GA:
    VolumeHealthCheck:
      用途: 实时监控卷健康状态
      指标:
        - 磁盘状态
        - 性能指标
        - 错误率
      告警: 自动触发
  
  卷修改:
    VolumeModification:
      用途: 在线修改卷属性
      支持:
        - IOPS调整
        - 吞吐量变更
        - QoS策略
      无需重启: 热修改

  CSI_Addons:
    功能扩展:
      - 卷复制 (Replication)
      - 卷加密 (Encryption)
      - 卷监控 (Monitoring)
      - 网络隔离 (NetworkFencing)
```

### VolumeGroupSnapshot 示例 (2025新特性)

```yaml
# volumegroupsnapshot-example.yaml
# 场景：一致性快照MySQL主从库

# 1. VolumeGroupSnapshotClass
apiVersion: groupsnapshot.storage.k8s.io/v1alpha1
kind: VolumeGroupSnapshotClass
metadata:
  name: csi-group-snapclass
driver: rook-ceph.rbd.csi.ceph.com
deletionPolicy: Delete
parameters:
  clusterID: rook-ceph
  csi.storage.k8s.io/snapshotter-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/snapshotter-secret-namespace: rook-ceph

---
# 2. MySQL StatefulSet (主从)
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: database
spec:
  serviceName: mysql
  replicas: 2
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: mysql
        snapshot-group: mysql-cluster
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: rook-ceph-block
      resources:
        requests:
          storage: 10Gi

---
# 3. VolumeGroupSnapshot (一致性快照)
apiVersion: groupsnapshot.storage.k8s.io/v1alpha1
kind: VolumeGroupSnapshot
metadata:
  name: mysql-consistent-snapshot
  namespace: database
spec:
  volumeGroupSnapshotClassName: csi-group-snapclass
  source:
    selector:
      matchLabels:
        app: mysql
        snapshot-group: mysql-cluster

---
# 4. CronJob自动化快照
apiVersion: batch/v1
kind: CronJob
metadata:
  name: mysql-group-snapshot
  namespace: database
spec:
  schedule: "0 */6 * * *"  # 每6小时
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: snapshot-controller
          containers:
          - name: snapshot
            image: bitnami/kubectl:latest
            command:
              - /bin/bash
              - -c
              - |
                TIMESTAMP=$(date +%Y%m%d-%H%M%S)
                kubectl apply -f - <<EOF
                apiVersion: groupsnapshot.storage.k8s.io/v1alpha1
                kind: VolumeGroupSnapshot
                metadata:
                  name: mysql-snapshot-${TIMESTAMP}
                  namespace: database
                spec:
                  volumeGroupSnapshotClassName: csi-group-snapclass
                  source:
                    selector:
                      matchLabels:
                        app: mysql
                        snapshot-group: mysql-cluster
                EOF
                
                # 清理旧快照 (保留最近7个)
                kubectl get volumegroupsnapshot -n database \
                  --sort-by=.metadata.creationTimestamp \
                  -o jsonpath='{range .items[:-7]}{.metadata.name}{"\n"}{end}' | \
                  xargs -r kubectl delete volumegroupsnapshot -n database
          restartPolicy: OnFailure
```

### 卷健康检查 (CSI 1.9 GA)

```yaml
# volume-health-monitor.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: csi-volume-health
  namespace: kube-system
data:
  config.yaml: |
    # CSI卷健康检查配置
    health_check_interval: "60s"
    
    metrics:
      - name: volume_abnormal
        type: gauge
        help: "Volume abnormal condition"
      
      - name: volume_health_status
        type: gauge
        labels: ["pv_name", "namespace", "pvc_name"]
        help: "Volume health status (0=unknown, 1=healthy, 2=degraded, 3=unhealthy)"
      
      - name: volume_io_errors
        type: counter
        help: "Volume I/O errors count"
    
    alerts:
      - name: VolumeUnhealthy
        condition: volume_health_status == 3
        severity: critical
        message: "Volume {{ $labels.pv_name }} is unhealthy"
      
      - name: VolumeDegraded
        condition: volume_health_status == 2
        duration: "5m"
        severity: warning
        message: "Volume {{ $labels.pv_name }} is degraded"

---
# Prometheus告警规则
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-volume-health-rules
  namespace: monitoring
data:
  volume-health.yaml: |
    groups:
    - name: volume_health
      interval: 30s
      rules:
      - alert: PersistentVolumeUnhealthy
        expr: csi_volume_health_status{status="unhealthy"} == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "PV {{ $labels.pv_name }} is unhealthy"
          description: "Volume has been unhealthy for more than 5 minutes"
      
      - alert: PersistentVolumeDegraded
        expr: csi_volume_health_status{status="degraded"} == 1
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "PV {{ $labels.pv_name }} is degraded"
          description: "Volume performance is degraded"
      
      - alert: VolumeHighIOErrors
        expr: rate(csi_volume_io_errors_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High I/O error rate on {{ $labels.pv_name }}"
          description: "I/O error rate: {{ $value }} errors/sec"
```

---

## 6. 存储性能优化 2025

### NVMe优化

```yaml
Storage_Performance_2025:
  NVMe优化:
    设备选择:
      - NVMe SSD (优于SATA SSD)
      - 企业级NVMe (高耐久度)
      - PCIe 4.0/5.0
      - NVMe-oF网络存储
    
    内核参数:
      io_scheduler: none  # NVMe不需要调度器
      nr_requests: 256
      queue_depth: 256
      add_random: 0
    
    文件系统:
      推荐: XFS, ext4
      挂载选项:
        - noatime
        - nodiratime
        - discard  # TRIM支持
  
  网络优化:
    RDMA_RoCE:
      - 低延迟 (<5μs)
      - 高带宽 (100Gbps+)
      - CPU卸载
      - 零拷贝
    
    网卡配置:
      - 巨型帧 (MTU 9000)
      - RSS (Receive Side Scaling)
      - TSO/GSO
      - 中断合并
  
  应用层优化:
    数据库:
      - Direct I/O
      - 异步I/O
      - 批量提交
      - WAL优化
    
    AI_ML:
      - GPU Direct Storage
      - 流式I/O
      - 预读优化
      - 数据局部性
```

### 性能调优脚本

```bash
#!/bin/bash
# storage-performance-tuning.sh - 2025年存储性能优化

set -e

echo "=== 存储性能优化 2025 ==="

# 1. NVMe设备优化
optimize_nvme() {
    local device=$1
    echo "优化NVMe设备: $device"
    
    # 禁用调度器
    echo none > /sys/block/$device/queue/scheduler
    
    # 队列深度
    echo 256 > /sys/block/$device/queue/nr_requests
    
    # 预读
    echo 256 > /sys/block/$device/queue/read_ahead_kb
    
    # NUMA绑定
    echo 0 > /sys/block/$device/queue/add_random
    
    # 合并请求
    echo 2 > /sys/block/$device/queue/nomerges
}

# 遍历所有NVMe设备
for nvme in $(ls /sys/block/ | grep nvme); do
    optimize_nvme $nvme
done

# 2. 内核参数优化
cat > /etc/sysctl.d/99-storage-performance.conf <<EOF
# 虚拟内存
vm.dirty_ratio = 15
vm.dirty_background_ratio = 5
vm.swappiness = 10
vm.vfs_cache_pressure = 50

# 网络缓冲区
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 4096 87380 67108864
net.ipv4.tcp_wmem = 4096 65536 67108864

# 网络优化
net.ipv4.tcp_congestion_control = bbr
net.core.default_qdisc = fq
net.ipv4.tcp_mtu_probing = 1

# 文件描述符
fs.file-max = 2097152
fs.nr_open = 2097152
EOF

sysctl -p /etc/sysctl.d/99-storage-performance.conf

# 3. RDMA/RoCE配置 (如果支持)
if lsmod | grep -q rdma; then
    echo "配置RDMA..."
    
    # 加载内核模块
    modprobe ib_core
    modprobe ib_uverbs
    modprobe rdma_ucm
    
    # 设置网卡MTU
    for iface in $(ibdev2netdev | awk '{print $5}'); do
        ip link set $iface mtu 9000
    done
fi

# 4. 巨型帧配置
configure_jumbo_frames() {
    for iface in $(ip -o link show | awk -F': ' '{print $2}' | grep -E '^(eth|ens|enp)'); do
        echo "配置巨型帧: $iface"
        ip link set $iface mtu 9000 || true
    done
}

configure_jumbo_frames

# 5. CPU性能模式
echo "设置CPU性能模式..."
for cpu in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; do
    echo performance > $cpu 2>/dev/null || true
done

# 6. 中断亲和性
set_irq_affinity() {
    local device=$1
    local cpus=$2
    
    for irq in $(grep $device /proc/interrupts | awk '{print $1}' | tr -d ':'); do
        echo $cpus > /proc/irq/$irq/smp_affinity
    done
}

# 为NVMe设备设置中断亲和性 (绑定到特定CPU)
for nvme in $(lsblk -d -n -o NAME | grep nvme); do
    set_irq_affinity $nvme "f"  # 使用CPU 0-3
done

echo "=== 存储性能优化完成 ==="

# 7. 性能测试
echo ""
echo "运行性能测试..."
fio --name=randread --ioengine=libaio --iodepth=32 --rw=randread \
    --bs=4k --direct=1 --size=1G --numjobs=4 --runtime=30 \
    --group_reporting --filename=/dev/nvme0n1 --output=fio-results.txt

echo "性能测试结果已保存到 fio-results.txt"
```

### GPU Direct Storage (GDS) 配置

```yaml
# gpu-direct-storage-config.yaml
# AI/ML工作负载的存储优化

apiVersion: v1
kind: ConfigMap
metadata:
  name: gds-config
  namespace: gpu-workloads
data:
  gds.conf: |
    # GPU Direct Storage配置
    [gds]
    enable = true
    
    # 直通路径
    direct_io = true
    bypass_page_cache = true
    
    # RDMA配置
    rdma_enabled = true
    rdma_device = mlx5_0
    
    # 性能调优
    batch_size = 64MB
    num_threads = 8
    prefetch_size = 256MB

---
# StorageClass for AI/ML workloads
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nvme-gds-optimized
provisioner: io.openebs.csi-mayastor
parameters:
  protocol: nvmf
  repl: "3"
  
  # GDS优化
  mount_options: "noatime,nodiratime,discard"
  filesystem: "xfs"
  
  # 高性能配置
  thin: "false"
  ioTimeout: "30"
  
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer

---
# PyTorch DDP训练Job with GDS
apiVersion: batch/v1
kind: Job
metadata:
  name: pytorch-ddp-gds
  namespace: gpu-workloads
spec:
  parallelism: 4
  completions: 4
  template:
    metadata:
      labels:
        app: pytorch-ddp
    spec:
      containers:
      - name: pytorch
        image: nvcr.io/nvidia/pytorch:23.10-py3
        command:
          - python
          - -m
          - torch.distributed.launch
          - --nproc_per_node=4
          - train.py
          - --data-path=/data
          - --gds-enabled  # 启用GDS
        resources:
          requests:
            nvidia.com/gpu: 4
            memory: 64Gi
            cpu: 16
          limits:
            nvidia.com/gpu: 4
            memory: 128Gi
        volumeMounts:
        - name: dataset
          mountPath: /data
        - name: gds-config
          mountPath: /etc/gds
      volumes:
      - name: dataset
        persistentVolumeClaim:
          claimName: imagenet-dataset-pvc
      - name: gds-config
        configMap:
          name: gds-config
      nodeSelector:
        nvidia.com/gpu.present: "true"
        storage-type: nvme
```

---

## 7. 存储安全增强

### 加密卷实现

```yaml
Storage_Security_2025:
  加密技术:
    LUKS2:
      - Linux统一密钥设置
      - 全盘加密
      - 多密钥支持
      - 性能影响: <5%
    
    KMS集成:
      - HashiCorp Vault
      - AWS KMS
      - Azure Key Vault
      - Google Cloud KMS
    
    加密层级:
      - 存储池加密
      - 卷级加密
      - 文件系统加密
      - 对象加密

  访问控制:
    RBAC:
      - 细粒度权限
      - 命名空间隔离
      - PVC访问控制
      - StorageClass权限
    
    Pod_Security:
      - 只读根文件系统
      - 非特权容器
      - securityContext
      - SELinux/AppArmor

  审计:
    审计日志:
      - 卷创建/删除
      - 快照操作
      - 访问记录
      - 失败尝试
```

### KMS集成示例 (Vault)

```yaml
# vault-kms-integration.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: csi-vault-sa
  namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: csi-vault-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: csi-vault-sa
  namespace: kube-system

---
# Vault配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: vault-kms-config
  namespace: kube-system
data:
  vault.conf: |
    vault {
      address = "https://vault.example.com:8200"
      auth {
        method = "kubernetes"
        path   = "auth/kubernetes/login"
        role   = "csi-encryption"
      }
      
      # 密钥路径
      transit {
        mount_path = "transit"
        key_name   = "csi-encryption-key"
      }
      
      # TLS配置
      tls {
        ca_cert = "/etc/vault/ca.crt"
        skip_verify = false
      }
    }

---
# StorageClass with encryption
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: encrypted-storage
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: encrypted-pool
  
  # 加密配置
  encrypted: "true"
  encryptionKMSID: "vault-kms"
  encryptionType: "luks2"
  
  # Vault KMS配置
  encryptionKMSType: "vaulttokens"
  vaultAddress: "https://vault.example.com:8200"
  vaultAuthPath: "/v1/auth/kubernetes/login"
  vaultRole: "csi-encryption"
  vaultNamespace: ""
  vaultCAFromSecret: "vault-ca"
  
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
```

---

## 8. 多集群存储联邦

### 跨集群存储复制

```yaml
Multi_Cluster_Storage_2025:
  架构模式:
    主备模式:
      - 主集群：生产环境
      - 备集群：灾难恢复
      - 单向复制
      - 自动故障转移
    
    多主模式:
      - 多个活跃集群
      - 双向复制
      - 就近访问
      - 负载均衡
    
    分布式模式:
      - 地理分布
      - 数据分片
      - 一致性哈希
      - 跨域访问

  复制方案:
    Rook_Ceph:
      - RBD镜像 (mirroring)
      - CephFS快照镜像
      - RGW多站点
      - 异步/同步复制
    
    Longhorn:
      - 远程备份
      - S3同步
      - 灾难恢复
      - 跨区域备份
    
    Velero:
      - 集群级备份
      - 应用迁移
      - 跨云复制
      - 定期备份
```

### Rook-Ceph 跨集群镜像

```yaml
# rbd-mirroring-cluster-a.yaml (主集群)
apiVersion: ceph.rook.io/v1
kind: CephRBDMirror
metadata:
  name: my-rbd-mirror
  namespace: rook-ceph
spec:
  count: 2
  peers:
    secretNames:
      - rbd-mirror-peer-secret  # 对端集群凭证
  
  placement:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: role
            operator: In
            values:
            - storage-node
  
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      memory: "4Gi"

---
# Secret for peer cluster (集群B信息)
apiVersion: v1
kind: Secret
metadata:
  name: rbd-mirror-peer-secret
  namespace: rook-ceph
type: Opaque
stringData:
  # 对端集群监控地址
  mon_host: "10.20.30.40:6789,10.20.30.41:6789,10.20.30.42:6789"
  # 对端集群名称
  cluster: "cluster-b"
  # 对端集群密钥
  key: "AQC...=="  # ceph auth get client.rbd-mirror 获取

---
# 启用镜像的存储池
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: mirrored-pool
  namespace: rook-ceph
spec:
  replicated:
    size: 3
    requireSafeReplicaSize: true
  
  # 启用RBD镜像
  mirroring:
    enabled: true
    mode: "image"  # 或 "pool" (全池镜像)
    
    # 2025新增：快照镜像
    snapshotSchedules:
      - interval: "24h"  # 每天一次
        startTime: "02:00:00Z"

---
# StorageClass for mirrored volumes
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block-mirrored
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: mirrored-pool
  imageFeatures: layering
  
  # 自动启用镜像
  imageFeatures: "layering,exclusive-lock,object-map,fast-diff,journaling"
  
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

allowVolumeExpansion: true
reclaimPolicy: Retain  # 重要：保留数据
volumeBindingMode: Immediate
```

### Longhorn 跨集群备份

```yaml
# longhorn-dr-volume.yaml
apiVersion: longhorn.io/v1beta2
kind: Volume
metadata:
  name: critical-data-volume
  namespace: longhorn-system
spec:
  size: "100Gi"
  numberOfReplicas: 3
  dataLocality: "best-effort"
  
  # 2025新增：灾难恢复配置
  dr:
    enabled: true
    remoteBackupTarget: "s3://longhorn-dr@us-west-2/"  # 不同区域
    scheduledBackup:
      cron: "0 */6 * * *"  # 每6小时
      retain: 14  # 保留14个备份
    
    # 故障转移设置
    failover:
      autoPromote: true  # 自动提升备份为主卷
      healthCheckInterval: "5m"
      unhealthyThreshold: 3

---
# RecurringJob for DR备份
apiVersion: longhorn.io/v1beta2
kind: RecurringJob
metadata:
  name: dr-backup-6h
  namespace: longhorn-system
spec:
  cron: "0 */6 * * *"
  task: "backup"
  groups:
    - dr-critical
  retain: 28  # 保留28个 = 7天
  concurrency: 2
  labels:
    disaster-recovery: "true"
    tier: "critical"

---
# 灾难恢复演练CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dr-drill-monthly
  namespace: longhorn-system
spec:
  schedule: "0 0 1 * *"  # 每月1号
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: longhorn-dr
          containers:
          - name: dr-drill
            image: longhornio/longhorn-manager:v1.6.0
            command:
              - /bin/bash
              - -c
              - |
                echo "=== 灾难恢复演练 ==="
                
                # 1. 列出所有DR备份
                longhorn-manager backup list --backup-target s3://longhorn-dr@us-west-2/
                
                # 2. 选择最新的critical备份
                LATEST_BACKUP=$(longhorn-manager backup list \
                  --backup-target s3://longhorn-dr@us-west-2/ \
                  --volume critical-data-volume \
                  --output json | jq -r '.[0].name')
                
                # 3. 在DR集群创建测试卷
                kubectl apply -f - <<EOF
                apiVersion: longhorn.io/v1beta2
                kind: Volume
                metadata:
                  name: dr-test-$(date +%Y%m%d)
                  namespace: longhorn-system
                spec:
                  fromBackup: "s3://longhorn-dr@us-west-2/${LATEST_BACKUP}"
                  numberOfReplicas: 1
                EOF
                
                # 4. 验证数据完整性
                # ... (应用特定的验证逻辑)
                
                # 5. 清理测试资源
                kubectl delete volume dr-test-$(date +%Y%m%d) -n longhorn-system
                
                echo "=== 演练完成 ==="
          restartPolicy: OnFailure
```

---

## 9. 存储成本优化 (FinOps)

### 成本分析

```yaml
Storage_FinOps_2025:
  成本维度:
    存储容量:
      - 已使用容量
      - 已分配容量
      - 超额分配率
      - 增长趋势
    
    性能层级:
      - 热数据 (NVMe)
      - 温数据 (SSD)
      - 冷数据 (HDD)
      - 归档数据 (S3/Glacier)
    
    副本策略:
      - 3副本 (高可用)
      - 2副本 (平衡)
      - EC编码 (省空间)
      - 单副本 (非关键)

  优化策略:
    自动分层:
      - 访问频率监控
      - 自动数据迁移
      - 冷热数据分离
      - 成本降低50-70%
    
    容量回收:
      - 未使用PVC清理
      - 孤儿卷删除
      - 快照过期清理
      - 空间压缩
    
    按需分配:
      - Thin Provisioning
      - 动态扩容
      - 避免过度分配
      - 使用率监控

  成本监控工具:
    Kubecost:
      - 存储成本分析
      - 按命名空间/应用
      - 趋势预测
      - 优化建议
    
    OpenCost:
      - CNCF开源项目
      - 多云成本
      - 实时监控
      - API集成
```

### Kubecost 存储监控部署

```bash
#!/bin/bash
# kubecost-storage-monitoring.sh

set -e

echo "=== 部署Kubecost存储成本监控 ==="

# 1. 添加Helm仓库
helm repo add kubecost https://kubecost.github.io/cost-analyzer/
helm repo update

# 2. 部署Kubecost
kubectl create namespace kubecost --dry-run=client -o yaml | kubectl apply -f -

helm install kubecost kubecost/cost-analyzer \
  --namespace kubecost \
  --set kubecostToken="your-token-here" \
  --set prometheus.server.persistentVolume.enabled=true \
  --set prometheus.server.persistentVolume.size=32Gi \
  --set persistentVolume.enabled=true \
  --set persistentVolume.size=32Gi \
  --set ingress.enabled=true \
  --set ingress.hosts[0]=kubecost.example.com \
  --set networkPolicy.enabled=false \
  --set global.prometheus.enabled=true \
  --set global.prometheus.fqdn=http://prometheus-server.prometheus.svc.cluster.local

# 3. 配置存储成本
kubectl apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: storage-cost-config
  namespace: kubecost
data:
  pricing.json: |
    {
      "storage": {
        "classes": {
          "rook-ceph-block": {
            "cost_per_gb_month": 0.10,
            "performance_tier": "high",
            "notes": "Rook-Ceph SSD storage"
          },
          "longhorn": {
            "cost_per_gb_month": 0.08,
            "performance_tier": "medium",
            "notes": "Longhorn distributed storage"
          },
          "nfs-client": {
            "cost_per_gb_month": 0.05,
            "performance_tier": "low",
            "notes": "NFS shared storage"
          },
          "local-path": {
            "cost_per_gb_month": 0.03,
            "performance_tier": "local",
            "notes": "Local node storage"
          }
        },
        "snapshots": {
          "cost_per_gb_month": 0.02
        },
        "backups": {
          "s3_standard": 0.023,
          "s3_ia": 0.0125,
          "s3_glacier": 0.004
        }
      }
    }
EOF

# 4. 存储成本分析Job
kubectl apply -f - <<EOF
apiVersion: batch/v1
kind: CronJob
metadata:
  name: storage-cost-report
  namespace: kubecost
spec:
  schedule: "0 0 * * 0"  # 每周日
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: kubecost
          containers:
          - name: cost-report
            image: curlimages/curl:latest
            command:
              - /bin/sh
              - -c
              - |
                # 调用Kubecost API生成报告
                curl -X GET "http://kubecost-cost-analyzer.kubecost:9090/model/allocation" \
                  -d window=7d \
                  -d aggregate=storageclass \
                  -d accumulate=true \
                  -d shareIdle=false \
                  | jq '.' > /tmp/storage-cost-report.json
                
                # 发送到Slack/Email
                # ...
          restartPolicy: OnFailure
EOF

# 5. 访问Kubecost UI
echo ""
echo "=== Kubecost部署完成 ==="
echo "访问UI: kubectl port-forward -n kubecost svc/kubecost-cost-analyzer 9090:9090"
echo "浏览器: http://localhost:9090"
```

### 自动清理未使用存储

```yaml
# storage-cleanup-automation.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: storage-cleanup
  namespace: kube-system
spec:
  schedule: "0 2 * * *"  # 每天凌晨2点
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: storage-admin
          containers:
          - name: cleanup
            image: bitnami/kubectl:latest
            command:
              - /bin/bash
              - -c
              - |
                echo "=== 存储清理任务 ==="
                
                # 1. 清理未绑定PVC (超过7天)
                echo "1. 清理未绑定PVC..."
                kubectl get pvc --all-namespaces -o json | \
                jq -r '.items[] | 
                  select(.status.phase=="Pending") | 
                  select((now - (.metadata.creationTimestamp | fromdateiso8601)) > 604800) | 
                  "\(.metadata.namespace) \(.metadata.name)"' | \
                while read ns name; do
                  echo "删除未绑定PVC: $ns/$name"
                  kubectl delete pvc $name -n $ns
                done
                
                # 2. 清理孤儿PV (Released状态超过3天)
                echo "2. 清理孤儿PV..."
                kubectl get pv -o json | \
                jq -r '.items[] | 
                  select(.status.phase=="Released") | 
                  select((now - (.metadata.creationTimestamp | fromdateiso8601)) > 259200) | 
                  .metadata.name' | \
                while read pv; do
                  echo "删除孤儿PV: $pv"
                  kubectl delete pv $pv
                done
                
                # 3. 清理过期快照 (超过30天)
                echo "3. 清理过期快照..."
                kubectl get volumesnapshot --all-namespaces -o json | \
                jq -r '.items[] | 
                  select(.metadata.labels.auto=="true") | 
                  select((now - (.metadata.creationTimestamp | fromdateiso8601)) > 2592000) | 
                  "\(.metadata.namespace) \(.metadata.name)"' | \
                while read ns name; do
                  echo "删除过期快照: $ns/$name"
                  kubectl delete volumesnapshot $name -n $ns
                done
                
                # 4. Longhorn卷清理 (未使用)
                echo "4. 清理Longhorn未使用卷..."
                kubectl -n longhorn-system get volumes.longhorn.io -o json | \
                jq -r '.items[] | 
                  select(.status.state=="detached") | 
                  select(.status.kubernetesStatus.pvStatus=="" or .status.kubernetesStatus.pvStatus==null) | 
                  .metadata.name' | \
                while read vol; do
                  echo "删除未使用Longhorn卷: $vol"
                  kubectl -n longhorn-system delete volumes.longhorn.io $vol
                done
                
                # 5. 生成清理报告
                echo ""
                echo "=== 清理统计 ==="
                echo "PVC数量: $(kubectl get pvc --all-namespaces --no-headers | wc -l)"
                echo "PV数量: $(kubectl get pv --no-headers | wc -l)"
                echo "快照数量: $(kubectl get volumesnapshot --all-namespaces --no-headers | wc -l)"
                echo "Longhorn卷: $(kubectl -n longhorn-system get volumes.longhorn.io --no-headers | wc -l)"
          restartPolicy: OnFailure

---
# RBAC for cleanup job
apiVersion: v1
kind: ServiceAccount
metadata:
  name: storage-admin
  namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: storage-admin
rules:
- apiGroups: [""]
  resources: ["persistentvolumes", "persistentvolumeclaims"]
  verbs: ["get", "list", "delete"]
- apiGroups: ["snapshot.storage.k8s.io"]
  resources: ["volumesnapshots"]
  verbs: ["get", "list", "delete"]
- apiGroups: ["longhorn.io"]
  resources: ["volumes"]
  verbs: ["get", "list", "delete"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: storage-admin-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: storage-admin
subjects:
- kind: ServiceAccount
  name: storage-admin
  namespace: kube-system
```

---

## 10. 生产环境最佳实践 2025

### 架构设计清单

```yaml
Production_Best_Practices_2025:
  规划阶段:
    容量规划:
      - 初始容量评估
      - 3年增长预测
      - 性能需求分析
      - 成本预算
    
    高可用设计:
      - 至少3个存储节点
      - 跨可用区部署
      - 副本策略 (3副本或EC)
      - 故障域隔离
    
    性能规划:
      - IOPS需求
      - 吞吐量需求
      - 延迟要求
      - 网络带宽

  部署阶段:
    硬件选型:
      存储节点:
        - NVMe SSD企业级
        - 32GB+ 内存
        - 10GbE/25GbE网络
        - RAID控制器 (可选)
      
      网络:
        - 独立存储网络
        - 巨型帧支持
        - RDMA (可选)
        - 冗余链路
    
    软件配置:
      - 内核参数调优
      - 文件系统优化
      - 监控告警
      - 备份策略

  运维阶段:
    监控:
      - 容量使用率
      - 性能指标
      - 错误日志
      - 健康检查
    
    备份:
      - 每日增量备份
      - 每周全量备份
      - 异地备份
      - 恢复演练
    
    安全:
      - 加密卷
      - 访问控制
      - 审计日志
      - 定期扫描

  灾难恢复:
    RTO_RPO目标:
      - RTO: <1小时
      - RPO: <15分钟
      - 自动化恢复
      - 定期演练
```

### 生产环境部署检查清单

```yaml
# production-readiness-checklist.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: storage-production-checklist
  namespace: kube-system
data:
  checklist.yaml: |
    production_readiness:
      infrastructure:
        - item: "至少3个专用存储节点"
          status: "required"
          verify: "kubectl get nodes -l role=storage-node --no-headers | wc -l"
          expected: ">= 3"
        
        - item: "节点跨可用区分布"
          status: "required"
          verify: "kubectl get nodes -l role=storage-node -o json | jq '.items[].metadata.labels.\"topology.kubernetes.io/zone\"' | sort -u | wc -l"
          expected: ">= 2"
        
        - item: "每节点NVMe SSD配置"
          status: "required"
          verify: "lsblk -d -n -o name,rota | grep nvme | grep ' 0$' | wc -l"
          expected: ">= 1"
        
        - item: "存储网络独立VLAN"
          status: "recommended"
          verify: "ip link show | grep storage-net"
          expected: "存在"
      
      storage_cluster:
        - item: "Rook/Longhorn版本最新稳定版"
          status: "required"
          verify: "kubectl -n rook-ceph get deploy rook-ceph-operator -o jsonpath='{.spec.template.spec.containers[0].image}'"
          expected: "v1.13+"
        
        - item: "监控组件MON >= 3 (奇数)"
          status: "required"
          verify: "kubectl -n rook-ceph get deploy -l app=rook-ceph-mon --no-headers | wc -l"
          expected: "3 or 5"
        
        - item: "OSD数量 >= 3"
          status: "required"
          verify: "kubectl -n rook-ceph get deploy -l app=rook-ceph-osd --no-headers | wc -l"
          expected: ">= 3"
        
        - item: "副本数配置为3"
          status: "required"
          verify: "kubectl -n rook-ceph get cephblockpool replicapool -o jsonpath='{.spec.replicated.size}'"
          expected: "3"
      
      performance:
        - item: "NVMe调度器设置为none"
          status: "required"
          verify: "cat /sys/block/nvme0n1/queue/scheduler"
          expected: "[none]"
        
        - item: "内核参数vm.swappiness <= 10"
          status: "required"
          verify: "sysctl vm.swappiness"
          expected: "<= 10"
        
        - item: "巨型帧启用 (MTU 9000)"
          status: "recommended"
          verify: "ip link show eth0 | grep mtu"
          expected: "9000"
      
      security:
        - item: "加密卷StorageClass配置"
          status: "required"
          verify: "kubectl get sc -o json | jq '.items[] | select(.parameters.encrypted==\"true\") | .metadata.name'"
          expected: "存在至少一个"
        
        - item: "KMS集成 (Vault/KMS)"
          status: "recommended"
          verify: "kubectl -n rook-ceph get secret | grep kms"
          expected: "存在"
        
        - item: "RBAC细粒度权限"
          status: "required"
          verify: "kubectl get clusterrole | grep storage"
          expected: "存在"
      
      monitoring:
        - item: "Prometheus监控部署"
          status: "required"
          verify: "kubectl -n monitoring get deploy prometheus-server"
          expected: "Running"
        
        - item: "Grafana仪表板配置"
          status: "required"
          verify: "kubectl -n monitoring get cm grafana-dashboards"
          expected: "存在"
        
        - item: "告警规则配置"
          status: "required"
          verify: "kubectl -n monitoring get prometheusrule"
          expected: "存在"
      
      backup_dr:
        - item: "备份目标配置 (S3)"
          status: "required"
          verify: "kubectl -n longhorn-system get setting backup-target -o jsonpath='{.value}'"
          expected: "s3://"
        
        - item: "每日自动备份Job"
          status: "required"
          verify: "kubectl get cronjob | grep backup"
          expected: "存在"
        
        - item: "跨区域备份配置"
          status: "recommended"
          verify: "kubectl -n longhorn-system get recurringjob -l type=backup -o json | jq '.items[].spec.retain'"
          expected: ">= 7"
        
        - item: "灾难恢复演练计划"
          status: "required"
          verify: "kubectl get cronjob dr-drill"
          expected: "存在"
      
      cost_optimization:
        - item: "Kubecost/OpenCost部署"
          status: "recommended"
          verify: "kubectl -n kubecost get deploy kubecost-cost-analyzer"
          expected: "Running"
        
        - item: "自动清理Job配置"
          status: "recommended"
          verify: "kubectl -n kube-system get cronjob storage-cleanup"
          expected: "存在"
        
        - item: "存储使用率监控"
          status: "required"
          verify: "kubectl top node"
          expected: "< 80%"

---
# 自动检查脚本
apiVersion: v1
kind: ConfigMap
metadata:
  name: storage-readiness-script
  namespace: kube-system
data:
  check.sh: |
    #!/bin/bash
    # production-storage-readiness-check.sh
    
    set -e
    
    echo "======================================"
    echo "  存储生产环境就绪检查 - 2025"
    echo "======================================"
    echo ""
    
    PASS=0
    FAIL=0
    WARN=0
    
    check_item() {
        local name="$1"
        local command="$2"
        local expected="$3"
        local severity="$4"
        
        echo -n "检查: $name ... "
        
        result=$(eval "$command" 2>/dev/null || echo "FAILED")
        
        if [[ "$result" == "$expected" ]] || [[ "$result" =~ $expected ]]; then
            echo "✅ PASS"
            ((PASS++))
        else
            if [[ "$severity" == "required" ]]; then
                echo "❌ FAIL (期望: $expected, 实际: $result)"
                ((FAIL++))
            else
                echo "⚠️  WARN (期望: $expected, 实际: $result)"
                ((WARN++))
            fi
        fi
    }
    
    echo "## 1. 基础设施检查"
    check_item "存储节点数量" \
        "kubectl get nodes -l role=storage-node --no-headers | wc -l" \
        "[3-9]" \
        "required"
    
    check_item "可用区分布" \
        "kubectl get nodes -l role=storage-node -o json | jq '.items[].metadata.labels.\"topology.kubernetes.io/zone\"' | sort -u | wc -l" \
        "[2-9]" \
        "required"
    
    echo ""
    echo "## 2. 存储集群检查"
    check_item "Rook Operator运行" \
        "kubectl -n rook-ceph get deploy rook-ceph-operator -o jsonpath='{.status.availableReplicas}'" \
        "1" \
        "required"
    
    check_item "MON数量 (奇数)" \
        "kubectl -n rook-ceph get deploy -l app=rook-ceph-mon --no-headers | wc -l" \
        "[35]" \
        "required"
    
    echo ""
    echo "## 3. 性能配置检查"
    check_item "vm.swappiness" \
        "sysctl -n vm.swappiness" \
        "10" \
        "required"
    
    echo ""
    echo "## 4. 安全配置检查"
    check_item "加密StorageClass" \
        "kubectl get sc -o json | jq '.items[] | select(.parameters.encrypted==\"true\") | .metadata.name' | wc -l" \
        "[1-9]" \
        "required"
    
    echo ""
    echo "## 5. 监控告警检查"
    check_item "Prometheus运行" \
        "kubectl -n monitoring get deploy prometheus-server -o jsonpath='{.status.availableReplicas}'" \
        "1" \
        "required"
    
    echo ""
    echo "## 6. 备份灾难恢复检查"
    check_item "备份目标配置" \
        "kubectl -n longhorn-system get setting backup-target -o jsonpath='{.value}' | grep -o 's3://'" \
        "s3://" \
        "required"
    
    echo ""
    echo "======================================"
    echo "  检查结果汇总"
    echo "======================================"
    echo "✅ 通过: $PASS"
    echo "❌ 失败: $FAIL"
    echo "⚠️  警告: $WARN"
    echo ""
    
    if [ $FAIL -eq 0 ]; then
        echo "🎉 存储系统已准备好投入生产！"
        exit 0
    else
        echo "❌ 存储系统未达到生产标准，请修复失败项。"
        exit 1
    fi
```

---

## 相关文档

### 本模块文档

- [CSI存储概述](01_CSI存储概述.md) - CSI规范和存储类型
- [Rook-Ceph存储](02_Rook_Ceph存储.md) - Ceph分布式存储
- [Longhorn存储](03_Longhorn存储.md) - 云原生块存储
- [StorageClass最佳实践](04_StorageClass最佳实践.md) - 存储类配置

### 相关模块

- [容器网络](../03_容器网络/README.md) - CNI和Cilium
- [监控告警](../../04_运维管理/01_监控告警/README.md) - Prometheus监控
- [自动化运维](../../04_运维管理/03_自动化运维/README.md) - Ansible/Terraform

### 外部资源

- [CSI Spec](https://github.com/container-storage-interface/spec)
- [Rook Documentation](https://rook.io/docs/rook/latest/)
- [Longhorn Documentation](https://longhorn.io/docs/)
- [OpenEBS Documentation](https://openebs.io/docs)
- [CNCF Storage Landscape](https://landscape.cncf.io/card-mode?category=cloud-native-storage)

---

**文档版本**: v1.0  
**更新日期**: 2025-10-20  
**状态**: ✅ **2025年最新技术标准对齐完成**
