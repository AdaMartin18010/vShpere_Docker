# 2025å¹´äº‘åŸç”Ÿå­˜å‚¨æ–°ç‰¹æ€§

> **è¿”å›**: [å®¹å™¨å­˜å‚¨ç›®å½•](README.md) | [å®¹å™¨åŒ–éƒ¨ç½²é¦–é¡µ](../README.md) | [éƒ¨ç½²æŒ‡å—é¦–é¡µ](../../00_ç´¢å¼•å¯¼èˆª/README.md)

---

## ğŸ“‹ ç›®å½•

- [2025å¹´äº‘åŸç”Ÿå­˜å‚¨æ–°ç‰¹æ€§](#2025å¹´äº‘åŸç”Ÿå­˜å‚¨æ–°ç‰¹æ€§)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. 2025å¹´å­˜å‚¨æŠ€æœ¯æ¦‚è§ˆ](#1-2025å¹´å­˜å‚¨æŠ€æœ¯æ¦‚è§ˆ)
    - [æ ¸å¿ƒæŠ€æœ¯è¶‹åŠ¿](#æ ¸å¿ƒæŠ€æœ¯è¶‹åŠ¿)
    - [æŠ€æœ¯é€‰å‹çŸ©é˜µ](#æŠ€æœ¯é€‰å‹çŸ©é˜µ)
  - [2. Rook 1.13+ æ–°ç‰¹æ€§](#2-rook-113-æ–°ç‰¹æ€§)
    - [æ ¸å¿ƒå¢å¼º](#æ ¸å¿ƒå¢å¼º)
    - [Rook 1.13 éƒ¨ç½²ç¤ºä¾‹](#rook-113-éƒ¨ç½²ç¤ºä¾‹)
    - [RBD åŠ å¯†å·ç¤ºä¾‹ (2025æ–°ç‰¹æ€§)](#rbd-åŠ å¯†å·ç¤ºä¾‹-2025æ–°ç‰¹æ€§)
  - [3. Longhorn 1.6+ æ–°ç‰¹æ€§](#3-longhorn-16-æ–°ç‰¹æ€§)
    - [æ ¸å¿ƒå¢å¼º](#æ ¸å¿ƒå¢å¼º-1)
    - [Longhorn 1.6 éƒ¨ç½²](#longhorn-16-éƒ¨ç½²)
    - [V2æ•°æ®å¼•æ“é…ç½® (2025å®éªŒæ€§åŠŸèƒ½)](#v2æ•°æ®å¼•æ“é…ç½®-2025å®éªŒæ€§åŠŸèƒ½)
    - [Longhorn å¤‡ä»½åˆ°S3 (2025æœ€ä½³å®è·µ)](#longhorn-å¤‡ä»½åˆ°s3-2025æœ€ä½³å®è·µ)
  - [4. OpenEBS 4.0+ äº‘åŸç”Ÿå­˜å‚¨](#4-openebs-40-äº‘åŸç”Ÿå­˜å‚¨)
    - [æ ¸å¿ƒæ¶æ„](#æ ¸å¿ƒæ¶æ„)
    - [OpenEBS Mayastor 2.0 éƒ¨ç½²](#openebs-mayastor-20-éƒ¨ç½²)
    - [Mayastor æ€§èƒ½ä¼˜åŒ–é…ç½®](#mayastor-æ€§èƒ½ä¼˜åŒ–é…ç½®)
  - [5. CSI 1.9+ æ–°è§„èŒƒ](#5-csi-19-æ–°è§„èŒƒ)
    - [æ ¸å¿ƒæ–°å¢åŠŸèƒ½](#æ ¸å¿ƒæ–°å¢åŠŸèƒ½)
    - [VolumeGroupSnapshot ç¤ºä¾‹ (2025æ–°ç‰¹æ€§)](#volumegroupsnapshot-ç¤ºä¾‹-2025æ–°ç‰¹æ€§)
    - [å·å¥åº·æ£€æŸ¥ (CSI 1.9 GA)](#å·å¥åº·æ£€æŸ¥-csi-19-ga)
  - [6. å­˜å‚¨æ€§èƒ½ä¼˜åŒ– 2025](#6-å­˜å‚¨æ€§èƒ½ä¼˜åŒ–-2025)
    - [NVMeä¼˜åŒ–](#nvmeä¼˜åŒ–)
    - [æ€§èƒ½è°ƒä¼˜è„šæœ¬](#æ€§èƒ½è°ƒä¼˜è„šæœ¬)
    - [GPU Direct Storage (GDS) é…ç½®](#gpu-direct-storage-gds-é…ç½®)
  - [7. å­˜å‚¨å®‰å…¨å¢å¼º](#7-å­˜å‚¨å®‰å…¨å¢å¼º)
    - [åŠ å¯†å·å®ç°](#åŠ å¯†å·å®ç°)
    - [KMSé›†æˆç¤ºä¾‹ (Vault)](#kmsé›†æˆç¤ºä¾‹-vault)
  - [8. å¤šé›†ç¾¤å­˜å‚¨è”é‚¦](#8-å¤šé›†ç¾¤å­˜å‚¨è”é‚¦)
    - [è·¨é›†ç¾¤å­˜å‚¨å¤åˆ¶](#è·¨é›†ç¾¤å­˜å‚¨å¤åˆ¶)
    - [Rook-Ceph è·¨é›†ç¾¤é•œåƒ](#rook-ceph-è·¨é›†ç¾¤é•œåƒ)
    - [Longhorn è·¨é›†ç¾¤å¤‡ä»½](#longhorn-è·¨é›†ç¾¤å¤‡ä»½)
  - [9. å­˜å‚¨æˆæœ¬ä¼˜åŒ– (FinOps)](#9-å­˜å‚¨æˆæœ¬ä¼˜åŒ–-finops)
    - [æˆæœ¬åˆ†æ](#æˆæœ¬åˆ†æ)
    - [Kubecost å­˜å‚¨ç›‘æ§éƒ¨ç½²](#kubecost-å­˜å‚¨ç›‘æ§éƒ¨ç½²)
    - [è‡ªåŠ¨æ¸…ç†æœªä½¿ç”¨å­˜å‚¨](#è‡ªåŠ¨æ¸…ç†æœªä½¿ç”¨å­˜å‚¨)
  - [10. ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ 2025](#10-ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ-2025)
    - [æ¶æ„è®¾è®¡æ¸…å•](#æ¶æ„è®¾è®¡æ¸…å•)
    - [ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ£€æŸ¥æ¸…å•](#ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ£€æŸ¥æ¸…å•)
  - [ç›¸å…³æ–‡æ¡£](#ç›¸å…³æ–‡æ¡£)
    - [æœ¬æ¨¡å—æ–‡æ¡£](#æœ¬æ¨¡å—æ–‡æ¡£)
    - [ç›¸å…³æ¨¡å—](#ç›¸å…³æ¨¡å—)
    - [å¤–éƒ¨èµ„æº](#å¤–éƒ¨èµ„æº)

---

## 1. 2025å¹´å­˜å‚¨æŠ€æœ¯æ¦‚è§ˆ

### æ ¸å¿ƒæŠ€æœ¯è¶‹åŠ¿

```yaml
2025_Storage_Trends:
  æŠ€æœ¯æ¼”è¿›:
    å®¹å™¨åŸç”Ÿå­˜å‚¨:
      - Rook 1.13+ (CNCFæ¯•ä¸šé¡¹ç›®)
      - Longhorn 1.6+ (è½»é‡çº§äº‘åŸç”Ÿ)
      - OpenEBS 4.0+ (CASæ¶æ„)
      - Portworx 3.0+ (ä¼ä¸šçº§)
    
    CSIè§„èŒƒå‡çº§:
      - CSI 1.9+ (æœ€æ–°è§„èŒƒ)
      - å·ç»„å¿«ç…§ (VolumeGroupSnapshot)
      - è·¨å‘½åç©ºé—´å·å…‹éš†
      - å·å¥åº·æ£€æŸ¥ GA
      - å·ä¿®æ”¹ (VolumeModification)
    
    æ€§èƒ½çªç ´:
      - NVMe over Fabrics (NVMe-oF)
      - SPDKç”¨æˆ·æ€é©±åŠ¨
      - io_uringå¼‚æ­¥I/O
      - eBPFå­˜å‚¨ç›‘æ§
      - RDMA/RoCEé«˜é€Ÿç½‘ç»œ
    
    å®‰å…¨å¢å¼º:
      - åŠ å¯†å· (LUKS2)
      - å¯†é’¥ç®¡ç† (KMSé›†æˆ)
      - å­˜å‚¨è®¿é—®æ§åˆ¶ (RBAC)
      - æ•°æ®ä¸å¯å˜æ€§ (WORM)
      - ä¾›åº”é“¾å®‰å…¨ (é•œåƒç­¾å)

  æ–°å…´ç‰¹æ€§:
    AI_MLå­˜å‚¨ä¼˜åŒ–:
      - GPU Direct Storage (GDS)
      - é«˜ååæ•°æ®ç®¡é“
      - æ•°æ®é›†ç¼“å­˜
      - åˆ†å¸ƒå¼è®­ç»ƒå­˜å‚¨
    
    è¾¹ç¼˜å­˜å‚¨:
      - K3s + Longhorn
      - è½»é‡çº§CSIé©±åŠ¨
      - æ–­ç½‘è‡ªæ²»
      - æ•°æ®åŒæ­¥
    
    FinOpsæˆæœ¬ä¼˜åŒ–:
      - å­˜å‚¨ä½¿ç”¨åˆ†æ
      - è‡ªåŠ¨æ•°æ®åˆ†å±‚
      - å†·çƒ­æ•°æ®åˆ†ç¦»
      - æˆæœ¬å¯è§†åŒ–

  æŠ€æœ¯å¯¹æ¯”_2025:
    | å­˜å‚¨æ–¹æ¡ˆ | ç‰ˆæœ¬ | æ¶æ„ | é€‚ç”¨åœºæ™¯ | æ€§èƒ½ | å¤æ‚åº¦ |
    |---------|------|------|---------|------|--------|
    | Rook-Ceph | 1.13+ | åˆ†å¸ƒå¼ | å¤§è§„æ¨¡ç”Ÿäº§ | â­â­â­â­ | é«˜ |
    | Longhorn | 1.6+ | è¶…èåˆ | ä¸­å°è§„æ¨¡ | â­â­â­â­ | ä¸­ |
    | OpenEBS | 4.0+ | CAS | äº‘åŸç”Ÿåº”ç”¨ | â­â­â­â­â­ | ä¸­ |
    | Portworx | 3.0+ | ä¼ä¸šçº§ | å…³é”®ä¸šåŠ¡ | â­â­â­â­â­ | é«˜ |
```

### æŠ€æœ¯é€‰å‹çŸ©é˜µ

```yaml
Storage_Selection_Matrix_2025:
  åœºæ™¯ä¸€_å¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒ:
    æ¨è: Rook-Ceph 1.13+
    åŸå› :
      - æˆç†Ÿç¨³å®šï¼ŒCNCFæ¯•ä¸š
      - æ”¯æŒå—/æ–‡ä»¶/å¯¹è±¡å­˜å‚¨
      - è‡ªåŠ¨åŒ–è¿ç»´
      - ä¼ä¸šçº§åŠŸèƒ½å®Œå–„
    è§„æ¨¡: 100+ èŠ‚ç‚¹ï¼ŒPBçº§å­˜å‚¨
  
  åœºæ™¯äºŒ_ä¸­å°è§„æ¨¡äº‘åŸç”Ÿ:
    æ¨è: Longhorn 1.6+
    åŸå› :
      - è½»é‡çº§ï¼Œæ˜“éƒ¨ç½²
      - UIå‹å¥½ï¼Œè¿ç»´ç®€å•
      - å†…ç½®å¤‡ä»½æ¢å¤
      - K3så®˜æ–¹æ¨è
    è§„æ¨¡: 10-100 èŠ‚ç‚¹ï¼ŒTBçº§å­˜å‚¨
  
  åœºæ™¯ä¸‰_é«˜æ€§èƒ½åº”ç”¨:
    æ¨è: OpenEBS 4.0+ (Mayastor)
    åŸå› :
      - NVMeæ€§èƒ½ä¼˜åŒ–
      - ç”¨æˆ·æ€é©±åŠ¨ (SPDK)
      - ä½å»¶è¿Ÿ (<100Î¼s)
      - CASæ¶æ„
    è§„æ¨¡: æ€§èƒ½æ•æ„Ÿåº”ç”¨
  
  åœºæ™¯å››_è¾¹ç¼˜è®¡ç®—:
    æ¨è: Longhorn 1.6+ / Local PV
    åŸå› :
      - èµ„æºå ç”¨å°
      - ç¦»çº¿è‡ªæ²»
      - è½»é‡çº§
      - K3sé›†æˆ
    è§„æ¨¡: è¾¹ç¼˜èŠ‚ç‚¹ï¼Œå•æœº/å°é›†ç¾¤
  
  åœºæ™¯äº”_AI_MLå·¥ä½œè´Ÿè½½:
    æ¨è: OpenEBS + NFS / WekaFS
    åŸå› :
      - é«˜ååé‡
      - GPU Direct Storage
      - æ•°æ®é›†å…±äº«
      - åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
    è§„æ¨¡: GPUé›†ç¾¤
```

---

## 2. Rook 1.13+ æ–°ç‰¹æ€§

### æ ¸å¿ƒå¢å¼º

```yaml
Rook_1.13_Features:
  æ–°å¢åŠŸèƒ½:
    Ceph_Quincy_17.2+:
      - Crimson OSD (ä¸‹ä¸€ä»£OSD)
      - RBDé•œåƒåŠ å¯†
      - CephFSå¿«ç…§é•œåƒ
      - RGWå¤šç«™ç‚¹å¢å¼º
      - æ€§èƒ½æå‡20-30%
    
    Operatorå¢å¼º:
      - å¤šé›†ç¾¤ç®¡ç†æ”¹è¿›
      - è‡ªåŠ¨åŒ–å‡çº§ç­–ç•¥
      - å¥åº·æ£€æŸ¥å¢å¼º
      - æ•…éšœè‡ªæ„ˆæ”¹è¿›
      - èµ„æºä½¿ç”¨ä¼˜åŒ–
    
    CSIé©±åŠ¨å‡çº§:
      - CSI 1.9+æ”¯æŒ
      - å·å…‹éš†æ€§èƒ½æå‡
      - å¿«ç…§æ¢å¤åŠ é€Ÿ
      - RWXå·æ€§èƒ½ä¼˜åŒ–
      - åŠ å¯†å·æ”¯æŒ
    
    ç›‘æ§å¯è§‚æµ‹æ€§:
      - PrometheusæŒ‡æ ‡å¢å¼º
      - Grafanaä»ªè¡¨æ¿æ›´æ–°
      - Ceph Dashboardé›†æˆ
      - å‘Šè­¦è§„åˆ™ä¼˜åŒ–
      - æ€§èƒ½åˆ†æå·¥å…·

  æ€§èƒ½ä¼˜åŒ–:
    BlueStoreä¼˜åŒ–:
      - RocksDBè°ƒä¼˜
      - å‹ç¼©ç®—æ³•æ”¹è¿›
      - ç¼“å­˜ç­–ç•¥ä¼˜åŒ–
      - å†…å­˜ä½¿ç”¨é™ä½
    
    ç½‘ç»œæ€§èƒ½:
      - RDMAæ”¯æŒå¢å¼º
      - å¤šç½‘ç»œæ¥å£
      - æµé‡åˆ†ç¦»
      - QoSæ§åˆ¶
```

### Rook 1.13 éƒ¨ç½²ç¤ºä¾‹

```yaml
# rook-ceph-cluster-1.13.yaml - 2025å¹´é…ç½®
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v17.2.7  # Ceph Quincy
    allowUnsupported: false
  
  dataDirHostPath: /var/lib/rook
  
  # 2025æ–°å¢ï¼šCrimson OSDæ”¯æŒ
  experimental:
    crimsonOSD: false  # å®éªŒæ€§åŠŸèƒ½ï¼Œé«˜æ€§èƒ½OSD
  
  mon:
    count: 3
    allowMultiplePerNode: false
    volumeClaimTemplate:
      spec:
        storageClassName: local-storage
        resources:
          requests:
            storage: 10Gi
  
  mgr:
    count: 2
    modules:
      - name: pg_autoscaler  # è‡ªåŠ¨PGè°ƒæ•´
        enabled: true
      - name: dashboard
        enabled: true
      - name: prometheus
        enabled: true
      - name: telemetry
        enabled: false
  
  # 2025å¢å¼ºï¼šå¤šç½‘ç»œé…ç½®
  network:
    provider: host
    connections:
      encryption:
        enabled: true  # ç½‘ç»œåŠ å¯†
      compression:
        enabled: false
    # åˆ†ç¦»å…¬å…±ç½‘ç»œå’Œé›†ç¾¤ç½‘ç»œ
    addressRanges:
      public:
        - 10.0.0.0/16
      cluster:
        - 10.1.0.0/16
  
  storage:
    useAllNodes: false
    useAllDevices: false
    nodes:
      - name: "node1"
        devices:
          - name: "nvme0n1"
            config:
              # 2025ä¼˜åŒ–ï¼šBlueStoreé…ç½®
              osdsPerDevice: "1"
              encryptedDevice: "true"  # è®¾å¤‡åŠ å¯†
              metadataDevice: "nvme0n2"  # ç‹¬ç«‹å…ƒæ•°æ®è®¾å¤‡
      - name: "node2"
        devices:
          - name: "nvme0n1"
      - name: "node3"
        devices:
          - name: "nvme0n1"
  
  # 2025æ–°å¢ï¼šèµ„æºè¯·æ±‚å’Œé™åˆ¶
  resources:
    mon:
      requests:
        cpu: "1000m"
        memory: "2Gi"
      limits:
        memory: "4Gi"
    osd:
      requests:
        cpu: "2000m"
        memory: "4Gi"
      limits:
        memory: "8Gi"
    mgr:
      requests:
        cpu: "1000m"
        memory: "2Gi"
      limits:
        memory: "4Gi"
  
  # å¥åº·æ£€æŸ¥å¢å¼º
  healthCheck:
    daemonHealth:
      mon:
        interval: 45s
        timeout: 10s
      osd:
        interval: 60s
        timeout: 10s
      status:
        interval: 60s
  
  # 2025æ–°å¢ï¼šè‡ªåŠ¨åŒ–å‡çº§ç­–ç•¥
  upgradeOSDRequiresHealthyPGs: true
  
  # 2025æ–°å¢ï¼šå­˜å‚¨ç±»é…ç½®
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: role
              operator: In
              values:
              - storage-node
      tolerations:
      - key: storage-node
        operator: Exists
```

### RBD åŠ å¯†å·ç¤ºä¾‹ (2025æ–°ç‰¹æ€§)

```yaml
# rbd-encrypted-storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block-encrypted
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: replicapool
  imageFeatures: layering
  
  # 2025æ–°å¢ï¼šLUKS2åŠ å¯†æ”¯æŒ
  encrypted: "true"
  encryptionType: "luks2"
  
  # KMSé›†æˆ (Vault/AWS KMS/Azure Key Vault)
  encryptionKMSID: "vault"
  
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  
  csi.storage.k8s.io/fstype: ext4

allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate

---
# KMS Secreté…ç½® (Vaultç¤ºä¾‹)
apiVersion: v1
kind: Secret
metadata:
  name: ceph-csi-kms-config
  namespace: rook-ceph
stringData:
  config.json: |-
    {
      "vault": {
        "VAULT_ADDR": "https://vault.example.com:8200",
        "VAULT_BACKEND_PATH": "rook",
        "VAULT_CACERT": "/etc/vault/ca.crt",
        "VAULT_AUTH_METHOD": "kubernetes",
        "VAULT_AUTH_PATH": "auth/kubernetes/login",
        "VAULT_ROLE": "ceph-csi"
      }
    }
```

---

## 3. Longhorn 1.6+ æ–°ç‰¹æ€§

### æ ¸å¿ƒå¢å¼º

```yaml
Longhorn_1.6_Features:
  é‡å¤§æ–°å¢:
    V2æ•°æ®å¼•æ“:
      - åŸºäºSPDKç”¨æˆ·æ€é©±åŠ¨
      - æ€§èƒ½æå‡2-3å€
      - å»¶è¿Ÿé™ä½50%
      - NVMeåŸç”Ÿæ”¯æŒ
      - å®éªŒæ€§åŠŸèƒ½ (1.6å¼€å§‹)
    
    å¿«ç…§å¢å¼º:
      - å¢é‡å¿«ç…§
      - å¿«ç…§å‹ç¼©
      - å¿«ç…§åŠ å¯†
      - è·¨é›†ç¾¤å¿«ç…§å¤åˆ¶
      - è‡ªåŠ¨å¿«ç…§æ¸…ç†
    
    å¤‡ä»½æ¢å¤:
      - S3å…¼å®¹å­˜å‚¨å¢å¼º
      - å¢é‡å¤‡ä»½ä¼˜åŒ–
      - å¤‡ä»½åŠ å¯†
      - ç¾éš¾æ¢å¤æ¼”ç»ƒ
      - è·¨åŒºåŸŸå¤‡ä»½
    
    é«˜å¯ç”¨æ€§:
      - å¤šå‰¯æœ¬ä¼˜åŒ–
      - è‡ªåŠ¨åŒ–æ•…éšœè½¬ç§»
      - æ•°æ®é‡å»ºåŠ é€Ÿ
      - èŠ‚ç‚¹é©±é€ç­–ç•¥
      - ç½‘ç»œéš”ç¦»æ¢å¤
    
    ç›‘æ§å¯è§‚æµ‹æ€§:
      - PrometheusæŒ‡æ ‡å®Œå–„
      - Grafanaä»ªè¡¨æ¿
      - å·æ€§èƒ½åˆ†æ
      - å®æ—¶å‘Šè­¦
      - å®¡è®¡æ—¥å¿—

  æ€§èƒ½ä¼˜åŒ–:
    V2å¼•æ“ç‰¹æ€§:
      - SPDKç”¨æˆ·æ€I/O
      - é›¶æ‹·è´æ•°æ®è·¯å¾„
      - æ‰¹é‡I/Oå¤„ç†
      - NVMeé˜Ÿåˆ—ä¼˜åŒ–
      - CPUäº²å’Œæ€§
    
    ç½‘ç»œä¼˜åŒ–:
      - RDMAæ”¯æŒ (å®éªŒæ€§)
      - å¤šè·¯å¾„I/O
      - ç½‘ç»œå‹ç¼©
      - æµé‡æ§åˆ¶
```

### Longhorn 1.6 éƒ¨ç½²

```bash
#!/bin/bash
# longhorn-1.6-install.sh - 2025å¹´å®‰è£…è„šæœ¬

set -e

echo "=== Longhorn 1.6+ å®‰è£… (2025) ==="

# 1. å‰ç½®æ£€æŸ¥
echo "1. æ£€æŸ¥å‰ç½®æ¡ä»¶..."
kubectl get nodes

# æ£€æŸ¥ä¾èµ–
for node in $(kubectl get nodes -o jsonpath='{.items[*].metadata.name}'); do
  echo "æ£€æŸ¥èŠ‚ç‚¹: $node"
  kubectl exec -n kube-system $(kubectl get pods -n kube-system -l component=kube-proxy --field-selector spec.nodeName=$node -o jsonpath='{.items[0].metadata.name}') -- \
    sh -c "command -v iscsiadm && command -v nfs-common || echo 'éœ€è¦å®‰è£… open-iscsi nfs-common'"
done

# 2. æ·»åŠ Longhorn Helmä»“åº“
echo "2. æ·»åŠ Helmä»“åº“..."
helm repo add longhorn https://charts.longhorn.io
helm repo update

# 3. åˆ›å»ºå‘½åç©ºé—´
kubectl create namespace longhorn-system --dry-run=client -o yaml | kubectl apply -f -

# 4. å®‰è£…Longhorn 1.6+ (2025é…ç½®)
echo "3. å®‰è£…Longhorn 1.6+..."
helm install longhorn longhorn/longhorn \
  --namespace longhorn-system \
  --version "1.6.0" \
  --set defaultSettings.backupTarget="s3://longhorn-backup@us-east-1/" \
  --set defaultSettings.backupTargetCredentialSecret="aws-secret" \
  --set defaultSettings.defaultDataPath="/var/lib/longhorn/" \
  --set defaultSettings.defaultReplicaCount="3" \
  --set defaultSettings.guaranteedInstanceManagerCPU="10" \
  --set defaultSettings.createDefaultDiskLabeledNodes="true" \
  --set defaultSettings.storageMinimalAvailablePercentage="10" \
  --set defaultSettings.storageOverProvisioningPercentage="200" \
  --set defaultSettings.upgradeChecker="false" \
  --set defaultSettings.nodeDownPodDeletionPolicy="delete-both-statefulset-and-deployment-pod" \
  --set defaultSettings.autoSalvage="true" \
  --set defaultSettings.autoDeletePodWhenVolumeDetachedUnexpectedly="true" \
  --set defaultSettings.disableSchedulingOnCordonedNode="true" \
  --set defaultSettings.replicaSoftAntiAffinity="false" \
  --set defaultSettings.systemManagedComponentsNodeSelector="node-role.kubernetes.io/storage:true" \
  --set defaultSettings.v2DataEngine="true" \
  --set persistence.defaultClass="true" \
  --set persistence.defaultClassReplicaCount="3" \
  --set csi.kubeletRootDir="/var/lib/kubelet" \
  --set defaultSettings.snapshotMaxCount="25" \
  --set defaultSettings.backupCompressionMethod="lz4" \
  --set defaultSettings.backupConcurrentLimit="5" \
  --set defaultSettings.restoreConcurrentLimit="5"

# 5. ç­‰å¾…éƒ¨ç½²å®Œæˆ
echo "4. ç­‰å¾…Longhornéƒ¨ç½²å®Œæˆ..."
kubectl -n longhorn-system rollout status deployment/longhorn-driver-deployer
kubectl -n longhorn-system rollout status daemonset/longhorn-manager

# 6. éªŒè¯å®‰è£…
echo "5. éªŒè¯å®‰è£…..."
kubectl -n longhorn-system get pods
kubectl -n longhorn-system get storageclass

# 7. è®¿é—®UI
echo "6. Longhorn UIè®¿é—®æ–¹å¼:"
echo "   kubectl -n longhorn-system port-forward svc/longhorn-frontend 8080:80"
echo "   æµè§ˆå™¨è®¿é—®: http://localhost:8080"

echo ""
echo "=== Longhorn 1.6å®‰è£…å®Œæˆ ==="
```

### V2æ•°æ®å¼•æ“é…ç½® (2025å®éªŒæ€§åŠŸèƒ½)

```yaml
# longhorn-v2-engine-storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-v2-fast
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "30"
  fromBackup: ""
  fsType: "ext4"
  
  # 2025æ–°å¢ï¼šV2æ•°æ®å¼•æ“ (SPDK)
  dataEngine: "v2"
  
  # V2å¼•æ“å‚æ•°
  diskSelector: "ssd,nvme"  # ä»…ä½¿ç”¨NVMeè®¾å¤‡
  nodeSelector: "storage-node"
  
  # æ€§èƒ½ä¼˜åŒ–
  unmapMarkSnapChainRemoved: "enabled"
  disableRevisionCounter: "false"
  
  # å¿«ç…§é…ç½®
  recurringJobSelector: '[
    {
      "name": "snapshot-daily",
      "isGroup": false
    }
  ]'

---
# RecurringJob for automated snapshots
apiVersion: longhorn.io/v1beta2
kind: RecurringJob
metadata:
  name: snapshot-daily
  namespace: longhorn-system
spec:
  cron: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨2ç‚¹
  task: "snapshot"
  groups:
    - default
  retain: 7  # ä¿ç•™7å¤©
  concurrency: 5
  labels:
    type: snapshot
```

### Longhorn å¤‡ä»½åˆ°S3 (2025æœ€ä½³å®è·µ)

```yaml
# longhorn-backup-s3-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: aws-secret
  namespace: longhorn-system
type: Opaque
stringData:
  AWS_ACCESS_KEY_ID: "YOUR_ACCESS_KEY"
  AWS_SECRET_ACCESS_KEY: "YOUR_SECRET_KEY"
  AWS_ENDPOINTS: "https://s3.us-east-1.amazonaws.com"
  # æˆ–ä½¿ç”¨MinIO/å…¶ä»–S3å…¼å®¹å­˜å‚¨
  # AWS_ENDPOINTS: "https://minio.example.com"

---
# BackupTargeté…ç½®
apiVersion: longhorn.io/v1beta2
kind: Setting
metadata:
  name: backup-target
  namespace: longhorn-system
value: "s3://longhorn-backups@us-east-1/"

---
apiVersion: longhorn.io/v1beta2
kind: Setting
metadata:
  name: backup-target-credential-secret
  namespace: longhorn-system
value: "aws-secret"

---
# 2025æ–°å¢ï¼šå¤‡ä»½ç­–ç•¥
apiVersion: longhorn.io/v1beta2
kind: RecurringJob
metadata:
  name: backup-daily
  namespace: longhorn-system
spec:
  cron: "0 3 * * *"  # æ¯å¤©å‡Œæ™¨3ç‚¹
  task: "backup"
  groups:
    - default
  retain: 30  # ä¿ç•™30å¤©
  concurrency: 3
  labels:
    type: backup
    encrypted: "true"  # åŠ å¯†å¤‡ä»½

---
# ç¾éš¾æ¢å¤æ¼”ç»ƒJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dr-drill
  namespace: longhorn-system
spec:
  schedule: "0 0 1 * *"  # æ¯æœˆ1å·
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: longhorn-dr
          containers:
          - name: dr-drill
            image: longhornio/longhorn-manager:v1.6.0
            command: ["/bin/sh"]
            args:
              - -c
              - |
                # éªŒè¯å¤‡ä»½å¯ç”¨æ€§
                longhorn-manager backup list --backup-target s3://longhorn-backups@us-east-1/
                # æµ‹è¯•æ¢å¤ (åˆ°æµ‹è¯•å‘½åç©ºé—´)
                # ...
          restartPolicy: OnFailure
```

---

## 4. OpenEBS 4.0+ äº‘åŸç”Ÿå­˜å‚¨

### æ ¸å¿ƒæ¶æ„

```yaml
OpenEBS_4.0_Architecture:
  æ•°æ®å¼•æ“:
    Mayastor_2.0+:
      - NVMe-oFåŸç”Ÿ
      - SPDKç”¨æˆ·æ€é©±åŠ¨
      - è¶…ä½å»¶è¿Ÿ (<100Î¼s)
      - é«˜IOPS (>1M)
      - é€‚ç”¨: é«˜æ€§èƒ½åº”ç”¨ã€æ•°æ®åº“
    
    cStor:
      - ZFSå­˜å‚¨
      - å¿«ç…§å…‹éš†
      - æ•°æ®å®Œæ•´æ€§
      - é€‚ç”¨: é€šç”¨åº”ç”¨
    
    Jiva:
      - è½»é‡çº§
      - æ˜“éƒ¨ç½²
      - é€‚ç”¨: å°è§„æ¨¡
    
    Local_PV:
      - æœ¬åœ°å­˜å‚¨
      - æœ€é«˜æ€§èƒ½
      - é€‚ç”¨: StatefulSet

  CASæ¶æ„:
    Container_Attached_Storage:
      ä¼˜åŠ¿:
        - å®¹å™¨ç²’åº¦å­˜å‚¨
        - å¾®æœåŠ¡åŒ–
        - è‡ªåŠ¨åŒ–è¿ç»´
        - äº‘åŸç”Ÿè®¾è®¡
      
      ç»„ä»¶:
        - Control Plane: KubernetesåŸç”Ÿ
        - Data Plane: æ¯ä¸ªå·ç‹¬ç«‹å®¹å™¨
        - å­˜å‚¨ç­–ç•¥: CRDå®šä¹‰

  2025æ–°ç‰¹æ€§:
    Mayastor_2.0:
      - NVMe-oF TCP/RDMA
      - io_uringå¼‚æ­¥I/O
      - å¿«ç…§å¢å¼º
      - å‰¯æœ¬é‡å»ºä¼˜åŒ–
      - HAæ‹“æ‰‘æ„ŸçŸ¥
    
    ç›‘æ§å¯è§‚æµ‹æ€§:
      - Prometheuså¯¼å‡ºå™¨
      - Grafanaä»ªè¡¨æ¿
      - å®æ—¶æ€§èƒ½åˆ†æ
      - å®¹é‡è§„åˆ’
```

### OpenEBS Mayastor 2.0 éƒ¨ç½²

```bash
#!/bin/bash
# openebs-mayastor-2.0-install.sh

set -e

echo "=== OpenEBS Mayastor 2.0 å®‰è£… (2025) ==="

# 1. å‰ç½®æ£€æŸ¥
echo "1. æ£€æŸ¥å‰ç½®æ¡ä»¶..."

# æ£€æŸ¥å†…æ ¸ç‰ˆæœ¬ (éœ€è¦ >= 5.10)
uname -r

# æ£€æŸ¥Hugepagesé…ç½®
echo "æ£€æŸ¥Hugepages..."
grep HugePages /proc/meminfo

# å¦‚æœæœªé…ç½®ï¼Œè®¾ç½®Hugepages (æ¯ä¸ªå­˜å‚¨èŠ‚ç‚¹)
for node in $(kubectl get nodes -l openebs.io/data-plane=true -o jsonpath='{.items[*].metadata.name}'); do
  echo "é…ç½®èŠ‚ç‚¹: $node"
  kubectl label node $node openebs.io/engine=mayastor --overwrite
  
  # é€šè¿‡DaemonSeté…ç½®Hugepages
  kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: mayastor-hugepages-setup
  namespace: openebs
spec:
  selector:
    matchLabels:
      app: mayastor-hugepages
  template:
    metadata:
      labels:
        app: mayastor-hugepages
    spec:
      nodeSelector:
        openebs.io/engine: mayastor
      hostPID: true
      hostNetwork: true
      containers:
      - name: setup
        image: alpine:3.18
        command:
          - sh
          - -c
          - |
            echo 1024 > /proc/sys/vm/nr_hugepages
            sleep infinity
        securityContext:
          privileged: true
EOF
done

# 2. å®‰è£…OpenEBS Operator
echo "2. å®‰è£…OpenEBS Operator..."
kubectl create namespace openebs --dry-run=client -o yaml | kubectl apply -f -

helm repo add openebs https://openebs.github.io/charts
helm repo update

# å®‰è£…openebs-operator
helm install openebs openebs/openebs \
  --namespace openebs \
  --set engines.replicated.mayastor.enabled=true \
  --set mayastor.csi.node.kubeletDir="/var/lib/kubelet" \
  --set mayastor.etcd.replicaCount=3

# 3. åˆ›å»ºMayastorå­˜å‚¨æ± 
echo "3. åˆ›å»ºMayastor DiskPool..."

# ä¸ºæ¯ä¸ªå­˜å‚¨èŠ‚ç‚¹åˆ›å»ºDiskPool
kubectl apply -f - <<EOF
apiVersion: openebs.io/v1beta2
kind: DiskPool
metadata:
  name: pool-on-node1
  namespace: openebs
spec:
  node: node1  # å­˜å‚¨èŠ‚ç‚¹åç§°
  disks:
    - /dev/nvme0n1  # NVMeè®¾å¤‡è·¯å¾„
---
apiVersion: openebs.io/v1beta2
kind: DiskPool
metadata:
  name: pool-on-node2
  namespace: openebs
spec:
  node: node2
  disks:
    - /dev/nvme0n1
---
apiVersion: openebs.io/v1beta2
kind: DiskPool
metadata:
  name: pool-on-node3
  namespace: openebs
spec:
  node: node3
  disks:
    - /dev/nvme0n1
EOF

# 4. éªŒè¯DiskPool
echo "4. éªŒè¯DiskPoolçŠ¶æ€..."
kubectl -n openebs get diskpool

# 5. åˆ›å»ºStorageClass
echo "5. åˆ›å»ºé«˜æ€§èƒ½StorageClass..."
kubectl apply -f - <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: mayastor-nvme-3replica
parameters:
  protocol: nvmf
  repl: "3"  # 3å‰¯æœ¬
  ioTimeout: "60"
  # 2025æ–°å¢ï¼šæ‹“æ‰‘æ„ŸçŸ¥
  thin: "false"
provisioner: io.openebs.csi-mayastor
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
EOF

# 6. æ€§èƒ½æµ‹è¯•
echo "6. è¿è¡Œæ€§èƒ½æµ‹è¯•..."
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mayastor-test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: mayastor-nvme-3replica
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: fio-test
spec:
  containers:
  - name: fio
    image: ljishen/fio:latest
    command:
      - sh
      - -c
      - |
        fio --name=randwrite --ioengine=libaio --iodepth=32 --rw=randwrite \\
            --bs=4k --direct=1 --size=1G --numjobs=4 --runtime=60 \\
            --group_reporting --filename=/data/test
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: mayastor-test-pvc
EOF

echo "=== OpenEBS Mayastor 2.0 å®‰è£…å®Œæˆ ==="
echo "æŸ¥çœ‹DiskPool: kubectl -n openebs get diskpool"
echo "æŸ¥çœ‹StorageClass: kubectl get sc"
```

### Mayastor æ€§èƒ½ä¼˜åŒ–é…ç½®

```yaml
# mayastor-performance-tuning.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mayastor-tuning
  namespace: openebs
data:
  # I/Oè·¯å¾„ä¼˜åŒ–
  io-engine.yaml: |
    # io_uringé˜Ÿåˆ—æ·±åº¦
    io_uring_queue_depth: 512
    
    # CPUäº²å’Œæ€§
    core_mask: "0x0F"  # ä½¿ç”¨CPU 0-3
    
    # å†…å­˜
    mem_size: 4096  # MB
    hugepage_size: 2048  # KB
    
    # NVMe-oFé…ç½®
    nvmf:
      max_queue_depth: 128
      num_io_queues: 4
      tcp_nodelay: true

---
# DaemonSetåº”ç”¨æ€§èƒ½è°ƒä¼˜
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: mayastor-io-engine-tuned
  namespace: openebs
spec:
  selector:
    matchLabels:
      app: mayastor-io-engine
  template:
    metadata:
      labels:
        app: mayastor-io-engine
    spec:
      nodeSelector:
        openebs.io/engine: mayastor
      hostNetwork: true
      containers:
      - name: io-engine
        image: openebs/mayastor-io-engine:v2.0.0
        env:
          - name: MAYASTOR_HPA_SIZE
            value: "4096Mi"
          - name: MAYASTOR_CORE_MASK
            value: "0x0F"
          - name: MAYASTOR_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
        resources:
          requests:
            cpu: "4"
            memory: "8Gi"
            hugepages-2Mi: "4Gi"
          limits:
            cpu: "8"
            memory: "16Gi"
            hugepages-2Mi: "4Gi"
        securityContext:
          privileged: true
        volumeMounts:
          - name: device
            mountPath: /dev
          - name: hugepages
            mountPath: /dev/hugepages
      volumes:
        - name: device
          hostPath:
            path: /dev
        - name: hugepages
          emptyDir:
            medium: HugePages
```

---

## 5. CSI 1.9+ æ–°è§„èŒƒ

### æ ¸å¿ƒæ–°å¢åŠŸèƒ½

```yaml
CSI_1.9_Features:
  å·ç»„å¿«ç…§:
    VolumeGroupSnapshot:
      ç”¨é€”: ä¸€è‡´æ€§å¿«ç…§å¤šä¸ªå·
      åœºæ™¯: æ•°æ®åº“é›†ç¾¤ã€åº”ç”¨ä¸€è‡´æ€§å¤‡ä»½
      æ”¯æŒ: Rook 1.13+, Longhorn 1.6+
      
      ç¤ºä¾‹:
        - ä¸€æ¬¡æ€§å¿«ç…§å¤šä¸ªPV
        - ä¿è¯æ•°æ®ä¸€è‡´æ€§
        - ç®€åŒ–å¤‡ä»½æµç¨‹
  
  è·¨å‘½åç©ºé—´å·å…‹éš†:
    CrossNamespaceVolumeClone:
      ç”¨é€”: è·¨å‘½åç©ºé—´å…‹éš†å·
      åœºæ™¯: ç¯å¢ƒå¤åˆ¶ã€æ•°æ®å…±äº«
      é™åˆ¶: éœ€è¦RBACæˆæƒ
  
  å·å¥åº·æ£€æŸ¥_GA:
    VolumeHealthCheck:
      ç”¨é€”: å®æ—¶ç›‘æ§å·å¥åº·çŠ¶æ€
      æŒ‡æ ‡:
        - ç£ç›˜çŠ¶æ€
        - æ€§èƒ½æŒ‡æ ‡
        - é”™è¯¯ç‡
      å‘Šè­¦: è‡ªåŠ¨è§¦å‘
  
  å·ä¿®æ”¹:
    VolumeModification:
      ç”¨é€”: åœ¨çº¿ä¿®æ”¹å·å±æ€§
      æ”¯æŒ:
        - IOPSè°ƒæ•´
        - ååé‡å˜æ›´
        - QoSç­–ç•¥
      æ— éœ€é‡å¯: çƒ­ä¿®æ”¹

  CSI_Addons:
    åŠŸèƒ½æ‰©å±•:
      - å·å¤åˆ¶ (Replication)
      - å·åŠ å¯† (Encryption)
      - å·ç›‘æ§ (Monitoring)
      - ç½‘ç»œéš”ç¦» (NetworkFencing)
```

### VolumeGroupSnapshot ç¤ºä¾‹ (2025æ–°ç‰¹æ€§)

```yaml
# volumegroupsnapshot-example.yaml
# åœºæ™¯ï¼šä¸€è‡´æ€§å¿«ç…§MySQLä¸»ä»åº“

# 1. VolumeGroupSnapshotClass
apiVersion: groupsnapshot.storage.k8s.io/v1alpha1
kind: VolumeGroupSnapshotClass
metadata:
  name: csi-group-snapclass
driver: rook-ceph.rbd.csi.ceph.com
deletionPolicy: Delete
parameters:
  clusterID: rook-ceph
  csi.storage.k8s.io/snapshotter-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/snapshotter-secret-namespace: rook-ceph

---
# 2. MySQL StatefulSet (ä¸»ä»)
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: database
spec:
  serviceName: mysql
  replicas: 2
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: mysql
        snapshot-group: mysql-cluster
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: rook-ceph-block
      resources:
        requests:
          storage: 10Gi

---
# 3. VolumeGroupSnapshot (ä¸€è‡´æ€§å¿«ç…§)
apiVersion: groupsnapshot.storage.k8s.io/v1alpha1
kind: VolumeGroupSnapshot
metadata:
  name: mysql-consistent-snapshot
  namespace: database
spec:
  volumeGroupSnapshotClassName: csi-group-snapclass
  source:
    selector:
      matchLabels:
        app: mysql
        snapshot-group: mysql-cluster

---
# 4. CronJobè‡ªåŠ¨åŒ–å¿«ç…§
apiVersion: batch/v1
kind: CronJob
metadata:
  name: mysql-group-snapshot
  namespace: database
spec:
  schedule: "0 */6 * * *"  # æ¯6å°æ—¶
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: snapshot-controller
          containers:
          - name: snapshot
            image: bitnami/kubectl:latest
            command:
              - /bin/bash
              - -c
              - |
                TIMESTAMP=$(date +%Y%m%d-%H%M%S)
                kubectl apply -f - <<EOF
                apiVersion: groupsnapshot.storage.k8s.io/v1alpha1
                kind: VolumeGroupSnapshot
                metadata:
                  name: mysql-snapshot-${TIMESTAMP}
                  namespace: database
                spec:
                  volumeGroupSnapshotClassName: csi-group-snapclass
                  source:
                    selector:
                      matchLabels:
                        app: mysql
                        snapshot-group: mysql-cluster
                EOF
                
                # æ¸…ç†æ—§å¿«ç…§ (ä¿ç•™æœ€è¿‘7ä¸ª)
                kubectl get volumegroupsnapshot -n database \
                  --sort-by=.metadata.creationTimestamp \
                  -o jsonpath='{range .items[:-7]}{.metadata.name}{"\n"}{end}' | \
                  xargs -r kubectl delete volumegroupsnapshot -n database
          restartPolicy: OnFailure
```

### å·å¥åº·æ£€æŸ¥ (CSI 1.9 GA)

```yaml
# volume-health-monitor.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: csi-volume-health
  namespace: kube-system
data:
  config.yaml: |
    # CSIå·å¥åº·æ£€æŸ¥é…ç½®
    health_check_interval: "60s"
    
    metrics:
      - name: volume_abnormal
        type: gauge
        help: "Volume abnormal condition"
      
      - name: volume_health_status
        type: gauge
        labels: ["pv_name", "namespace", "pvc_name"]
        help: "Volume health status (0=unknown, 1=healthy, 2=degraded, 3=unhealthy)"
      
      - name: volume_io_errors
        type: counter
        help: "Volume I/O errors count"
    
    alerts:
      - name: VolumeUnhealthy
        condition: volume_health_status == 3
        severity: critical
        message: "Volume {{ $labels.pv_name }} is unhealthy"
      
      - name: VolumeDegraded
        condition: volume_health_status == 2
        duration: "5m"
        severity: warning
        message: "Volume {{ $labels.pv_name }} is degraded"

---
# Prometheuså‘Šè­¦è§„åˆ™
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-volume-health-rules
  namespace: monitoring
data:
  volume-health.yaml: |
    groups:
    - name: volume_health
      interval: 30s
      rules:
      - alert: PersistentVolumeUnhealthy
        expr: csi_volume_health_status{status="unhealthy"} == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "PV {{ $labels.pv_name }} is unhealthy"
          description: "Volume has been unhealthy for more than 5 minutes"
      
      - alert: PersistentVolumeDegraded
        expr: csi_volume_health_status{status="degraded"} == 1
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "PV {{ $labels.pv_name }} is degraded"
          description: "Volume performance is degraded"
      
      - alert: VolumeHighIOErrors
        expr: rate(csi_volume_io_errors_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High I/O error rate on {{ $labels.pv_name }}"
          description: "I/O error rate: {{ $value }} errors/sec"
```

---

## 6. å­˜å‚¨æ€§èƒ½ä¼˜åŒ– 2025

### NVMeä¼˜åŒ–

```yaml
Storage_Performance_2025:
  NVMeä¼˜åŒ–:
    è®¾å¤‡é€‰æ‹©:
      - NVMe SSD (ä¼˜äºSATA SSD)
      - ä¼ä¸šçº§NVMe (é«˜è€ä¹…åº¦)
      - PCIe 4.0/5.0
      - NVMe-oFç½‘ç»œå­˜å‚¨
    
    å†…æ ¸å‚æ•°:
      io_scheduler: none  # NVMeä¸éœ€è¦è°ƒåº¦å™¨
      nr_requests: 256
      queue_depth: 256
      add_random: 0
    
    æ–‡ä»¶ç³»ç»Ÿ:
      æ¨è: XFS, ext4
      æŒ‚è½½é€‰é¡¹:
        - noatime
        - nodiratime
        - discard  # TRIMæ”¯æŒ
  
  ç½‘ç»œä¼˜åŒ–:
    RDMA_RoCE:
      - ä½å»¶è¿Ÿ (<5Î¼s)
      - é«˜å¸¦å®½ (100Gbps+)
      - CPUå¸è½½
      - é›¶æ‹·è´
    
    ç½‘å¡é…ç½®:
      - å·¨å‹å¸§ (MTU 9000)
      - RSS (Receive Side Scaling)
      - TSO/GSO
      - ä¸­æ–­åˆå¹¶
  
  åº”ç”¨å±‚ä¼˜åŒ–:
    æ•°æ®åº“:
      - Direct I/O
      - å¼‚æ­¥I/O
      - æ‰¹é‡æäº¤
      - WALä¼˜åŒ–
    
    AI_ML:
      - GPU Direct Storage
      - æµå¼I/O
      - é¢„è¯»ä¼˜åŒ–
      - æ•°æ®å±€éƒ¨æ€§
```

### æ€§èƒ½è°ƒä¼˜è„šæœ¬

```bash
#!/bin/bash
# storage-performance-tuning.sh - 2025å¹´å­˜å‚¨æ€§èƒ½ä¼˜åŒ–

set -e

echo "=== å­˜å‚¨æ€§èƒ½ä¼˜åŒ– 2025 ==="

# 1. NVMeè®¾å¤‡ä¼˜åŒ–
optimize_nvme() {
    local device=$1
    echo "ä¼˜åŒ–NVMeè®¾å¤‡: $device"
    
    # ç¦ç”¨è°ƒåº¦å™¨
    echo none > /sys/block/$device/queue/scheduler
    
    # é˜Ÿåˆ—æ·±åº¦
    echo 256 > /sys/block/$device/queue/nr_requests
    
    # é¢„è¯»
    echo 256 > /sys/block/$device/queue/read_ahead_kb
    
    # NUMAç»‘å®š
    echo 0 > /sys/block/$device/queue/add_random
    
    # åˆå¹¶è¯·æ±‚
    echo 2 > /sys/block/$device/queue/nomerges
}

# éå†æ‰€æœ‰NVMeè®¾å¤‡
for nvme in $(ls /sys/block/ | grep nvme); do
    optimize_nvme $nvme
done

# 2. å†…æ ¸å‚æ•°ä¼˜åŒ–
cat > /etc/sysctl.d/99-storage-performance.conf <<EOF
# è™šæ‹Ÿå†…å­˜
vm.dirty_ratio = 15
vm.dirty_background_ratio = 5
vm.swappiness = 10
vm.vfs_cache_pressure = 50

# ç½‘ç»œç¼“å†²åŒº
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 4096 87380 67108864
net.ipv4.tcp_wmem = 4096 65536 67108864

# ç½‘ç»œä¼˜åŒ–
net.ipv4.tcp_congestion_control = bbr
net.core.default_qdisc = fq
net.ipv4.tcp_mtu_probing = 1

# æ–‡ä»¶æè¿°ç¬¦
fs.file-max = 2097152
fs.nr_open = 2097152
EOF

sysctl -p /etc/sysctl.d/99-storage-performance.conf

# 3. RDMA/RoCEé…ç½® (å¦‚æœæ”¯æŒ)
if lsmod | grep -q rdma; then
    echo "é…ç½®RDMA..."
    
    # åŠ è½½å†…æ ¸æ¨¡å—
    modprobe ib_core
    modprobe ib_uverbs
    modprobe rdma_ucm
    
    # è®¾ç½®ç½‘å¡MTU
    for iface in $(ibdev2netdev | awk '{print $5}'); do
        ip link set $iface mtu 9000
    done
fi

# 4. å·¨å‹å¸§é…ç½®
configure_jumbo_frames() {
    for iface in $(ip -o link show | awk -F': ' '{print $2}' | grep -E '^(eth|ens|enp)'); do
        echo "é…ç½®å·¨å‹å¸§: $iface"
        ip link set $iface mtu 9000 || true
    done
}

configure_jumbo_frames

# 5. CPUæ€§èƒ½æ¨¡å¼
echo "è®¾ç½®CPUæ€§èƒ½æ¨¡å¼..."
for cpu in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; do
    echo performance > $cpu 2>/dev/null || true
done

# 6. ä¸­æ–­äº²å’Œæ€§
set_irq_affinity() {
    local device=$1
    local cpus=$2
    
    for irq in $(grep $device /proc/interrupts | awk '{print $1}' | tr -d ':'); do
        echo $cpus > /proc/irq/$irq/smp_affinity
    done
}

# ä¸ºNVMeè®¾å¤‡è®¾ç½®ä¸­æ–­äº²å’Œæ€§ (ç»‘å®šåˆ°ç‰¹å®šCPU)
for nvme in $(lsblk -d -n -o NAME | grep nvme); do
    set_irq_affinity $nvme "f"  # ä½¿ç”¨CPU 0-3
done

echo "=== å­˜å‚¨æ€§èƒ½ä¼˜åŒ–å®Œæˆ ==="

# 7. æ€§èƒ½æµ‹è¯•
echo ""
echo "è¿è¡Œæ€§èƒ½æµ‹è¯•..."
fio --name=randread --ioengine=libaio --iodepth=32 --rw=randread \
    --bs=4k --direct=1 --size=1G --numjobs=4 --runtime=30 \
    --group_reporting --filename=/dev/nvme0n1 --output=fio-results.txt

echo "æ€§èƒ½æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° fio-results.txt"
```

### GPU Direct Storage (GDS) é…ç½®

```yaml
# gpu-direct-storage-config.yaml
# AI/MLå·¥ä½œè´Ÿè½½çš„å­˜å‚¨ä¼˜åŒ–

apiVersion: v1
kind: ConfigMap
metadata:
  name: gds-config
  namespace: gpu-workloads
data:
  gds.conf: |
    # GPU Direct Storageé…ç½®
    [gds]
    enable = true
    
    # ç›´é€šè·¯å¾„
    direct_io = true
    bypass_page_cache = true
    
    # RDMAé…ç½®
    rdma_enabled = true
    rdma_device = mlx5_0
    
    # æ€§èƒ½è°ƒä¼˜
    batch_size = 64MB
    num_threads = 8
    prefetch_size = 256MB

---
# StorageClass for AI/ML workloads
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nvme-gds-optimized
provisioner: io.openebs.csi-mayastor
parameters:
  protocol: nvmf
  repl: "3"
  
  # GDSä¼˜åŒ–
  mount_options: "noatime,nodiratime,discard"
  filesystem: "xfs"
  
  # é«˜æ€§èƒ½é…ç½®
  thin: "false"
  ioTimeout: "30"
  
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer

---
# PyTorch DDPè®­ç»ƒJob with GDS
apiVersion: batch/v1
kind: Job
metadata:
  name: pytorch-ddp-gds
  namespace: gpu-workloads
spec:
  parallelism: 4
  completions: 4
  template:
    metadata:
      labels:
        app: pytorch-ddp
    spec:
      containers:
      - name: pytorch
        image: nvcr.io/nvidia/pytorch:23.10-py3
        command:
          - python
          - -m
          - torch.distributed.launch
          - --nproc_per_node=4
          - train.py
          - --data-path=/data
          - --gds-enabled  # å¯ç”¨GDS
        resources:
          requests:
            nvidia.com/gpu: 4
            memory: 64Gi
            cpu: 16
          limits:
            nvidia.com/gpu: 4
            memory: 128Gi
        volumeMounts:
        - name: dataset
          mountPath: /data
        - name: gds-config
          mountPath: /etc/gds
      volumes:
      - name: dataset
        persistentVolumeClaim:
          claimName: imagenet-dataset-pvc
      - name: gds-config
        configMap:
          name: gds-config
      nodeSelector:
        nvidia.com/gpu.present: "true"
        storage-type: nvme
```

---

## 7. å­˜å‚¨å®‰å…¨å¢å¼º

### åŠ å¯†å·å®ç°

```yaml
Storage_Security_2025:
  åŠ å¯†æŠ€æœ¯:
    LUKS2:
      - Linuxç»Ÿä¸€å¯†é’¥è®¾ç½®
      - å…¨ç›˜åŠ å¯†
      - å¤šå¯†é’¥æ”¯æŒ
      - æ€§èƒ½å½±å“: <5%
    
    KMSé›†æˆ:
      - HashiCorp Vault
      - AWS KMS
      - Azure Key Vault
      - Google Cloud KMS
    
    åŠ å¯†å±‚çº§:
      - å­˜å‚¨æ± åŠ å¯†
      - å·çº§åŠ å¯†
      - æ–‡ä»¶ç³»ç»ŸåŠ å¯†
      - å¯¹è±¡åŠ å¯†

  è®¿é—®æ§åˆ¶:
    RBAC:
      - ç»†ç²’åº¦æƒé™
      - å‘½åç©ºé—´éš”ç¦»
      - PVCè®¿é—®æ§åˆ¶
      - StorageClassæƒé™
    
    Pod_Security:
      - åªè¯»æ ¹æ–‡ä»¶ç³»ç»Ÿ
      - éç‰¹æƒå®¹å™¨
      - securityContext
      - SELinux/AppArmor

  å®¡è®¡:
    å®¡è®¡æ—¥å¿—:
      - å·åˆ›å»º/åˆ é™¤
      - å¿«ç…§æ“ä½œ
      - è®¿é—®è®°å½•
      - å¤±è´¥å°è¯•
```

### KMSé›†æˆç¤ºä¾‹ (Vault)

```yaml
# vault-kms-integration.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: csi-vault-sa
  namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: csi-vault-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: csi-vault-sa
  namespace: kube-system

---
# Vaulté…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: vault-kms-config
  namespace: kube-system
data:
  vault.conf: |
    vault {
      address = "https://vault.example.com:8200"
      auth {
        method = "kubernetes"
        path   = "auth/kubernetes/login"
        role   = "csi-encryption"
      }
      
      # å¯†é’¥è·¯å¾„
      transit {
        mount_path = "transit"
        key_name   = "csi-encryption-key"
      }
      
      # TLSé…ç½®
      tls {
        ca_cert = "/etc/vault/ca.crt"
        skip_verify = false
      }
    }

---
# StorageClass with encryption
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: encrypted-storage
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: encrypted-pool
  
  # åŠ å¯†é…ç½®
  encrypted: "true"
  encryptionKMSID: "vault-kms"
  encryptionType: "luks2"
  
  # Vault KMSé…ç½®
  encryptionKMSType: "vaulttokens"
  vaultAddress: "https://vault.example.com:8200"
  vaultAuthPath: "/v1/auth/kubernetes/login"
  vaultRole: "csi-encryption"
  vaultNamespace: ""
  vaultCAFromSecret: "vault-ca"
  
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: Immediate
```

---

## 8. å¤šé›†ç¾¤å­˜å‚¨è”é‚¦

### è·¨é›†ç¾¤å­˜å‚¨å¤åˆ¶

```yaml
Multi_Cluster_Storage_2025:
  æ¶æ„æ¨¡å¼:
    ä¸»å¤‡æ¨¡å¼:
      - ä¸»é›†ç¾¤ï¼šç”Ÿäº§ç¯å¢ƒ
      - å¤‡é›†ç¾¤ï¼šç¾éš¾æ¢å¤
      - å•å‘å¤åˆ¶
      - è‡ªåŠ¨æ•…éšœè½¬ç§»
    
    å¤šä¸»æ¨¡å¼:
      - å¤šä¸ªæ´»è·ƒé›†ç¾¤
      - åŒå‘å¤åˆ¶
      - å°±è¿‘è®¿é—®
      - è´Ÿè½½å‡è¡¡
    
    åˆ†å¸ƒå¼æ¨¡å¼:
      - åœ°ç†åˆ†å¸ƒ
      - æ•°æ®åˆ†ç‰‡
      - ä¸€è‡´æ€§å“ˆå¸Œ
      - è·¨åŸŸè®¿é—®

  å¤åˆ¶æ–¹æ¡ˆ:
    Rook_Ceph:
      - RBDé•œåƒ (mirroring)
      - CephFSå¿«ç…§é•œåƒ
      - RGWå¤šç«™ç‚¹
      - å¼‚æ­¥/åŒæ­¥å¤åˆ¶
    
    Longhorn:
      - è¿œç¨‹å¤‡ä»½
      - S3åŒæ­¥
      - ç¾éš¾æ¢å¤
      - è·¨åŒºåŸŸå¤‡ä»½
    
    Velero:
      - é›†ç¾¤çº§å¤‡ä»½
      - åº”ç”¨è¿ç§»
      - è·¨äº‘å¤åˆ¶
      - å®šæœŸå¤‡ä»½
```

### Rook-Ceph è·¨é›†ç¾¤é•œåƒ

```yaml
# rbd-mirroring-cluster-a.yaml (ä¸»é›†ç¾¤)
apiVersion: ceph.rook.io/v1
kind: CephRBDMirror
metadata:
  name: my-rbd-mirror
  namespace: rook-ceph
spec:
  count: 2
  peers:
    secretNames:
      - rbd-mirror-peer-secret  # å¯¹ç«¯é›†ç¾¤å‡­è¯
  
  placement:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: role
            operator: In
            values:
            - storage-node
  
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      memory: "4Gi"

---
# Secret for peer cluster (é›†ç¾¤Bä¿¡æ¯)
apiVersion: v1
kind: Secret
metadata:
  name: rbd-mirror-peer-secret
  namespace: rook-ceph
type: Opaque
stringData:
  # å¯¹ç«¯é›†ç¾¤ç›‘æ§åœ°å€
  mon_host: "10.20.30.40:6789,10.20.30.41:6789,10.20.30.42:6789"
  # å¯¹ç«¯é›†ç¾¤åç§°
  cluster: "cluster-b"
  # å¯¹ç«¯é›†ç¾¤å¯†é’¥
  key: "AQC...=="  # ceph auth get client.rbd-mirror è·å–

---
# å¯ç”¨é•œåƒçš„å­˜å‚¨æ± 
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: mirrored-pool
  namespace: rook-ceph
spec:
  replicated:
    size: 3
    requireSafeReplicaSize: true
  
  # å¯ç”¨RBDé•œåƒ
  mirroring:
    enabled: true
    mode: "image"  # æˆ– "pool" (å…¨æ± é•œåƒ)
    
    # 2025æ–°å¢ï¼šå¿«ç…§é•œåƒ
    snapshotSchedules:
      - interval: "24h"  # æ¯å¤©ä¸€æ¬¡
        startTime: "02:00:00Z"

---
# StorageClass for mirrored volumes
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block-mirrored
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: mirrored-pool
  imageFeatures: layering
  
  # è‡ªåŠ¨å¯ç”¨é•œåƒ
  imageFeatures: "layering,exclusive-lock,object-map,fast-diff,journaling"
  
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

allowVolumeExpansion: true
reclaimPolicy: Retain  # é‡è¦ï¼šä¿ç•™æ•°æ®
volumeBindingMode: Immediate
```

### Longhorn è·¨é›†ç¾¤å¤‡ä»½

```yaml
# longhorn-dr-volume.yaml
apiVersion: longhorn.io/v1beta2
kind: Volume
metadata:
  name: critical-data-volume
  namespace: longhorn-system
spec:
  size: "100Gi"
  numberOfReplicas: 3
  dataLocality: "best-effort"
  
  # 2025æ–°å¢ï¼šç¾éš¾æ¢å¤é…ç½®
  dr:
    enabled: true
    remoteBackupTarget: "s3://longhorn-dr@us-west-2/"  # ä¸åŒåŒºåŸŸ
    scheduledBackup:
      cron: "0 */6 * * *"  # æ¯6å°æ—¶
      retain: 14  # ä¿ç•™14ä¸ªå¤‡ä»½
    
    # æ•…éšœè½¬ç§»è®¾ç½®
    failover:
      autoPromote: true  # è‡ªåŠ¨æå‡å¤‡ä»½ä¸ºä¸»å·
      healthCheckInterval: "5m"
      unhealthyThreshold: 3

---
# RecurringJob for DRå¤‡ä»½
apiVersion: longhorn.io/v1beta2
kind: RecurringJob
metadata:
  name: dr-backup-6h
  namespace: longhorn-system
spec:
  cron: "0 */6 * * *"
  task: "backup"
  groups:
    - dr-critical
  retain: 28  # ä¿ç•™28ä¸ª = 7å¤©
  concurrency: 2
  labels:
    disaster-recovery: "true"
    tier: "critical"

---
# ç¾éš¾æ¢å¤æ¼”ç»ƒCronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dr-drill-monthly
  namespace: longhorn-system
spec:
  schedule: "0 0 1 * *"  # æ¯æœˆ1å·
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: longhorn-dr
          containers:
          - name: dr-drill
            image: longhornio/longhorn-manager:v1.6.0
            command:
              - /bin/bash
              - -c
              - |
                echo "=== ç¾éš¾æ¢å¤æ¼”ç»ƒ ==="
                
                # 1. åˆ—å‡ºæ‰€æœ‰DRå¤‡ä»½
                longhorn-manager backup list --backup-target s3://longhorn-dr@us-west-2/
                
                # 2. é€‰æ‹©æœ€æ–°çš„criticalå¤‡ä»½
                LATEST_BACKUP=$(longhorn-manager backup list \
                  --backup-target s3://longhorn-dr@us-west-2/ \
                  --volume critical-data-volume \
                  --output json | jq -r '.[0].name')
                
                # 3. åœ¨DRé›†ç¾¤åˆ›å»ºæµ‹è¯•å·
                kubectl apply -f - <<EOF
                apiVersion: longhorn.io/v1beta2
                kind: Volume
                metadata:
                  name: dr-test-$(date +%Y%m%d)
                  namespace: longhorn-system
                spec:
                  fromBackup: "s3://longhorn-dr@us-west-2/${LATEST_BACKUP}"
                  numberOfReplicas: 1
                EOF
                
                # 4. éªŒè¯æ•°æ®å®Œæ•´æ€§
                # ... (åº”ç”¨ç‰¹å®šçš„éªŒè¯é€»è¾‘)
                
                # 5. æ¸…ç†æµ‹è¯•èµ„æº
                kubectl delete volume dr-test-$(date +%Y%m%d) -n longhorn-system
                
                echo "=== æ¼”ç»ƒå®Œæˆ ==="
          restartPolicy: OnFailure
```

---

## 9. å­˜å‚¨æˆæœ¬ä¼˜åŒ– (FinOps)

### æˆæœ¬åˆ†æ

```yaml
Storage_FinOps_2025:
  æˆæœ¬ç»´åº¦:
    å­˜å‚¨å®¹é‡:
      - å·²ä½¿ç”¨å®¹é‡
      - å·²åˆ†é…å®¹é‡
      - è¶…é¢åˆ†é…ç‡
      - å¢é•¿è¶‹åŠ¿
    
    æ€§èƒ½å±‚çº§:
      - çƒ­æ•°æ® (NVMe)
      - æ¸©æ•°æ® (SSD)
      - å†·æ•°æ® (HDD)
      - å½’æ¡£æ•°æ® (S3/Glacier)
    
    å‰¯æœ¬ç­–ç•¥:
      - 3å‰¯æœ¬ (é«˜å¯ç”¨)
      - 2å‰¯æœ¬ (å¹³è¡¡)
      - ECç¼–ç  (çœç©ºé—´)
      - å•å‰¯æœ¬ (éå…³é”®)

  ä¼˜åŒ–ç­–ç•¥:
    è‡ªåŠ¨åˆ†å±‚:
      - è®¿é—®é¢‘ç‡ç›‘æ§
      - è‡ªåŠ¨æ•°æ®è¿ç§»
      - å†·çƒ­æ•°æ®åˆ†ç¦»
      - æˆæœ¬é™ä½50-70%
    
    å®¹é‡å›æ”¶:
      - æœªä½¿ç”¨PVCæ¸…ç†
      - å­¤å„¿å·åˆ é™¤
      - å¿«ç…§è¿‡æœŸæ¸…ç†
      - ç©ºé—´å‹ç¼©
    
    æŒ‰éœ€åˆ†é…:
      - Thin Provisioning
      - åŠ¨æ€æ‰©å®¹
      - é¿å…è¿‡åº¦åˆ†é…
      - ä½¿ç”¨ç‡ç›‘æ§

  æˆæœ¬ç›‘æ§å·¥å…·:
    Kubecost:
      - å­˜å‚¨æˆæœ¬åˆ†æ
      - æŒ‰å‘½åç©ºé—´/åº”ç”¨
      - è¶‹åŠ¿é¢„æµ‹
      - ä¼˜åŒ–å»ºè®®
    
    OpenCost:
      - CNCFå¼€æºé¡¹ç›®
      - å¤šäº‘æˆæœ¬
      - å®æ—¶ç›‘æ§
      - APIé›†æˆ
```

### Kubecost å­˜å‚¨ç›‘æ§éƒ¨ç½²

```bash
#!/bin/bash
# kubecost-storage-monitoring.sh

set -e

echo "=== éƒ¨ç½²Kubecostå­˜å‚¨æˆæœ¬ç›‘æ§ ==="

# 1. æ·»åŠ Helmä»“åº“
helm repo add kubecost https://kubecost.github.io/cost-analyzer/
helm repo update

# 2. éƒ¨ç½²Kubecost
kubectl create namespace kubecost --dry-run=client -o yaml | kubectl apply -f -

helm install kubecost kubecost/cost-analyzer \
  --namespace kubecost \
  --set kubecostToken="your-token-here" \
  --set prometheus.server.persistentVolume.enabled=true \
  --set prometheus.server.persistentVolume.size=32Gi \
  --set persistentVolume.enabled=true \
  --set persistentVolume.size=32Gi \
  --set ingress.enabled=true \
  --set ingress.hosts[0]=kubecost.example.com \
  --set networkPolicy.enabled=false \
  --set global.prometheus.enabled=true \
  --set global.prometheus.fqdn=http://prometheus-server.prometheus.svc.cluster.local

# 3. é…ç½®å­˜å‚¨æˆæœ¬
kubectl apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: storage-cost-config
  namespace: kubecost
data:
  pricing.json: |
    {
      "storage": {
        "classes": {
          "rook-ceph-block": {
            "cost_per_gb_month": 0.10,
            "performance_tier": "high",
            "notes": "Rook-Ceph SSD storage"
          },
          "longhorn": {
            "cost_per_gb_month": 0.08,
            "performance_tier": "medium",
            "notes": "Longhorn distributed storage"
          },
          "nfs-client": {
            "cost_per_gb_month": 0.05,
            "performance_tier": "low",
            "notes": "NFS shared storage"
          },
          "local-path": {
            "cost_per_gb_month": 0.03,
            "performance_tier": "local",
            "notes": "Local node storage"
          }
        },
        "snapshots": {
          "cost_per_gb_month": 0.02
        },
        "backups": {
          "s3_standard": 0.023,
          "s3_ia": 0.0125,
          "s3_glacier": 0.004
        }
      }
    }
EOF

# 4. å­˜å‚¨æˆæœ¬åˆ†æJob
kubectl apply -f - <<EOF
apiVersion: batch/v1
kind: CronJob
metadata:
  name: storage-cost-report
  namespace: kubecost
spec:
  schedule: "0 0 * * 0"  # æ¯å‘¨æ—¥
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: kubecost
          containers:
          - name: cost-report
            image: curlimages/curl:latest
            command:
              - /bin/sh
              - -c
              - |
                # è°ƒç”¨Kubecost APIç”ŸæˆæŠ¥å‘Š
                curl -X GET "http://kubecost-cost-analyzer.kubecost:9090/model/allocation" \
                  -d window=7d \
                  -d aggregate=storageclass \
                  -d accumulate=true \
                  -d shareIdle=false \
                  | jq '.' > /tmp/storage-cost-report.json
                
                # å‘é€åˆ°Slack/Email
                # ...
          restartPolicy: OnFailure
EOF

# 5. è®¿é—®Kubecost UI
echo ""
echo "=== Kubecostéƒ¨ç½²å®Œæˆ ==="
echo "è®¿é—®UI: kubectl port-forward -n kubecost svc/kubecost-cost-analyzer 9090:9090"
echo "æµè§ˆå™¨: http://localhost:9090"
```

### è‡ªåŠ¨æ¸…ç†æœªä½¿ç”¨å­˜å‚¨

```yaml
# storage-cleanup-automation.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: storage-cleanup
  namespace: kube-system
spec:
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨2ç‚¹
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: storage-admin
          containers:
          - name: cleanup
            image: bitnami/kubectl:latest
            command:
              - /bin/bash
              - -c
              - |
                echo "=== å­˜å‚¨æ¸…ç†ä»»åŠ¡ ==="
                
                # 1. æ¸…ç†æœªç»‘å®šPVC (è¶…è¿‡7å¤©)
                echo "1. æ¸…ç†æœªç»‘å®šPVC..."
                kubectl get pvc --all-namespaces -o json | \
                jq -r '.items[] | 
                  select(.status.phase=="Pending") | 
                  select((now - (.metadata.creationTimestamp | fromdateiso8601)) > 604800) | 
                  "\(.metadata.namespace) \(.metadata.name)"' | \
                while read ns name; do
                  echo "åˆ é™¤æœªç»‘å®šPVC: $ns/$name"
                  kubectl delete pvc $name -n $ns
                done
                
                # 2. æ¸…ç†å­¤å„¿PV (ReleasedçŠ¶æ€è¶…è¿‡3å¤©)
                echo "2. æ¸…ç†å­¤å„¿PV..."
                kubectl get pv -o json | \
                jq -r '.items[] | 
                  select(.status.phase=="Released") | 
                  select((now - (.metadata.creationTimestamp | fromdateiso8601)) > 259200) | 
                  .metadata.name' | \
                while read pv; do
                  echo "åˆ é™¤å­¤å„¿PV: $pv"
                  kubectl delete pv $pv
                done
                
                # 3. æ¸…ç†è¿‡æœŸå¿«ç…§ (è¶…è¿‡30å¤©)
                echo "3. æ¸…ç†è¿‡æœŸå¿«ç…§..."
                kubectl get volumesnapshot --all-namespaces -o json | \
                jq -r '.items[] | 
                  select(.metadata.labels.auto=="true") | 
                  select((now - (.metadata.creationTimestamp | fromdateiso8601)) > 2592000) | 
                  "\(.metadata.namespace) \(.metadata.name)"' | \
                while read ns name; do
                  echo "åˆ é™¤è¿‡æœŸå¿«ç…§: $ns/$name"
                  kubectl delete volumesnapshot $name -n $ns
                done
                
                # 4. Longhornå·æ¸…ç† (æœªä½¿ç”¨)
                echo "4. æ¸…ç†Longhornæœªä½¿ç”¨å·..."
                kubectl -n longhorn-system get volumes.longhorn.io -o json | \
                jq -r '.items[] | 
                  select(.status.state=="detached") | 
                  select(.status.kubernetesStatus.pvStatus=="" or .status.kubernetesStatus.pvStatus==null) | 
                  .metadata.name' | \
                while read vol; do
                  echo "åˆ é™¤æœªä½¿ç”¨Longhornå·: $vol"
                  kubectl -n longhorn-system delete volumes.longhorn.io $vol
                done
                
                # 5. ç”Ÿæˆæ¸…ç†æŠ¥å‘Š
                echo ""
                echo "=== æ¸…ç†ç»Ÿè®¡ ==="
                echo "PVCæ•°é‡: $(kubectl get pvc --all-namespaces --no-headers | wc -l)"
                echo "PVæ•°é‡: $(kubectl get pv --no-headers | wc -l)"
                echo "å¿«ç…§æ•°é‡: $(kubectl get volumesnapshot --all-namespaces --no-headers | wc -l)"
                echo "Longhornå·: $(kubectl -n longhorn-system get volumes.longhorn.io --no-headers | wc -l)"
          restartPolicy: OnFailure

---
# RBAC for cleanup job
apiVersion: v1
kind: ServiceAccount
metadata:
  name: storage-admin
  namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: storage-admin
rules:
- apiGroups: [""]
  resources: ["persistentvolumes", "persistentvolumeclaims"]
  verbs: ["get", "list", "delete"]
- apiGroups: ["snapshot.storage.k8s.io"]
  resources: ["volumesnapshots"]
  verbs: ["get", "list", "delete"]
- apiGroups: ["longhorn.io"]
  resources: ["volumes"]
  verbs: ["get", "list", "delete"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: storage-admin-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: storage-admin
subjects:
- kind: ServiceAccount
  name: storage-admin
  namespace: kube-system
```

---

## 10. ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ 2025

### æ¶æ„è®¾è®¡æ¸…å•

```yaml
Production_Best_Practices_2025:
  è§„åˆ’é˜¶æ®µ:
    å®¹é‡è§„åˆ’:
      - åˆå§‹å®¹é‡è¯„ä¼°
      - 3å¹´å¢é•¿é¢„æµ‹
      - æ€§èƒ½éœ€æ±‚åˆ†æ
      - æˆæœ¬é¢„ç®—
    
    é«˜å¯ç”¨è®¾è®¡:
      - è‡³å°‘3ä¸ªå­˜å‚¨èŠ‚ç‚¹
      - è·¨å¯ç”¨åŒºéƒ¨ç½²
      - å‰¯æœ¬ç­–ç•¥ (3å‰¯æœ¬æˆ–EC)
      - æ•…éšœåŸŸéš”ç¦»
    
    æ€§èƒ½è§„åˆ’:
      - IOPSéœ€æ±‚
      - ååé‡éœ€æ±‚
      - å»¶è¿Ÿè¦æ±‚
      - ç½‘ç»œå¸¦å®½

  éƒ¨ç½²é˜¶æ®µ:
    ç¡¬ä»¶é€‰å‹:
      å­˜å‚¨èŠ‚ç‚¹:
        - NVMe SSDä¼ä¸šçº§
        - 32GB+ å†…å­˜
        - 10GbE/25GbEç½‘ç»œ
        - RAIDæ§åˆ¶å™¨ (å¯é€‰)
      
      ç½‘ç»œ:
        - ç‹¬ç«‹å­˜å‚¨ç½‘ç»œ
        - å·¨å‹å¸§æ”¯æŒ
        - RDMA (å¯é€‰)
        - å†—ä½™é“¾è·¯
    
    è½¯ä»¶é…ç½®:
      - å†…æ ¸å‚æ•°è°ƒä¼˜
      - æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–
      - ç›‘æ§å‘Šè­¦
      - å¤‡ä»½ç­–ç•¥

  è¿ç»´é˜¶æ®µ:
    ç›‘æ§:
      - å®¹é‡ä½¿ç”¨ç‡
      - æ€§èƒ½æŒ‡æ ‡
      - é”™è¯¯æ—¥å¿—
      - å¥åº·æ£€æŸ¥
    
    å¤‡ä»½:
      - æ¯æ—¥å¢é‡å¤‡ä»½
      - æ¯å‘¨å…¨é‡å¤‡ä»½
      - å¼‚åœ°å¤‡ä»½
      - æ¢å¤æ¼”ç»ƒ
    
    å®‰å…¨:
      - åŠ å¯†å·
      - è®¿é—®æ§åˆ¶
      - å®¡è®¡æ—¥å¿—
      - å®šæœŸæ‰«æ

  ç¾éš¾æ¢å¤:
    RTO_RPOç›®æ ‡:
      - RTO: <1å°æ—¶
      - RPO: <15åˆ†é’Ÿ
      - è‡ªåŠ¨åŒ–æ¢å¤
      - å®šæœŸæ¼”ç»ƒ
```

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ£€æŸ¥æ¸…å•

```yaml
# production-readiness-checklist.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: storage-production-checklist
  namespace: kube-system
data:
  checklist.yaml: |
    production_readiness:
      infrastructure:
        - item: "è‡³å°‘3ä¸ªä¸“ç”¨å­˜å‚¨èŠ‚ç‚¹"
          status: "required"
          verify: "kubectl get nodes -l role=storage-node --no-headers | wc -l"
          expected: ">= 3"
        
        - item: "èŠ‚ç‚¹è·¨å¯ç”¨åŒºåˆ†å¸ƒ"
          status: "required"
          verify: "kubectl get nodes -l role=storage-node -o json | jq '.items[].metadata.labels.\"topology.kubernetes.io/zone\"' | sort -u | wc -l"
          expected: ">= 2"
        
        - item: "æ¯èŠ‚ç‚¹NVMe SSDé…ç½®"
          status: "required"
          verify: "lsblk -d -n -o name,rota | grep nvme | grep ' 0$' | wc -l"
          expected: ">= 1"
        
        - item: "å­˜å‚¨ç½‘ç»œç‹¬ç«‹VLAN"
          status: "recommended"
          verify: "ip link show | grep storage-net"
          expected: "å­˜åœ¨"
      
      storage_cluster:
        - item: "Rook/Longhornç‰ˆæœ¬æœ€æ–°ç¨³å®šç‰ˆ"
          status: "required"
          verify: "kubectl -n rook-ceph get deploy rook-ceph-operator -o jsonpath='{.spec.template.spec.containers[0].image}'"
          expected: "v1.13+"
        
        - item: "ç›‘æ§ç»„ä»¶MON >= 3 (å¥‡æ•°)"
          status: "required"
          verify: "kubectl -n rook-ceph get deploy -l app=rook-ceph-mon --no-headers | wc -l"
          expected: "3 or 5"
        
        - item: "OSDæ•°é‡ >= 3"
          status: "required"
          verify: "kubectl -n rook-ceph get deploy -l app=rook-ceph-osd --no-headers | wc -l"
          expected: ">= 3"
        
        - item: "å‰¯æœ¬æ•°é…ç½®ä¸º3"
          status: "required"
          verify: "kubectl -n rook-ceph get cephblockpool replicapool -o jsonpath='{.spec.replicated.size}'"
          expected: "3"
      
      performance:
        - item: "NVMeè°ƒåº¦å™¨è®¾ç½®ä¸ºnone"
          status: "required"
          verify: "cat /sys/block/nvme0n1/queue/scheduler"
          expected: "[none]"
        
        - item: "å†…æ ¸å‚æ•°vm.swappiness <= 10"
          status: "required"
          verify: "sysctl vm.swappiness"
          expected: "<= 10"
        
        - item: "å·¨å‹å¸§å¯ç”¨ (MTU 9000)"
          status: "recommended"
          verify: "ip link show eth0 | grep mtu"
          expected: "9000"
      
      security:
        - item: "åŠ å¯†å·StorageClassé…ç½®"
          status: "required"
          verify: "kubectl get sc -o json | jq '.items[] | select(.parameters.encrypted==\"true\") | .metadata.name'"
          expected: "å­˜åœ¨è‡³å°‘ä¸€ä¸ª"
        
        - item: "KMSé›†æˆ (Vault/KMS)"
          status: "recommended"
          verify: "kubectl -n rook-ceph get secret | grep kms"
          expected: "å­˜åœ¨"
        
        - item: "RBACç»†ç²’åº¦æƒé™"
          status: "required"
          verify: "kubectl get clusterrole | grep storage"
          expected: "å­˜åœ¨"
      
      monitoring:
        - item: "Prometheusç›‘æ§éƒ¨ç½²"
          status: "required"
          verify: "kubectl -n monitoring get deploy prometheus-server"
          expected: "Running"
        
        - item: "Grafanaä»ªè¡¨æ¿é…ç½®"
          status: "required"
          verify: "kubectl -n monitoring get cm grafana-dashboards"
          expected: "å­˜åœ¨"
        
        - item: "å‘Šè­¦è§„åˆ™é…ç½®"
          status: "required"
          verify: "kubectl -n monitoring get prometheusrule"
          expected: "å­˜åœ¨"
      
      backup_dr:
        - item: "å¤‡ä»½ç›®æ ‡é…ç½® (S3)"
          status: "required"
          verify: "kubectl -n longhorn-system get setting backup-target -o jsonpath='{.value}'"
          expected: "s3://"
        
        - item: "æ¯æ—¥è‡ªåŠ¨å¤‡ä»½Job"
          status: "required"
          verify: "kubectl get cronjob | grep backup"
          expected: "å­˜åœ¨"
        
        - item: "è·¨åŒºåŸŸå¤‡ä»½é…ç½®"
          status: "recommended"
          verify: "kubectl -n longhorn-system get recurringjob -l type=backup -o json | jq '.items[].spec.retain'"
          expected: ">= 7"
        
        - item: "ç¾éš¾æ¢å¤æ¼”ç»ƒè®¡åˆ’"
          status: "required"
          verify: "kubectl get cronjob dr-drill"
          expected: "å­˜åœ¨"
      
      cost_optimization:
        - item: "Kubecost/OpenCostéƒ¨ç½²"
          status: "recommended"
          verify: "kubectl -n kubecost get deploy kubecost-cost-analyzer"
          expected: "Running"
        
        - item: "è‡ªåŠ¨æ¸…ç†Jobé…ç½®"
          status: "recommended"
          verify: "kubectl -n kube-system get cronjob storage-cleanup"
          expected: "å­˜åœ¨"
        
        - item: "å­˜å‚¨ä½¿ç”¨ç‡ç›‘æ§"
          status: "required"
          verify: "kubectl top node"
          expected: "< 80%"

---
# è‡ªåŠ¨æ£€æŸ¥è„šæœ¬
apiVersion: v1
kind: ConfigMap
metadata:
  name: storage-readiness-script
  namespace: kube-system
data:
  check.sh: |
    #!/bin/bash
    # production-storage-readiness-check.sh
    
    set -e
    
    echo "======================================"
    echo "  å­˜å‚¨ç”Ÿäº§ç¯å¢ƒå°±ç»ªæ£€æŸ¥ - 2025"
    echo "======================================"
    echo ""
    
    PASS=0
    FAIL=0
    WARN=0
    
    check_item() {
        local name="$1"
        local command="$2"
        local expected="$3"
        local severity="$4"
        
        echo -n "æ£€æŸ¥: $name ... "
        
        result=$(eval "$command" 2>/dev/null || echo "FAILED")
        
        if [[ "$result" == "$expected" ]] || [[ "$result" =~ $expected ]]; then
            echo "âœ… PASS"
            ((PASS++))
        else
            if [[ "$severity" == "required" ]]; then
                echo "âŒ FAIL (æœŸæœ›: $expected, å®é™…: $result)"
                ((FAIL++))
            else
                echo "âš ï¸  WARN (æœŸæœ›: $expected, å®é™…: $result)"
                ((WARN++))
            fi
        fi
    }
    
    echo "## 1. åŸºç¡€è®¾æ–½æ£€æŸ¥"
    check_item "å­˜å‚¨èŠ‚ç‚¹æ•°é‡" \
        "kubectl get nodes -l role=storage-node --no-headers | wc -l" \
        "[3-9]" \
        "required"
    
    check_item "å¯ç”¨åŒºåˆ†å¸ƒ" \
        "kubectl get nodes -l role=storage-node -o json | jq '.items[].metadata.labels.\"topology.kubernetes.io/zone\"' | sort -u | wc -l" \
        "[2-9]" \
        "required"
    
    echo ""
    echo "## 2. å­˜å‚¨é›†ç¾¤æ£€æŸ¥"
    check_item "Rook Operatorè¿è¡Œ" \
        "kubectl -n rook-ceph get deploy rook-ceph-operator -o jsonpath='{.status.availableReplicas}'" \
        "1" \
        "required"
    
    check_item "MONæ•°é‡ (å¥‡æ•°)" \
        "kubectl -n rook-ceph get deploy -l app=rook-ceph-mon --no-headers | wc -l" \
        "[35]" \
        "required"
    
    echo ""
    echo "## 3. æ€§èƒ½é…ç½®æ£€æŸ¥"
    check_item "vm.swappiness" \
        "sysctl -n vm.swappiness" \
        "10" \
        "required"
    
    echo ""
    echo "## 4. å®‰å…¨é…ç½®æ£€æŸ¥"
    check_item "åŠ å¯†StorageClass" \
        "kubectl get sc -o json | jq '.items[] | select(.parameters.encrypted==\"true\") | .metadata.name' | wc -l" \
        "[1-9]" \
        "required"
    
    echo ""
    echo "## 5. ç›‘æ§å‘Šè­¦æ£€æŸ¥"
    check_item "Prometheusè¿è¡Œ" \
        "kubectl -n monitoring get deploy prometheus-server -o jsonpath='{.status.availableReplicas}'" \
        "1" \
        "required"
    
    echo ""
    echo "## 6. å¤‡ä»½ç¾éš¾æ¢å¤æ£€æŸ¥"
    check_item "å¤‡ä»½ç›®æ ‡é…ç½®" \
        "kubectl -n longhorn-system get setting backup-target -o jsonpath='{.value}' | grep -o 's3://'" \
        "s3://" \
        "required"
    
    echo ""
    echo "======================================"
    echo "  æ£€æŸ¥ç»“æœæ±‡æ€»"
    echo "======================================"
    echo "âœ… é€šè¿‡: $PASS"
    echo "âŒ å¤±è´¥: $FAIL"
    echo "âš ï¸  è­¦å‘Š: $WARN"
    echo ""
    
    if [ $FAIL -eq 0 ]; then
        echo "ğŸ‰ å­˜å‚¨ç³»ç»Ÿå·²å‡†å¤‡å¥½æŠ•å…¥ç”Ÿäº§ï¼"
        exit 0
    else
        echo "âŒ å­˜å‚¨ç³»ç»Ÿæœªè¾¾åˆ°ç”Ÿäº§æ ‡å‡†ï¼Œè¯·ä¿®å¤å¤±è´¥é¡¹ã€‚"
        exit 1
    fi
```

---

## ç›¸å…³æ–‡æ¡£

### æœ¬æ¨¡å—æ–‡æ¡£

- [CSIå­˜å‚¨æ¦‚è¿°](01_CSIå­˜å‚¨æ¦‚è¿°.md) - CSIè§„èŒƒå’Œå­˜å‚¨ç±»å‹
- [Rook-Cephå­˜å‚¨](02_Rook_Cephå­˜å‚¨.md) - Cephåˆ†å¸ƒå¼å­˜å‚¨
- [Longhornå­˜å‚¨](03_Longhornå­˜å‚¨.md) - äº‘åŸç”Ÿå—å­˜å‚¨
- [StorageClassæœ€ä½³å®è·µ](04_StorageClassæœ€ä½³å®è·µ.md) - å­˜å‚¨ç±»é…ç½®

### ç›¸å…³æ¨¡å—

- [å®¹å™¨ç½‘ç»œ](../03_å®¹å™¨ç½‘ç»œ/README.md) - CNIå’ŒCilium
- [ç›‘æ§å‘Šè­¦](../../04_è¿ç»´ç®¡ç†/01_ç›‘æ§å‘Šè­¦/README.md) - Prometheusç›‘æ§
- [è‡ªåŠ¨åŒ–è¿ç»´](../../04_è¿ç»´ç®¡ç†/03_è‡ªåŠ¨åŒ–è¿ç»´/README.md) - Ansible/Terraform

### å¤–éƒ¨èµ„æº

- [CSI Spec](https://github.com/container-storage-interface/spec)
- [Rook Documentation](https://rook.io/docs/rook/latest/)
- [Longhorn Documentation](https://longhorn.io/docs/)
- [OpenEBS Documentation](https://openebs.io/docs)
- [CNCF Storage Landscape](https://landscape.cncf.io/card-mode?category=cloud-native-storage)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æ›´æ–°æ—¥æœŸ**: 2025-10-20  
**çŠ¶æ€**: âœ… **2025å¹´æœ€æ–°æŠ€æœ¯æ ‡å‡†å¯¹é½å®Œæˆ**
