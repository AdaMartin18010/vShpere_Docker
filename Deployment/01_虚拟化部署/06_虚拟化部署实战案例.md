# 虚拟化部署完整实战案例

> **返回**: [虚拟化部署首页](README.md) | [部署指南首页](../00_索引导航/README.md)

---

## 📋 目录

- [虚拟化部署完整实战案例](#虚拟化部署完整实战案例)
  - [📋 目录](#-目录)
  - [案例概述](#案例概述)
  - [案例一: 中小企业VMware vSphere部署](#案例一-中小企业vmware-vsphere部署)
    - [业务需求分析](#业务需求分析)
    - [硬件选型与采购](#硬件选型与采购)
    - [部署实施步骤](#部署实施步骤)
    - [配置验证与测试](#配置验证与测试)
  - [案例二: 互联网公司KVM虚拟化集群](#案例二-互联网公司kvm虚拟化集群)
    - [业务需求分析](#业务需求分析-1)
    - [架构设计](#架构设计)
    - [部署实施](#部署实施)
    - [自动化运维](#自动化运维)
  - [案例三: 金融行业超融合HCI方案](#案例三-金融行业超融合hci方案)
    - [业务需求分析](#业务需求分析-2)
    - [vSAN架构设计](#vsan架构设计)
    - [部署实施](#部署实施-1)
    - [高可用验证](#高可用验证)
  - [案例四: 混合云虚拟化架构](#案例四-混合云虚拟化架构)
    - [业务场景](#业务场景)
    - [架构设计](#架构设计-1)
    - [部署实施](#部署实施-2)
  - [常见问题与解决方案](#常见问题与解决方案)
  - [最佳实践总结](#最佳实践总结)
    - [硬件选型](#硬件选型)
    - [软件配置](#软件配置)
    - [运维管理](#运维管理)
    - [安全加固](#安全加固)

---

## 案例概述

本文档提供4个真实的虚拟化部署案例，涵盖不同规模和需求场景：

| 案例 | 适用场景 | 规模 | 技术栈 | 预算 |
|------|---------|------|--------|------|
| 案例一 | 中小企业 | 50-100 VM | VMware vSphere | ¥50万 |
| 案例二 | 互联网公司 | 500+ VM | KVM + Ceph | ¥200万 |
| 案例三 | 金融行业 | 200 VM | vSAN HCI | ¥350万 |
| 案例四 | 混合云 | 300 VM | VMware + 公有云 | ¥150万 |

---

## 案例一: 中小企业VMware vSphere部署

### 业务需求分析

**企业背景**:

- 制造业企业，员工500人
- 现有应用: ERP、OA、CRM、文件服务器
- 目标: 整合20台物理服务器到虚拟化平台
- 预期VM数量: 80台

**技术需求**:

```yaml
性能要求:
  CPU总核心: 200+ vCPU
  内存总量: 800GB+
  存储容量: 50TB (原始)
  网络带宽: 10Gbps

可用性要求:
  业务连续性: 99.9%
  计划内停机: <4小时/月
  数据备份: 每日备份
  恢复时间: <4小时

安全要求:
  网络隔离: VLAN隔离
  数据加密: 静态加密
  访问控制: RBAC
  审计日志: 完整审计
```

### 硬件选型与采购

**主机配置** (3台Dell PowerEdge R750):

```yaml
服务器配置:
  型号: Dell PowerEdge R750
  数量: 3台
  
  CPU配置:
    型号: 2x Intel Xeon Silver 4316 (20核 2.3GHz)
    总核心: 40核80线程/台
    虚拟化特性: VT-x, VT-d, EPT
  
  内存配置:
    容量: 512GB DDR4-3200 ECC RDIMM
    配置: 16x 32GB (填满所有通道)
    预留扩展: 可扩展至1TB
  
  存储配置:
    系统盘: 2x 480GB SATA SSD (RAID1)
    缓存盘: 2x 1.6TB NVMe SSD (vSAN缓存层)
    容量盘: 6x 4TB SATA SSD (vSAN容量层)
    RAID卡: PERC H755 (HBA模式用于vSAN)
  
  网络配置:
    管理网络: 2x 1GbE (板载, 冗余)
    业务网络: 2x 10GbE SFP+ (PCIe网卡, 冗余)
    vMotion网络: 使用10GbE接口
    vSAN网络: 使用10GbE接口
  
  电源: 2x 1100W 白金级冗余电源
  
  单台成本: ¥95,000
  总成本: ¥285,000
```

**网络设备**:

```yaml
核心交换机:
  型号: Dell N3248TE-ON
  规格: 48x 1GbE + 4x 10GbE SFP+
  数量: 2台 (堆叠)
  成本: ¥45,000
  
接入交换机:
  型号: Dell N1548P
  规格: 48x 1GbE PoE+
  数量: 3台
  成本: ¥30,000

网络配件:
  10G光纤模块: 12个 SFP+ SR
  万兆光纤: 20米 OM3
  网线: Cat6A
  成本: ¥15,000
```

**存储与备份**:

```yaml
备份存储:
  型号: Synology RS3621xs+
  配置: 12盘位NAS
  硬盘: 12x 8TB SATA HDD (RAID6)
  可用容量: 80TB
  网络: 4x 1GbE (链路聚合)
  成本: ¥80,000

UPS:
  型号: APC Smart-UPS SRT 10KVA
  数量: 2台
  成本: ¥60,000
```

**软件许可**:

```yaml
VMware许可:
  vSphere Standard: 6 CPU许可
  vCenter Standard: 1实例
  vSAN Standard: 6 CPU许可
  成本: ¥120,000

备份软件:
  Veeam Backup & Replication Standard
  许可: 100 VM
  成本: ¥45,000
```

**总成本汇总**:

```text
主机: ¥285,000
网络: ¥90,000
存储: ¥80,000
UPS: ¥60,000
软件: ¥165,000
实施: ¥50,000
============
总计: ¥730,000
```

### 部署实施步骤

**阶段1: 硬件部署 (第1-2天)**:

```bash
# Day 1: 硬件安装
1. 机柜规划与设备上架
   - 服务器: U1-U6 (每台2U)
   - 交换机: U40-U42
   - NAS: U38-U39
   - UPS: 独立机柜

2. 网络布线
   - 管理网络 (VLAN 10): 连接到管理交换机
   - 业务网络 (VLAN 20-50): 10GbE万兆连接
   - vMotion (VLAN 100): 10GbE专用VLAN
   - vSAN (VLAN 200): 10GbE专用VLAN

3. 电源连接
   - 双电源分别连接两台UPS
   - 验证电源冗余

# Day 2: BIOS配置
esxcli system settings advanced list
```

**BIOS关键配置**:

```yaml
虚拟化设置:
  Intel VT-x: Enabled
  Intel VT-d: Enabled
  SR-IOV: Enabled (如需PCIe直通)

电源管理:
  Power Profile: Maximum Performance
  C-States: Disabled
  P-States: Disabled
  Turbo Boost: Enabled

内存设置:
  Node Interleaving: Disabled (启用NUMA)
  Memory Operating Mode: Optimizer Mode
  Memory Patrol Scrub: Enabled

启动选项:
  Boot Mode: UEFI
  Secure Boot: Disabled
```

**阶段2: ESXi安装 (第3天)**:

```bash
# 1. 准备ESXi安装介质
# 下载ESXi 8.0 U2 ISO
# 使用Rufus制作USB启动盘

# 2. 安装ESXi (每台主机)
# - 选择安装磁盘: RAID1 (2x 480GB SSD)
# - 设置管理员密码
# - 配置管理网络:
#   ESXi-01: 192.168.10.11/24
#   ESXi-02: 192.168.10.12/24
#   ESXi-03: 192.168.10.13/24
#   Gateway: 192.168.10.1
#   DNS: 192.168.10.53

# 3. 安装后配置 (SSH登录每台主机)
# 启用SSH和Shell
vim-cmd hostsvc/enable_ssh
vim-cmd hostsvc/enable_esx_shell

# 配置NTP
esxcli system ntp set --server=ntp.aliyun.com --server=ntp1.aliyun.com
esxcli system ntp set --enabled=yes
esxcli system ntp start

# 设置主机名
esxcli system hostname set --fqdn=esxi-01.company.local

# 添加DNS服务器
esxcli network ip dns server add --server=192.168.10.53
esxcli network ip dns search add --domain=company.local
```

**阶段3: vCenter部署 (第4天)**:

```bash
# 1. 部署vCenter Server Appliance (VCSA)
# 使用任意ESXi主机部署

# 从Windows管理机执行:
# 挂载VCSA ISO
# 运行: vcsa-deploy\win32\installer.exe

# 部署参数:
VCSA_CONFIG = {
    "target_esxi": "192.168.10.11",
    "appliance_name": "vcenter.company.local",
    "vm_size": "small",  # small: 10主机/100VM
    "storage_size": "default",  # 740GB
    "network": {
        "ip": "192.168.10.10",
        "prefix": "24",
        "gateway": "192.168.10.1",
        "dns": "192.168.10.53",
        "fqdn": "vcenter.company.local"
    },
    "sso": {
        "domain": "vsphere.local",
        "password": "Strong@Pass123",
        "site_name": "Company-DC"
    }
}

# 2. 访问vCenter管理界面
# https://vcenter.company.local:443
# 用户: administrator@vsphere.local
```

**阶段4: 数据中心和集群配置 (第4天)**:

```yaml
# 在vCenter中配置:

1. 创建数据中心:
   名称: Company-DC
   位置: Datacenter-01

2. 创建集群:
   名称: Prod-Cluster
   位置: Company-DC
   
   启用功能:
     vSphere HA:
       准入控制: 主机故障容错数: 1
       主机监控: 已启用
       VM监控: 已启用
     
     vSphere DRS:
       自动化级别: 全自动
       迁移阈值: 保守
       VM分布: 已启用
     
     vSAN:
       后续配置

3. 添加主机到集群:
   - 添加esxi-01.company.local (192.168.10.11)
   - 添加esxi-02.company.local (192.168.10.12)
   - 添加esxi-03.company.local (192.168.10.13)
   - 维护模式: 否
```

**阶段5: 网络配置 (第5天)**:

```yaml
# 分布式交换机配置

1. 创建分布式交换机:
   名称: DSwitch-Prod
   版本: 8.0.0
   上行链路: 4个 (2x 1GbE管理 + 2x 10GbE业务)
   MTU: 9000 (启用Jumbo Frame)

2. 添加主机到DSwitch:
   - 添加所有3台ESXi主机
   - 配置物理适配器映射:
     vmnic0 -> Uplink1 (1GbE 管理)
     vmnic1 -> Uplink2 (1GbE 管理冗余)
     vmnic2 -> Uplink3 (10GbE 业务)
     vmnic3 -> Uplink4 (10GbE 业务冗余)

3. 创建端口组:
   
   管理网络 (已存在 - 迁移到DSwitch):
     名称: PG-Management
     VLAN: 10
     绑定: vmnic0, vmnic1
     负载均衡: Route based on originating virtual port
   
   vMotion网络:
     名称: PG-vMotion
     VLAN: 100
     绑定: vmnic2, vmnic3
     负载均衡: Route based on physical NIC load
     注意: 勾选"启用vMotion流量"
   
   vSAN网络:
     名称: PG-vSAN
     VLAN: 200
     绑定: vmnic2, vmnic3
     负载均衡: Route based on physical NIC load
     注意: 勾选"启用vSAN流量"
   
   生产业务网络:
     名称: PG-Production
     VLAN: 20
     绑定: vmnic2, vmnic3
     负载均衡: Route based on originating virtual port
   
   测试网络:
     名称: PG-Test
     VLAN: 30
     绑定: vmnic2, vmnic3
   
   DMZ网络:
     名称: PG-DMZ
     VLAN: 50
     绑定: vmnic2, vmnic3

4. 配置VMkernel适配器:
   
   每台主机创建:
     vmk1 - vMotion:
       IP: 192.168.100.11/24 (esxi-01)
       IP: 192.168.100.12/24 (esxi-02)
       IP: 192.168.100.13/24 (esxi-03)
       端口组: PG-vMotion
       服务: vMotion
     
     vmk2 - vSAN:
       IP: 192.168.200.11/24 (esxi-01)
       IP: 192.168.200.12/24 (esxi-02)
       IP: 192.168.200.13/24 (esxi-03)
       端口组: PG-vSAN
       服务: vSAN
```

**交换机VLAN配置** (Cisco示例):

```cisco
! 核心交换机配置
enable
configure terminal

! 创建VLANs
vlan 10
 name Management
vlan 20
 name Production
vlan 30
 name Test
vlan 50
 name DMZ
vlan 100
 name vMotion
vlan 200
 name vSAN

! 配置Trunk端口连接ESXi主机
interface range GigabitEthernet1/0/1-6
 description ESXi-Hosts-1GbE
 switchport mode trunk
 switchport trunk native vlan 999
 switchport trunk allowed vlan 10,20,30,50,100,200
 spanning-tree portfast trunk

interface range TenGigabitEthernet1/0/1-6
 description ESXi-Hosts-10GbE
 switchport mode trunk
 switchport trunk native vlan 999
 switchport trunk allowed vlan 10,20,30,50,100,200
 mtu 9000
 spanning-tree portfast trunk

! 启用Jumbo Frame
system mtu jumbo 9000

! 保存配置
end
write memory
```

**阶段6: vSAN配置 (第6天)**:

```yaml
# vSAN集群配置

1. 准备磁盘:
   每台主机磁盘组:
     缓存层: 2x 1.6TB NVMe SSD
     容量层: 6x 4TB SATA SSD
   
   验证磁盘:
     - 检查磁盘是否被识别
     - 确认HBA模式 (不使用RAID)
     - 验证SSD标记正确

2. 启用vSAN:
   集群 -> 配置 -> vSAN -> 服务
   - 勾选"启用vSAN"
   - 类型: 单站点集群
   - 去重和压缩: 启用
   - 加密: 启用 (使用vCenter密钥)

3. 创建磁盘组:
   每台主机:
     磁盘组1:
       缓存: 1x 1.6TB NVMe
       容量: 3x 4TB SATA SSD
     
     磁盘组2:
       缓存: 1x 1.6TB NVMe
       容量: 3x 4TB SATA SSD

4. vSAN原始容量计算:
   容量盘总量: 3台 x 6块 x 4TB = 72TB
   去重压缩比: 约1.5x
   RAID容错: FTT=1 RAID1 (50%空间)
   实际可用: 72TB x 1.5 / 2 = ~54TB

5. 创建存储策略:
   
   策略1 - 生产关键业务:
     名称: vSAN-Production-FTT1
     故障容忍: FTT=1 (RAID1镜像)
     每对象磁盘条带数: 1
     对象空间预留: 50% (厚置备)
     闪存读取缓存预留: 10%
   
   策略2 - 一般业务:
     名称: vSAN-Standard-FTT1
     故障容忍: FTT=1 (RAID1镜像)
     每对象磁盘条带数: 1
     对象空间预留: 0% (精简置备)
   
   策略3 - 测试开发:
     名称: vSAN-Test-FTT0
     故障容忍: FTT=0 (无冗余)
     对象空间预留: 0%

6. 创建vSAN数据存储:
   名称: vsanDatastore
   容量: 会自动显示可用容量
   默认策略: vSAN-Standard-FTT1
```

**阶段7: 虚拟机模板创建 (第7天)**:

```bash
# 1. 创建Windows Server 2022模板

# 创建新虚拟机:
VM_CONFIG_WIN = {
    "name": "Template-Win2022",
    "guest_os": "Microsoft Windows Server 2022 (64-bit)",
    "cpu": 2,
    "memory": 4096,  # MB
    "disk": 60,  # GB
    "network": "PG-Production",
    "datastore": "vsanDatastore",
    "storage_policy": "vSAN-Standard-FTT1"
}

# 安装Windows Server 2022:
# - 挂载ISO
# - 安装操作系统
# - 激活Windows
# - 安装VMware Tools
# - 运行Windows Update

# 系统优化:
# 禁用IPv6 (如不使用)
Disable-NetAdapterBinding -Name "*" -ComponentID ms_tcpip6

# 禁用休眠
powercfg /h off

# 设置电源计划为高性能
powercfg /s 8c5e7fda-e8bf-4a96-9a85-a6e23a8c635c

# 优化服务
Get-Service -Name "wuauserv" | Set-Service -StartupType Manual

# 运行Sysprep:
C:\Windows\System32\Sysprep\sysprep.exe /oobe /generalize /shutdown

# 转换为模板:
# 右键VM -> 模板 -> 转换为模板


# 2. 创建Linux (Rocky Linux 9) 模板

VM_CONFIG_LINUX = {
    "name": "Template-Rocky9",
    "guest_os": "Red Hat Enterprise Linux 9 (64-bit)",
    "cpu": 2,
    "memory": 2048,
    "disk": 30,
    "network": "PG-Production",
    "datastore": "vsanDatastore"
}

# 安装Rocky Linux 9
# 最小化安装 + 标准系统工具

# SSH登录后执行优化脚本:
cat > /root/vm-optimize.sh << 'EOF'
#!/bin/bash

# 更新系统
dnf update -y

# 安装必要工具
dnf install -y \
    vim \
    wget \
    curl \
    net-tools \
    bind-utils \
    open-vm-tools \
    perl

# 启用open-vm-tools
systemctl enable --now vmtoolsd

# 配置网络
nmcli connection modify ens192 connection.autoconnect yes

# 禁用SELinux (可选)
sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config

# 配置时间同步
timedatectl set-timezone Asia/Shanghai
systemctl enable --now chronyd

# 内核参数优化
cat >> /etc/sysctl.conf << 'SYSCTL'
# 网络优化
net.ipv4.tcp_fin_timeout = 30
net.ipv4.tcp_keepalive_time = 1200
net.ipv4.tcp_max_syn_backlog = 8192
net.ipv4.tcp_tw_reuse = 1
net.core.somaxconn = 32768

# 文件系统优化
fs.file-max = 2097152
SYSCTL

sysctl -p

# 资源限制
cat >> /etc/security/limits.conf << 'LIMITS'
* soft nofile 65536
* hard nofile 65536
* soft nproc 65536
* hard nproc 65536
LIMITS

# 清理历史
history -c
rm -f /root/.bash_history
rm -f /home/*/.bash_history

echo "VM优化完成!"
EOF

chmod +x /root/vm-optimize.sh
/root/vm-optimize.sh

# 关机前清理
virt-sysprep operations (手动):
- 清理网络配置中的MAC地址
- 清理machine-id
- 清理SSH主机密钥

rm -f /etc/ssh/ssh_host_*
echo -n > /etc/machine-id
poweroff

# 转换为模板 (vCenter界面操作)
```

### 配置验证与测试

**功能验证清单**:

```yaml
# 阶段1: 基础功能验证

1. vSphere HA测试:
   测试步骤:
     - 在集群中创建测试VM
     - 模拟主机故障 (拔电源/关机)
     - 验证VM在其他主机自动重启
     - 时间: <5分钟
   
   预期结果:
     ✅ VM在故障后自动重启
     ✅ 重启时间 < 3分钟
     ✅ 业务数据无丢失

2. vSphere DRS测试:
   测试步骤:
     - 在一台主机创建多个VM负载
     - 观察DRS自动负载均衡
     - 手动触发vMotion迁移
   
   预期结果:
     ✅ DRS自动均衡负载
     ✅ vMotion迁移成功
     ✅ 迁移过程VM无感知

3. vSAN存储测试:
   测试步骤:
     - 使用fio测试IOPS和延迟
     - 模拟磁盘故障 (移除一块盘)
     - 验证数据恢复
   
   性能基准:
     随机读IOPS: >50,000
     随机写IOPS: >30,000
     延迟: <5ms
   
   预期结果:
     ✅ 性能符合预期
     ✅ 磁盘故障后数据自动恢复
     ✅ 无数据丢失

4. 网络性能测试:
   测试步骤:
     - 使用iperf3测试VM间网络带宽
     - 测试vMotion迁移速度
   
   预期结果:
     VM间带宽: >9 Gbps (10GbE网络)
     vMotion速度: >800 MB/s
```

**性能测试脚本**:

```bash
# 在Linux VM中运行性能测试

#!/bin/bash
# storage-benchmark.sh

echo "=== vSAN存储性能测试 ==="

# 安装fio
dnf install -y fio

# 随机读测试
echo "1. 随机读IOPS测试 (4K块)"
fio --name=randread --ioengine=libaio --direct=1 --bs=4k --iodepth=64 \
    --rw=randread --size=10G --numjobs=4 --runtime=60 --group_reporting

# 随机写测试
echo "2. 随机写IOPS测试 (4K块)"
fio --name=randwrite --ioengine=libaio --direct=1 --bs=4k --iodepth=64 \
    --rw=randwrite --size=10G --numjobs=4 --runtime=60 --group_reporting

# 顺序读测试
echo "3. 顺序读带宽测试 (1M块)"
fio --name=seqread --ioengine=libaio --direct=1 --bs=1m --iodepth=32 \
    --rw=read --size=10G --runtime=60 --group_reporting

# 顺序写测试
echo "4. 顺序写带宽测试 (1M块)"
fio --name=seqwrite --ioengine=libaio --direct=1 --bs=1m --iodepth=32 \
    --rw=write --size=10G --runtime=60 --group_reporting

echo "性能测试完成!"

# 网络性能测试
# 在一台VM运行服务器端:
iperf3 -s

# 在另一台VM运行客户端:
iperf3 -c <server-ip> -t 60 -P 10
```

---

## 案例二: 互联网公司KVM虚拟化集群

### 业务需求分析

**企业背景**:

- 互联网游戏公司
- 日活用户100万+
- 微服务架构,容器化部署
- 需要大规模虚拟化支撑

**技术需求**:

```yaml
规模要求:
  物理主机: 20台
  虚拟机数量: 500+
  容器节点: 100+

性能要求:
  CPU: 2000+ vCPU
  内存: 10TB+
  存储: 500TB (Ceph分布式存储)
  网络: 25Gbps

特殊需求:
  开源方案: 降低许可成本
  自动化: Ansible/Terraform
  监控: Prometheus + Grafana
  CI/CD集成: Jenkins/GitLab
```

### 架构设计

**集群架构**:

```yaml
计算节点池:
  Web应用池:
    节点: 10台
    规格: 2x AMD EPYC 7543 (32核) + 512GB内存
    用途: Web应用VM
  
  数据库池:
    节点: 5台
    规格: 2x AMD EPYC 7643 (48核) + 1TB内存
    用途: 数据库VM (MySQL/Redis)
  
  容器节点池:
    节点: 5台
    规格: 2x AMD EPYC 7543 + 256GB内存
    用途: Kubernetes Worker节点

Ceph存储集群:
  OSD节点: 复用计算节点
  每节点: 12x 4TB NVMe SSD
  总容量: 20节点 x 12 x 4TB = 960TB原始
  可用容量: ~320TB (3副本)
  
  MON节点: 3台独立节点
  MGR节点: 与MON共用
  
网络架构:
  管理网络: 1GbE
  业务网络: 25GbE (双链路冗余)
  存储网络: 25GbE (专用)
  
管理节点:
  KVM管理: 3台 (WebVirtMgr/oVirt)
  Ansible: 1台
  监控: 3台 (Prometheus HA)
```

### 部署实施

**阶段1: 操作系统部署 (使用PXE自动化)**:

```bash
# PXE服务器配置 (在管理节点)

# 1. 安装DHCP + TFTP + HTTP服务
dnf install -y dhcp-server tftp-server httpd syslinux

# 2. 配置DHCP
cat > /etc/dhcp/dhcpd.conf << 'EOF'
subnet 192.168.10.0 netmask 255.255.255.0 {
    range 192.168.10.100 192.168.10.200;
    option routers 192.168.10.1;
    option domain-name-servers 192.168.10.53;
    next-server 192.168.10.5;  # TFTP服务器IP
    filename "pxelinux.0";
}
EOF

# 3. 准备安装镜像
mkdir -p /var/www/html/rocky9
mount -o loop Rocky-9.3-x86_64-dvd.iso /mnt
cp -r /mnt/* /var/www/html/rocky9/
umount /mnt

# 4. 配置Kickstart自动安装
cat > /var/www/html/rocky9-ks.cfg << 'EOF'
# Rocky Linux 9 Kickstart for KVM Host

# 系统语言和键盘
lang en_US.UTF-8
keyboard us
timezone Asia/Shanghai --utc

# 网络配置
network --bootproto=dhcp --device=enp1s0 --onboot=on --ipv6=auto
network --hostname=kvm-node.cluster.local

# 安装模式
text
install
url --url=http://192.168.10.5/rocky9/

# 磁盘分区
clearpart --all --initlabel
part /boot/efi --fstype=efi --size=512
part /boot --fstype=xfs --size=1024
part pv.01 --size=1 --grow
volgroup vg_system pv.01
logvol / --fstype=xfs --vgname=vg_system --name=lv_root --size=51200
logvol /var --fstype=xfs --vgname=vg_system --name=lv_var --size=20480
logvol swap --vgname=vg_system --name=lv_swap --size=16384

# 软件包选择
%packages
@^minimal-environment
@virtualization-host-environment
@virtualization-platform
vim
wget
git
%end

# 安装后脚本
%post
# 启用嵌套虚拟化
echo "options kvm_intel nested=1" > /etc/modprobe.d/kvm-nested.conf

# 优化内核参数
cat >> /etc/sysctl.conf << 'SYSCTL'
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
vm.swappiness = 10
vm.overcommit_memory = 1
SYSCTL

# 禁用防火墙 (测试环境)
systemctl disable firewalld

# 启用libvirtd
systemctl enable libvirtd

# 配置SSH密钥 (用于Ansible管理)
mkdir -p /root/.ssh
cat >> /root/.ssh/authorized_keys << 'SSH_KEY'
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABA... ansible-管理密钥
SSH_KEY
chmod 600 /root/.ssh/authorized_keys

%end

reboot
EOF

# 5. 启动服务
systemctl enable --now dhcpd tftp httpd

# 6. 批量部署所有节点 (网络PXE启动)
```

**阶段2: KVM主机配置 (Ansible自动化)**:

```yaml
# ansible/kvm-setup.yml

---
- name: KVM主机标准化配置
  hosts: kvm_hosts
  become: yes
  
  vars:
    bridge_interface: br0
    physical_interface: enp1s0
    storage_network_interface: enp2s0
  
  tasks:
    - name: 安装虚拟化软件包
      dnf:
        name:
          - qemu-kvm
          - libvirt
          - libvirt-daemon-kvm
          - virt-install
          - virt-manager
          - virt-top
          - libguestfs-tools
          - python3-libvirt
        state: latest
    
    - name: 启用并启动libvirtd
      systemd:
        name: libvirtd
        enabled: yes
        state: started
    
    - name: 配置网桥接口
      template:
        src: templates/ifcfg-br0.j2
        dest: /etc/sysconfig/network-scripts/ifcfg-{{ bridge_interface }}
      notify: restart network
    
    - name: 配置物理接口
      template:
        src: templates/ifcfg-eth.j2
        dest: /etc/sysconfig/network-scripts/ifcfg-{{ physical_interface }}
      notify: restart network
    
    - name: 配置libvirt网络
      copy:
        content: |
          <network>
            <name>br0</name>
            <forward mode="bridge"/>
            <bridge name="br0"/>
          </network>
        dest: /tmp/br0.xml
    
    - name: 定义libvirt网络
      command: virsh net-define /tmp/br0.xml
      ignore_errors: yes
    
    - name: 启动libvirt网络
      command: virsh net-start br0
      ignore_errors: yes
    
    - name: 设置libvirt网络自启动
      command: virsh net-autostart br0
    
    - name: 创建VM存储池
      command: |
        virsh pool-define-as local-pool dir - - - - "/var/lib/libvirt/images"
        virsh pool-build local-pool
        virsh pool-start local-pool
        virsh pool-autostart local-pool
      ignore_errors: yes
    
    - name: 配置KVM CPU性能模式
      lineinfile:
        path: /etc/default/grub
        regexp: '^GRUB_CMDLINE_LINUX='
        line: 'GRUB_CMDLINE_LINUX="intel_iommu=on iommu=pt default_hugepagesz=1G hugepagesz=1G hugepages=32"'
      notify: rebuild grub
    
    - name: 优化KVM参数
      blockinfile:
        path: /etc/sysctl.d/99-kvm.conf
        create: yes
        block: |
          # KVM优化
          vm.nr_hugepages = 4096
          vm.hugetlb_shm_group = 36
          kernel.sched_migration_cost_ns = 5000000
          kernel.sched_autogroup_enabled = 0
    
    - name: 应用sysctl参数
      command: sysctl -p /etc/sysctl.d/99-kvm.conf
  
  handlers:
    - name: restart network
      systemd:
        name: NetworkManager
        state: restarted
    
    - name: rebuild grub
      command: grub2-mkconfig -o /boot/grub2/grub.cfg

# 执行部署:
# ansible-playbook -i inventory/production kvm-setup.yml
```

**阶段3: Ceph存储集群部署**:

```bash
# 使用cephadm部署Ceph Quincy

# 1. 在第一个节点安装cephadm
dnf install -y centos-release-ceph-quincy
dnf install -y cephadm

# 2. 初始化Ceph集群
cephadm bootstrap \
  --mon-ip 192.168.10.11 \
  --cluster-network 192.168.20.0/24 \
  --public-network 192.168.10.0/24

# 输出:
# Ceph Dashboard: https://192.168.10.11:8443/
# User: admin
# Password: <生成的密码>

# 3. 添加其他节点
# 复制SSH公钥到所有节点
ssh-copy-id root@192.168.10.12
ssh-copy-id root@192.168.10.13
# ... 其他节点

# 添加主机到集群
ceph orch host add kvm-node-02 192.168.10.12
ceph orch host add kvm-node-03 192.168.10.13
# ... 添加所有20台节点

# 4. 部署MON守护进程 (至少3个)
ceph orch apply mon --placement="kvm-node-01,kvm-node-02,kvm-node-03"

# 5. 部署MGR守护进程
ceph orch apply mgr --placement="kvm-node-01,kvm-node-02,kvm-node-03"

# 6. 添加OSD (在每个节点添加12块NVMe SSD)
# 自动发现并添加所有可用磁盘
ceph orch apply osd --all-available-devices

# 或手动添加特定磁盘:
for node in kvm-node-{01..20}; do
  for disk in nvme{0..11}n1; do
    ceph orch daemon add osd $node:/dev/$disk
  done
done

# 7. 创建RBD存储池
ceph osd pool create kvm-pool 128 128
ceph osd pool set kvm-pool size 3  # 3副本
ceph osd pool set kvm-pool min_size 2
ceph osd pool application enable kvm-pool rbd

# 8. 创建Ceph RBD客户端密钥
ceph auth get-or-create client.kvm \
  mon 'allow r' \
  osd 'allow class-read object_prefix rbd_children, allow rwx pool=kvm-pool' \
  -o /etc/ceph/ceph.client.kvm.keyring

# 9. 在KVM主机配置Ceph
# 分发配置文件到所有KVM主机
for node in kvm-node-{01..20}; do
  scp /etc/ceph/ceph.conf $node:/etc/ceph/
  scp /etc/ceph/ceph.client.kvm.keyring $node:/etc/ceph/
done

# 10. 验证Ceph集群状态
ceph -s
# 预期输出:
#   cluster:
#     id:     xxx
#     health: HEALTH_OK
#   services:
#     mon: 3 daemons
#     mgr: 3 daemons
#     osd: 240 osds: 240 up, 240 in
#   data:
#     pools:   1 pools, 128 pgs
#     objects: 0 objects, 0 B
#     usage:   960 TB used, 0 B / 960 TB avail
```

### 自动化运维

**VM自动化创建脚本**:

```bash
#!/bin/bash
# create-vm-from-ceph.sh
# 从Ceph RBD创建KVM虚拟机

VM_NAME=$1
VM_VCPU=${2:-4}
VM_MEMORY=${3:-8192}  # MB
VM_DISK=${4:-50}      # GB

if [ -z "$VM_NAME" ]; then
    echo "用法: $0 <VM名称> [vCPU数] [内存MB] [磁盘GB]"
    exit 1
fi

echo "创建虚拟机: $VM_NAME"
echo "  vCPU: $VM_VCPU"
echo "  内存: ${VM_MEMORY}MB"
echo "  磁盘: ${VM_DISK}GB"

# 1. 创建Ceph RBD卷
echo "创建Ceph RBD卷..."
rbd create kvm-pool/${VM_NAME}-disk --size=${VM_DISK}G

# 2. 映射RBD卷
rbd map kvm-pool/${VM_NAME}-disk

# 3. 使用virt-install创建VM
echo "创建虚拟机..."
virt-install \
  --name ${VM_NAME} \
  --virt-type kvm \
  --vcpus ${VM_VCPU} \
  --memory ${VM_MEMORY} \
  --disk path=/dev/rbd/kvm-pool/${VM_NAME}-disk,bus=virtio,cache=writeback \
  --network bridge=br0,model=virtio \
  --graphics vnc,listen=0.0.0.0 \
  --cdrom /var/lib/libvirt/images/Rocky-9.3-x86_64-dvd.iso \
  --os-variant rhel9.0 \
  --noautoconsole

echo "虚拟机 $VM_NAME 创建成功!"
echo "使用以下命令查看VNC端口:"
echo "  virsh vncdisplay $VM_NAME"
```

---

## 案例三: 金融行业超融合HCI方案

### 业务需求分析

**合规要求**:

```yaml
金融行业特殊要求:
  等保三级: 必须满足
  数据加密: 静态+传输加密
  审计日志: 完整操作审计
  容灾要求: 同城双活 + 异地灾备
  可用性: 99.99% (年停机<53分钟)
  
业务系统:
  核心系统: 账务、支付、风控
  渠道系统: 网银、手机银行、ATM
  管理系统: CRM、OA、BI分析
```

### vSAN架构设计

```yaml
双数据中心架构:

主数据中心 (DC1):
  计算+存储节点: 6台
  规格: Dell PowerEdge R750
  CPU: 2x Intel Xeon Gold 6346 (16核)
  内存: 768GB DDR4 ECC
  存储: 2x 1.6TB NVMe (缓存) + 8x 3.84TB SAS SSD (容量)
  网络: 4x 25GbE

备数据中心 (DC2):
  计算+存储节点: 6台
  配置同DC1

vSAN配置:
  类型: vSAN延伸集群 (Stretched Cluster)
  故障域: DC1 (主) + DC2 (辅) + 见证节点
  见证节点: 放置在第三地 (云端或第三机房)
  数据保护: FTT=1 RAID1 (主辅各一份副本)
  网络延迟要求: <5ms (DC1 <-> DC2)
```

### 部署实施

详细配置和验证过程略，参考案例一的部署流程，额外注意：

```yaml
金融行业额外配置:

1. 数据加密:
   - vSAN加密: 启用
   - VM加密: 对核心业务VM启用
   - 密钥管理: 外部KMS (如HyTrust/Thales)

2. 网络隔离:
   - 核心区: 不出互联网
   - DMZ区: 严格ACL控制
   - 管理区: 跳板机+堡垒机

3. 审计日志:
   - vCenter操作日志: 转发到SIEM
   - ESXi系统日志: 集中收集
   - VM审计: 开启vCenter Change Tracking

4. 容灾演练:
   - 频率: 每季度
   - 类型: 计划切换 + 故障切换
   - RTO目标: <30分钟
   - RPO目标: <15分钟
```

### 高可用验证

```yaml
测试场景1: 单台主机故障
  操作: 关闭DC1一台主机
  预期: VM在其他主机重启 (<3分钟)
  实际: 通过 ✅

测试场景2: 存储设备故障
  操作: 拔除一块SSD
  预期: vSAN自动重建数据
  实际: 通过 ✅

测试场景3: 数据中心故障
  操作: 断开DC1所有网络
  预期: 业务切换到DC2 (<5分钟)
  实际: 通过 ✅

测试场景4: 灾难恢复
  操作: 从备份恢复完整业务系统
  预期: RTO <4小时
  实际: 通过 ✅
```

---

## 案例四: 混合云虚拟化架构

### 业务场景

```yaml
企业背景:
  类型: 电商平台
  特点: 业务波动大 (促销期10x流量)
  
架构目标:
  本地IDC: 承载常驻业务 (100台VM)
  公有云: 弹性扩展 (促销期+200台VM)
  混合管理: 统一管理本地+云端资源
```

### 架构设计

```yaml
本地IDC:
  平台: VMware vSphere 8.0
  主机: 8台高配服务器
  存储: vSAN
  
公有云:
  提供商: 阿里云/AWS/Azure
  服务: EC2/ECS实例
  
混合管理:
  工具: VMware Cloud Director / vRealize
  网络: VPN/专线 (Aliyun Express Connect)
  监控: 统一监控平台
```

### 部署实施

```yaml
阶段1: 本地IDC部署
  - 参考案例一部署vSphere环境

阶段2: 云端网络打通
  1. 配置VPN/专线:
     本地: 192.168.0.0/16
     阿里云VPC: 172.16.0.0/16
     互联: IPsec VPN或专线
  
  2. 路由配置:
     - 本地IDC路由表添加172.16.0.0/16 -> VPN
     - 阿里云VPC路由表添加192.168.0.0/16 -> VPN

阶段3: 混合负载均衡
  - 使用阿里云SLB或F5 BIG-IP
  - 后端池包含本地VM + 云端ECS
  - 健康检查: HTTP/TCP探测

阶段4: 自动弹性伸缩
  - 监控指标: CPU/内存/请求数
  - 扩容策略: 阈值触发云端ECS启动
  - 缩容策略: 流量下降后回收ECS
```

---

## 常见问题与解决方案

```yaml
问题1: ESXi主机无法识别NVMe SSD
  原因: ESXi版本过旧或驱动不兼容
  解决:
    - 升级到ESXi 8.0 U2
    - 或安装社区NVMe驱动 (Community NVMe Driver Fling)
  参考: VMware KB 2147714

问题2: vSAN性能不达标
  排查:
    - 检查HCL兼容性
    - 验证网络带宽 (至少10GbE)
    - 确认RAID卡设置为HBA模式/直通
    - 检查缓存/容量比 (推荐至少1:10)
  优化:
    - 启用Jumbo Frame (MTU 9000)
    - 启用vSAN去重压缩
    - 调整磁盘条带数

问题3: vMotion迁移失败
  原因:
    - vMotion网络不通
    - CPU兼容性问题
    - 存储访问权限问题
  解决:
    - ping测试vMotion网络
    - 启用EVC (Enhanced vMotion Compatibility)
    - 检查共享存储权限

问题4: KVM虚拟机嵌套虚拟化不工作
  解决:
    # Intel CPU
    modprobe -r kvm_intel
    modprobe kvm_intel nested=1
    
    # AMD CPU  
    modprobe -r kvm_amd
    modprobe kvm_amd nested=1
    
    # 持久化
    echo "options kvm_intel nested=1" > /etc/modprobe.d/kvm.conf

问题5: Ceph集群HEALTH_WARN
  常见原因:
    - 时钟不同步: 配置NTP
    - OSD down: 检查OSD日志
    - PG不平衡: ceph balancer on
    - 容量不足: 扩容或删除数据
  
  排查命令:
    ceph health detail
    ceph osd tree
    ceph osd df
    ceph pg dump
```

---

## 最佳实践总结

### 硬件选型

✅ **CPU**: 虚拟化优先选择多核心，核心数比主频更重要  
✅ **内存**: 必须ECC，填满所有内存通道以获得最大带宽  
✅ **存储**: SSD/NVMe是虚拟化基础，不要为省钱用HDD  
✅ **网络**: 10GbE是起步标准，关键业务25GbE起  
✅ **冗余**: 电源、网卡、存储控制器必须冗余  

### 软件配置

✅ **vSphere HA**: 生产环境必须启用，建议FTT=1  
✅ **vSphere DRS**: 启用全自动模式，优化资源分配  
✅ **存储策略**: 根据业务重要性分级 (关键/一般/测试)  
✅ **网络隔离**: 管理/vMotion/vSAN/业务网络必须隔离  
✅ **监控告警**: 全面监控CPU/内存/存储/网络，及时告警  

### 运维管理

✅ **标准化**: VM模板、命名规范、配置标准  
✅ **自动化**: Ansible/Terraform for IaC  
✅ **备份**: 3-2-1规则 (3份副本,2种介质,1份异地)  
✅ **文档化**: 架构图、配置清单、操作手册  
✅ **演练**: 定期容灾演练，验证RTO/RPO  

### 安全加固

✅ **访问控制**: 最小权限原则 + RBAC  
✅ **网络安全**: 防火墙 + ACL + VLAN隔离  
✅ **数据加密**: 静态加密 (vSAN/VM) + 传输加密 (TLS)  
✅ **补丁管理**: 定期打补丁，但先在测试环境验证  
✅ **审计日志**: 开启完整审计，集中收集分析  

---

**文档版本**: v1.0  
**创建时间**: 2025-10-19  
**适用环境**: VMware vSphere 7.0+, KVM, vSAN, Ceph  
**状态**: ✅ 生产就绪
