# Kubernetesé«˜å¯ç”¨æ¶æ„

> **è¿”å›**: [é«˜å¯ç”¨å®¹ç¾ç›®å½•](README.md) | [è™šæ‹ŸåŒ–éƒ¨ç½²é¦–é¡µ](../README.md) | [éƒ¨ç½²æŒ‡å—é¦–é¡µ](../../00_ç´¢å¼•å¯¼èˆª/README.md)

---

## ğŸ“‹ ç›®å½•

- [Kubernetesé«˜å¯ç”¨æ¶æ„](#kubernetesé«˜å¯ç”¨æ¶æ„)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. Kubernetes HAæ¦‚è¿°](#1-kubernetes-haæ¦‚è¿°)
  - [2. æ§åˆ¶å¹³é¢é«˜å¯ç”¨](#2-æ§åˆ¶å¹³é¢é«˜å¯ç”¨)
    - [2.1 ä½¿ç”¨kubeadméƒ¨ç½²HAé›†ç¾¤](#21-ä½¿ç”¨kubeadméƒ¨ç½²haé›†ç¾¤)
    - [2.2 æ‰‹åŠ¨éƒ¨ç½²HAé›†ç¾¤ (å¤–éƒ¨etcd)](#22-æ‰‹åŠ¨éƒ¨ç½²haé›†ç¾¤-å¤–éƒ¨etcd)
  - [3. etcdé›†ç¾¤é«˜å¯ç”¨](#3-etcdé›†ç¾¤é«˜å¯ç”¨)
    - [3.1 etcdç»´æŠ¤ä¸ç›‘æ§](#31-etcdç»´æŠ¤ä¸ç›‘æ§)
    - [3.2 etcdæ•…éšœæ¢å¤](#32-etcdæ•…éšœæ¢å¤)
  - [4. å·¥ä½œèŠ‚ç‚¹é«˜å¯ç”¨](#4-å·¥ä½œèŠ‚ç‚¹é«˜å¯ç”¨)
    - [4.1 é…ç½®Podåäº²å’Œæ€§](#41-é…ç½®podåäº²å’Œæ€§)
  - [5. è´Ÿè½½å‡è¡¡é…ç½®](#5-è´Ÿè½½å‡è¡¡é…ç½®)
    - [5.1 Keepalived + HAProxyé…ç½®](#51-keepalived--haproxyé…ç½®)
  - [6. åº”ç”¨é«˜å¯ç”¨](#6-åº”ç”¨é«˜å¯ç”¨)
    - [6.1 åº”ç”¨é«˜å¯ç”¨é…ç½®ç¤ºä¾‹](#61-åº”ç”¨é«˜å¯ç”¨é…ç½®ç¤ºä¾‹)
  - [7. å­˜å‚¨é«˜å¯ç”¨](#7-å­˜å‚¨é«˜å¯ç”¨)
    - [7.1 Ceph RBD StorageClassé…ç½®](#71-ceph-rbd-storageclassé…ç½®)
  - [8. ç›‘æ§ä¸æ•…éšœæ¢å¤](#8-ç›‘æ§ä¸æ•…éšœæ¢å¤)
    - [8.1 Prometheuså‘Šè­¦è§„åˆ™](#81-prometheuså‘Šè­¦è§„åˆ™)
    - [8.2 æ•…éšœæ¢å¤è„šæœ¬](#82-æ•…éšœæ¢å¤è„šæœ¬)
  - [9. æœ€ä½³å®è·µ](#9-æœ€ä½³å®è·µ)
  - [ç›¸å…³æ–‡æ¡£](#ç›¸å…³æ–‡æ¡£)

---

## 1. Kubernetes HAæ¦‚è¿°

```yaml
Kubernetes_HA_Overview:
  æ¶æ„ç»„ä»¶:
    æ§åˆ¶å¹³é¢ (Control Plane):
      - kube-apiserver: APIæœåŠ¡å™¨ (å¯å¤šå®ä¾‹)
      - kube-controller-manager: æ§åˆ¶å™¨ç®¡ç†å™¨
      - kube-scheduler: è°ƒåº¦å™¨
      - etcd: åˆ†å¸ƒå¼é”®å€¼å­˜å‚¨
    
    å·¥ä½œèŠ‚ç‚¹ (Worker Nodes):
      - kubelet: èŠ‚ç‚¹ä»£ç†
      - kube-proxy: ç½‘ç»œä»£ç†
      - å®¹å™¨è¿è¡Œæ—¶: containerd/CRI-O/Docker
    
    è´Ÿè½½å‡è¡¡:
      - å¤–éƒ¨LB: HAProxy/Nginx/F5/äº‘LB
      - å†…éƒ¨LB: kube-proxy (Service)
  
  é«˜å¯ç”¨ç­–ç•¥:
    æ§åˆ¶å¹³é¢HA:
      - è‡³å°‘3ä¸ªMasterèŠ‚ç‚¹ (å¥‡æ•°)
      - API Serverå¤šå®ä¾‹ä¸»åŠ¨-ä¸»åŠ¨
      - Controller Manager/Schedulerä¸»å¤‡æ¨¡å¼
      - è´Ÿè½½å‡è¡¡åˆ†å‘APIè¯·æ±‚
    
    etcd HA:
      - è‡³å°‘3èŠ‚ç‚¹é›†ç¾¤ (æ¨è5èŠ‚ç‚¹)
      - Raftå…±è¯†ç®—æ³•
      - è‡ªåŠ¨Leaderé€‰ä¸¾
      - æ•°æ®å¤šå‰¯æœ¬
    
    å·¥ä½œèŠ‚ç‚¹HA:
      - å¤šå‰¯æœ¬Podéƒ¨ç½²
      - èŠ‚ç‚¹æ•…éšœè‡ªåŠ¨é‡æ–°è°ƒåº¦
      - å¥åº·æ£€æŸ¥ä¸è‡ªæ„ˆ
    
    åº”ç”¨HA:
      - Deployment/StatefulSetå¤šå‰¯æœ¬
      - Podäº²å’Œæ€§ä¸åäº²å’Œæ€§
      - PodDisruptionBudget (PDB)
      - HPAè‡ªåŠ¨æ‰©ç¼©å®¹

HAæ‹“æ‰‘æ¨¡å¼:
  å †å etcdæ‹“æ‰‘ (Stacked etcd):
    æ¶æ„:
      - etcdä¸æ§åˆ¶å¹³é¢ç»„ä»¶å…±äº«èŠ‚ç‚¹
      - æ¯ä¸ªMasterèŠ‚ç‚¹è¿è¡Œetcd
    
    ä¼˜ç‚¹:
      - éƒ¨ç½²ç®€å•
      - èµ„æºéœ€æ±‚è¾ƒå°‘
      - ç®¡ç†æ–¹ä¾¿
    
    ç¼ºç‚¹:
      - æ§åˆ¶å¹³é¢ä¸etcdè€¦åˆ
      - å•èŠ‚ç‚¹æ•…éšœå½±å“æ›´å¤§
      - ä¸é€‚åˆè¶…å¤§è§„æ¨¡é›†ç¾¤
    
    é€‚ç”¨åœºæ™¯:
      - ä¸­å°å‹é›†ç¾¤ (< 100èŠ‚ç‚¹)
      - èµ„æºæœ‰é™
      - ç®€åŒ–ç®¡ç†
  
  å¤–éƒ¨etcdæ‹“æ‰‘ (External etcd):
    æ¶æ„:
      - etcdç‹¬ç«‹é›†ç¾¤
      - æ§åˆ¶å¹³é¢å•ç‹¬éƒ¨ç½²
    
    ä¼˜ç‚¹:
      - è§£è€¦æ§åˆ¶å¹³é¢ä¸etcd
      - æ›´é«˜å¯é æ€§
      - ç‹¬ç«‹æ‰©å±•
      - é€‚åˆå¤§è§„æ¨¡é›†ç¾¤
    
    ç¼ºç‚¹:
      - éƒ¨ç½²å¤æ‚
      - èµ„æºéœ€æ±‚é«˜
      - ç®¡ç†å¤æ‚
    
    é€‚ç”¨åœºæ™¯:
      - å¤§å‹é›†ç¾¤ (> 100èŠ‚ç‚¹)
      - ç”Ÿäº§ç¯å¢ƒ
      - é«˜å¯ç”¨è¦æ±‚

HAé›†ç¾¤è§„æ¨¡å»ºè®®:
  å°å‹HA (æµ‹è¯•/å¼€å‘):
    Master: 3èŠ‚ç‚¹
    Worker: 3-10èŠ‚ç‚¹
    etcd: 3èŠ‚ç‚¹ (å †å )
    
  ä¸­å‹HA (ç”Ÿäº§):
    Master: 3èŠ‚ç‚¹
    Worker: 10-50èŠ‚ç‚¹
    etcd: 3-5èŠ‚ç‚¹ (å †å æˆ–å¤–éƒ¨)
    
  å¤§å‹HA (ä¼ä¸šç”Ÿäº§):
    Master: 3-5èŠ‚ç‚¹
    Worker: 50-500èŠ‚ç‚¹
    etcd: 5èŠ‚ç‚¹ (å¤–éƒ¨)
```

---

## 2. æ§åˆ¶å¹³é¢é«˜å¯ç”¨

```yaml
Control_Plane_HA:
  kube-apiserver:
    ç‰¹ç‚¹:
      - æ— çŠ¶æ€æœåŠ¡
      - å¯å¤šå®ä¾‹è¿è¡Œ
      - ä¸»åŠ¨-ä¸»åŠ¨æ¨¡å¼
      - é€šè¿‡LBåˆ†å‘è¯·æ±‚
    
    éƒ¨ç½²æ–¹å¼:
      - æ¯ä¸ªMasterèŠ‚ç‚¹è¿è¡Œ1ä¸ªå®ä¾‹
      - ç›‘å¬ä¸åŒIP (èŠ‚ç‚¹IP)
      - å…±äº«åŒä¸€ç«¯å£ (6443)
      - å‰ç«¯LBè´Ÿè½½å‡è¡¡
    
    é…ç½®ç¤ºä¾‹:
      - --advertise-address=<NodeIP>
      - --secure-port=6443
      - --etcd-servers=<etcdé›†ç¾¤åœ°å€>
      - --service-cluster-ip-range=10.96.0.0/12
  
  kube-controller-manager:
    ç‰¹ç‚¹:
      - æœ‰çŠ¶æ€æœåŠ¡
      - ä¸»å¤‡æ¨¡å¼ (Leader Election)
      - åŒæ—¶åªæœ‰1ä¸ªå®ä¾‹æ´»åŠ¨
      - è‡ªåŠ¨æ•…éšœåˆ‡æ¢
    
    Leaderé€‰ä¸¾:
      - é€šè¿‡etcdæˆ–K8s Leaseå®ç°
      - ç§Ÿçº¦æœºåˆ¶
      - å¿ƒè·³ç»­çº¦
      - æ•…éšœè‡ªåŠ¨åˆ‡æ¢
    
    é…ç½®ç¤ºä¾‹:
      - --leader-elect=true
      - --leader-elect-lease-duration=15s
      - --leader-elect-renew-deadline=10s
      - --leader-elect-retry-period=2s
  
  kube-scheduler:
    ç‰¹ç‚¹:
      - æœ‰çŠ¶æ€æœåŠ¡
      - ä¸»å¤‡æ¨¡å¼ (Leader Election)
      - åŒæ—¶åªæœ‰1ä¸ªå®ä¾‹æ´»åŠ¨
      - è°ƒåº¦å†³ç­–éœ€è¦ä¸€è‡´æ€§
    
    é…ç½®:
      ä¸controller-managerç±»ä¼¼
      ä½¿ç”¨Leader Election

æ§åˆ¶å¹³é¢éƒ¨ç½²è„šæœ¬:
  SystemDæœåŠ¡:
    kube-apiserver.service:
      - å¤šå®ä¾‹åŒæ—¶è¿è¡Œ
      - ç›‘å¬æœ¬åœ°IP
      - è¿æ¥etcdé›†ç¾¤
    
    kube-controller-manager.service:
      - å¯ç”¨Leaderé€‰ä¸¾
      - è¿æ¥æœ¬åœ°API Server
      - è‡ªåŠ¨æ•…éšœåˆ‡æ¢
    
    kube-scheduler.service:
      - å¯ç”¨Leaderé€‰ä¸¾
      - è¿æ¥æœ¬åœ°API Server
      - è‡ªåŠ¨æ•…éšœåˆ‡æ¢
```

### 2.1 ä½¿ç”¨kubeadméƒ¨ç½²HAé›†ç¾¤

```bash
#!/bin/bash
# ========================================
# kubeadméƒ¨ç½²K8s HAé›†ç¾¤ (å †å etcd)
# ========================================

# ç¯å¢ƒè§„åˆ’:
# - 3ä¸ªMasterèŠ‚ç‚¹ (master01/02/03)
# - 3ä¸ªWorkerèŠ‚ç‚¹ (worker01/02/03)
# - 1ä¸ªè´Ÿè½½å‡è¡¡å™¨ (HAProxy)

# ========================================
# å‡†å¤‡å·¥ä½œ (æ‰€æœ‰èŠ‚ç‚¹)
# ========================================

# 1. ç³»ç»Ÿé…ç½®
cat <<EOF | tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

modprobe overlay
modprobe br_netfilter

cat <<EOF | tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sysctl --system

# 2. ç¦ç”¨swap
swapoff -a
sed -i '/ swap / s/^/#/' /etc/fstab

# 3. å®‰è£…å®¹å™¨è¿è¡Œæ—¶ (containerd)
apt update
apt install -y containerd

mkdir -p /etc/containerd
containerd config default > /etc/containerd/config.toml

# é…ç½®containerdä½¿ç”¨systemd cgroup driver
sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

systemctl restart containerd
systemctl enable containerd

# 4. å®‰è£…kubeadm, kubelet, kubectl
apt update
apt install -y apt-transport-https ca-certificates curl

curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | \
    gpg --dearmor -o /etc/apt/keyrings/kubernetes-archive-keyring.gpg

echo "deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] \
    https://apt.kubernetes.io/ kubernetes-xenial main" | \
    tee /etc/apt/sources.list.d/kubernetes.list

apt update
apt install -y kubelet=1.28.0-00 kubeadm=1.28.0-00 kubectl=1.28.0-00
apt-mark hold kubelet kubeadm kubectl

systemctl enable kubelet

# ========================================
# é…ç½®è´Ÿè½½å‡è¡¡å™¨ (HAProxyèŠ‚ç‚¹)
# ========================================

# å®‰è£…HAProxy
apt install -y haproxy

# é…ç½®HAProxy
cat > /etc/haproxy/haproxy.cfg << 'EOF'
global
    log /dev/log local0
    log /dev/log local1 notice
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin
    stats timeout 30s
    user haproxy
    group haproxy
    daemon

defaults
    log     global
    mode    http
    option  httplog
    option  dontlognull
    timeout connect 5000
    timeout client  50000
    timeout server  50000

# Kubernetes API Server Frontend
frontend k8s-api
    bind *:6443
    mode tcp
    option tcplog
    default_backend k8s-api-backend

# Kubernetes API Server Backend
backend k8s-api-backend
    mode tcp
    balance roundrobin
    option tcp-check
    server master01 192.168.10.101:6443 check fall 3 rise 2
    server master02 192.168.10.102:6443 check fall 3 rise 2
    server master03 192.168.10.103:6443 check fall 3 rise 2

# Statsé¡µé¢
listen stats
    bind *:8080
    mode http
    stats enable
    stats uri /stats
    stats refresh 30s
    stats realm HAProxy\ Statistics
    stats auth admin:admin
EOF

# å¯åŠ¨HAProxy
systemctl restart haproxy
systemctl enable haproxy

# éªŒè¯HAProxy
curl http://localhost:8080/stats

# ========================================
# åˆå§‹åŒ–ç¬¬ä¸€ä¸ªMasterèŠ‚ç‚¹ (master01)
# ========================================

# åˆ›å»ºkubeadmé…ç½®æ–‡ä»¶
cat > kubeadm-config.yaml << 'EOF'
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0
controlPlaneEndpoint: "192.168.10.100:6443"  # HAProxy VIP
networking:
  serviceSubnet: "10.96.0.0/12"
  podSubnet: "10.244.0.0/16"
apiServer:
  certSANs:
    - "192.168.10.100"  # HAProxy VIP
    - "192.168.10.101"  # master01
    - "192.168.10.102"  # master02
    - "192.168.10.103"  # master03
    - "master01"
    - "master02"
    - "master03"
    - "k8s-api.example.com"
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: "192.168.10.101"  # master01 IP
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  taints:
    - key: node-role.kubernetes.io/control-plane
      effect: NoSchedule
EOF

# åˆå§‹åŒ–é›†ç¾¤
kubeadm init --config=kubeadm-config.yaml --upload-certs

# é…ç½®kubectl (master01)
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

# ä¿å­˜joinå‘½ä»¤ (ç”¨äºå…¶ä»–èŠ‚ç‚¹åŠ å…¥)
# kubeadmä¼šè¾“å‡ºç±»ä¼¼ä»¥ä¸‹å‘½ä»¤:

# MasterèŠ‚ç‚¹åŠ å…¥å‘½ä»¤:
# kubeadm join 192.168.10.100:6443 --token <token> \
#     --discovery-token-ca-cert-hash sha256:<hash> \
#     --control-plane --certificate-key <cert-key>

# WorkerèŠ‚ç‚¹åŠ å…¥å‘½ä»¤:
# kubeadm join 192.168.10.100:6443 --token <token> \
#     --discovery-token-ca-cert-hash sha256:<hash>

# å®‰è£…CNIç½‘ç»œæ’ä»¶ (Calico)
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

# éªŒè¯ç¬¬ä¸€ä¸ªMasterèŠ‚ç‚¹
kubectl get nodes
kubectl get pods -A

# ========================================
# åŠ å…¥å…¶ä»–MasterèŠ‚ç‚¹ (master02, master03)
# ========================================

# åœ¨master02å’Œmaster03ä¸Šæ‰§è¡Œ (ä½¿ç”¨ä¸Šé¢ä¿å­˜çš„joinå‘½ä»¤):
kubeadm join 192.168.10.100:6443 --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash> \
    --control-plane --certificate-key <cert-key> \
    --apiserver-advertise-address=<master0X-IP>

# é…ç½®kubectl (master02, master03)
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

# ========================================
# åŠ å…¥WorkerèŠ‚ç‚¹ (worker01/02/03)
# ========================================

# åœ¨æ‰€æœ‰WorkerèŠ‚ç‚¹ä¸Šæ‰§è¡Œ:
kubeadm join 192.168.10.100:6443 --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash>

# ========================================
# éªŒè¯é›†ç¾¤
# ========================================

# æ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹
kubectl get nodes
# åº”è¯¥çœ‹åˆ°3ä¸ªMasterèŠ‚ç‚¹å’Œ3ä¸ªWorkerèŠ‚ç‚¹éƒ½æ˜¯ReadyçŠ¶æ€

# æ£€æŸ¥æ§åˆ¶å¹³é¢ç»„ä»¶
kubectl get pods -n kube-system
# åº”è¯¥çœ‹åˆ°:
# - æ¯ä¸ªMasterèŠ‚ç‚¹ä¸Š: kube-apiserver, kube-controller-manager, kube-scheduler, etcd
# - æ‰€æœ‰èŠ‚ç‚¹: calico, kube-proxy, coredns

# æ£€æŸ¥etcdé›†ç¾¤å¥åº·
kubectl exec -n kube-system etcd-master01 -- etcdctl \
    --endpoints=https://127.0.0.1:2379 \
    --cacert=/etc/kubernetes/pki/etcd/ca.crt \
    --cert=/etc/kubernetes/pki/etcd/server.crt \
    --key=/etc/kubernetes/pki/etcd/server.key \
    endpoint health

# æ£€æŸ¥Leaderé€‰ä¸¾
kubectl get lease -n kube-system
# åº”è¯¥çœ‹åˆ°kube-controller-managerå’Œkube-schedulerçš„lease

echo "âœ… Kubernetes HAé›†ç¾¤éƒ¨ç½²å®Œæˆ"
```

### 2.2 æ‰‹åŠ¨éƒ¨ç½²HAé›†ç¾¤ (å¤–éƒ¨etcd)

```bash
#!/bin/bash
# ========================================
# æ‰‹åŠ¨éƒ¨ç½²K8s HAé›†ç¾¤ (å¤–éƒ¨etcd)
# ========================================

# æ¶æ„:
# - 3ä¸ªç‹¬ç«‹etcdèŠ‚ç‚¹ (etcd01/02/03)
# - 3ä¸ªMasterèŠ‚ç‚¹ (master01/02/03)
# - 3ä¸ªWorkerèŠ‚ç‚¹ (worker01/02/03)

# ========================================
# 1. éƒ¨ç½²ç‹¬ç«‹etcdé›†ç¾¤
# ========================================

# åœ¨etcd01èŠ‚ç‚¹:
ETCD_NAME=etcd01
ETCD_IP=192.168.10.201
INITIAL_CLUSTER="etcd01=https://192.168.10.201:2380,etcd02=https://192.168.10.202:2380,etcd03=https://192.168.10.203:2380"

# ä¸‹è½½etcd
ETCD_VER=v3.5.10
wget https://github.com/etcd-io/etcd/releases/download/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz
tar xzf etcd-${ETCD_VER}-linux-amd64.tar.gz
cp etcd-${ETCD_VER}-linux-amd64/etcd* /usr/local/bin/

# ç”Ÿæˆè¯ä¹¦ (ä½¿ç”¨cfsslæˆ–openssl)
# ... (è¯ä¹¦ç”Ÿæˆæ­¥éª¤ç•¥)

# åˆ›å»ºetcd systemdæœåŠ¡
cat > /etc/systemd/system/etcd.service << EOF
[Unit]
Description=etcd
Documentation=https://github.com/etcd-io/etcd

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAME} \\
  --cert-file=/etc/etcd/ssl/server.pem \\
  --key-file=/etc/etcd/ssl/server-key.pem \\
  --peer-cert-file=/etc/etcd/ssl/peer.pem \\
  --peer-key-file=/etc/etcd/ssl/peer-key.pem \\
  --trusted-ca-file=/etc/etcd/ssl/ca.pem \\
  --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${ETCD_IP}:2380 \\
  --listen-peer-urls https://${ETCD_IP}:2380 \\
  --listen-client-urls https://${ETCD_IP}:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://${ETCD_IP}:2379 \\
  --initial-cluster-token etcd-cluster-1 \\
  --initial-cluster ${INITIAL_CLUSTER} \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable etcd
systemctl start etcd

# åœ¨etcd02å’Œetcd03é‡å¤ä¸Šè¿°æ­¥éª¤ (ä¿®æ”¹ETCD_NAMEå’ŒETCD_IP)

# éªŒè¯etcdé›†ç¾¤
etcdctl --endpoints=https://192.168.10.201:2379,https://192.168.10.202:2379,https://192.168.10.203:2379 \
    --cacert=/etc/etcd/ssl/ca.pem \
    --cert=/etc/etcd/ssl/server.pem \
    --key=/etc/etcd/ssl/server-key.pem \
    endpoint health

# ========================================
# 2. éƒ¨ç½²Kubernetesæ§åˆ¶å¹³é¢ (master01)
# ========================================

# kubeadmé…ç½®æ–‡ä»¶ (å¤–éƒ¨etcd)
cat > kubeadm-config-external-etcd.yaml << 'EOF'
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0
controlPlaneEndpoint: "192.168.10.100:6443"
networking:
  serviceSubnet: "10.96.0.0/12"
  podSubnet: "10.244.0.0/16"
etcd:
  external:
    endpoints:
      - https://192.168.10.201:2379
      - https://192.168.10.202:2379
      - https://192.168.10.203:2379
    caFile: /etc/kubernetes/pki/etcd/ca.crt
    certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
    keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
apiServer:
  certSANs:
    - "192.168.10.100"
    - "192.168.10.101"
    - "192.168.10.102"
    - "192.168.10.103"
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: "192.168.10.101"
  bindPort: 6443
EOF

# åˆå§‹åŒ–
kubeadm init --config=kubeadm-config-external-etcd.yaml --upload-certs

# é…ç½®kubectl
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

# å®‰è£…CNI
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

# å…¶ä»–MasterèŠ‚ç‚¹åŠ å…¥ (master02, master03)
# ... (ä½¿ç”¨kubeadm joinå‘½ä»¤)

echo "âœ… å¤–éƒ¨etcdæ¨¡å¼K8s HAé›†ç¾¤éƒ¨ç½²å®Œæˆ"
```

---

## 3. etcdé›†ç¾¤é«˜å¯ç”¨

```yaml
etcd_HA:
  æ¶æ„ç‰¹ç‚¹:
    - åˆ†å¸ƒå¼é”®å€¼å­˜å‚¨
    - Raftå…±è¯†ç®—æ³•
    - Leaderé€‰ä¸¾
    - å¼ºä¸€è‡´æ€§
  
  é›†ç¾¤è§„æ¨¡:
    3èŠ‚ç‚¹:
      - æœ€å°HAé…ç½®
      - å®¹å¿1ä¸ªèŠ‚ç‚¹æ•…éšœ
      - æ¨èä¸­å°å‹é›†ç¾¤
    
    5èŠ‚ç‚¹:
      - æ¨èç”Ÿäº§é…ç½®
      - å®¹å¿2ä¸ªèŠ‚ç‚¹æ•…éšœ
      - æ›´é«˜å¯ç”¨æ€§
      - æ¨èå¤§å‹é›†ç¾¤
    
    7èŠ‚ç‚¹:
      - è¶…å¤§è§„æ¨¡é›†ç¾¤
      - å®¹å¿3ä¸ªèŠ‚ç‚¹æ•…éšœ
      - æ³¨æ„æ€§èƒ½å¼€é”€
  
  Raftå…±è¯†:
    è§’è‰²:
      Leader:
        - å¤„ç†æ‰€æœ‰å†™è¯·æ±‚
        - åŒæ­¥åˆ°Follower
        - å‘é€å¿ƒè·³
      
      Follower:
        - æ¥æ”¶Leaderæ—¥å¿—
        - å‚ä¸é€‰ä¸¾
        - è½¬å‘å†™è¯·æ±‚åˆ°Leader
      
      Candidate:
        - é€‰ä¸¾ä¸­çš„ä¸´æ—¶è§’è‰²
    
    é€‰ä¸¾è¿‡ç¨‹:
      1. Followerè¶…æ—¶æœªæ”¶åˆ°å¿ƒè·³
      2. è½¬å˜ä¸ºCandidate
      3. è¯·æ±‚å…¶ä»–èŠ‚ç‚¹æŠ•ç¥¨
      4. è·å¾—å¤šæ•°ç¥¨æˆä¸ºLeader
      5. å¼€å§‹å‘é€å¿ƒè·³
    
    æ•°æ®ä¸€è‡´æ€§:
      - æ—¥å¿—å¤åˆ¶
      - å¤šæ•°æ´¾ç¡®è®¤
      - å†™å…¥æŒä¹…åŒ–
      - è¯»å–ä¸€è‡´æ€§ä¿è¯

etcdé…ç½®ä¼˜åŒ–:
  æ€§èƒ½è°ƒä¼˜:
    --heartbeat-interval: 100      # å¿ƒè·³é—´éš” (ms)
    --election-timeout: 1000        # é€‰ä¸¾è¶…æ—¶ (ms)
    --snapshot-count: 10000         # å¿«ç…§è§¦å‘é˜ˆå€¼
    --quota-backend-bytes: 8589934592  # 8GBå­˜å‚¨é…é¢
  
  ç½‘ç»œé…ç½®:
    - ä¸“ç”¨ç½‘ç»œ
    - ä½å»¶è¿Ÿ (<5ms)
    - é«˜å¸¦å®½ (1Gbps+)
    - é˜²ç«å¢™å¼€æ”¾ç«¯å£ (2379, 2380)
  
  ç£ç›˜é…ç½®:
    - SSDç£ç›˜ (æ¨èNVMe)
    - ç‹¬ç«‹ç£ç›˜ (ä¸ä¸å…¶ä»–IOç«äº‰)
    - ä½å»¶è¿Ÿ (<10ms)
    - é«˜IOPS
  
  å¤‡ä»½ç­–ç•¥:
    å®šæœŸå¤‡ä»½:
      - æ¯å¤©å…¨é‡å¤‡ä»½
      - ä¿ç•™7å¤©
      - å¼‚åœ°å­˜å‚¨
    
    å¿«ç…§å¤‡ä»½:
      - è‡ªåŠ¨å¿«ç…§
      - å¢é‡å¤‡ä»½
      - å¿«é€Ÿæ¢å¤
```

### 3.1 etcdç»´æŠ¤ä¸ç›‘æ§

```bash
#!/bin/bash
# ========================================
# etcdé›†ç¾¤ç»´æŠ¤è„šæœ¬
# ========================================

ETCDCTL_API=3
ENDPOINTS="https://192.168.10.201:2379,https://192.168.10.202:2379,https://192.168.10.203:2379"
CACERT="/etc/etcd/ssl/ca.pem"
CERT="/etc/etcd/ssl/server.pem"
KEY="/etc/etcd/ssl/server-key.pem"

# 1. å¥åº·æ£€æŸ¥
echo "=== etcdå¥åº·æ£€æŸ¥ ==="
etcdctl --endpoints=$ENDPOINTS \
    --cacert=$CACERT --cert=$CERT --key=$KEY \
    endpoint health

# 2. æˆå‘˜åˆ—è¡¨
echo -e "\n=== etcdæˆå‘˜åˆ—è¡¨ ==="
etcdctl --endpoints=$ENDPOINTS \
    --cacert=$CACERT --cert=$CERT --key=$KEY \
    member list -w table

# 3. LeaderæŸ¥è¯¢
echo -e "\n=== etcd Leader ==="
etcdctl --endpoints=$ENDPOINTS \
    --cacert=$CACERT --cert=$CERT --key=$KEY \
    endpoint status -w table

# 4. æ€§èƒ½ç›‘æ§
echo -e "\n=== etcdæ€§èƒ½æŒ‡æ ‡ ==="
etcdctl --endpoints=$ENDPOINTS \
    --cacert=$CACERT --cert=$CERT --key=$KEY \
    endpoint status --write-out=table

# 5. ç¢ç‰‡æ•´ç†
echo -e "\n=== ç¢ç‰‡æ•´ç† (å®šæœŸæ‰§è¡Œ) ==="
for endpoint in $(echo $ENDPOINTS | tr ',' ' '); do
    echo "Defragmenting $endpoint"
    etcdctl --endpoints=$endpoint \
        --cacert=$CACERT --cert=$CERT --key=$KEY \
        defrag
done

# 6. å¿«ç…§å¤‡ä»½
echo -e "\n=== åˆ›å»ºå¿«ç…§å¤‡ä»½ ==="
BACKUP_DIR="/backup/etcd"
BACKUP_FILE="$BACKUP_DIR/etcd-snapshot-$(date +%Y%m%d-%H%M%S).db"

mkdir -p $BACKUP_DIR
etcdctl --endpoints=$ENDPOINTS \
    --cacert=$CACERT --cert=$CERT --key=$KEY \
    snapshot save $BACKUP_FILE

# éªŒè¯å¿«ç…§
etcdctl --write-out=table snapshot status $BACKUP_FILE

# 7. æ•°æ®åº“å¤§å°
echo -e "\n=== etcdæ•°æ®åº“å¤§å° ==="
etcdctl --endpoints=$ENDPOINTS \
    --cacert=$CACERT --cert=$CERT --key=$KEY \
    endpoint status -w table | awk '{print $3, $5}'

echo "âœ… etcdç»´æŠ¤å®Œæˆ"
```

### 3.2 etcdæ•…éšœæ¢å¤

```bash
#!/bin/bash
# ========================================
# etcdç¾éš¾æ¢å¤
# ========================================

# åœºæ™¯1: å•ä¸ªèŠ‚ç‚¹æ•…éšœ
# å¦‚æœèŠ‚ç‚¹å¯æ¢å¤ï¼Œç›´æ¥é‡å¯etcdæœåŠ¡
systemctl restart etcd

# å¦‚æœèŠ‚ç‚¹ä¸å¯æ¢å¤ï¼Œç§»é™¤æ•…éšœèŠ‚ç‚¹å¹¶æ·»åŠ æ–°èŠ‚ç‚¹
# 1. ç§»é™¤æ•…éšœèŠ‚ç‚¹
etcdctl member remove <member-id>

# 2. æ·»åŠ æ–°èŠ‚ç‚¹
etcdctl member add etcd-new \
    --peer-urls=https://192.168.10.204:2380

# 3. åœ¨æ–°èŠ‚ç‚¹å¯åŠ¨etcd (ä½¿ç”¨--initial-cluster-state existing)

# åœºæ™¯2: å¤šæ•°èŠ‚ç‚¹æ•…éšœ (éœ€è¦ä»å¿«ç…§æ¢å¤)
# 1. åœæ­¢æ‰€æœ‰etcdèŠ‚ç‚¹
systemctl stop etcd

# 2. ä»å¿«ç…§æ¢å¤ (åœ¨æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œ)
ETCD_NAME=etcd01
ETCD_INITIAL_CLUSTER="etcd01=https://192.168.10.201:2380,etcd02=https://192.168.10.202:2380,etcd03=https://192.168.10.203:2380"
SNAPSHOT_FILE="/backup/etcd/etcd-snapshot-20251019.db"

etcdctl snapshot restore $SNAPSHOT_FILE \
    --name $ETCD_NAME \
    --initial-cluster $ETCD_INITIAL_CLUSTER \
    --initial-cluster-token etcd-cluster-1 \
    --initial-advertise-peer-urls https://192.168.10.201:2380 \
    --data-dir /var/lib/etcd-restore

# 3. æ›¿æ¢æ•°æ®ç›®å½•
rm -rf /var/lib/etcd
mv /var/lib/etcd-restore /var/lib/etcd

# 4. å¯åŠ¨etcd
systemctl start etcd

# 5. éªŒè¯æ¢å¤
etcdctl endpoint health

echo "âœ… etcdæ¢å¤å®Œæˆ"
```

---

## 4. å·¥ä½œèŠ‚ç‚¹é«˜å¯ç”¨

```yaml
Worker_Node_HA:
  èŠ‚ç‚¹å†—ä½™:
    æœ€å°é…ç½®:
      - è‡³å°‘3ä¸ªWorkerèŠ‚ç‚¹
      - N+1å†—ä½™
      - è´Ÿè½½åˆ†æ•£
    
    æ¨èé…ç½®:
      - 5ä¸ªä»¥ä¸ŠWorkerèŠ‚ç‚¹
      - N+2å†—ä½™
      - æ›´é«˜å®¹é”™èƒ½åŠ›
  
  Podé«˜å¯ç”¨:
    å‰¯æœ¬æ•°:
      - Deployment: replicas >= 2
      - StatefulSet: replicas >= 2
      - DaemonSet: æ¯èŠ‚ç‚¹1ä¸ª
    
    åˆ†å¸ƒç­–ç•¥:
      podAntiAffinity:
        - preferredDuringSchedulingIgnoredDuringExecution
        - requiredDuringSchedulingIgnoredDuringExecution
      
      topologySpreadConstraints:
        - è·¨å¯ç”¨åŒºåˆ†å¸ƒ
        - è·¨èŠ‚ç‚¹åˆ†å¸ƒ
        - å‡åŒ€åˆ†å¸ƒ
    
    PodDisruptionBudget (PDB):
      - minAvailable: æœ€å°‘å¯ç”¨Podæ•°
      - maxUnavailable: æœ€å¤šä¸å¯ç”¨Podæ•°
      - ä¿æŠ¤åº”ç”¨å¯ç”¨æ€§
      - æ§åˆ¶æ»šåŠ¨æ›´æ–°é€Ÿåº¦
  
  å¥åº·æ£€æŸ¥:
    livenessProbe:
      - æ£€æµ‹Podå­˜æ´»
      - å¤±è´¥åˆ™é‡å¯Pod
      - HTTP/TCP/Execæ¢æµ‹
    
    readinessProbe:
      - æ£€æµ‹Podå°±ç»ª
      - æœªå°±ç»ªåˆ™ç§»å‡ºService
      - æµé‡éš”ç¦»
    
    startupProbe:
      - æ£€æµ‹åº”ç”¨å¯åŠ¨
      - æ…¢å¯åŠ¨åº”ç”¨
      - é¿å…è¿‡æ—©æ£€æŸ¥

èŠ‚ç‚¹æ•…éšœå¤„ç†:
  æ•…éšœæ£€æµ‹:
    kubeletå¿ƒè·³:
      - é»˜è®¤10ç§’
      - node-status-update-frequency
    
    èŠ‚ç‚¹çŠ¶æ€:
      - Ready: æ­£å¸¸
      - NotReady: æ•…éšœ
      - Unknown: å¤±è”
  
  Podé©±é€:
    é©±é€æ¡ä»¶:
      - èŠ‚ç‚¹NotReadyè¶…è¿‡40ç§’
      - --pod-eviction-timeout=5m (å·²åºŸå¼ƒ)
    
    é©±é€è¡Œä¸º:
      - Podæ ‡è®°ä¸ºTerminating
      - åœ¨å…¶ä»–èŠ‚ç‚¹é‡æ–°åˆ›å»º
      - StatefulSetæŒ‰åºé‡å»º
  
  èŠ‚ç‚¹æ¢å¤:
    è‡ªåŠ¨æ¢å¤:
      - èŠ‚ç‚¹é‡å¯åè‡ªåŠ¨åŠ å…¥
      - Podè‡ªåŠ¨è°ƒåº¦å›æ¥ (å¦‚æœä»éœ€è¦)
    
    æ‰‹åŠ¨å¹²é¢„:
      - æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
      - æ¸…ç†æ•…éšœPod
      - éªŒè¯é›†ç¾¤å¥åº·
```

### 4.1 é…ç½®Podåäº²å’Œæ€§

```yaml
# ========================================
# Deployment with Pod Anti-Affinity
# ========================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      # Podåäº²å’Œæ€§ (å¼ºåˆ¶)
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - web
              topologyKey: kubernetes.io/hostname
      
      containers:
        - name: web
          image: nginx:1.25
          ports:
            - containerPort: 80
          
          # å¥åº·æ£€æŸ¥
          livenessProbe:
            httpGet:
              path: /healthz
              port: 80
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          
          readinessProbe:
            httpGet:
              path: /ready
              port: 80
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
          
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 500m
              memory: 512Mi

---
# ========================================
# PodDisruptionBudget
# ========================================
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: web-app-pdb
spec:
  minAvailable: 2  # è‡³å°‘ä¿æŒ2ä¸ªPodå¯ç”¨
  selector:
    matchLabels:
      app: web

---
# ========================================
# Topology Spread Constraints
# ========================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: distributed-app
spec:
  replicas: 6
  selector:
    matchLabels:
      app: distributed
  template:
    metadata:
      labels:
        app: distributed
    spec:
      # æ‹“æ‰‘åˆ†å¸ƒçº¦æŸ
      topologySpreadConstraints:
        # è·¨èŠ‚ç‚¹å‡åŒ€åˆ†å¸ƒ
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app: distributed
        
        # è·¨å¯ç”¨åŒºåˆ†å¸ƒ
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: distributed
      
      containers:
        - name: app
          image: myapp:latest
          ports:
            - containerPort: 8080
```

---

## 5. è´Ÿè½½å‡è¡¡é…ç½®

```yaml
Load_Balancer_Types:
  å¤–éƒ¨è´Ÿè½½å‡è¡¡:
    ç”¨é€”: åˆ†å‘API Serverè¯·æ±‚
    
    HAProxy:
      ä¼˜ç‚¹:
        - å¼€æºå…è´¹
        - é«˜æ€§èƒ½
        - çµæ´»é…ç½®
        - TCP/HTTPæ”¯æŒ
      
      é…ç½®:
        - frontendç›‘å¬6443
        - backendå¤šä¸ªAPI Server
        - å¥åº·æ£€æŸ¥
        - ä¼šè¯ä¿æŒ
    
    Nginx:
      ä¼˜ç‚¹:
        - æˆç†Ÿç¨³å®š
        - æ˜“äºé…ç½®
        - streamæ¨¡å—æ”¯æŒTCP
      
      é…ç½®:
        - streamå—é…ç½®
        - upstreamå¤šä¸ªAPI Server
        - å¥åº·æ£€æŸ¥
    
    äº‘LB:
      - AWS ELB/NLB
      - Azure Load Balancer
      - GCP Load Balancer
      - è‡ªåŠ¨æ•…éšœåˆ‡æ¢
      - é«˜å¯ç”¨æ€§
  
  å†…éƒ¨è´Ÿè½½å‡è¡¡:
    kube-proxy:
      æ¨¡å¼:
        iptables:
          - é»˜è®¤æ¨¡å¼
          - åŸºäºiptablesè§„åˆ™
          - éšæœºè´Ÿè½½å‡è¡¡
        
        ipvs:
          - é«˜æ€§èƒ½
          - å¤šç§è´Ÿè½½å‡è¡¡ç®—æ³•
          - æ¨èå¤§è§„æ¨¡é›†ç¾¤
        
        userspace:
          - å·²åºŸå¼ƒ
          - æ€§èƒ½è¾ƒå·®
      
      Serviceç±»å‹:
        ClusterIP:
          - é›†ç¾¤å†…éƒ¨è®¿é—®
          - è™šæ‹ŸIP
          - è´Ÿè½½å‡è¡¡
        
        NodePort:
          - èŠ‚ç‚¹ç«¯å£æš´éœ²
          - å¤–éƒ¨è®¿é—®
          - è‡ªåŠ¨è´Ÿè½½å‡è¡¡
        
        LoadBalancer:
          - äº‘ç¯å¢ƒ
          - å¤–éƒ¨LBé›†æˆ
          - è‡ªåŠ¨é…ç½®

Keepalived + HAProxy HA:
  æ¶æ„:
    - 2ä¸ªHAProxyèŠ‚ç‚¹
    - Keepalivedæä¾›VIP
    - VRRPåè®®
    - è‡ªåŠ¨æ•…éšœåˆ‡æ¢
  
  VIP (Virtual IP):
    - 192.168.10.100
    - æµ®åŠ¨IP
    - MasteræŒæœ‰
    - Backupè‡ªåŠ¨æ¥ç®¡
```

### 5.1 Keepalived + HAProxyé…ç½®

```bash
#!/bin/bash
# ========================================
# Keepalived + HAProxy HAé…ç½®
# ========================================

# åœ¨HAProxy-01 (Master)èŠ‚ç‚¹

# 1. å®‰è£…è½¯ä»¶
apt install -y haproxy keepalived

# 2. é…ç½®HAProxy (ä¸å‰é¢ç›¸åŒ)
cat > /etc/haproxy/haproxy.cfg << 'EOF'
# ... (HAProxyé…ç½®ï¼Œè§å‰é¢ç« èŠ‚)
EOF

# 3. é…ç½®Keepalived (Master)
cat > /etc/keepalived/keepalived.conf << 'EOF'
global_defs {
    router_id LB-MASTER
}

vrrp_script check_haproxy {
    script "/usr/bin/killall -0 haproxy"
    interval 2
    weight -5
    fall 2
    rise 2
}

vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 51
    priority 100
    advert_int 1
    
    authentication {
        auth_type PASS
        auth_pass K8s@2025
    }
    
    virtual_ipaddress {
        192.168.10.100/24
    }
    
    track_script {
        check_haproxy
    }
    
    notify_master "/etc/keepalived/notify.sh MASTER"
    notify_backup "/etc/keepalived/notify.sh BACKUP"
    notify_fault "/etc/keepalived/notify.sh FAULT"
}
EOF

# 4. åˆ›å»ºé€šçŸ¥è„šæœ¬
cat > /etc/keepalived/notify.sh << 'EOF'
#!/bin/bash
TYPE=$1
NAME=$2
STATE=$3

case $STATE in
    "MASTER") echo "$(date) - Became MASTER" >> /var/log/keepalived-notify.log
              ;;
    "BACKUP") echo "$(date) - Became BACKUP" >> /var/log/keepalived-notify.log
              ;;
    "FAULT")  echo "$(date) - Fault detected" >> /var/log/keepalived-notify.log
              ;;
    *)        echo "$(date) - Unknown state: $STATE" >> /var/log/keepalived-notify.log
              ;;
esac
EOF

chmod +x /etc/keepalived/notify.sh

# 5. å¯åŠ¨æœåŠ¡
systemctl restart haproxy
systemctl enable haproxy

systemctl restart keepalived
systemctl enable keepalived

# 6. éªŒè¯VIP
ip addr show eth0
# åº”è¯¥çœ‹åˆ°192.168.10.100

# ========================================
# åœ¨HAProxy-02 (Backup)èŠ‚ç‚¹
# ========================================

# å®‰è£…è½¯ä»¶ (åŒMaster)
apt install -y haproxy keepalived

# é…ç½®HAProxy (åŒMaster)
# ...

# é…ç½®Keepalived (Backup - priorityè¾ƒä½)
cat > /etc/keepalived/keepalived.conf << 'EOF'
global_defs {
    router_id LB-BACKUP
}

vrrp_script check_haproxy {
    script "/usr/bin/killall -0 haproxy"
    interval 2
    weight -5
    fall 2
    rise 2
}

vrrp_instance VI_1 {
    state BACKUP
    interface eth0
    virtual_router_id 51
    priority 90  # ä½äºMaster
    advert_int 1
    
    authentication {
        auth_type PASS
        auth_pass K8s@2025
    }
    
    virtual_ipaddress {
        192.168.10.100/24
    }
    
    track_script {
        check_haproxy
    }
    
    notify_master "/etc/keepalived/notify.sh MASTER"
    notify_backup "/etc/keepalived/notify.sh BACKUP"
    notify_fault "/etc/keepalived/notify.sh FAULT"
}
EOF

# é€šçŸ¥è„šæœ¬ (åŒMaster)
# ...

# å¯åŠ¨æœåŠ¡
systemctl restart haproxy keepalived
systemctl enable haproxy keepalived

# ========================================
# æµ‹è¯•æ•…éšœåˆ‡æ¢
# ========================================

# åœ¨MasterèŠ‚ç‚¹åœæ­¢HAProxy
systemctl stop haproxy

# åœ¨BackupèŠ‚ç‚¹æ£€æŸ¥VIPæ˜¯å¦æ¼‚ç§»
ip addr show eth0
# åº”è¯¥çœ‹åˆ°192.168.10.100å·²ç»åœ¨BackupèŠ‚ç‚¹

# æ¢å¤MasterèŠ‚ç‚¹
systemctl start haproxy
# VIPåº”è¯¥è‡ªåŠ¨æ¼‚å›Master (å¦‚æœé…ç½®äº†æŠ¢å )

echo "âœ… Keepalived + HAProxy HAé…ç½®å®Œæˆ"
```

---

## 6. åº”ç”¨é«˜å¯ç”¨

```yaml
Application_HA_Strategies:
  æ— çŠ¶æ€åº”ç”¨:
    Deployment:
      - å¤šå‰¯æœ¬éƒ¨ç½²
      - æ»šåŠ¨æ›´æ–°
      - è‡ªåŠ¨é‡å¯
      - å¥åº·æ£€æŸ¥
    
    HPA (Horizontal Pod Autoscaler):
      - CPU/å†…å­˜è‡ªåŠ¨æ‰©ç¼©å®¹
      - è‡ªå®šä¹‰æŒ‡æ ‡æ‰©ç¼©å®¹
      - æœ€å°/æœ€å¤§å‰¯æœ¬æ•°
      - æ‰©ç¼©å®¹ç­–ç•¥
    
    é…ç½®ç¤ºä¾‹:
      replicas: 3
      strategy:
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0
      
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
  
  æœ‰çŠ¶æ€åº”ç”¨:
    StatefulSet:
      - ç¨³å®šçš„ç½‘ç»œæ ‡è¯†
      - æœ‰åºéƒ¨ç½²å’Œæ‰©å±•
      - æŒä¹…åŒ–å­˜å‚¨
      - æœ‰åºç»ˆæ­¢
    
    Headless Service:
      - DNSè®°å½•æŒ‡å‘Pod
      - ç¨³å®šçš„ç½‘ç»œèº«ä»½
      - æ”¯æŒä¸»ä»æ¶æ„
    
    æŒä¹…åŒ–:
      - PVC/PV
      - StorageClass
      - æ•°æ®æŒä¹…åŒ–
      - å¤‡ä»½æ¢å¤
  
  æ•°æ®åº“HA:
    MySQL:
      - MySQL Group Replication
      - MySQL InnoDB Cluster
      - æˆ–ä½¿ç”¨MySQL Operator
      - ä¸»ä»å¤åˆ¶
    
    PostgreSQL:
      - Patroni + etcd
      - Streaming Replication
      - Pgpool-II
      - Zalando Postgres Operator
    
    MongoDB:
      - Replica Set
      - Sharding
      - MongoDB Operator
      - è‡ªåŠ¨æ•…éšœåˆ‡æ¢
    
    Redis:
      - Redis Sentinel
      - Redis Cluster
      - Redis Operator
      - é«˜å¯ç”¨æ€§

æœåŠ¡å‘ç°ä¸æ³¨å†Œ:
  Service:
    - ClusterIP (é»˜è®¤)
    - NodePort
    - LoadBalancer
    - ExternalName
  
  Endpoints:
    - è‡ªåŠ¨ç®¡ç†
    - å¥åº·Pod
    - è´Ÿè½½å‡è¡¡
  
  DNS:
    - CoreDNS
    - æœåŠ¡å‘ç°
    - Pod DNS
    - è‡ªåŠ¨è§£æ
```

### 6.1 åº”ç”¨é«˜å¯ç”¨é…ç½®ç¤ºä¾‹

```yaml
# ========================================
# æ— çŠ¶æ€åº”ç”¨ HAé…ç½®
# ========================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ha
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  
  # æ»šåŠ¨æ›´æ–°ç­–ç•¥
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # æ›´æ–°æ—¶æœ€å¤šé¢å¤–åˆ›å»º1ä¸ªPod
      maxUnavailable: 0  # æ›´æ–°æ—¶æœ€å¤š0ä¸ªPodä¸å¯ç”¨
  
  template:
    metadata:
      labels:
        app: nginx
    spec:
      # Podåäº²å’Œæ€§
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - nginx
                topologyKey: kubernetes.io/hostname
      
      containers:
        - name: nginx
          image: nginx:1.25
          ports:
            - containerPort: 80
              name: http
          
          # èµ„æºé™åˆ¶
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 500m
              memory: 512Mi
          
          # å­˜æ´»æ¢é’ˆ
          livenessProbe:
            httpGet:
              path: /
              port: 80
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          
          # å°±ç»ªæ¢é’ˆ
          readinessProbe:
            httpGet:
              path: /
              port: 80
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 3

---
# Service
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: ClusterIP
  selector:
    app: nginx
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
  sessionAffinity: ClientIP  # ä¼šè¯ä¿æŒ
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800

---
# PodDisruptionBudget
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: nginx-pdb
spec:
  minAvailable: 2  # è‡³å°‘2ä¸ªPodå¯ç”¨
  selector:
    matchLabels:
      app: nginx

---
# HPA (Horizontal Pod Autoscaler)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-ha
  minReplicas: 3
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # 5åˆ†é’Ÿç¨³å®šæœŸ
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
        - type: Pods
          value: 2
          periodSeconds: 30
      selectPolicy: Max

---
# ========================================
# æœ‰çŠ¶æ€åº”ç”¨ HAé…ç½® (MySQLç¤ºä¾‹)
# ========================================
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  ports:
    - port: 3306
  clusterIP: None  # Headless Service
  selector:
    app: mysql

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
        - name: init-mysql
          image: mysql:8.0
          command:
            - bash
            - "-c"
            - |
              set -ex
              # æ ¹æ®Podåºå·ç”Ÿæˆserver-id
              [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
              ordinal=${BASH_REMATCH[1]}
              echo [mysqld] > /mnt/conf.d/server-id.cnf
              echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf
              # å¤åˆ¶é…ç½®åˆ°æ­£ç¡®ä½ç½®
              if [[ $ordinal -eq 0 ]]; then
                cp /mnt/config-map/master.cnf /mnt/conf.d/
              else
                cp /mnt/config-map/slave.cnf /mnt/conf.d/
              fi
          volumeMounts:
            - name: conf
              mountPath: /mnt/conf.d
            - name: config-map
              mountPath: /mnt/config-map
      
      containers:
        - name: mysql
          image: mysql:8.0
          env:
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mysql-secret
                  key: password
          ports:
            - name: mysql
              containerPort: 3306
          
          volumeMounts:
            - name: data
              mountPath: /var/lib/mysql
              subPath: mysql
            - name: conf
              mountPath: /etc/mysql/conf.d
          
          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: 2000m
              memory: 4Gi
          
          livenessProbe:
            exec:
              command: ["mysqladmin", "ping"]
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
          
          readinessProbe:
            exec:
              command: ["mysql", "-h", "127.0.0.1", "-e", "SELECT 1"]
            initialDelaySeconds: 5
            periodSeconds: 2
            timeoutSeconds: 1
      
      volumes:
        - name: conf
          emptyDir: {}
        - name: config-map
          configMap:
            name: mysql-config
  
  # æŒä¹…åŒ–å­˜å‚¨
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: "fast-ssd"
        resources:
          requests:
            storage: 10Gi
```

---

## 7. å­˜å‚¨é«˜å¯ç”¨

```yaml
Storage_HA:
  æŒä¹…åŒ–å­˜å‚¨:
    æœ¬åœ°å­˜å‚¨:
      - Local PV
      - HostPath (ä¸æ¨èç”Ÿäº§)
      - æ€§èƒ½å¥½
      - æ— å†—ä½™
    
    ç½‘ç»œå­˜å‚¨:
      NFS:
        - ReadWriteMany
        - å…±äº«å­˜å‚¨
        - ç®€å•æ˜“ç”¨
        - æ€§èƒ½ä¸€èˆ¬
      
      iSCSI:
        - ReadWriteOnce
        - å—å­˜å‚¨
        - æ€§èƒ½å¥½
        - éœ€è¦å¤šè·¯å¾„
      
      Ceph RBD:
        - åˆ†å¸ƒå¼å­˜å‚¨
        - é«˜å¯ç”¨
        - è‡ªåŠ¨å‰¯æœ¬
        - æ€§èƒ½å¥½
      
      äº‘å­˜å‚¨:
        - AWS EBS
        - Azure Disk
        - GCP Persistent Disk
        - è‡ªåŠ¨é«˜å¯ç”¨
  
  StorageClass:
    åŠ¨æ€é…ç½®:
      - è‡ªåŠ¨åˆ›å»ºPV
      - æŒ‰éœ€åˆ†é…
      - å›æ”¶ç­–ç•¥
    
    å‚æ•°:
      - provisioner: å­˜å‚¨æä¾›è€…
      - parameters: å­˜å‚¨å‚æ•°
      - reclaimPolicy: Retain/Delete
      - volumeBindingMode: Immediate/WaitForFirstConsumer
  
  CSI (Container Storage Interface):
    ä¼˜ç‚¹:
      - æ ‡å‡†åŒ–æ¥å£
      - æ’ä»¶åŒ–
      - å‚å•†ä¸­ç«‹
      - åŠŸèƒ½ä¸°å¯Œ
    
    å¸¸ç”¨CSI:
      - Ceph CSI
      - NFS CSI
      - Local Path Provisioner
      - Cloud Provider CSI

å¤‡ä»½ç­–ç•¥:
  åº”ç”¨çº§å¤‡ä»½:
    - Velero
    - å¤‡ä»½K8sèµ„æº
    - å¤‡ä»½PVæ•°æ®
    - ç¾éš¾æ¢å¤
  
  å­˜å‚¨çº§å¤‡ä»½:
    - å­˜å‚¨å¿«ç…§
    - å®šæœŸå¤‡ä»½
    - å¼‚åœ°å¤åˆ¶
    - å¿«é€Ÿæ¢å¤
```

### 7.1 Ceph RBD StorageClassé…ç½®

```yaml
# ========================================
# Ceph RBD StorageClass (é«˜å¯ç”¨å­˜å‚¨)
# ========================================
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-rbd-ssd
provisioner: rbd.csi.ceph.com
parameters:
  clusterID: <ceph-cluster-id>
  pool: kubernetes
  imageFeatures: layering
  csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
  csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi
  csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
  csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi
  csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
  csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi
  csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Retain  # ä¿ç•™æ•°æ®
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer

---
# ========================================
# NFS StorageClass (ReadWriteMany)
# ========================================
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client
provisioner: cluster.local/nfs-client-provisioner
parameters:
  archiveOnDelete: "true"
reclaimPolicy: Retain
volumeBindingMode: Immediate

---
# ========================================
# Local Path Provisioner (æœ¬åœ°SSD)
# ========================================
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-path
provisioner: rancher.io/local-path
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
```

---

## 8. ç›‘æ§ä¸æ•…éšœæ¢å¤

```yaml
Monitoring_Stack:
  Prometheus:
    ç»„ä»¶:
      - Prometheus Server: æŒ‡æ ‡æ”¶é›†å’Œå­˜å‚¨
      - Alertmanager: å‘Šè­¦ç®¡ç†
      - Node Exporter: èŠ‚ç‚¹æŒ‡æ ‡
      - kube-state-metrics: K8så¯¹è±¡æŒ‡æ ‡
      - Grafana: å¯è§†åŒ–
    
    å…³é”®æŒ‡æ ‡:
      é›†ç¾¤çº§åˆ«:
        - èŠ‚ç‚¹çŠ¶æ€
        - PodçŠ¶æ€
        - API Serverè¯·æ±‚é€Ÿç‡
        - etcdæ€§èƒ½
      
      èŠ‚ç‚¹çº§åˆ«:
        - CPUä½¿ç”¨ç‡
        - å†…å­˜ä½¿ç”¨ç‡
        - ç£ç›˜IO
        - ç½‘ç»œæµé‡
      
      åº”ç”¨çº§åˆ«:
        - Podé‡å¯æ¬¡æ•°
        - å®¹å™¨CPU/å†…å­˜
        - è¯·æ±‚å»¶è¿Ÿ
        - é”™è¯¯ç‡
  
  æ—¥å¿—èšåˆ:
    EFK Stack:
      - Elasticsearch: å­˜å‚¨
      - Fluentd/Filebeat: æ”¶é›†
      - Kibana: æŸ¥è¯¢åˆ†æ
    
    Loki:
      - Promtail: æ”¶é›†
      - Loki: å­˜å‚¨
      - Grafana: æŸ¥è¯¢
      - è½»é‡çº§

æ•…éšœè‡ªæ„ˆ:
  è‡ªåŠ¨é‡å¯:
    - livenessProbeå¤±è´¥
    - å®¹å™¨å´©æºƒ
    - OOM Killed
    - è‡ªåŠ¨é‡å¯Pod
  
  è‡ªåŠ¨é‡æ–°è°ƒåº¦:
    - èŠ‚ç‚¹æ•…éšœ
    - Podè¢«é©±é€
    - èµ„æºä¸è¶³
    - è°ƒåº¦åˆ°å…¶ä»–èŠ‚ç‚¹
  
  è‡ªåŠ¨æ‰©ç¼©å®¹:
    - HPA (Podè‡ªåŠ¨æ‰©ç¼©å®¹)
    - VPA (å‚ç›´æ‰©ç¼©å®¹)
    - Cluster Autoscaler (èŠ‚ç‚¹è‡ªåŠ¨æ‰©ç¼©å®¹)
```

### 8.1 Prometheuså‘Šè­¦è§„åˆ™

```yaml
# ========================================
# Prometheus AlertRules for K8s HA
# ========================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules
  namespace: monitoring
data:
  kubernetes.rules: |
    groups:
      - name: kubernetes-ha-alerts
        interval: 30s
        rules:
          # èŠ‚ç‚¹æ•…éšœå‘Šè­¦
          - alert: NodeDown
            expr: up{job="node-exporter"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Node {{ $labels.instance }} is down"
              description: "Node {{ $labels.instance }} has been down for more than 2 minutes."
          
          # èŠ‚ç‚¹èµ„æºå‘Šè­¦
          - alert: NodeHighCPU
            expr: (100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Node {{ $labels.instance }} high CPU usage"
              description: "CPU usage is above 80% (current: {{ $value }}%)"
          
          - alert: NodeHighMemory
            expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Node {{ $labels.instance }} high memory usage"
              description: "Memory usage is above 85% (current: {{ $value }}%)"
          
          # Podæ•…éšœå‘Šè­¦
          - alert: PodCrashLooping
            expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
              description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is restarting {{ $value }} times per second."
          
          - alert: PodNotReady
            expr: kube_pod_status_phase{phase!="Running",phase!="Succeeded"} > 0
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} not ready"
              description: "Pod has been in {{ $labels.phase }} state for more than 10 minutes."
          
          # API Serverå‘Šè­¦
          - alert: APIServerDown
            expr: up{job="kubernetes-apiservers"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "API Server {{ $labels.instance }} is down"
              description: "API Server has been down for more than 1 minute."
          
          - alert: APIServerHighLatency
            expr: histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{verb!~"WATCH"}[5m])) by (verb, le)) > 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "API Server high latency"
              description: "99th percentile latency is {{ $value }}s for {{ $labels.verb }} requests."
          
          # etcdå‘Šè­¦
          - alert: etcdDown
            expr: up{job="etcd"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "etcd {{ $labels.instance }} is down"
              description: "etcd instance has been down for more than 1 minute."
          
          - alert: etcdNoLeader
            expr: etcd_server_has_leader == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "etcd cluster has no leader"
              description: "etcd cluster has no leader for more than 1 minute."
          
          - alert: etcdHighFsyncDuration
            expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) > 0.5
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "etcd high fsync duration"
              description: "99th percentile fsync duration is {{ $value }}s."
          
          # Deploymentå‰¯æœ¬å‘Šè­¦
          - alert: DeploymentReplicasMismatch
            expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replicas mismatch"
              description: "Deployment has {{ $value }} available replicas, expected {{ $labels.spec_replicas }}."
          
          # PVå®¹é‡å‘Šè­¦
          - alert: PersistentVolumeAlmostFull
            expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.9
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "PersistentVolume {{ $labels.persistentvolumeclaim }} almost full"
              description: "PV usage is above 90% (current: {{ $value | humanizePercentage }})"
```

### 8.2 æ•…éšœæ¢å¤è„šæœ¬

```bash
#!/bin/bash
# ========================================
# Kubernetesé›†ç¾¤æ•…éšœæ¢å¤è„šæœ¬
# ========================================

# åœºæ™¯1: MasterèŠ‚ç‚¹æ•…éšœ
# å¦‚æœä½¿ç”¨kubeadméƒ¨ç½²ï¼Œetcdåœ¨MasterèŠ‚ç‚¹ä¸Š

# 1. ç§»é™¤æ•…éšœMasterèŠ‚ç‚¹
kubectl delete node master02

# 2. åœ¨æ•…éšœèŠ‚ç‚¹é‡ç½®kubeadm
kubeadm reset -f

# 3. é‡æ–°åŠ å…¥é›†ç¾¤ (ä½¿ç”¨ä¹‹å‰ä¿å­˜çš„joinå‘½ä»¤æˆ–é‡æ–°ç”Ÿæˆtoken)
kubeadm token create --print-join-command

# 4. åœ¨æ•…éšœèŠ‚ç‚¹æ‰§è¡Œjoinå‘½ä»¤
kubeadm join <control-plane-endpoint>:6443 --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash> \
    --control-plane --certificate-key <cert-key>

# åœºæ™¯2: etcdæ•°æ®æŸå (ä½¿ç”¨å¿«ç…§æ¢å¤)
# è§å‰é¢etcdæ¢å¤ç« èŠ‚

# åœºæ™¯3: æ‰€æœ‰MasterèŠ‚ç‚¹æ•…éšœ
# 1. ä»å¤‡ä»½æ¢å¤etcdå¿«ç…§
# 2. é‡æ–°éƒ¨ç½²æ§åˆ¶å¹³é¢
# 3. WorkerèŠ‚ç‚¹ä¼šè‡ªåŠ¨é‡æ–°è¿æ¥

# åœºæ™¯4: WorkerèŠ‚ç‚¹æ•…éšœ
# èŠ‚ç‚¹æ•…éšœä¼šè‡ªåŠ¨è§¦å‘Podé‡æ–°è°ƒåº¦ï¼Œæ— éœ€æ‰‹åŠ¨å¹²é¢„
# å¦‚æœèŠ‚ç‚¹æ¢å¤ï¼Œé‡æ–°åŠ å…¥é›†ç¾¤:
systemctl start kubelet

# åœºæ™¯5: CNIç½‘ç»œæ•…éšœ
# é‡æ–°éƒ¨ç½²CNIæ’ä»¶
kubectl delete -f https://docs.projectcalico.org/manifests/calico.yaml
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

# åœºæ™¯6: CoreDNSæ•…éšœ
kubectl rollout restart deployment coredns -n kube-system

# ========================================
# å¥åº·æ£€æŸ¥è„šæœ¬
# ========================================

check_cluster_health() {
    echo "=== é›†ç¾¤å¥åº·æ£€æŸ¥ ==="
    
    # æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
    echo -e "\n1. èŠ‚ç‚¹çŠ¶æ€:"
    kubectl get nodes
    
    NOT_READY=$(kubectl get nodes --no-headers | grep -v Ready | wc -l)
    if [ $NOT_READY -gt 0 ]; then
        echo "âš ï¸  è­¦å‘Š: $NOT_READY ä¸ªèŠ‚ç‚¹Not Ready"
    else
        echo "âœ… æ‰€æœ‰èŠ‚ç‚¹Ready"
    fi
    
    # æ£€æŸ¥æ§åˆ¶å¹³é¢Pod
    echo -e "\n2. æ§åˆ¶å¹³é¢ç»„ä»¶:"
    kubectl get pods -n kube-system | grep -E "kube-apiserver|kube-controller|kube-scheduler|etcd"
    
    # æ£€æŸ¥CoreDNS
    echo -e "\n3. CoreDNSçŠ¶æ€:"
    kubectl get pods -n kube-system -l k8s-app=kube-dns
    
    # æ£€æŸ¥CNI
    echo -e "\n4. CNIæ’ä»¶çŠ¶æ€:"
    kubectl get pods -n kube-system | grep -E "calico|flannel|cilium|weave"
    
    # æ£€æŸ¥PV/PVC
    echo -e "\n5. æŒä¹…åŒ–å­˜å‚¨:"
    kubectl get pv,pvc --all-namespaces
    
    # æ£€æŸ¥æœ‰é—®é¢˜çš„Pod
    echo -e "\n6. é—®é¢˜Pod:"
    kubectl get pods --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded
    
    # æ£€æŸ¥æœ€è¿‘äº‹ä»¶
    echo -e "\n7. æœ€è¿‘å‘Šè­¦äº‹ä»¶:"
    kubectl get events --all-namespaces --sort-by='.metadata.creationTimestamp' | grep -i "error\|warning" | tail -20
}

# æ‰§è¡Œå¥åº·æ£€æŸ¥
check_cluster_health

echo "âœ… å¥åº·æ£€æŸ¥å®Œæˆ"
```

---

## 9. æœ€ä½³å®è·µ

```yaml
Kubernetes_HA_Best_Practices:
  é›†ç¾¤è§„åˆ’:
    âœ… MasterèŠ‚ç‚¹å¥‡æ•°ä¸ª (3æˆ–5)
    âœ… MasterèŠ‚ç‚¹åˆ†å¸ƒåœ¨ä¸åŒæœºæ¶/å¯ç”¨åŒº
    âœ… WorkerèŠ‚ç‚¹è‡³å°‘3ä¸ª
    âœ… ä½¿ç”¨å¤–éƒ¨etcd (å¤§å‹é›†ç¾¤)
    âœ… é…ç½®è´Ÿè½½å‡è¡¡å™¨å†—ä½™ (Keepalived)
  
  é«˜å¯ç”¨é…ç½®:
    âœ… å¯ç”¨Leaderé€‰ä¸¾ (controller-manager/scheduler)
    âœ… é…ç½®etcdå®šæœŸå¤‡ä»½
    âœ… é…ç½®API Serverå®¡è®¡æ—¥å¿—
    âœ… ä½¿ç”¨RBACè¿›è¡Œæƒé™æ§åˆ¶
    âœ… å¯ç”¨Pod Security Policy/Standards
  
  åº”ç”¨éƒ¨ç½²:
    âœ… å…³é”®åº”ç”¨: replicas >= 3
    âœ… é…ç½®Podåäº²å’Œæ€§
    âœ… é…ç½®PodDisruptionBudget
    âœ… é…ç½®å¥åº·æ£€æŸ¥ (liveness/readiness)
    âœ… é…ç½®èµ„æºè¯·æ±‚å’Œé™åˆ¶
    âœ… ä½¿ç”¨HPAè‡ªåŠ¨æ‰©ç¼©å®¹
  
  å­˜å‚¨é…ç½®:
    âœ… ä½¿ç”¨æŒä¹…åŒ–å­˜å‚¨ (PV/PVC)
    âœ… é€‰æ‹©åˆé€‚çš„StorageClass
    âœ… é…ç½®å¤‡ä»½ç­–ç•¥
    âœ… ä½¿ç”¨å‰¯æœ¬æ•°>1çš„å­˜å‚¨æ–¹æ¡ˆ
  
  ç›‘æ§å‘Šè­¦:
    âœ… éƒ¨ç½²Prometheusç›‘æ§
    âœ… é…ç½®å…³é”®æŒ‡æ ‡å‘Šè­¦
    âœ… é›†ä¸­æ—¥å¿—æ”¶é›† (EFK/Loki)
    âœ… å®šæœŸå®¡æŸ¥ç›‘æ§æ•°æ®
    âœ… å»ºç«‹æ•…éšœå“åº”æµç¨‹
  
  å®‰å…¨åŠ å›º:
    âœ… å¯ç”¨TLSåŠ å¯† (æ‰€æœ‰ç»„ä»¶)
    âœ… å®šæœŸè½®æ¢è¯ä¹¦
    âœ… ç½‘ç»œç­–ç•¥éš”ç¦»
    âœ… é•œåƒæ‰«æ
    âœ… å®šæœŸæ›´æ–°K8sç‰ˆæœ¬
    âœ… æœ€å°æƒé™åŸåˆ™
  
  å¤‡ä»½æ¢å¤:
    âœ… å®šæœŸå¤‡ä»½etcd (æ¯å¤©)
    âœ… å¤‡ä»½K8sèµ„æºå®šä¹‰
    âœ… å¤‡ä»½æŒä¹…åŒ–æ•°æ®
    âœ… å®šæœŸæµ‹è¯•æ¢å¤æµç¨‹
    âœ… å¼‚åœ°å¤‡ä»½
  
  å®¹é‡è§„åˆ’:
    âœ… MasterèŠ‚ç‚¹: 4C8G (æœ€å°), 8C16G (æ¨è)
    âœ… WorkerèŠ‚ç‚¹: æ ¹æ®è´Ÿè½½è§„åˆ’
    âœ… etcd: SSDç£ç›˜, ä½å»¶è¿Ÿç½‘ç»œ
    âœ… é¢„ç•™30%èµ„æºç”¨äºæ•…éšœåˆ‡æ¢
  
  æ•…éšœæ¼”ç»ƒ:
    âœ… å®šæœŸMasterèŠ‚ç‚¹æ•…éšœæ¼”ç»ƒ
    âœ… etcdæ¢å¤æ¼”ç»ƒ
    âœ… ç½‘ç»œåˆ†åŒºæµ‹è¯•
    âœ… å­˜å‚¨æ•…éšœæµ‹è¯•
    âœ… æ–‡æ¡£åŒ–æ¼”ç»ƒç»“æœ

å¸¸è§é™·é˜±:
  âŒ MasterèŠ‚ç‚¹æ•°é‡ä¸ºå¶æ•°
  âŒ æ‰€æœ‰MasterèŠ‚ç‚¹åœ¨åŒä¸€æœºæ¶
  âŒ æœªé…ç½®èµ„æºrequests/limits
  âŒ å•å‰¯æœ¬éƒ¨ç½²å…³é”®åº”ç”¨
  âŒ æœªé…ç½®å¥åº·æ£€æŸ¥
  âŒ æœªå¤‡ä»½etcd
  âŒ ä½¿ç”¨HostPathå­˜å‚¨
  âŒ æœªé…ç½®PDB
  âŒ æœªå¯ç”¨ç›‘æ§å‘Šè­¦
  âŒ æœªå®šæœŸæµ‹è¯•æ¢å¤æµç¨‹

ç”Ÿäº§çº§æ£€æŸ¥æ¸…å•:
  éƒ¨ç½²å‰:
    âœ… ç¡¬ä»¶èµ„æºå……è¶³
    âœ… ç½‘ç»œè§„åˆ’åˆç†
    âœ… è´Ÿè½½å‡è¡¡é…ç½®æ­£ç¡®
    âœ… etcdå¤‡ä»½ç­–ç•¥å°±ç»ª
    âœ… ç›‘æ§å‘Šè­¦é…ç½®å®Œæˆ
    âœ… æ–‡æ¡£å®Œæ•´
  
  éƒ¨ç½²å:
    âœ… æ‰€æœ‰èŠ‚ç‚¹Ready
    âœ… æ‰€æœ‰ç³»ç»ŸPod Running
    âœ… API Serverå¯è®¿é—®
    âœ… DNSè§£ææ­£å¸¸
    âœ… ç½‘ç»œè¿é€šæ€§æ­£å¸¸
    âœ… å­˜å‚¨å¯ç”¨
  
  å®šæœŸæ£€æŸ¥:
    âœ… é›†ç¾¤ç»„ä»¶å¥åº·
    âœ… èµ„æºä½¿ç”¨ç‡
    âœ… è¯ä¹¦è¿‡æœŸæ—¶é—´
    âœ… å¤‡ä»½å®Œæ•´æ€§
    âœ… æ—¥å¿—å‘Šè­¦
    âœ… ç‰ˆæœ¬æ›´æ–°
```

---

## ç›¸å…³æ–‡æ¡£

- [VMwareé«˜å¯ç”¨é…ç½®](01_VMwareé«˜å¯ç”¨é…ç½®.md)
- [å¤‡ä»½æ¢å¤æ–¹æ¡ˆ](03_å¤‡ä»½æ¢å¤æ–¹æ¡ˆ.md)
- [å®¹ç¾æ¼”ç»ƒæµç¨‹](04_å®¹ç¾æ¼”ç»ƒæµç¨‹.md)
- [å­˜å‚¨å®¹ç¾ä¸å¤‡ä»½](../03_å­˜å‚¨æ¶æ„/07_å­˜å‚¨å®¹ç¾ä¸å¤‡ä»½.md)
- [ç½‘ç»œé«˜å¯ç”¨](../04_ç½‘ç»œæ¶æ„/03_è´Ÿè½½å‡è¡¡ä¸é«˜å¯ç”¨.md)

---

**æ›´æ–°æ—¶é—´**: 2025-10-19  
**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª
