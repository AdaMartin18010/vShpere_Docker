# Cephåˆ†å¸ƒå¼å­˜å‚¨

> **è¿”å›**: [å­˜å‚¨æ¶æ„ç›®å½•](README.md) | [è™šæ‹ŸåŒ–éƒ¨ç½²é¦–é¡µ](../README.md) | [éƒ¨ç½²æŒ‡å—é¦–é¡µ](../../00_ç´¢å¼•å¯¼èˆª/README.md)

---

## ğŸ“‹ ç›®å½•

- [Cephåˆ†å¸ƒå¼å­˜å‚¨](#cephåˆ†å¸ƒå¼å­˜å‚¨)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [Cephæ¦‚è¿°](#cephæ¦‚è¿°)
  - [Cephæ¶æ„ä¸ç»„ä»¶](#cephæ¶æ„ä¸ç»„ä»¶)
  - [Cephç¡¬ä»¶è¦æ±‚](#cephç¡¬ä»¶è¦æ±‚)
  - [Cephé›†ç¾¤éƒ¨ç½²](#cephé›†ç¾¤éƒ¨ç½²)
  - [å­˜å‚¨æ± ä¸æ•°æ®ç®¡ç†](#å­˜å‚¨æ± ä¸æ•°æ®ç®¡ç†)
  - [RBDå—å­˜å‚¨](#rbdå—å­˜å‚¨)
  - [CephFSæ–‡ä»¶ç³»ç»Ÿ](#cephfsæ–‡ä»¶ç³»ç»Ÿ)
  - [å¯¹è±¡å­˜å‚¨ç½‘å…³](#å¯¹è±¡å­˜å‚¨ç½‘å…³)
  - [Cephæ€§èƒ½ä¼˜åŒ–](#cephæ€§èƒ½ä¼˜åŒ–)
  - [ç›‘æ§ä¸è¿ç»´](#ç›‘æ§ä¸è¿ç»´)
  - [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
  - [ç›¸å…³æ–‡æ¡£](#ç›¸å…³æ–‡æ¡£)

---

## Cephæ¦‚è¿°

```yaml
Cephç®€ä»‹:
  å®šä¹‰:
    å¼€æºåˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿ
    ç»Ÿä¸€å­˜å‚¨å¹³å° (å—/æ–‡ä»¶/å¯¹è±¡)
    è½¯ä»¶å®šä¹‰å­˜å‚¨ (SDS)
    CRUSHç®—æ³•é©±åŠ¨
  
  æ ¸å¿ƒç‰¹æ€§:
    âœ… ç»Ÿä¸€å­˜å‚¨æ¥å£
    âœ… é«˜å¯ç”¨æ€§ (æ— å•ç‚¹æ•…éšœ)
    âœ… è‡ªåŠ¨æ•°æ®åˆ†å¸ƒä¸å¹³è¡¡
    âœ… è‡ªæˆ‘ä¿®å¤
    âœ… æ¨ªå‘æ‰©å±• (Scale-Out)
    âœ… å¼€æºå…è´¹ (Apache 2.0)
  
  ç‰ˆæœ¬å†å²:
    Pacific (16.x):
      å‘å¸ƒ: 2021å¹´
      çŠ¶æ€: ç¨³å®š LTS
      ç‰¹æ€§: cephadméƒ¨ç½², æ”¹è¿›ç›‘æ§
    
    Quincy (17.x):
      å‘å¸ƒ: 2022å¹´
      çŠ¶æ€: å½“å‰ LTS
      ç‰¹æ€§: æ€§èƒ½æå‡, RBDé•œåƒæ”¹è¿›
    
    Reef (18.x):
      å‘å¸ƒ: 2023å¹´
      çŠ¶æ€: æœ€æ–°ç¨³å®š
      ç‰¹æ€§: ç®€åŒ–ç®¡ç†, å¢å¼ºå®‰å…¨
  
  å­˜å‚¨æ¥å£:
    å—å­˜å‚¨ (RBD - RADOS Block Device):
      åè®®: Kernel RBD, librbd
      ç”¨é€”: è™šæ‹Ÿæœºç£ç›˜, Kubernetes PV
      æ€§èƒ½: ä¼˜ç§€ (ä½å»¶è¿Ÿ)
      å®¢æˆ·ç«¯: Linuxå†…æ ¸, QEMU/KVM, OpenStack
    
    æ–‡ä»¶å­˜å‚¨ (CephFS):
      åè®®: POSIXå…¼å®¹
      ç”¨é€”: å…±äº«æ–‡ä»¶ç³»ç»Ÿ
      æ€§èƒ½: è‰¯å¥½
      å®¢æˆ·ç«¯: Linuxå†…æ ¸, FUSE, Windows (Samba)
    
    å¯¹è±¡å­˜å‚¨ (RGW - RADOS Gateway):
      åè®®: S3, Swift
      ç”¨é€”: å¯¹è±¡å­˜å‚¨, äº‘å­˜å‚¨
      æ€§èƒ½: è‰¯å¥½
      å®¢æˆ·ç«¯: S3 APIå®¢æˆ·ç«¯

  é€‚ç”¨åœºæ™¯:
    âœ… äº‘å¹³å°å­˜å‚¨ (OpenStack, CloudStack)
    âœ… KubernetesæŒä¹…å·
    âœ… è™šæ‹ŸåŒ–å­˜å‚¨ (Proxmox, oVirt)
    âœ… å¤§æ•°æ®å­˜å‚¨
    âœ… å¤‡ä»½å½’æ¡£
    âœ… å¯¹è±¡å­˜å‚¨æœåŠ¡
  
  ä¸é€‚ç”¨åœºæ™¯:
    âš ï¸ æä½å»¶è¿Ÿ (<0.5ms)
    âš ï¸ å°å‹ç¯å¢ƒ (<3èŠ‚ç‚¹)
    âš ï¸ äº‹åŠ¡å‹æ•°æ®åº“ (OLTP)
    âš ï¸ èµ„æºå—é™ç¯å¢ƒ

  ä¼˜åŠ¿:
    âœ… å¼€æºå…è´¹
    âœ… ç»Ÿä¸€å­˜å‚¨å¹³å°
    âœ… æ— å•ç‚¹æ•…éšœ
    âœ… æ¨ªå‘æ‰©å±•
    âœ… è‡ªåŠ¨ä¿®å¤
    âœ… ç¤¾åŒºæ´»è·ƒ
  
  åŠ£åŠ¿:
    âŒ å­¦ä¹ æ›²çº¿é™¡å³­
    âŒ ç¡¬ä»¶èµ„æºæ¶ˆè€—é«˜
    âŒ å°é›†ç¾¤æ€§èƒ½ä¸€èˆ¬
    âŒ è¿ç»´å¤æ‚åº¦é«˜
```

---

## Cephæ¶æ„ä¸ç»„ä»¶

```yaml
Cephæ ¸å¿ƒç»„ä»¶:
  Monitor (MON):
    è§’è‰²: é›†ç¾¤çŠ¶æ€ç®¡ç†
    åŠŸèƒ½:
      - ç»´æŠ¤é›†ç¾¤æ˜ å°„ (cluster map)
      - æä¾›è®¤è¯æœåŠ¡
      - ç®¡ç†é…ç½®å˜æ›´
      - ç›‘æ§é›†ç¾¤å¥åº·
    
    æ•°é‡: 
      æœ€å°‘: 1 (æµ‹è¯•)
      æ¨è: 3 (ç”Ÿäº§)
      å¤§å‹: 5-7 (é«˜å¯ç”¨)
    
    é…ç½®:
      CPU: 2-4æ ¸å¿ƒ
      å†…å­˜: 2-4GB
      å­˜å‚¨: 50GB SSD (å­˜å‚¨é›†ç¾¤æ˜ å°„)
      ç½‘ç»œ: 1GbE+
    
    æ³¨æ„: Monitorä¸å­˜å‚¨ç”¨æˆ·æ•°æ®
  
  OSD (Object Storage Daemon):
    è§’è‰²: æ•°æ®å­˜å‚¨ä¸ç®¡ç†
    åŠŸèƒ½:
      - å­˜å‚¨å¯¹è±¡æ•°æ®
      - æ•°æ®å¤åˆ¶
      - æ•°æ®ä¿®å¤
      - æ•°æ®å¹³è¡¡
      - æä¾›å®¹é‡
    
    æ•°é‡:
      æœ€å°‘: 3 OSD
      æ¨è: æ¯èŠ‚ç‚¹3-10 OSD
      å¤§å‹: æ•°ç™¾åˆ°æ•°åƒ OSD
    
    é…ç½®:
      1 OSD = 1 ç£ç›˜ (æ¨è)
      CPU: 0.5-2æ ¸å¿ƒ/OSD
      å†…å­˜: 4-8GB/OSD (å–å†³äºæ•°æ®é‡)
      å­˜å‚¨: HDDæˆ–SSD
      ç½‘ç»œ: 10GbE+ (ä¸“ç”¨åç«¯ç½‘ç»œ)
    
    ç±»å‹:
      BlueStore (é»˜è®¤):
        ç›´æ¥ç®¡ç†è£¸è®¾å¤‡
        æ€§èƒ½ä¼˜ç§€
        æ¨è: Ceph 12.2+
      
      FileStore (å·²å¼ƒç”¨):
        åŸºäºæ–‡ä»¶ç³»ç»Ÿ (XFS)
        Ceph 14+å·²ç§»é™¤
  
  Manager (MGR):
    è§’è‰²: ç®¡ç†ä¸ç›‘æ§
    åŠŸèƒ½:
      - Dashboard Webç•Œé¢
      - RESTful API
      - ç›‘æ§æŒ‡æ ‡æ”¶é›†
      - æ’ä»¶ç³»ç»Ÿ (Prometheus, Zabbix)
    
    æ•°é‡:
      æœ€å°‘: 1
      æ¨è: 2 (ä¸»å¤‡)
    
    é…ç½®:
      CPU: 2-4æ ¸å¿ƒ
      å†…å­˜: 2-4GB
  
  MDS (Metadata Server):
    è§’è‰²: CephFSå…ƒæ•°æ®ç®¡ç†
    åŠŸèƒ½:
      - ç®¡ç†æ–‡ä»¶ç³»ç»Ÿå…ƒæ•°æ®
      - æ–‡ä»¶/ç›®å½•ç»“æ„
      - æƒé™ç®¡ç†
    
    æ•°é‡:
      CephFSéœ€è¦: è‡³å°‘1ä¸ª
      é«˜å¯ç”¨: 2+ (ä¸»å¤‡)
    
    é…ç½®:
      CPU: 4-8æ ¸å¿ƒ
      å†…å­˜: 2-32GB (å–å†³äºæ–‡ä»¶æ•°é‡)
    
    æ³¨æ„: ä»…CephFSéœ€è¦MDS

  RGW (RADOS Gateway):
    è§’è‰²: å¯¹è±¡å­˜å‚¨æ¥å£
    åŠŸèƒ½:
      - S3 API
      - Swift API
      - ç”¨æˆ·ç®¡ç†
      - å¤šç§Ÿæˆ·
    
    æ•°é‡:
      å¯é€‰: æŒ‰éœ€éƒ¨ç½²
      é«˜å¯ç”¨: 2+ (è´Ÿè½½å‡è¡¡)
    
    é…ç½®:
      CPU: 2-4æ ¸å¿ƒ
      å†…å­˜: 4-8GB

Cephæ•°æ®åˆ†å¸ƒ:
  CRUSHç®—æ³•:
    å®šä¹‰: Controlled Replication Under Scalable Hashing
    ä½œç”¨: ä¼ªéšæœºæ•°æ®åˆ†å¸ƒ
    ç‰¹ç‚¹:
      - ç¡®å®šæ€§ (ç»™å®šè¾“å…¥ï¼Œè¾“å‡ºå”¯ä¸€)
      - å»ä¸­å¿ƒåŒ– (å®¢æˆ·ç«¯ç›´æ¥è®¡ç®—)
      - è‡ªåŠ¨å¹³è¡¡
      - æ”¯æŒå¤šå±‚æ¬¡æ‹“æ‰‘
  
  CRUSH Map:
    å®šä¹‰: é›†ç¾¤ç‰©ç†æ‹“æ‰‘
    å±‚æ¬¡: root â†’ datacenter â†’ rack â†’ host â†’ osd
    è§„åˆ™: å®šä¹‰æ•°æ®æ”¾ç½®ç­–ç•¥
  
  æ•°æ®æµ:
    1. å®¢æˆ·ç«¯å†™å…¥æ•°æ®åˆ°Pool
    2. Poolæ˜ å°„åˆ°PG (Placement Group)
    3. CRUSHç®—æ³•è®¡ç®—PG â†’ OSDæ˜ å°„
    4. æ•°æ®å†™å…¥åˆ°ä¸»OSD
    5. ä¸»OSDå¤åˆ¶åˆ°å‰¯æœ¬OSD
    6. å…¨éƒ¨OSDç¡®è®¤åè¿”å›æˆåŠŸ

  å­˜å‚¨æ±  (Pool):
    å®šä¹‰: é€»è¾‘å­˜å‚¨åˆ†åŒº
    ç±»å‹:
      å‰¯æœ¬æ±  (Replicated):
        æ•°æ®å®Œæ•´å¤åˆ¶
        å‰¯æœ¬æ•°: 2-3
        ç©ºé—´æ•ˆç‡: 33-50%
      
      çº åˆ ç æ±  (Erasure Coded):
        æ•°æ®åˆ†ç‰‡+æ ¡éªŒ
        é…ç½®: k+m (å¦‚4+2)
        ç©ºé—´æ•ˆç‡: 67-80%
        æ€§èƒ½: å†™æ€§èƒ½è¾ƒä½
  
  PG (Placement Group):
    å®šä¹‰: å¯¹è±¡åˆ°OSDçš„ä¸­é—´å±‚
    ä½œç”¨: ç®€åŒ–æ•°æ®ç®¡ç†
    æ•°é‡è®¡ç®—:
      PGæ•° = (OSDæ€»æ•° Ã— 100) / å‰¯æœ¬æ•°
      å‘ä¸Šå–2çš„å¹‚æ¬¡
    
    ç¤ºä¾‹:
      30 OSD, 3å‰¯æœ¬: (30 Ã— 100) / 3 = 1000 â†’ 1024 PG
```

---

## Cephç¡¬ä»¶è¦æ±‚

```yaml
æœ€å°é›†ç¾¤é…ç½® (æµ‹è¯•):
  èŠ‚ç‚¹æ•°: 3
  æ¯èŠ‚ç‚¹:
    CPU: 4æ ¸å¿ƒ
    å†…å­˜: 8GB
    ç³»ç»Ÿç›˜: 50GB SSD
    OSDç›˜: 2x 1TB HDD
    ç½‘ç»œ: 1GbE
  
  é›†ç¾¤æ€»è®¡:
    12æ ¸å¿ƒ, 24GBå†…å­˜, 6 OSD
    å®¹é‡: 6TBåŸå§‹ (2TBå¯ç”¨, 3å‰¯æœ¬)

ç”Ÿäº§é›†ç¾¤é…ç½® (å°å‹):
  èŠ‚ç‚¹æ•°: 3-5
  æ¯èŠ‚ç‚¹:
    CPU: 16æ ¸å¿ƒ
    å†…å­˜: 64GB
    ç³»ç»Ÿç›˜: 240GB SSD (RAID1)
    OSDç›˜: 10x 4TB HDD
    æ—¥å¿—/DBç›˜: 1x 800GB SSD (å…±äº«)
    ç½‘ç»œ: 2x 10GbE (å‰ç«¯+åç«¯)
  
  é›†ç¾¤æ€»è®¡ (5èŠ‚ç‚¹):
    80æ ¸å¿ƒ, 320GBå†…å­˜, 50 OSD
    å®¹é‡: 200TBåŸå§‹ (66TBå¯ç”¨, 3å‰¯æœ¬)

ç”Ÿäº§é›†ç¾¤é…ç½® (ä¸­å‹å…¨é—ªå­˜):
  èŠ‚ç‚¹æ•°: 5-7
  æ¯èŠ‚ç‚¹:
    CPU: 32æ ¸å¿ƒ
    å†…å­˜: 128GB
    ç³»ç»Ÿç›˜: 480GB SSD (RAID1)
    OSDç›˜: 12x 3.84TB NVMe SSD
    ç½‘ç»œ: 2x 25GbE (å‰ç«¯+åç«¯)
  
  é›†ç¾¤æ€»è®¡ (7èŠ‚ç‚¹):
    224æ ¸å¿ƒ, 896GBå†…å­˜, 84 OSD
    å®¹é‡: 323TBåŸå§‹ (107TBå¯ç”¨, 3å‰¯æœ¬)
    æ€§èƒ½: IOPS >500K

ç¡¬ä»¶é€‰å‹å»ºè®®:
  CPU:
    æ¨è: Intel Xeonæˆ–AMD EPYC
    æ ¸å¿ƒæ•°: 1-2æ ¸å¿ƒ/OSD + åŸºç¡€å¼€é”€
    ç¤ºä¾‹: 10 OSD â†’ 16æ ¸å¿ƒ
  
  å†…å­˜:
    å…¬å¼: 4-8GB/OSD (BlueStore)
    åŸºç¡€: MON/MGR 4GB
    ç¤ºä¾‹:
      10 OSD â†’ 64GB (4GBÃ—10 + 24GBç³»ç»Ÿ)
      MDS (CephFS) â†’ é¢å¤–8-32GB
  
  å­˜å‚¨:
    OSDç£ç›˜:
      HDD:
        å®¹é‡: 4TB-12TB
        è½¬é€Ÿ: 7.2K (ç»æµ) æˆ– 10K (æ€§èƒ½)
        æ¥å£: SATA/SAS
        é€‚ç”¨: å¤§å®¹é‡åœºæ™¯
      
      SSD:
        å®¹é‡: 1.92TB-7.68TB
        æ¥å£: SATA/SAS/NVMe
        è€ä¹…æ€§: 1+ DWPD
        é€‚ç”¨: é«˜æ€§èƒ½åœºæ™¯
      
      NVMe:
        å®¹é‡: 1.92TB-15.36TB
        æ¥å£: PCIe 3.0/4.0
        è€ä¹…æ€§: 3+ DWPD
        é€‚ç”¨: æé«˜æ€§èƒ½
    
    BlueStore DB/WALç›˜:
      ç±»å‹: SSDæˆ–NVMe
      å¤§å°:
        DB: OSDå®¹é‡çš„4% (HDDåç«¯)
        WAL: 512MB-2GB
      å…±äº«: 1ä¸ªSSDå¯æœåŠ¡4-6ä¸ªHDD OSD
    
    ç³»ç»Ÿç›˜:
      å®¹é‡: 240GB+ SSD
      RAID: RAID1 (å¯é€‰)
  
  ç½‘ç»œ:
    å‰ç«¯ç½‘ç»œ (Public Network):
      ç”¨é€”: å®¢æˆ·ç«¯è®¿é—®
      å¸¦å®½: 10GbE+
      VLAN: ç‹¬ç«‹VLAN
    
    åç«¯ç½‘ç»œ (Cluster Network):
      ç”¨é€”: OSDé—´å¤åˆ¶
      å¸¦å®½: 10/25GbE
      VLAN: ç‹¬ç«‹VLAN (ä¸å‰ç«¯éš”ç¦»)
      æ¨è: 2x NIC (å†—ä½™)
    
    æ‹“æ‰‘:
      å°å‹: å•10GbE (å‰åç«¯å…±äº«)
      ä¸­å‹: å‰ç«¯10GbE + åç«¯10GbE
      å¤§å‹: å‰ç«¯25GbE + åç«¯25GbE
  
  æ§åˆ¶å™¨:
    æ¨è: HBAç›´é€šæ¨¡å¼
    å‹å·: LSI 9300/9400ç³»åˆ—
    é¿å…: RAIDæ§åˆ¶å™¨ (é™¤éJBOD)

å­˜å‚¨è§„åˆ’:
  å‰¯æœ¬æ±  (3å‰¯æœ¬):
    åŸå§‹å®¹é‡ â†’ å¯ç”¨å®¹é‡ â‰ˆ åŸå§‹ Ã— 0.33 Ã— 0.7
    # 0.33=1/3, 0.7=é¢„ç•™30%
  
  çº åˆ ç æ±  (4+2):
    åŸå§‹å®¹é‡ â†’ å¯ç”¨å®¹é‡ â‰ˆ åŸå§‹ Ã— 0.67 Ã— 0.7
  
  ç¤ºä¾‹:
    60 OSD Ã— 4TB HDD, 3å‰¯æœ¬:
      åŸå§‹: 240TB
      å¯ç”¨: 240TB Ã— 0.33 Ã— 0.7 â‰ˆ 55TB
```

---

## Cephé›†ç¾¤éƒ¨ç½²

```yaml
éƒ¨ç½²æ–¹æ³•å¯¹æ¯”:
  cephadm (æ¨è):
    ç‰ˆæœ¬: Ceph 15.2+ (Octopus)
    å·¥å…·: å®¹å™¨åŒ–éƒ¨ç½²
    ä¼˜ç‚¹:
      âœ… å®˜æ–¹æ¨è
      âœ… ç®€åŒ–éƒ¨ç½²
      âœ… å®¹å™¨éš”ç¦»
      âœ… è‡ªåŠ¨åŒ–ç®¡ç†
    
    é€‚ç”¨: æ–°é›†ç¾¤
  
  ceph-ansible:
    å·¥å…·: Ansible playbook
    ä¼˜ç‚¹:
      âœ… çµæ´»
      âœ… é€‚åˆå¤§è§„æ¨¡
      âœ… å¯å®šåˆ¶
    
    ç¼ºç‚¹:
      âš ï¸ éœ€è¦Ansibleç»éªŒ
      âš ï¸ ç»´æŠ¤å¤æ‚
  
  æ‰‹åŠ¨éƒ¨ç½²:
    é€‚ç”¨: å­¦ä¹ /å°å‹æµ‹è¯•
    ä¸æ¨è: ç”Ÿäº§ç¯å¢ƒ

éƒ¨ç½²å‰å‡†å¤‡:
  æ“ä½œç³»ç»Ÿ:
    æ¨è:
      âœ… Ubuntu 20.04/22.04 LTS
      âœ… Rocky Linux 8/9
      âœ… CentOS Stream 9
    
    å†…æ ¸: 5.4+
  
  æ—¶é—´åŒæ­¥:
    è¦æ±‚: æ‰€æœ‰èŠ‚ç‚¹æ—¶é—´ä¸€è‡´
    é…ç½®: NTPæˆ–Chrony
    éªŒè¯: timedatectl
  
  ä¸»æœºåä¸DNS:
    è¦æ±‚: è§£ææ­£ç¡®
    é…ç½®: /etc/hostsæˆ–DNS
    éªŒè¯: ping ä¸»æœºå
  
  é˜²ç«å¢™:
    é€‰æ‹©:
      å…³é—­é˜²ç«å¢™ (æµ‹è¯•)
      é…ç½®é˜²ç«å¢™è§„åˆ™ (ç”Ÿäº§)
    
    ç«¯å£:
      MON: 3300, 6789
      MGR: 6800-7300, 8443 (Dashboard)
      OSD: 6800-7300
      MDS: 6800
      RGW: 7480, 8080
  
  SSHå…å¯†:
    ç®¡ç†èŠ‚ç‚¹ â†’ æ‰€æœ‰èŠ‚ç‚¹
    é…ç½®: ssh-keygen + ssh-copy-id

ä½¿ç”¨cephadméƒ¨ç½²:
  ç¯å¢ƒ: 3èŠ‚ç‚¹é›†ç¾¤
    node1: 192.168.30.11 (MON, MGR, OSD)
    node2: 192.168.30.12 (MON, OSD)
    node3: 192.168.30.13 (MON, OSD)
```

```bash
#!/bin/bash
# Cephé›†ç¾¤éƒ¨ç½²è„šæœ¬ - cephadmæ–¹å¼
# åœ¨node1æ‰§è¡Œ

set -e

echo "========================================="
echo "  Cephé›†ç¾¤éƒ¨ç½² (cephadm)"
echo "========================================="
echo ""

# 1. å®‰è£…ä¾èµ–
echo "æ­¥éª¤1: å®‰è£…ä¾èµ–..."
if command -v apt &> /dev/null; then
    apt update
    apt install -y python3 python3-pip lvm2 podman
elif command -v dnf &> /dev/null; then
    dnf install -y python3 python3-pip lvm2 podman
fi

# 2. ä¸‹è½½cephadm
echo ""
echo "æ­¥éª¤2: ä¸‹è½½cephadm..."
curl --silent --remote-name --location https://github.com/ceph/ceph/raw/quincy/src/cephadm/cephadm
chmod +x cephadm

# 3. æ·»åŠ Cephä»“åº“
echo ""
echo "æ­¥éª¤3: æ·»åŠ Cephä»“åº“..."
./cephadm add-repo --release quincy
./cephadm install

# 4. Bootstrapå¼•å¯¼é›†ç¾¤
echo ""
echo "æ­¥éª¤4: Bootstrapå¼•å¯¼é›†ç¾¤..."
cephadm bootstrap \
  --mon-ip 192.168.30.11 \
  --cluster-network 192.168.30.0/24 \
  --initial-dashboard-user admin \
  --initial-dashboard-password 'AdminP@ssw0rd!' \
  --allow-fqdn-hostname \
  --skip-monitoring-stack

# è¾“å‡ºä¿¡æ¯
echo ""
echo "========================================="
echo "  Bootstrapå®Œæˆ"
echo "========================================="
echo ""
echo "Dashboard URL: https://192.168.30.11:8443"
echo "Username: admin"
echo "Password: AdminP@ssw0rd!"
echo ""
echo "æŸ¥çœ‹é›†ç¾¤çŠ¶æ€:"
echo "  ceph -s"
echo ""

# 5. æ·»åŠ Ceph CLIåˆ«å
echo "æ­¥éª¤5: é…ç½®CLI..."
cephadm shell -- ceph -v
alias ceph='cephadm shell -- ceph'

# 6. å®‰è£…ceph-common (å¯é€‰)
echo ""
echo "æ­¥éª¤6: å®‰è£…ceph-common..."
cephadm install ceph-common

# 7. æ·»åŠ å…¶ä»–ä¸»æœº
echo ""
echo "æ­¥éª¤7: æ·»åŠ å…¶ä»–ä¸»æœº..."

# å¤åˆ¶SSHå¯†é’¥åˆ°å…¶ä»–èŠ‚ç‚¹
ssh-copy-id -f -i /etc/ceph/ceph.pub root@192.168.30.12
ssh-copy-id -f -i /etc/ceph/ceph.pub root@192.168.30.13

# æ·»åŠ ä¸»æœº
ceph orch host add node2 192.168.30.12
ceph orch host add node3 192.168.30.13

# 8. éƒ¨ç½²MONå®ˆæŠ¤è¿›ç¨‹
echo ""
echo "æ­¥éª¤8: éƒ¨ç½²Monitor..."
ceph orch apply mon 3  # éƒ¨ç½²3ä¸ªMON

# 9. éƒ¨ç½²MGRå®ˆæŠ¤è¿›ç¨‹
echo ""
echo "æ­¥éª¤9: éƒ¨ç½²Manager..."
ceph orch apply mgr 2  # éƒ¨ç½²2ä¸ªMGR

# 10. éƒ¨ç½²OSD
echo ""
echo "æ­¥éª¤10: éƒ¨ç½²OSD..."

# æ–¹æ³•1: è‡ªåŠ¨å‘ç°å¹¶éƒ¨ç½²æ‰€æœ‰å¯ç”¨ç£ç›˜
ceph orch apply osd --all-available-devices

# æ–¹æ³•2: æ‰‹åŠ¨æŒ‡å®šç£ç›˜ (æ¨è)
# ceph orch daemon add osd node1:/dev/sdb
# ceph orch daemon add osd node1:/dev/sdc
# ceph orch daemon add osd node2:/dev/sdb
# ceph orch daemon add osd node2:/dev/sdc
# ceph orch daemon add osd node3:/dev/sdb
# ceph orch daemon add osd node3:/dev/sdc

echo ""
echo "ç­‰å¾…OSDåˆ›å»ºå®Œæˆ..."
sleep 30

# 11. éªŒè¯é›†ç¾¤
echo ""
echo "========================================="
echo "  é›†ç¾¤éªŒè¯"
echo "========================================="
echo ""

echo "é›†ç¾¤çŠ¶æ€:"
ceph -s

echo ""
echo "é›†ç¾¤æ‹“æ‰‘:"
ceph orch ps

echo ""
echo "OSDåˆ—è¡¨:"
ceph osd tree

echo ""
echo "å­˜å‚¨æ± :"
ceph osd lspools

echo ""
echo "========================================="
echo "  éƒ¨ç½²å®Œæˆ"
echo "========================================="
echo ""
echo "ä¸‹ä¸€æ­¥:"
echo "  1. è®¿é—®Dashboard: https://192.168.30.11:8443"
echo "  2. åˆ›å»ºå­˜å‚¨æ± : ceph osd pool create <pool-name> <pg-num>"
echo "  3. é…ç½®å®¢æˆ·ç«¯è®¿é—®"
```

**æ‰‹åŠ¨é…ç½®OSDï¼ˆå¸¦å•ç‹¬DB/WALç›˜ï¼‰**:

```bash
# åœºæ™¯: 10ä¸ªHDD OSD + 1ä¸ªSSDä½œä¸ºDB/WALç›˜

# 1. åˆ›å»ºé€»è¾‘å· (åœ¨SSDä¸Š)
SSD_DEV="/dev/nvme0n1"
for i in {1..10}; do
    lvcreate -L 40G -n db$i ceph-db $SSD_DEV
    lvcreate -L 2G -n wal$i ceph-wal $SSD_DEV
done

# 2. åˆ›å»ºOSD
for i in {1..10}; do
    HDD_DEV="/dev/sd${i}"
    ceph-volume lvm create \
      --data $HDD_DEV \
      --block.db ceph-db/db$i \
      --block.wal ceph-wal/wal$i
done
```

---

## å­˜å‚¨æ± ä¸æ•°æ®ç®¡ç†

```yaml
å­˜å‚¨æ± ç®¡ç†:
  åˆ›å»ºå‰¯æœ¬æ± :
    å‘½ä»¤:
      ceph osd pool create <pool-name> <pg-num> <pgp-num> replicated
    
    ç¤ºä¾‹:
      # åˆ›å»ºRBDæ± , 3å‰¯æœ¬, 128 PG
      ceph osd pool create rbd 128 128 replicated
      ceph osd pool set rbd size 3
      ceph osd pool set rbd min_size 2
      
      # åˆå§‹åŒ–RBDæ± 
      rbd pool init rbd
  
  åˆ›å»ºçº åˆ ç æ± :
    å‘½ä»¤:
      # å…ˆåˆ›å»ºçº åˆ ç profile
      ceph osd erasure-code-profile set <profile-name> k=4 m=2
      
      # åˆ›å»ºæ± 
      ceph osd pool create <pool-name> <pg-num> <pgp-num> erasure <profile-name>
    
    ç¤ºä¾‹:
      # 4+2çº åˆ ç æ± 
      ceph osd erasure-code-profile set ec42 k=4 m=2
      ceph osd pool create ec-pool 128 128 erasure ec42
  
  å¸¸ç”¨æ± é…ç½®:
    å‰¯æœ¬æ•°:
      ceph osd pool set <pool> size 3
      ceph osd pool set <pool> min_size 2
    
    PGæ•°è°ƒæ•´:
      ceph osd pool set <pool> pg_num 256
      ceph osd pool set <pool> pgp_num 256
    
    åº”ç”¨ç±»å‹:
      ceph osd pool application enable <pool> rbd
      # ç±»å‹: rbd, cephfs, rgw
  
  åˆ é™¤æ± :
    # éœ€è¦å…ˆå…è®¸åˆ é™¤
    ceph tell mon.* injectargs '--mon-allow-pool-delete=true'
    ceph osd pool delete <pool> <pool> --yes-i-really-really-mean-it

PGæ•°é‡è®¡ç®—:
  å…¬å¼:
    PGæ€»æ•° = (OSDæ€»æ•° Ã— 100) / æœ€å¤§å‰¯æœ¬æ•°
    æ¯ä¸ªæ± PGæ•° = PGæ€»æ•° / æ± æ•°é‡
    ç»“æœå‘ä¸Šå–2çš„å¹‚æ¬¡
  
  ç¤ºä¾‹:
    30 OSD, 3å‰¯æœ¬, 3ä¸ªæ± :
      PGæ€»æ•° = (30 Ã— 100) / 3 = 1000
      æ¯æ± PGæ•° = 1000 / 3 â‰ˆ 333 â†’ 512 (2çš„å¹‚æ¬¡)
  
  å·¥å…·:
    åœ¨çº¿è®¡ç®—å™¨:
      https://ceph.io/pgcalc/

å¿«ç…§ç®¡ç†:
  æ± å¿«ç…§:
    åˆ›å»º: ceph osd pool mksnap <pool> <snap-name>
    åˆ é™¤: ceph osd pool rmsnap <pool> <snap-name>
    åˆ—è¡¨: rados -p <pool> lssnap
  
  RBDå¿«ç…§:
    åˆ›å»º: rbd snap create <pool>/<image>@<snap>
    ä¿æŠ¤: rbd snap protect <pool>/<image>@<snap>
    å…‹éš†: rbd clone <pool>/<image>@<snap> <pool>/<new-image>
    åˆ é™¤: rbd snap rm <pool>/<image>@<snap>

é…é¢ç®¡ç†:
  è®¾ç½®æ± é…é¢:
    # æœ€å¤§å¯¹è±¡æ•°
    ceph osd pool set-quota <pool> max_objects 10000
    
    # æœ€å¤§å®¹é‡
    ceph osd pool set-quota <pool> max_bytes $((100 * 1024**3))  # 100GB
  
  æŸ¥çœ‹é…é¢:
    ceph osd pool get-quota <pool>
```

---

## RBDå—å­˜å‚¨

```yaml
RBDæ¦‚è¿°:
  å®šä¹‰: RADOS Block Device
  åŠŸèƒ½: å—è®¾å¤‡æ¥å£
  ç‰¹ç‚¹:
    âœ… ç²¾ç®€ç½®å¤‡
    âœ… å¿«ç…§
    âœ… å…‹éš†
    âœ… é•œåƒ (ç¾å¤‡)
    âœ… åŠ å¯†
  
  å®¢æˆ·ç«¯:
    Kernel RBD: Linuxå†…æ ¸æ¨¡å—
    librbd: ç”¨æˆ·ç©ºé—´åº“ (QEMU/KVM)
```

```bash
#!/bin/bash
# RBDå—å­˜å‚¨é…ç½®è„šæœ¬

echo "========================================="
echo "  RBDå—å­˜å‚¨é…ç½®"
echo "========================================="
echo ""

# 1. åˆ›å»ºRBDå­˜å‚¨æ± 
echo "æ­¥éª¤1: åˆ›å»ºRBDæ± ..."
ceph osd pool create rbd 128 128 replicated
ceph osd pool set rbd size 3
ceph osd pool set rbd min_size 2
ceph osd pool application enable rbd rbd
rbd pool init rbd

# 2. åˆ›å»ºRBDé•œåƒ
echo ""
echo "æ­¥éª¤2: åˆ›å»ºRBDé•œåƒ..."

# åˆ›å»º10GBé•œåƒ
rbd create --size 10240 rbd/disk01
rbd create --size 10240 rbd/disk02

# å¯ç”¨ç‰¹æ€§
rbd feature enable rbd/disk01 object-map fast-diff
rbd feature enable rbd/disk02 object-map fast-diff

# æŸ¥çœ‹é•œåƒ
echo ""
echo "RBDé•œåƒåˆ—è¡¨:"
rbd ls rbd
rbd info rbd/disk01

# 3. æ˜ å°„RBDåˆ°æœ¬åœ° (Linuxå®¢æˆ·ç«¯)
echo ""
echo "æ­¥éª¤3: æ˜ å°„RBDè®¾å¤‡..."

# æ˜ å°„
rbd map rbd/disk01

# æŸ¥çœ‹æ˜ å°„
rbd showmapped

# æ ¼å¼åŒ–å¹¶æŒ‚è½½
DEV=$(rbd showmapped | grep disk01 | awk '{print $5}')
mkfs.ext4 $DEV
mkdir -p /mnt/rbd-disk01
mount $DEV /mnt/rbd-disk01

echo ""
echo "RBDè®¾å¤‡å·²æŒ‚è½½åˆ°: /mnt/rbd-disk01"

# 4. å¿«ç…§ç®¡ç†
echo ""
echo "æ­¥éª¤4: å¿«ç…§ç®¡ç†..."

# åˆ›å»ºå¿«ç…§
rbd snap create rbd/disk01@snap1

# ä¿æŠ¤å¿«ç…§ (ç”¨äºå…‹éš†)
rbd snap protect rbd/disk01@snap1

# ä»å¿«ç…§å…‹éš†
rbd clone rbd/disk01@snap1 rbd/disk01-clone

# åˆ—å‡ºå¿«ç…§
rbd snap ls rbd/disk01

# 5. æ€§èƒ½æµ‹è¯•
echo ""
echo "æ­¥éª¤5: æ€§èƒ½æµ‹è¯•..."
rbd bench --io-type write rbd/disk01 --io-size 4096 --io-threads 16 --io-total 1G

echo ""
echo "========================================="
echo "  é…ç½®å®Œæˆ"
echo "========================================="
```

**Kubernetesé›†æˆï¼ˆRBDï¼‰**:

```yaml
# StorageClassé…ç½®
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
  clusterID: <ceph-cluster-id>
  pool: kubernetes
  imageFeatures: layering
  csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
  csi.storage.k8s.io/provisioner-secret-namespace: default
  csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
  csi.storage.k8s.io/controller-expand-secret-namespace: default
  csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
  csi.storage.k8s.io/node-stage-secret-namespace: default
  csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
  - discard
```

```yaml
# PVCç¤ºä¾‹
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: rbd-sc
  resources:
    requests:
      storage: 10Gi
```

---

## CephFSæ–‡ä»¶ç³»ç»Ÿ

```yaml
CephFSæ¦‚è¿°:
  å®šä¹‰: Cephæ–‡ä»¶ç³»ç»Ÿ
  åè®®: POSIXå…¼å®¹
  æ¶æ„: å…ƒæ•°æ®(MDS) + æ•°æ®(OSD)
  ç‰¹ç‚¹:
    âœ… å…±äº«è®¿é—® (å¤šå®¢æˆ·ç«¯)
    âœ… ç›®å½•å¿«ç…§
    âœ… é…é¢ç®¡ç†
    âœ… é«˜å¯ç”¨MDS
```

```bash
#!/bin/bash
# CephFSéƒ¨ç½²è„šæœ¬

echo "========================================="
echo "  CephFSéƒ¨ç½²"
echo "========================================="
echo ""

# 1. åˆ›å»ºå­˜å‚¨æ± 
echo "æ­¥éª¤1: åˆ›å»ºCephFSå­˜å‚¨æ± ..."

# å…ƒæ•°æ®æ±  (å°, SSD)
ceph osd pool create cephfs_metadata 64 64 replicated
ceph osd pool set cephfs_metadata size 3

# æ•°æ®æ±  (å¤§)
ceph osd pool create cephfs_data 256 256 replicated
ceph osd pool set cephfs_data size 3

# 2. åˆ›å»ºæ–‡ä»¶ç³»ç»Ÿ
echo ""
echo "æ­¥éª¤2: åˆ›å»ºCephFS..."
ceph fs new cephfs cephfs_metadata cephfs_data

# 3. éƒ¨ç½²MDS
echo ""
echo "æ­¥éª¤3: éƒ¨ç½²MDS..."
ceph orch apply mds cephfs --placement="3"

# ç­‰å¾…MDSå¯åŠ¨
sleep 10

# æŸ¥çœ‹MDSçŠ¶æ€
ceph fs status cephfs

# 4. æŒ‚è½½CephFS (Kernel driver)
echo ""
echo "æ­¥éª¤4: æŒ‚è½½CephFS..."

# åˆ›å»ºæŒ‚è½½ç‚¹
mkdir -p /mnt/cephfs

# è·å–å¯†é’¥
KEY=$(ceph auth get-key client.admin)

# æŒ‚è½½
mount -t ceph 192.168.30.11:6789:/ /mnt/cephfs \
  -o name=admin,secret=$KEY

echo ""
echo "CephFSå·²æŒ‚è½½åˆ°: /mnt/cephfs"

# 5. æ°¸ä¹…æŒ‚è½½ (/etc/fstab)
cat >> /etc/fstab <<EOF
192.168.30.11:6789:/  /mnt/cephfs  ceph  name=admin,secretfile=/etc/ceph/admin.secret,noatime,_netdev  0  0
EOF

echo $KEY > /etc/ceph/admin.secret
chmod 600 /etc/ceph/admin.secret

# 6. é…ç½®é…é¢
echo ""
echo "æ­¥éª¤6: é…ç½®é…é¢..."

# åˆ›å»ºç›®å½•
mkdir -p /mnt/cephfs/projects/project1

# è®¾ç½®é…é¢
setfattr -n ceph.quota.max_bytes -v 107374182400 /mnt/cephfs/projects/project1  # 100GB
setfattr -n ceph.quota.max_files -v 1000000 /mnt/cephfs/projects/project1

# æŸ¥çœ‹é…é¢
getfattr -n ceph.quota.max_bytes /mnt/cephfs/projects/project1

echo ""
echo "========================================="
echo "  éƒ¨ç½²å®Œæˆ"
echo "========================================="
```

**Kubernetesé›†æˆï¼ˆCephFSï¼‰**:

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: cephfs-sc
provisioner: cephfs.csi.ceph.com
parameters:
  clusterID: <ceph-cluster-id>
  fsName: cephfs
  pool: cephfs_data
  csi.storage.k8s.io/provisioner-secret-name: csi-cephfs-secret
  csi.storage.k8s.io/provisioner-secret-namespace: default
  csi.storage.k8s.io/controller-expand-secret-name: csi-cephfs-secret
  csi.storage.k8s.io/controller-expand-secret-namespace: default
  csi.storage.k8s.io/node-stage-secret-name: csi-cephfs-secret
  csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
  - debug
```

---

## å¯¹è±¡å­˜å‚¨ç½‘å…³

```yaml
RGWæ¦‚è¿°:
  å®šä¹‰: RADOS Gateway
  åè®®: S3, Swift
  ç”¨é€”: å¯¹è±¡å­˜å‚¨API
  ç‰¹ç‚¹:
    âœ… S3å…¼å®¹
    âœ… å¤šç§Ÿæˆ·
    âœ… ç”¨æˆ·ç®¡ç†
    âœ… é…é¢
```

```bash
#!/bin/bash
# RGWéƒ¨ç½²è„šæœ¬

echo "========================================="
echo "  RADOS Gatewayéƒ¨ç½²"
echo "========================================="
echo ""

# 1. éƒ¨ç½²RGW
echo "æ­¥éª¤1: éƒ¨ç½²RGW..."
ceph orch apply rgw myrgw --placement="2 node1 node2" --port=8080

# ç­‰å¾…å¯åŠ¨
sleep 15

# 2. åˆ›å»ºç”¨æˆ·
echo ""
echo "æ­¥éª¤2: åˆ›å»ºS3ç”¨æˆ·..."
radosgw-admin user create \
  --uid=testuser \
  --display-name="Test User" \
  --email=test@example.com \
  --access-key=TESTKEY123 \
  --secret-key=TESTSECRET456

# 3. åˆ›å»ºå­˜å‚¨æ¡¶
echo ""
echo "æ­¥éª¤3: åˆ›å»ºå­˜å‚¨æ¡¶..."

# å®‰è£…s3cmd
pip3 install s3cmd

# é…ç½®s3cmd
cat > ~/.s3cfg <<EOF
[default]
access_key = TESTKEY123
secret_key = TESTSECRET456
host_base = 192.168.30.11:8080
host_bucket = 192.168.30.11:8080
use_https = False
EOF

# åˆ›å»ºbucket
s3cmd mb s3://mybucket

# ä¸Šä¼ æ–‡ä»¶
echo "Hello Ceph RGW" > test.txt
s3cmd put test.txt s3://mybucket/

# åˆ—å‡ºå¯¹è±¡
s3cmd ls s3://mybucket/

echo ""
echo "========================================="
echo "  RGWéƒ¨ç½²å®Œæˆ"
echo "========================================="
echo ""
echo "S3 Endpoint: http://192.168.30.11:8080"
echo "Access Key: TESTKEY123"
echo "Secret Key: TESTSECRET456"
```

---

## Cephæ€§èƒ½ä¼˜åŒ–

```yaml
OSDæ€§èƒ½ä¼˜åŒ–:
  BlueStoreé…ç½®:
    # ceph.confæˆ–è¦†ç›–
    [osd]
    osd_op_num_threads_per_shard = 2
    osd_op_num_shards = 8
    bluestore_cache_size = 4GB  # HDD
    bluestore_cache_size = 8GB  # SSD
  
  ç½‘ç»œä¼˜åŒ–:
    [global]
    ms_async_op_threads = 5
    ms_async_max_op_threads = 10
  
  æ—¥å¿—ä¼˜åŒ–:
    [osd]
    osd_max_write_size = 512
    osd_client_message_size_cap = 2147483648
    osd_deep_scrub_stride = 131072

å®¢æˆ·ç«¯ä¼˜åŒ–:
  RBDç¼“å­˜:
    [client]
    rbd_cache = true
    rbd_cache_size = 33554432  # 32MB
    rbd_cache_max_dirty = 25165824
    rbd_cache_target_dirty = 16777216
  
  å†…æ ¸RBD:
    echo 4096 > /sys/module/rbd/parameters/single_major

PGä¼˜åŒ–:
  è‡ªåŠ¨è°ƒæ•´:
    ceph osd pool set <pool> pg_autoscale_mode on
  
  æ‰‹åŠ¨è°ƒæ•´:
    ceph osd pool set <pool> pg_num 256
    ceph osd pool set <pool> pgp_num 256

SSDä¼˜åŒ–:
  # è°ƒåº¦å™¨
  echo none > /sys/block/sdb/queue/scheduler
  
  # å…³é—­æœºæ¢°ç›˜ä¼˜åŒ–
  echo 0 > /sys/block/sdb/queue/rotational
  
  # é˜Ÿåˆ—æ·±åº¦
  echo 1024 > /sys/block/sdb/queue/nr_requests

æ€§èƒ½æµ‹è¯•:
  RADOS Bench:
    # å†™æµ‹è¯•
    rados bench -p rbd 10 write --no-cleanup
    
    # é¡ºåºè¯»
    rados bench -p rbd 10 seq
    
    # éšæœºè¯»
    rados bench -p rbd 10 rand
  
  RBD Bench:
    rbd bench --io-type write rbd/image1 --io-size 4096 --io-threads 16 --io-total 10G
  
  FIO:
    fio --name=rbd-test --ioengine=rbd --pool=rbd --rbdname=fio-test --rw=randwrite --bs=4k --size=10G --runtime=60 --time_based
```

---

## ç›‘æ§ä¸è¿ç»´

```yaml
é›†ç¾¤ç›‘æ§:
  Dashboard:
    URL: https://<mon-ip>:8443
    åŠŸèƒ½:
      - é›†ç¾¤çŠ¶æ€
      - æ€§èƒ½å›¾è¡¨
      - OSDç®¡ç†
      - æ± ç®¡ç†
      - RBD/CephFSç®¡ç†
  
  Prometheus:
    å¯ç”¨:
      ceph mgr module enable prometheus
      ceph mgr module enable alerts
    
    Endpoint: http://<mgr-ip>:9283/metrics
  
  å‘½ä»¤è¡Œç›‘æ§:
    å®æ—¶çŠ¶æ€:
      ceph -w
    
    é›†ç¾¤çŠ¶æ€:
      ceph -s
      ceph health detail
    
    OSDçŠ¶æ€:
      ceph osd stat
      ceph osd tree
      ceph osd df
    
    æ€§èƒ½:
      ceph osd perf
      ceph osd pool stats

æ—¥å¿—ç®¡ç†:
  æŸ¥çœ‹æ—¥å¿—:
    # Cephadmå®¹å™¨æ—¥å¿—
    cephadm logs --name <daemon>
    
    # ç¤ºä¾‹
    cephadm logs --name osd.0
    cephadm logs --name mon.node1
  
  è°ƒæ•´æ—¥å¿—çº§åˆ«:
    ceph tell osd.* config set debug_osd 10/10
    ceph tell mon.* config set debug_mon 10/10

å®¹é‡ç®¡ç†:
  æ‰©å®¹:
    æ·»åŠ OSD:
      ceph orch daemon add osd node4:/dev/sdb
    
    æ·»åŠ èŠ‚ç‚¹:
      ceph orch host add node4 192.168.30.14
  
  ç¼©å®¹:
    ç§»é™¤OSD:
      ceph orch osd rm <osd-id>
      # ç­‰å¾…æ•°æ®è¿ç§»
      ceph osd purge <osd-id> --yes-i-really-mean-it
    
    ç§»é™¤èŠ‚ç‚¹:
      ceph orch host drain node4
      ceph orch host rm node4

å¤‡ä»½ç­–ç•¥:
  é…ç½®å¤‡ä»½:
    ceph config-key dump > ceph-config-backup.json
    ceph osd getcrushmap -o crushmap-backup
  
  æ•°æ®å¤‡ä»½:
    RBD: rbd export <pool>/<image> <file>
    CephFS: rsync/tarå¤‡ä»½
    RGW: s3cmd sync

å‡çº§:
  cephadmæ–¹å¼:
    # æ£€æŸ¥æ›´æ–°
    ceph orch upgrade check <version>
    
    # å¼€å§‹å‡çº§
    ceph orch upgrade start --image <image>
    
    # ç›‘æ§å‡çº§
    ceph orch upgrade status
```

---

## æ•…éšœæ’æŸ¥

```yaml
å¸¸è§é—®é¢˜:
  é—®é¢˜1: OSD Down
    ç°è±¡:
      ceph -sæ˜¾ç¤ºOSD down
    
    æ’æŸ¥:
      1. æ£€æŸ¥OSDè¿›ç¨‹
         systemctl status ceph-osd@<id>
      
      2. æŸ¥çœ‹OSDæ—¥å¿—
         cephadm logs --name osd.<id>
      
      3. æ£€æŸ¥ç£ç›˜å¥åº·
         smartctl -a /dev/sdX
      
      4. æ£€æŸ¥ç½‘ç»œ
         ping <osd-host>
    
    è§£å†³:
      - é‡å¯OSD: systemctl restart ceph-osd@<id>
      - æ›´æ¢æ•…éšœç£ç›˜
      - ä¿®å¤ç½‘ç»œ
  
  é—®é¢˜2: PG Inactive
    ç°è±¡:
      ceph -sæ˜¾ç¤ºPG not active+clean
    
    æ’æŸ¥:
      ceph pg dump | grep -v active+clean
      ceph pg <pg-id> query
    
    è§£å†³:
      - ç­‰å¾…PGæ¢å¤
      - æ£€æŸ¥OSDçŠ¶æ€
      - é‡å¯ç›¸å…³OSD
  
  é—®é¢˜3: MON Quorum Lost
    ç°è±¡:
      æ— æ³•è¿æ¥é›†ç¾¤
    
    æ’æŸ¥:
      ceph mon stat
      ceph quorum_status --format json-pretty
    
    è§£å†³:
      - æ¢å¤å¤šæ•°MON
      - ç´§æ€¥: æå–å¹¶é‡å»ºMON
  
  é—®é¢˜4: æ€§èƒ½å·®
    ç°è±¡:
      IOPSä½, å»¶è¿Ÿé«˜
    
    æ’æŸ¥:
      1. æ£€æŸ¥PGåˆ†å¸ƒ
         ceph pg dump_stuck
      
      2. æ£€æŸ¥OSDè´Ÿè½½
         ceph osd perf
      
      3. æ£€æŸ¥ç½‘ç»œ
         iperf3æµ‹è¯•
      
      4. æ£€æŸ¥ç£ç›˜
         iostat -x 1
    
    è§£å†³:
      - è°ƒæ•´PGæ•°é‡
      - å¹³è¡¡æ•°æ®åˆ†å¸ƒ
      - å‡çº§ç½‘ç»œ
      - æ›´æ¢æ…¢ç›˜
  
  é—®é¢˜5: å®¹é‡ä¸å¹³è¡¡
    ç°è±¡:
      éƒ¨åˆ†OSDä½¿ç”¨ç‡é«˜
    
    æ’æŸ¥:
      ceph osd df tree
    
    è§£å†³:
      # è°ƒæ•´æƒé‡
      ceph osd reweight <osd-id> 0.95
      
      # æˆ–ä½¿ç”¨è‡ªåŠ¨å¹³è¡¡
      ceph balancer on
      ceph balancer mode upmap

è¯Šæ–­å‘½ä»¤:
  é›†ç¾¤å¥åº·:
    ceph health detail
    ceph -s
  
  PGçŠ¶æ€:
    ceph pg stat
    ceph pg dump_stuck
    ceph pg <pg-id> query
  
  OSDè¯Šæ–­:
    ceph osd tree
    ceph osd df
    ceph tell osd.* version
  
  æ—¥å¿—:
    cephadm logs --name <daemon>
    journalctl -u ceph-osd@<id>

ç´§æ€¥æ¢å¤:
  OSDä¸¢å¤± (å°‘äºmin_size):
    # ä¸´æ—¶é™ä½min_size
    ceph osd pool set <pool> min_size 1
    
    # æ¢å¤æ•°æ®åæ”¹å›
    ceph osd pool set <pool> min_size 2
  
  MONæ•°æ®æŸå:
    # ä»å¥åº·MONæå–
    ceph-mon --extract-monmap <file>
    
    # é‡å»ºMON
    ceph-mon --mkfs -i <new-mon> --monmap <file>
```

---

## ç›¸å…³æ–‡æ¡£

- [å­˜å‚¨ç±»å‹ä¸é€‰å‹æ ‡å‡†](01_å­˜å‚¨ç±»å‹ä¸é€‰å‹æ ‡å‡†.md)
- [iSCSIé…ç½®ä¸ä¼˜åŒ–](02_iSCSIé…ç½®ä¸ä¼˜åŒ–.md)
- [NFSé…ç½®ä¸ä¼˜åŒ–](03_NFSé…ç½®ä¸ä¼˜åŒ–.md)
- [VMware vSANé…ç½®](04_VMware_vSANé…ç½®.md)
- [å­˜å‚¨æ€§èƒ½ä¼˜åŒ–](06_å­˜å‚¨æ€§èƒ½ä¼˜åŒ–.md)
- [å­˜å‚¨å®¹ç¾ä¸å¤‡ä»½](07_å­˜å‚¨å®¹ç¾ä¸å¤‡ä»½.md)

---

**æ›´æ–°æ—¶é—´**: 2025-10-19  
**æ–‡æ¡£ç‰ˆæœ¬**: v3.0  
**çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª
