# 存储性能优化综合指南

> **返回**: [存储架构目录](README.md) | [虚拟化部署首页](../README.md) | [部署指南首页](../../00_索引导航/README.md)

---

## 📋 目录

- [存储性能优化综合指南](#存储性能优化综合指南)
  - [📋 目录](#-目录)
  - [性能优化概述](#性能优化概述)
  - [硬件层性能优化](#硬件层性能优化)
  - [操作系统层优化](#操作系统层优化)
  - [虚拟化层优化](#虚拟化层优化)
  - [网络层优化](#网络层优化)
  - [存储协议优化](#存储协议优化)
  - [应用层优化](#应用层优化)
  - [性能测试与基准](#性能测试与基准)
  - [监控与诊断](#监控与诊断)
  - [故障排查](#故障排查)
  - [相关文档](#相关文档)

---

## 性能优化概述

```yaml
性能指标体系:
  IOPS (每秒IO操作数):
    定义: Input/Output Operations Per Second
    类型:
      随机读IOPS: 数据库查询
      随机写IOPS: 数据库写入
      顺序读IOPS: 大文件读取
      顺序写IOPS: 日志写入
    
    性能基准:
      HDD (7.2K RPM): 100-200 IOPS
      HDD (10K RPM): 150-300 IOPS
      HDD (15K RPM): 200-400 IOPS
      SATA SSD: 50K-100K IOPS
      NVMe SSD: 500K-1M+ IOPS
  
  延迟 (Latency):
    定义: IO请求响应时间
    单位: 毫秒 (ms) 或微秒 (μs)
    
    性能基准:
      HDD: 5-10ms
      SATA SSD: 0.1-0.5ms
      NVMe SSD: 0.01-0.1ms (10-100μs)
    
    影响因素:
      - 磁盘类型
      - 队列深度
      - 网络延迟 (SAN/NAS)
      - 控制器性能
  
  吞吐量 (Throughput):
    定义: 每秒传输数据量
    单位: MB/s 或 GB/s
    
    性能基准:
      HDD (单盘): 100-200 MB/s
      SATA SSD: 500-600 MB/s
      NVMe SSD: 3000-7000 MB/s
      10GbE网络: ~1.2 GB/s
      25GbE网络: ~3 GB/s
  
  队列深度 (Queue Depth):
    定义: 同时处理的IO请求数
    影响: 更高QD通常提升IOPS
    
    推荐值:
      HDD: 4-32
      SSD: 32-128
      NVMe: 128-256

性能优化层次:
  L1 - 硬件层:
    优先级: 最高
    投资: 最大
    效果: 最显著
    措施:
      ✅ 升级到SSD/NVMe
      ✅ 增加内存
      ✅ 升级网络到10/25GbE
      ✅ 使用RAID控制器
  
  L2 - 操作系统层:
    优先级: 高
    投资: 低
    效果: 显著
    措施:
      ✅ 文件系统选择与调优
      ✅ IO调度器优化
      ✅ 内核参数调优
      ✅ 关闭不必要服务
  
  L3 - 虚拟化层:
    优先级: 高
    投资: 低
    效果: 显著
    措施:
      ✅ 虚拟磁盘格式优化
      ✅ 虚拟控制器选择
      ✅ 存储策略配置
      ✅ 资源分配优化
  
  L4 - 网络层:
    优先级: 中
    投资: 中
    效果: 中等
    措施:
      ✅ Jumbo Frame
      ✅ 网络隔离
      ✅ 负载均衡
      ✅ QoS配置
  
  L5 - 协议层:
    优先级: 中
    投资: 低
    效果: 中等
    措施:
      ✅ iSCSI参数调优
      ✅ NFS挂载选项
      ✅ 多路径配置
  
  L6 - 应用层:
    优先级: 中
    投资: 低
    效果: 应用相关
    措施:
      ✅ 数据库调优
      ✅ 缓存策略
      ✅ 批量操作
      ✅ 异步IO

性能优化原则:
  1. 测量先于优化:
     使用工具建立性能基准
     识别瓶颈
  
  2. 从底层到上层:
     硬件 → 操作系统 → 应用
     底层优化效果最明显
  
  3. 渐进式优化:
     一次改变一个变量
     记录每次变化的效果
  
  4. 投资回报率:
     优先低成本高收益优化
     评估硬件升级成本
  
  5. 持续监控:
     建立监控体系
     定期性能评估
```

---

## 硬件层性能优化

```yaml
磁盘选型与配置:
  HDD优化:
    选型:
      转速: 10K/15K RPM优于7.2K
      接口: SAS优于SATA
      缓存: 128MB-256MB
    
    RAID配置:
      RAID10: 最佳性能+冗余
      RAID5: 读性能良好
      RAID6: 2个磁盘容错
      避免: RAID5写性能差
    
    优化建议:
      ✅ 短冲程 (Short Stroking)
        仅使用外圈磁道 (更快)
      ✅ 分散负载
        不同工作负载分不同磁盘组
      ✅ 预读取
        顺序读取场景
  
  SSD优化:
    选型:
      接口: NVMe > SAS > SATA
      NAND类型: SLC > MLC > TLC > QLC
      过度配置: 20-30%
      耐久性: 3+ DWPD (写密集)
    
    配置:
      RAID:
        RAID0: 最高性能 (无冗余)
        RAID10: 性能+冗余
        避免: RAID5/6 (写放大)
      
      Trim支持:
        定期Trim释放空间
        保持性能稳定
    
    优化建议:
      ✅ 预留空间
        保持20%+ 自由空间
      ✅ 磨损均衡
        监控写入量
      ✅ 队列深度
        设置64-128
  
  NVMe优化:
    选型:
      接口: PCIe 4.0 > 3.0
      通道: x4通道
      队列: 支持多队列
    
    配置:
      直连CPU:
        避免PCIe switch
        降低延迟
      
      中断绑定:
        CPU亲和性
        避免中断风暴
    
    优化建议:
      ✅ 使用最新驱动
      ✅ 启用多队列
      ✅ 调整队列深度

内存优化:
  容量规划:
    操作系统: 16GB基础
    虚拟化: 4-8GB/VM
    Ceph: 4-8GB/OSD
    vSAN: 32GB/TB容量
    数据库缓存: 应用需求
  
  性能配置:
    频率: DDR4-3200+ 或 DDR5
    通道: 多通道 (4/6/8通道)
    ECC: 生产环境推荐
    NUMA: 启用NUMA感知
  
  优化建议:
    ✅ 启用大页内存 (HugePages)
    ✅ 禁用交换 (Swap)
    ✅ 内存对齐
    ✅ NUMA绑定

CPU优化:
  选型:
    核心数: 更多核心 (横向扩展)
    频率: 高频率 (单线程性能)
    缓存: L3缓存越大越好
    
    推荐:
      Intel Xeon Gold/Platinum
      AMD EPYC 7003/9004系列
  
  配置:
    虚拟化: 启用VT-x/AMD-V, EPT/RVI
    超线程: 启用 (大多数场景)
    Turbo Boost: 启用
    C-States: 禁用 (低延迟) 或 启用 (节能)
  
  优化建议:
    ✅ CPU亲和性 (taskset)
    ✅ 中断绑定 (irqbalance)
    ✅ NUMA绑定

存储控制器:
  HBA模式 (推荐):
    类型: LSI 9300/9400系列
    功能: 直通，无RAID
    用途: Ceph, vSAN
    性能: 最佳
  
  RAID模式:
    类型: Dell PERC, HPE Smart Array
    缓存: 2-8GB, 电池保护
    写策略: Write-Back (性能)
    读策略: Read-Ahead (顺序)
  
  队列深度:
    HBA: 默认即可
    RAID: 增加到256-512
    
    配置示例 (Linux):
      echo 256 > /sys/block/sda/device/queue_depth

网络硬件:
  网卡:
    速率: 10GbE (最低), 25GbE (推荐)
    队列: 多队列支持
    卸载: TSO, LRO, RSS
    
    推荐型号:
      Intel X710 (10GbE)
      Mellanox ConnectX-5/6 (25/100GbE)
  
  交换机:
    缓冲: 深缓冲 (large buffer)
    延迟: 低延迟 (<5μs)
    Jumbo Frame: 支持
    
    推荐:
      Cisco Nexus
      Arista (低延迟)
      Mellanox Spectrum
```

---

## 操作系统层优化

```yaml
文件系统选择:
  XFS (推荐存储服务器):
    优势:
      ✅ 大文件性能优秀
      ✅ 并发性能好
      ✅ 日志型文件系统
      ✅ 在线扩展
    
    适用: Ceph, NFS服务器
    
    优化挂载选项:
      noatime,nodiratime: 不更新访问时间
      nobarrier: 禁用barrier (有UPS)
      inode64: 64位inode
      logbufs=8: 增加日志缓冲
      
      示例:
        mount -o noatime,nodiratime,nobarrier,inode64,logbufs=8 /dev/sdb1 /data
  
  ext4:
    优势:
      ✅ 稳定成熟
      ✅ 广泛支持
      ✅ 小文件性能好
    
    适用: 通用场景
    
    优化挂载选项:
      noatime,nodiratime
      data=writeback: 提升写性能 (安全性降低)
      commit=60: 延长提交间隔
      
      示例:
        mount -o noatime,data=writeback,commit=60 /dev/sdb1 /data
  
  Btrfs:
    优势:
      ✅ 快照
      ✅ 压缩
      ✅ COW
    
    劣势:
      ⚠️ 性能较XFS/ext4低
      ⚠️ RAID5/6不稳定
    
    适用: 备份场景
  
  ZFS:
    优势:
      ✅ 数据完整性
      ✅ 压缩
      ✅ 快照
      ✅ ARC缓存
    
    劣势:
      ⚠️ 内存消耗大
      ⚠️ 非Linux原生
    
    适用: FreeBSD/TrueNAS

IO调度器:
  None (推荐SSD/NVMe):
    特点: 无调度，直接下发
    适用: SSD, NVMe
    配置:
      echo none > /sys/block/nvme0n1/queue/scheduler
  
  mq-deadline (推荐HDD):
    特点: 多队列，deadline保证
    适用: HDD, 混合负载
    配置:
      echo mq-deadline > /sys/block/sda/queue/scheduler
  
  BFQ (预算公平队列):
    特点: 低延迟，公平
    适用: 桌面，交互式
    不推荐: 服务器
  
  Kyber:
    特点: 自适应
    适用: 通用

内核参数调优:
```

```bash
#!/bin/bash
# 存储性能内核参数优化脚本

cat >> /etc/sysctl.conf <<'EOF'

# ====================
# 存储性能优化
# ====================

# 虚拟内存
vm.dirty_ratio = 15                    # 脏页比例触发写回
vm.dirty_background_ratio = 5          # 后台写回比例
vm.dirty_expire_centisecs = 3000       # 脏页过期时间 (30秒)
vm.dirty_writeback_centisecs = 500     # 写回间隔 (5秒)
vm.swappiness = 10                     # 降低swap使用
vm.vfs_cache_pressure = 50             # inode/dentry缓存压力

# 块设备
vm.block_dump = 0                      # 禁用块dump (性能)

# 内存
vm.min_free_kbytes = 524288            # 最小空闲内存 (512MB)
vm.overcommit_memory = 1               # 允许内存过度分配
vm.overcommit_ratio = 50               # 过度分配比例

# 网络 (存储网络)
net.core.rmem_max = 134217728          # 最大接收缓冲 (128MB)
net.core.wmem_max = 134217728          # 最大发送缓冲
net.core.rmem_default = 33554432       # 默认接收缓冲 (32MB)
net.core.wmem_default = 33554432       # 默认发送缓冲
net.ipv4.tcp_rmem = 4096 87380 134217728
net.ipv4.tcp_wmem = 4096 65536 134217728
net.ipv4.tcp_mem = 134217728 134217728 134217728

# TCP优化
net.ipv4.tcp_window_scaling = 1        # 启用窗口扩展
net.ipv4.tcp_timestamps = 1            # 启用时间戳
net.ipv4.tcp_sack = 1                  # 启用SACK
net.ipv4.tcp_no_metrics_save = 1       # 不保存metrics
net.core.netdev_max_backlog = 10000    # 网络设备队列

# 文件系统
fs.file-max = 2097152                  # 最大文件句柄
fs.aio-max-nr = 1048576                # 异步IO

# HugePages (可选, 根据内存大小)
# vm.nr_hugepages = 1024               # 2GB (2MB×1024)

EOF

# 应用配置
sysctl -p

echo "内核参数优化完成"
```

```yaml
系统服务优化:
  禁用不必要服务:
    systemctl disable firewalld        # 或配置规则
    systemctl disable postfix
    systemctl disable bluetooth
    systemctl disable cups
  
  保留必要服务:
    ✅ sshd
    ✅ chronyd/ntpd
    ✅ rsyslog

磁盘预读优化:
  查看当前值:
    blockdev --getra /dev/sda
  
  设置预读 (KB):
    # HDD: 4096-8192 (4-8MB)
    blockdev --setra 4096 /dev/sda
    
    # SSD: 256-1024 (256KB-1MB)
    blockdev --setra 512 /dev/sdb
    
    # NVMe: 128-512 (128-512KB)
    blockdev --setra 256 /dev/nvme0n1
  
  永久配置:
    # /etc/udev/rules.d/60-readahead.rules
    ACTION=="add|change", KERNEL=="sd[a-z]", ATTR{queue/read_ahead_kb}="4096"
    ACTION=="add|change", KERNEL=="nvme[0-9]n[0-9]", ATTR{queue/read_ahead_kb}="256"

队列深度:
  查看:
    cat /sys/block/sda/device/queue_depth
  
  设置:
    echo 256 > /sys/block/sda/device/queue_depth
  
  推荐值:
    HDD: 32
    SSD: 128
    NVMe: 256

SSD优化:
  Trim支持:
    # 检查支持
    lsblk --discard
    
    # 手动Trim
    fstrim -v /
    
    # 定期Trim (cron)
    0 2 * * 0 /sbin/fstrim -av
  
  关闭磁盘调度:
    echo none > /sys/block/sdb/queue/scheduler
  
  标记为SSD:
    echo 0 > /sys/block/sdb/queue/rotational
```

---

## 虚拟化层优化

```yaml
VMware vSphere优化:
  虚拟磁盘格式:
    Thick Eager Zeroed:
      性能: 最佳
      原因: 预分配+归零
      适用: 生产数据库
    
    Thick Lazy Zeroed:
      性能: 良好
      原因: 预分配，按需归零
      适用: 通用虚拟机
    
    Thin:
      性能: 较差
      原因: 按需分配，元数据开销
      适用: 开发测试
  
  虚拟SCSI控制器:
    选择:
      LSI Logic Parallel (老旧)
      LSI Logic SAS (通用)
      VMware Paravirtual (PVSCSI) ✅ 推荐
    
    PVSCSI优势:
      ✅ 降低CPU开销
      ✅ 更高吞吐量
      ✅ 更高IOPS
      ✅ 支持256个队列
    
    配置:
      队列深度: 256
      环形缓冲: 64
  
  存储策略 (vSAN):
    高性能:
      FTT=1, RAID-5, 磁盘条带=4
      对象空间预留=100%
      闪存读缓存=100%
    
    平衡:
      FTT=1, RAID-1, 磁盘条带=1
      对象空间预留=0%
    
    容量优化:
      FTT=2, RAID-6
      重复数据删除+压缩
  
  高级参数:
    Disk.SchedNumReqOutstanding: 256
      说明: 每磁盘队列深度
      默认: 32
      推荐: 128-256 (SSD)
    
    Disk.DiskMaxIOSize: 4096
      说明: 最大IO大小 (KB)
      默认: 32768
      推荐: 保持默认 (大文件)
    
    NFS.MaxQueueDepth: 128
      说明: NFS队列深度
      默认: 64
      推荐: 128
  
  CPU/内存优化:
    CPU预留: 关键VM预留CPU
    内存预留: 数据库VM预留内存
    NUMA: 启用NUMA调度
    延迟敏感度: 高性能VM设置"高"

KVM优化:
  virtio驱动:
    virtio-blk:
      类型: 块设备
      性能: 优秀
      推荐: ✅
    
    virtio-scsi:
      类型: SCSI控制器
      功能: 更多特性 (Trim, 磁盘热插拔)
      性能: 优秀
      推荐: ✅ (生产环境)
  
  缓存模式:
    none:
      特点: 直接IO，绕过宿主缓存
      性能: 最佳 (共享存储)
      数据安全: 最高
      推荐: ✅ 生产环境
    
    writethrough:
      特点: 读缓存，写直通
      性能: 读快，写慢
      推荐: 本地存储
    
    writeback:
      特点: 读写都缓存
      性能: 最快
      风险: 断电数据丢失
      推荐: 仅测试
  
  IO线程:
    # 启用多队列
    <driver name='qemu' type='raw' cache='none' io='native' iothread='1'/>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native' queues='4'/>
      ...
    </disk>
  
  CPU固定:
    <vcpu placement='static' cpuset='2-5'>4</vcpu>
    <cputune>
      <vcpupin vcpu='0' cpuset='2'/>
      <vcpupin vcpu='1' cpuset='3'/>
      <vcpupin vcpu='2' cpuset='4'/>
      <vcpupin vcpu='3' cpuset='5'/>
    </cputune>

Hyper-V优化:
  虚拟硬盘:
    VHDX (推荐):
      优势: 更大容量, 更好性能
      格式: 固定 > 动态
    
    Pass-through磁盘:
      性能: 最佳
      用途: 数据库
  
  存储QoS:
    最小IOPS: 保证性能
    最大IOPS: 防止争用
  
  优化:
    禁用虚拟软盘
    使用SCSI控制器 (而非IDE)
    启用集成服务
```

---

## 网络层优化

```yaml
Jumbo Frame配置:
  MTU设置:
    标准: 1500
    Jumbo Frame: 9000
    
    好处:
      ✅ 减少CPU开销 20-30%
      ✅ 提升吞吐量 10-20%
      ✅ 降低延迟
  
  配置要求:
    ✅ 端到端支持
      客户端 MTU 9000
      交换机 MTU 9216
      存储服务器 MTU 9000
  
  Linux配置:
    临时:
      ip link set eth1 mtu 9000
    
    永久 (NetworkManager):
      nmcli connection modify eth1 802-3-ethernet.mtu 9000
    
    永久 (Netplan):
      network:
        ethernets:
          eth1:
            mtu: 9000
  
  VMware配置:
    vDS:
      编辑vDS → 高级 → MTU: 9000
    
    VMkernel:
      esxcli network ip interface set -i vmk1 -m 9000
  
  验证:
    Linux:
      ping -M do -s 8972 <target-ip>
      # 8972 = 9000 - 28 (IP+ICMP header)
    
    Windows:
      ping -f -l 8972 <target-ip>

网络隔离:
  VLAN规划:
    VLAN 10: 管理网络 (1GbE)
    VLAN 20: 存储网络 (10/25GbE)
    VLAN 30: vMotion网络 (10GbE)
    VLAN 40: 虚拟机网络 (10GbE)
  
  物理隔离:
    存储: 专用交换机
    业务: 独立交换机
    备份: 独立网段

网卡绑定 (Bonding/Teaming):
  模式选择:
    Mode 0 (Balance-RR):
      特点: 轮询
      性能: 最高
      限制: 需要交换机配置
    
    Mode 1 (Active-Backup):
      特点: 主备
      性能: 单链路
      优势: 无交换机要求
    
    Mode 4 (LACP 802.3ad):
      特点: 动态聚合
      性能: 高
      推荐: ✅ 生产环境
  
  Linux配置 (LACP):
    # /etc/network/interfaces (Debian/Ubuntu)
    auto bond0
    iface bond0 inet static
        address 192.168.20.10
        netmask 255.255.255.0
        bond-mode 802.3ad
        bond-miimon 100
        bond-lacp-rate fast
        bond-slaves eth2 eth3
  
  VMware配置 (LACP):
    vDS → LACP → 新建
      模式: Active
      负载均衡: 基于IP哈希

网卡队列优化:
  多队列 (RSS - Receive Side Scaling):
    查看队列数:
      ethtool -l eth0
    
    设置队列数:
      ethtool -L eth0 combined 8
    
    推荐: 队列数 = CPU核心数 (最多16)
  
  中断绑定:
    查看中断:
      cat /proc/interrupts | grep eth0
    
    绑定到特定CPU:
      echo 2 > /proc/irq/<IRQ>/smp_affinity_list
    
    自动平衡:
      systemctl start irqbalance

TCP/IP调优:
  拥塞控制:
    查看:
      sysctl net.ipv4.tcp_congestion_control
    
    设置:
      # BBR (推荐, 内核4.9+)
      sysctl -w net.ipv4.tcp_congestion_control=bbr
      sysctl -w net.core.default_qdisc=fq
  
  窗口大小:
    net.ipv4.tcp_window_scaling = 1
    net.ipv4.tcp_rmem = 4096 87380 134217728
    net.ipv4.tcp_wmem = 4096 65536 134217728
  
  快速打开:
    net.ipv4.tcp_fastopen = 3
```

---

## 存储协议优化

```yaml
iSCSI优化:
  Initiator优化:
    队列深度:
      查看:
        cat /sys/class/iscsi_host/host*/device/session*/iscsi_session*/target*/*/queue_depth
      
      设置:
        echo 128 > /sys/class/scsi_disk/*/device/queue_depth
    
    替换超时:
      node.session.timeo.replacement_timeout = 15
    
    登录超时:
      node.conn[0].timeo.login_timeout = 15
      node.conn[0].timeo.logout_timeout = 15
    
    NOP超时:
      node.conn[0].timeo.noop_out_interval = 5
      node.conn[0].timeo.noop_out_timeout = 5
  
  Target优化:
    # TGT
    tgtadm --op update --mode target --tid 1 \
      --name MaxRecvDataSegmentLength --value 262144
    
    tgtadm --op update --mode target --tid 1 \
      --name FirstBurstLength --value 262144
  
  ESXi iSCSI优化:
    esxcli system settings advanced set \
      -o /Disk/QFullSampleSize -i 128
    
    esxcli system settings advanced set \
      -o /Disk/SchedNumReqOutstanding -i 256

NFS优化:
  挂载选项:
    rsize/wsize:
      推荐: 131072 (128KB) 或 262144 (256KB)
      
      mount -o rsize=262144,wsize=262144 ...
    
    错误处理:
      hard,intr,timeo=600,retrans=2
    
    协议:
      tcp (推荐)
      vers=3 或 vers=4.1
    
    完整示例:
      mount -t nfs -o rw,hard,intr,rsize=262144,wsize=262144,tcp,timeo=600,vers=3 \
        192.168.20.10:/export/vmware /mnt/vmware
  
  服务器端优化:
    NFS线程数:
      # 推荐: 2-4 × CPU核心数
      echo 32 > /proc/fs/nfsd/threads
    
    异步写入 (谨慎):
      /etc/exports:
        /export/data *(rw,async,no_subtree_check)
  
  ESXi NFS优化:
    esxcli system settings advanced set \
      -o /NFS/MaxQueueDepth -i 128
    
    esxcli system settings advanced set \
      -o /NFS/SendBufferSize -i 262144

vSAN优化:
  存储策略:
    磁盘条带数: 1 (默认) → 4 (高性能)
    对象空间预留: 0% → 100% (厚置备)
    闪存读缓存: 0% → 100% (关键VM)
  
  重同步限流:
    降低重建优先级，避免影响业务
    配置: 40-60 IOPS限制
  
  重复数据删除+压缩:
    全闪存推荐启用
    节省容量50-70%
    CPU开销10-15%

Ceph优化:
  OSD:
    bluestore_cache_size: 4GB (HDD) / 8GB (SSD)
    osd_op_num_threads_per_shard: 2
    osd_op_num_shards: 8
  
  客户端 (RBD):
    rbd_cache: true
    rbd_cache_size: 33554432  # 32MB
    rbd_cache_max_dirty: 25165824
  
  PG autoscale:
    ceph osd pool set <pool> pg_autoscale_mode on
```

---

## 应用层优化

```yaml
数据库优化:
  MySQL/MariaDB:
    InnoDB配置:
      innodb_buffer_pool_size: 70-80%内存
      innodb_log_file_size: 256MB-2GB
      innodb_flush_method: O_DIRECT
      innodb_io_capacity: 2000 (HDD) / 20000 (SSD)
      innodb_read_io_threads: 8
      innodb_write_io_threads: 8
    
    存储:
      推荐: SSD/NVMe
      文件系统: XFS (noatime)
      挂载: nobarrier (有UPS)
  
  PostgreSQL:
    配置:
      shared_buffers: 25%内存
      effective_cache_size: 50-75%内存
      work_mem: 根据连接数
      maintenance_work_mem: 1-2GB
      wal_buffers: 16MB
      checkpoint_completion_target: 0.9
      random_page_cost: 1.1 (SSD) / 4.0 (HDD)
    
    存储:
      数据目录: SSD/NVMe
      WAL目录: 独立高速盘
      文件系统: XFS
  
  MongoDB:
    配置:
      storage.wiredTiger.engineConfig.cacheSizeGB: 50%内存
      storage.directoryPerDB: true
    
    存储:
      推荐: SSD/NVMe
      文件系统: XFS
      RAID: RAID10

虚拟机优化:
  磁盘IO优先级:
    VMware:
      shares: High (2000) / Normal (1000) / Low (500)
    
    KVM:
      <iotune>
        <total_iops_sec>5000</total_iops_sec>
      </iotune>
  
  虚拟机放置:
    反亲和性: 分散到不同主机
    存储DRS: 自动负载均衡
    NUMA绑定: 大内存VM

容器优化:
  Docker:
    存储驱动:
      推荐: overlay2
      配置: /etc/docker/daemon.json
        {
          "storage-driver": "overlay2",
          "storage-opts": [
            "overlay2.override_kernel_check=true"
          ]
        }
  
  Kubernetes:
    存储类:
      选择性能优化的StorageClass
      支持扩展: allowVolumeExpansion
    
    PV回收策略:
      reclaimPolicy: Retain (避免误删)
```

---

## 性能测试与基准

```bash
#!/bin/bash
# 存储性能测试工具集

echo "========================================="
echo "  存储性能测试"
echo "========================================="
echo ""

# 1. FIO - 最全面的存储测试工具
echo "=== FIO测试 ==="

# 随机读IOPS测试
fio --name=randread --ioengine=libaio --iodepth=32 --rw=randread \
    --bs=4k --direct=1 --size=10G --numjobs=4 --runtime=60 \
    --group_reporting --filename=/dev/sdb

# 随机写IOPS测试
fio --name=randwrite --ioengine=libaio --iodepth=32 --rw=randwrite \
    --bs=4k --direct=1 --size=10G --numjobs=4 --runtime=60 \
    --group_reporting --filename=/dev/sdb

# 顺序读吞吐量测试
fio --name=seqread --ioengine=libaio --iodepth=32 --rw=read \
    --bs=1m --direct=1 --size=10G --numjobs=1 --runtime=60 \
    --group_reporting --filename=/dev/sdb

# 混合读写测试 (70%读 30%写)
fio --name=randrw --ioengine=libaio --iodepth=32 --rw=randrw \
    --rwmixread=70 --bs=4k --direct=1 --size=10G --numjobs=4 \
    --runtime=60 --group_reporting --filename=/dev/sdb

# 2. dd - 简单吞吐量测试
echo ""
echo "=== dd测试 ==="
dd if=/dev/zero of=/mnt/testfile bs=1M count=10240 conv=fdatasync
rm -f /mnt/testfile

# 3. iozone - 文件系统测试
echo ""
echo "=== IOzone测试 ==="
iozone -a -s 10G -r 4k -i 0 -i 1 -i 2 -f /mnt/iozone-test

# 4. sysbench - 数据库场景测试
echo ""
echo "=== Sysbench文件IO测试 ==="
sysbench fileio --file-total-size=10G prepare
sysbench fileio --file-total-size=10G --file-test-mode=rndrw \
  --time=60 --max-requests=0 run
sysbench fileio --file-total-size=10G cleanup

# 5. 网络存储测试 (iperf3)
echo ""
echo "=== 网络带宽测试 ==="
# 在存储服务器运行: iperf3 -s
iperf3 -c 192.168.20.10 -t 60

# 6. NFS性能测试
echo ""
echo "=== NFS性能测试 ==="
time dd if=/dev/zero of=/mnt/nfs/testfile bs=1M count=5120
time dd if=/mnt/nfs/testfile of=/dev/null bs=1M
rm -f /mnt/nfs/testfile

# 7. iSCSI性能测试
echo ""
echo "=== iSCSI性能测试 ==="
fio --name=iscsi-test --ioengine=libaio --iodepth=128 --rw=randread \
    --bs=4k --direct=1 --size=10G --numjobs=4 --runtime=60 \
    --group_reporting --filename=/dev/disk/by-path/ip-*-iscsi-*-lun-0

echo ""
echo "========================================="
echo "  测试完成"
echo "========================================="
```

**性能基准参考**:

```yaml
IOPS基准:
  HDD (7.2K):
    随机读: 100-150 IOPS
    随机写: 80-120 IOPS
  
  SSD (SATA):
    随机读: 80K-100K IOPS
    随机写: 60K-80K IOPS
  
  NVMe SSD:
    随机读: 500K-1M IOPS
    随机写: 300K-800K IOPS

延迟基准:
  HDD: 5-10ms
  SATA SSD: 0.1-0.5ms
  NVMe SSD: 0.01-0.1ms

吞吐量基准:
  HDD (单盘): 150-200 MB/s
  SATA SSD: 500-600 MB/s
  NVMe SSD: 3000-7000 MB/s
  10GbE网络: 1100-1200 MB/s
  25GbE网络: 2800-3000 MB/s

网络延迟:
  同机架: <0.1ms
  同交换机: <1ms
  跨交换机: 1-5ms
```

---

## 监控与诊断

```bash
#!/bin/bash
# 存储性能监控脚本

echo "========================================="
echo "  存储性能实时监控"
echo "========================================="
echo ""

# 1. iostat - IO统计
echo "=== iostat (5秒间隔) ==="
iostat -xz 5 3

# 2. iotop - IO进程监控
echo ""
echo "=== iotop (实时IO) ==="
iotop -oP -d 5 -n 3

# 3. dstat - 综合监控
echo ""
echo "=== dstat ==="
dstat -cdngy --disk-util --fs 5 3

# 4. sar - 系统活动报告
echo ""
echo "=== sar (磁盘IO) ==="
sar -d 5 3

# 5. vmstat - 虚拟内存统计
echo ""
echo "=== vmstat ==="
vmstat 5 3

# 6. 磁盘SMART健康
echo ""
echo "=== SMART健康状态 ==="
for disk in /dev/sd?; do
    echo "Disk: $disk"
    smartctl -H $disk
    smartctl -A $disk | grep -E "Reallocated_Sector_Ct|Current_Pending_Sector|Offline_Uncorrectable"
done

# 7. 查看IO等待
echo ""
echo "=== 进程IO等待 ==="
ps aux | awk '$8 == "D" {print}'

# 8. 块设备队列深度
echo ""
echo "=== 队列深度 ==="
for dev in /sys/block/sd?; do
    echo "$(basename $dev): $(cat $dev/device/queue_depth)"
done

# 9. NFS统计 (如果使用NFS)
if mount | grep -q nfs; then
    echo ""
    echo "=== NFS统计 ==="
    nfsstat -c
    nfsstat -m
fi

# 10. 网络统计
echo ""
echo "=== 网络统计 ==="
netstat -i
ss -s

echo ""
echo "========================================="
echo "  监控完成"
echo "========================================="
```

**监控指标解读**:

```yaml
iostat关键指标:
  %util:
    说明: 设备利用率
    警告: >80%
    严重: >95%
  
  await:
    说明: 平均等待时间 (ms)
    正常: HDD <10ms, SSD <1ms
    警告: HDD >20ms, SSD >5ms
  
  svctm:
    说明: 平均服务时间 (ms)
    注意: 新版本已弃用
  
  r/s, w/s:
    说明: 每秒读写操作数
    对比: 与设备规格

iotop指标:
  DISK READ/WRITE:
    说明: 实际磁盘读写
    用途: 识别高IO进程
  
  SWAPIN:
    警告: >0 (表示内存不足)

监控告警阈值:
  IOPS:
    警告: >设备额定值80%
    严重: >设备额定值95%
  
  延迟:
    HDD:
      警告: >15ms
      严重: >30ms
    SSD:
      警告: >2ms
      严重: >10ms
  
  队列深度:
    警告: 持续满载
    严重: 持续满载+高延迟
  
  磁盘利用率:
    警告: >80%
    严重: >90%
  
  容量:
    警告: >75%
    严重: >85%
```

---

## 故障排查

```yaml
常见性能问题:
  问题1: 磁盘IO高延迟
    现象:
      iostat显示高await
      应用响应慢
    
    排查:
      1. 识别高IO进程
         iotop -oP
      
      2. 检查磁盘健康
         smartctl -a /dev/sda
      
      3. 查看队列深度
         cat /sys/block/sda/device/queue_depth
      
      4. 检查RAID状态
         MegaCli -LDInfo -Lall -aALL
    
    解决:
      - 优化应用查询
      - 增加缓存
      - 更换慢盘
      - 升级到SSD
  
  问题2: IOPS达到瓶颈
    现象:
      IOPS不再增长
      队列深度持续高
    
    排查:
      1. 确认磁盘规格
      2. 检查控制器限制
      3. 查看网络带宽 (SAN/NAS)
      4. 检查CPU负载
    
    解决:
      - 增加磁盘数量 (RAID0/10)
      - 升级到更快磁盘
      - 增加队列深度
      - 分散负载
  
  问题3: 网络存储性能差
    现象:
      NFS/iSCSI延迟高
      吞吐量低
    
    排查:
      1. 测试网络延迟
         ping -c 100 <storage-ip>
      
      2. 测试带宽
         iperf3 -c <storage-ip>
      
      3. 检查MTU
         ip link show
      
      4. 查看丢包
         netstat -i
    
    解决:
      - 启用Jumbo Frame
      - 升级网络到10/25GbE
      - 检查交换机配置
      - 优化TCP参数
  
  问题4: 虚拟机IO性能差
    现象:
      VM内IOPS低
      应用慢
    
    排查:
      1. 检查虚拟磁盘类型
      2. 查看存储策略
      3. 检查共享争用
      4. 查看快照链
    
    解决:
      - 使用Thick Eager Zeroed
      - 启用PVSCSI (VMware)
      - 删除旧快照
      - 迁移到性能更好存储
  
  问题5: 数据库IO等待高
    现象:
      数据库慢查询
      IO wait高
    
    排查:
      1. 分析慢查询日志
      2. 检查索引
      3. 查看IO模式
         iostat -x 1
      
      4. 检查缓冲池命中率
    
    解决:
      - 优化SQL查询
      - 添加索引
      - 增加缓冲池
      - 数据分区
      - 使用SSD

诊断工具:
  实时监控:
    iostat -xz 1
    iotop -oP
    atop
  
  历史数据:
    sar -d
    Grafana + Prometheus
  
  深度分析:
    blktrace / blkparse
    strace -e trace=file
    perf record / perf report
  
  磁盘健康:
    smartctl -a /dev/sda
    badblocks -sv /dev/sda
```

---

## 相关文档

- [存储类型与选型标准](01_存储类型与选型标准.md)
- [iSCSI配置与优化](02_iSCSI配置与优化.md)
- [NFS配置与优化](03_NFS配置与优化.md)
- [VMware vSAN配置](04_VMware_vSAN配置.md)
- [Ceph分布式存储](05_Ceph分布式存储.md)
- [存储容灾与备份](07_存储容灾与备份.md)

---

**更新时间**: 2025-10-19  
**文档版本**: v3.0  
**状态**: ✅ 生产就绪
