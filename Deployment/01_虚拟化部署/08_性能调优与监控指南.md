# 虚拟化平台性能调优与监控指南

> **返回**: [虚拟化部署首页](README.md) | [部署指南首页](../00_索引导航/README.md)

---

## 📋 目录

- [虚拟化平台性能调优与监控指南](#虚拟化平台性能调优与监控指南)
  - [📋 目录](#-目录)
  - [性能调优概述](#性能调优概述)
  - [CPU性能调优](#cpu性能调优)
    - [VMware vSphere CPU调优](#vmware-vsphere-cpu调优)
    - [KVM CPU调优](#kvm-cpu调优)
  - [内存性能调优](#内存性能调优)
    - [VMware内存调优](#vmware内存调优)
    - [KVM内存调优](#kvm内存调优)
  - [存储性能调优](#存储性能调优)
    - [VMware存储调优](#vmware存储调优)
    - [KVM存储调优](#kvm存储调优)
  - [网络性能调优](#网络性能调优)
  - [vSAN性能调优](#vsan性能调优)
  - [监控系统搭建](#监控系统搭建)
  - [性能基准测试](#性能基准测试)

---

## 性能调优概述

```yaml
性能调优金字塔 (从下到上):

第1层 - 硬件基础 (最重要):
  - 选择合适的硬件 (CPU/内存/存储/网络)
  - 正确的BIOS配置
  - 固件/驱动更新
  影响: 60-70%性能

第2层 - 虚拟化层配置:
  - ESXi/KVM优化配置
  - 资源分配策略
  - 虚拟硬件配置
  影响: 20-30%性能

第3层 - Guest OS优化:
  - 操作系统调优
  - 应用程序优化
  - 驱动程序更新
  影响: 5-10%性能

第4层 - 应用层优化:
  - 应用程序配置
  - 数据库调优
  - 缓存策略
  影响: 5-10%性能

性能调优原则:
  ✅ 先测量后优化 (Measure First)
  ✅ 一次改一个变量
  ✅ 记录所有变更
  ✅ 建立性能基准
  ✅ 持续监控反馈

性能瓶颈识别:
  CPU瓶颈特征:
    - CPU Ready时间高 (>5%)
    - CPU使用率持续>80%
    - 应用响应慢

  内存瓶颈特征:
    - 内存Balloon活跃
    - Swap使用增加
    - 内存压力状态为Hard/Low

  存储瓶颈特征:
    - 存储延迟高 (>20ms)
    - IOPS不足
    - 队列深度高

  网络瓶颈特征:
    - 丢包率高 (>0.1%)
    - 带宽使用率>80%
    - 网络延迟高
```

---

## CPU性能调优

### VMware vSphere CPU调优

```yaml
1. BIOS CPU设置:
  
  必须启用:
    ✅ Intel VT-x / AMD-V: 硬件虚拟化
    ✅ Intel VT-d / AMD-Vi: I/O虚拟化
    ✅ Intel EPT / AMD RVI: 扩展页表
  
  性能相关:
    ✅ Hyper-Threading: 启用 (通常提升20-30%)
    ✅ Turbo Boost: 启用 (动态超频)
    ✅ C-States: 禁用 (降低延迟,增加功耗)
    ✅ P-States: 禁用或OS Control
    ✅ Intel Speedstep: 禁用
  
  电源管理:
    设置: Maximum Performance
    原因: 避免CPU降频影响性能

2. ESXi主机CPU配置:
  
  电源策略:
    # 查看当前策略
    esxcli system settings advanced list -o /Power/CpuPolicy
    
    # 设置为高性能模式
    esxcli system settings advanced set -o /Power/CpuPolicy -s "High Performance"
    
    策略选项:
      - Balanced: 默认,平衡性能和功耗
      - High Performance: 最高性能
      - Low Power: 低功耗
      - Custom: 自定义
  
  CPU调度:
    # 启用CPU硬件辅助虚拟化
    已默认启用,无需配置
    
    # 检查NUMA配置
    esxcli hardware memory get
    # 确保NUMA已启用

3. 虚拟机CPU配置最佳实践:
  
  vCPU分配原则:
    ✅ 从少开始: 先分配1-2 vCPU,监控后按需增加
    ✅ 避免过度分配: vCPU数不要超过物理核心数
    ✅ 考虑应用特性:
       - 单线程应用: 1-2 vCPU
       - 多线程应用: 根据线程数分配
       - Web服务器: 4-8 vCPU
       - 数据库: 8-16 vCPU
    
    ❌ 错误做法: 给所有VM都分配8+ vCPU
       原因: 导致CPU调度开销,Co-stop增加
  
  CPU热插拔:
    # 启用CPU热添加 (需VM关机配置)
    Get-VM "web-server-01" | New-AdvancedSetting -Name "vcpu.hotadd" -Value "TRUE"
    
    # 启用CPU热移除 (vSphere 7.0+)
    Get-VM "web-server-01" | New-AdvancedSetting -Name "vcpu.hotremove" -Value "TRUE"
  
  CPU预留 (Reservation):
    # 为关键VM预留CPU
    Get-VM "db-server-01" | Get-VMResourceConfiguration | Set-VMResourceConfiguration -CpuReservationMhz 10000
    
    使用场景:
      ✅ 关键业务VM
      ✅ 实时性要求高的应用
      ❌ 测试/开发环境 (浪费资源)
  
  CPU限制 (Limit):
    # 一般不建议设置
    # 除非需要限制某些VM的CPU使用
    
    # 如需设置:
    Get-VM "test-vm" | Get-VMResourceConfiguration | Set-VMResourceConfiguration -CpuLimitMhz 4000
  
  CPU份额 (Shares):
    # 控制CPU争用时的优先级
    Get-VM "critical-app" | Get-VMResourceConfiguration | Set-VMResourceConfiguration -CpuSharesLevel High
    
    级别:
      - Low: 500 shares/vCPU
      - Normal: 1000 shares/vCPU (默认)
      - High: 2000 shares/vCPU
      - Custom: 自定义数值
  
  CPU亲和性 (Affinity):
    # 绑定VM到特定CPU (一般不推荐)
    # 会限制DRS和vMotion功能
    
    # 特殊场景: 许可证绑定/NUMA优化
    Get-VM "licensed-app" | New-AdvancedSetting -Name "sched.cpu.affinity" -Value "0-7"

4. NUMA优化:
  
  NUMA架构理解:
    - 现代服务器是NUMA (Non-Uniform Memory Access)
    - 每个CPU插槽是一个NUMA节点
    - 访问本地内存快,跨节点慢
  
  vNUMA配置:
    # 自动vNUMA (推荐):
    # 当VM vCPU数 > 8 时,ESXi自动启用vNUMA
    # 将VM拆分成多个虚拟NUMA节点
    
    # 手动配置vNUMA (高级):
    # 确保VM适合单个NUMA节点
    # 例如: 双路服务器,每路16核32GB
    # VM配置应 ≤16 vCPU + 32GB内存
    
    # 查看NUMA分配
    esxtop
    # 按'n'进入NUMA统计视图
  
  NUMA最佳实践:
    ✅ 大VM (>8 vCPU) 确保内存+vCPU适合NUMA节点
    ✅ 避免跨NUMA访问
    ✅ 监控NUMA Home节点
    ✅ 使用DRS自动平衡

5. CPU性能监控:
  
  关键指标:
    # 使用esxtop监控
    esxtop
    # 按'c'进入CPU视图
    
    指标含义:
      - %USED: CPU使用率
      - %RDY: Ready时间 (等待物理CPU)
      - %CSTP: Co-stop时间 (多vCPU同步等待)
      - %MLMTD: CPU限制导致的等待
      - %SWPWT: Swap等待时间
    
    健康值:
      - %RDY < 5%: 健康
      - %RDY 5-10%: 注意
      - %RDY > 10%: 需要优化
      
      - %CSTP < 3%: 健康
      - %CSTP > 3%: 考虑减少vCPU
  
  PowerCLI监控脚本:
    # 获取所有VM的CPU Ready
    Get-VM | Select Name, @{N="CPU_Ready";E={(Get-Stat -Entity $_ -Stat cpu.ready.summation -Realtime -MaxSamples 1).Value}}
```

### KVM CPU调优

```bash
# 1. 主机CPU配置

# 查看CPU信息
lscpu

# 启用性能模式
# 安装cpupower工具
dnf install -y kernel-tools

# 设置为performance governor
cpupower frequency-set -g performance

# 持久化
echo "GOVERNOR=performance" > /etc/sysconfig/cpupower
systemctl enable --now cpupower

# 2. 虚拟机CPU配置

# CPU模式选择 (virsh edit vm-name)
<cpu mode='host-passthrough' check='none'>
  <topology sockets='1' cores='4' threads='1'/>
  <cache mode='passthrough'/>
  <feature policy='require' name='invtsc'/>
</cpu>

# CPU模式对比:
# host-passthrough: 最佳性能,直接透传物理CPU特性
# host-model: 模拟物理CPU,兼容性好
# custom: 自定义CPU模型

# CPU绑定 (CPU Pinning)
# 提升性能,减少CPU切换
<vcpu placement='static' cpuset='0-3'>4</vcpu>
<cputune>
  <vcpupin vcpu='0' cpuset='0'/>
  <vcpupin vcpu='1' cpuset='1'/>
  <vcpupin vcpu='2' cpuset='2'/>
  <vcpupin vcpu='3' cpuset='3'/>
  <emulatorpin cpuset='4-5'/>
</cputune>

# NUMA绑定
<numatune>
  <memory mode='strict' nodeset='0'/>
</numatune>

# 3. CPU调度优化

# 隔离CPU核心 (用于高性能VM)
# 编辑GRUB
vi /etc/default/grub
# 添加: isolcpus=2-7
GRUB_CMDLINE_LINUX="... isolcpus=2-7"

# 更新GRUB
grub2-mkconfig -o /boot/grub2/grub.cfg

# 重启生效
reboot

# 4. 监控CPU性能

# 使用virt-top
virt-top

# 使用virsh
virsh domstats vm-name --cpu-total

# 详细统计
virsh cpu-stats vm-name
```

---

## 内存性能调优

### VMware内存调优

```yaml
1. 主机内存配置:
  
  内存类型选择:
    ✅ 必须使用ECC内存
    ✅ DDR4-3200或更高
    ✅ 填满所有内存通道 (最大化带宽)
    
    示例: 双路服务器,每路8通道
    推荐: 16 x 32GB = 512GB (全填满)
    不推荐: 8 x 64GB = 512GB (只用了一半通道)
  
  BIOS内存设置:
    ✅ Memory Operating Mode: Optimizer Mode
    ✅ Node Interleaving: Disabled (启用NUMA)
    ✅ Memory Patrol Scrub: Enabled (错误检测)

2. ESXi内存管理技术:
  
  透明页共享 (TPS):
    状态: ESXi 6.0+默认禁用 (安全原因)
    
    # 查看TPS状态
    esxcli system settings advanced list -o /Mem/ShareForceSalting
    # 值为2表示禁用TPS
    
    # 如果在受信任环境,可启用TPS以节省内存
    esxcli system settings advanced set -o /Mem/ShareForceSalting -i 0
    
    # Per-VM启用TPS
    Get-VM "vm-name" | New-AdvancedSetting -Name "Mem.ShareForceSalting" -Value "FALSE"
  
  内存气球 (Ballooning):
    作用: 当主机内存不足时,从VM回收内存
    
    # 查看Balloon驱动状态
    # 在VM内
    systemctl status vmtoolsd
    
    # 正常情况下Balloon应该为0或很小
    # 持续Balloon表示主机内存压力大
  
  内存压缩:
    作用: 在Swap前压缩内存页
    
    # 默认启用,无需配置
    # 监控压缩内存大小
    esxtop -> 'm' -> ZIP列
  
  内存Swap:
    # 最后手段,性能严重下降
    # 应避免Swap
    
    # 查看Swap文件位置
    # .vswp文件在VM工作目录
  
  技术触发顺序:
    1. TPS (如启用) - 影响小
    2. Balloon - 影响中等
    3. Compression - 影响较大
    4. Swap - 影响严重

3. 虚拟机内存配置:
  
  内存大小:
    # 分配略高于应用实际需求
    # 使用监控确定合适大小
    
    # 示例: 监控VM内存使用
    Get-Stat -Entity (Get-VM "web-01") -Stat mem.usage.average -Realtime
  
  内存预留:
    # 关键VM预留全部内存
    Get-VM "db-server-01" | Get-VMResourceConfiguration | Set-VMResourceConfiguration -MemReservationGB 64
    
    # 或预留百分比
    Get-VM "db-server-01" | Get-VMResourceConfiguration | Set-VMResourceConfiguration -MemReservationMB ($VM.MemoryGB * 1024 * 0.75)
  
  内存限制:
    # 一般不设置
    # 测试环境可限制防止过度使用
    Get-VM "test-vm" | Get-VMResourceConfiguration | Set-VMResourceConfiguration -MemLimitGB 16
  
  内存份额:
    # 控制内存争用时的优先级
    Get-VM "critical-app" | Get-VMResourceConfiguration | Set-VMResourceConfiguration -MemSharesLevel High
  
  内存热添加:
    # 启用内存热添加
    Get-VM "app-server" | Get-AdvancedSetting -Name "Mem.HotAddEnabled" | Set-AdvancedSetting -Value "true"
    
    # 注意: 启用热添加会禁用某些vNUMA优化

4. 大页内存 (Huge Pages):
  
  ESXi:
    # 默认启用大页内存
    # 透明大页 (Transparent Huge Pages, THP)
    
    # 检查配置
    esxcli system settings advanced list -o /Mem/AllocGuestLargePage
    # 值为1表示启用
  
  好处:
    - 减少TLB miss
    - 降低内存管理开销
    - 提升性能5-10%

5. 内存监控:
  
  esxtop监控:
    esxtop
    # 按'm'进入内存视图
    
    关键指标:
      - MCTLSZ: Balloon大小 (应为0)
      - SWCUR: Swap当前大小 (应为0)
      - SWR/s, SWW/s: Swap读写速率 (应为0)
      - ZIP/s: 压缩速率
      - %ACTV: 活动内存百分比
  
  vCenter监控:
    指标:
      - mem.usage.average: 内存使用率
      - mem.active.average: 活动内存
      - mem.consumed.average: 已消耗内存
      - mem.vmmemctl.average: Balloon大小
      - mem.swapused.average: Swap使用
  
  告警阈值:
    Warning: Balloon > 1GB 或 Swap > 0
    Critical: Balloon > 10% VM内存 或 Swap > 100MB
```

### KVM内存调优

```bash
# 1. 主机内存配置

# 启用Huge Pages
# 计算需要的Huge Pages数量
# 例如: 8个VM,每个16GB = 128GB
# Huge Page默认大小2MB
# 需要: 128GB / 2MB = 65536个

# 编辑sysctl
cat >> /etc/sysctl.conf << EOF
# 预留Huge Pages
vm.nr_hugepages = 65536

# Huge Pages管理组
vm.hugetlb_shm_group = 36
EOF

sysctl -p

# 验证
grep Huge /proc/meminfo

# 2. VM配置使用Huge Pages

virsh edit vm-name

<memoryBacking>
  <hugepages>
    <page size='2048' unit='KiB'/>
  </hugepages>
</memoryBacking>

# 或使用1GB Huge Pages (需CPU支持)
<memoryBacking>
  <hugepages>
    <page size='1048576' unit='KiB'/>
  </hugepages>
  <locked/>  # 锁定内存,避免swap
</memoryBacking>

# 3. NUMA内存绑定

<numatune>
  <memory mode='strict' nodeset='0'/>
  <memnode cellid='0' mode='strict' nodeset='0'/>
</numatune>

# 4. 内存气球

# 启用内存气球 (动态调整VM内存)
<memballoon model='virtio'>
  <stats period='10'/>
</memballoon>

# 调整VM内存
virsh setmem vm-name 8G --live

# 5. KSM (Kernel Samepage Merging)

# 类似VMware TPS,合并相同内存页
# 启用KSM
echo 1 > /sys/kernel/mm/ksm/run

# 配置扫描参数
echo 1000 > /sys/kernel/mm/ksm/pages_to_scan
echo 20 > /sys/kernel/mm/ksm/sleep_millisecs

# 持久化
systemctl enable --now ksm ksmtuned

# 6. 监控内存

# 查看VM内存统计
virsh dommemstat vm-name

# 查看系统内存
free -h

# 查看Huge Pages使用
grep Huge /proc/meminfo
```

---

## 存储性能调优

### VMware存储调优

```yaml
1. 存储硬件选择:
  
  存储介质:
    ⭐⭐⭐⭐⭐ Enterprise NVMe SSD: 最佳选择
      - IOPS: 1M+
      - 延迟: <0.1ms
      - 适用: 数据库,高性能应用
    
    ⭐⭐⭐⭐ NVMe SSD: 推荐
      - IOPS: 500K
      - 延迟: <0.2ms
      - 适用: 一般虚拟化环境
    
    ⭐⭐⭐ SATA SSD: 可用
      - IOPS: 50K-100K
      - 延迟: 0.5-1ms
      - 适用: 非关键应用
    
    ⭐⭐ SAS HDD 15K RPM: 不推荐
      - IOPS: 200-400
      - 延迟: 3-5ms
      - 仅适用: 归档存储
  
  RAID级别:
    RAID10: 最佳性能+可靠性 (推荐)
    RAID5: 平衡性能+容量
    RAID6: 双盘容错 (大容量)
    RAID0: 最高性能但无容错 (仅测试环境)

2. 存储协议优化:
  
  iSCSI优化:
    # Jumbo Frame
    esxcli system module parameters set -m iscsi_vmk -p iscsivmk_LunQDepth=128
    
    # 增加队列深度
    esxcli storage core device set -d naa.xxx --queue-full-sample-size 64
    
    # 优化网络
    - 使用10GbE及以上
    - 启用Jumbo Frame (MTU 9000)
    - 专用VLAN隔离存储流量
    - NIC绑定提供冗余和带宽
  
  NFS优化:
    # 使用NFSv4.1
    # ESXi 7.0+默认
    
    # 增加最大队列深度
    esxcli system settings advanced set -o /NFS/MaxQueueDepth -i 128
    
    # 增加NFS41.MaxVolumes
    esxcli system settings advanced set -o /NFS41/MaxVolumes -i 256
    
    # 启用Jumbo Frame
    # NFS服务器和ESXi主机都需要配置
  
  FC (Fibre Channel):
    # 最佳性能和可靠性
    # 8/16/32 Gbps
    
    # 多路径配置
    esxcli storage nmp device list
    
    # 设置Round Robin策略
    esxcli storage nmp device set -d naa.xxx --psp VMW_PSP_RR
    
    # 设置IOPS切换阈值
    esxcli storage nmp psp roundrobin deviceconfig set -d naa.xxx -t iops -I 1

3. 存储队列调优:
  
  HBA队列深度:
    # 查看当前值
    esxcli storage core adapter list
    
    # 增加队列深度 (FC HBA)
    esxcli system module parameters set -m lpfc -p "lpfc_lun_queue_depth=128"
    
    # 重启主机生效
  
  磁盘队列深度:
    # 查看
    esxcli storage core device list -d naa.xxx
    
    # 设置
    esxcli storage core device set -d naa.xxx --queue-full-sample-size 64 --queue-full-threshold 32

4. VMFS优化:
  
  VMFS版本:
    使用VMFS6 (支持4K原生, 自动空间回收)
  
  块大小:
    VMFS6默认1MB块大小 (适合大多数场景)
  
  自动空间回收:
    # VMFS6默认启用UNMAP
    # 当VM删除文件时,自动回收空间
    
    # 手动回收
    esxcli storage vmfs unmap -l datastore-name

5. 虚拟机存储配置:
  
  虚拟磁盘类型:
    精简置备 (Thin): 节省空间,但性能略低
    厚置备延迟置零: 平衡性能和速度
    厚置备置零 (Eager Zero): 最佳性能 (推荐关键VM)
  
  SCSI控制器:
    LSI Logic Parallel: 兼容性好,性能一般
    LSI Logic SAS: 兼容性好,性能较好
    VMware Paravirtual (PVSCSI): 最佳性能 (推荐)
    
    # 修改为PVSCSI
    # 需要先在Guest OS安装PVSCSI驱动
    Get-VM "vm-name" | Get-ScsiController | Set-ScsiController -Type ParaVirtual
  
  磁盘份额/限制/预留:
    # 关键VM设置高份额
    Get-HardDisk -VM "db-server" | Set-HardDisk -StorageIOShares High
    
    # 限制某些VM的IOPS (测试环境)
    Get-HardDisk -VM "test-vm" | Set-HardDisk -StorageIOLimitIOPerSecond 1000

6. Storage I/O Control (SIOC):
  
  启用SIOC:
    # 在数据存储级别启用
    Get-Datastore "datastore1" | Set-Datastore -StorageIOControlEnabled $true
    
    # 设置拥塞阈值
    Get-Datastore "datastore1" | Set-Datastore -CongestionThresholdMillisecond 30
  
  VM存储份额:
    Low: 500 shares
    Normal: 1000 shares
    High: 2000 shares

7. 存储性能监控:
  
  esxtop:
    esxtop
    # 按'u'进入磁盘视图
    
    关键指标:
      - DAVG/cmd: 设备延迟 (<20ms)
      - KAVG/cmd: VMkernel延迟
      - GAVG/cmd: Guest OS延迟
      - CMDS/s: IOPS
      - MBREAD/s, MBWRIT/s: 吞吐量
      - %USD: 队列使用率
  
  vCenter监控:
    - datastore.numberReadAveraged: 读IOPS
    - datastore.numberWriteAveraged: 写IOPS
    - datastore.totalReadLatency: 读延迟
    - datastore.totalWriteLatency: 写延迟
```

### KVM存储调优

```bash
# 1. 磁盘驱动选择

# 最佳: virtio (块设备)
<disk type='block' device='disk'>
  <driver name='qemu' type='raw' cache='none' io='native' discard='unmap'/>
  <source dev='/dev/vg_storage/vm-disk'/>
  <target dev='vda' bus='virtio'/>
</disk>

# 2. 缓存模式

# cache='none': 直接I/O, 无缓存 (推荐数据库)
# cache='writeback': 写回缓存 (推荐应用服务器)
# cache='writethrough': 写穿缓存 (平衡)

# 3. I/O模式

# io='native': 使用Linux AIO (异步I/O)
# io='threads': 使用线程池 (默认)

# 推荐配置:
<driver name='qemu' type='raw' cache='none' io='native'/>

# 4. I/O线程 (iothread)

# 提升并发I/O性能
<domain type='kvm'>
  <iothreads>4</iothreads>
  ...
  <disk ...>
    <driver ... iothread='1'/>
    ...
  </disk>
</domain>

# 5. 磁盘调度器

# 查看当前调度器
cat /sys/block/sda/queue/scheduler

# 设置为none (使用多队列)
echo none > /sys/block/sda/queue/scheduler

# 或mq-deadline (机械硬盘)
echo mq-deadline > /sys/block/sda/queue/scheduler

# 持久化 (udev规则)
cat > /etc/udev/rules.d/60-scheduler.rules << EOF
ACTION=="add|change", KERNEL=="sd[a-z]", ATTR{queue/scheduler}="none"
ACTION=="add|change", KERNEL=="nvme[0-9]n[0-9]", ATTR{queue/scheduler}="none"
EOF

# 6. 块设备优化

# LVM配置
# 对齐分区 (4K扇区)
pvcreate --dataalignment 4096 /dev/sdb

# 精简置备
lvcreate -L 100G --thinpool thinpool vg_storage
lvcreate -V 50G --thin -n vm-disk vg_storage/thinpool

# 7. Ceph RBD优化

# RBD缓存
<disk type='network' device='disk'>
  <driver name='qemu' type='raw' cache='writeback'/>
  <source protocol='rbd' name='pool/image'>
    ...
  </source>
  <target dev='vda' bus='virtio'/>
</disk>

# Ceph客户端配置
cat >> /etc/ceph/ceph.conf << EOF
[client]
rbd_cache = true
rbd_cache_size = 67108864  # 64MB
rbd_cache_max_dirty = 50331648  # 48MB
rbd_cache_target_dirty = 33554432  # 32MB
rbd_cache_writethrough_until_flush = false
EOF
```

---

## 网络性能调优

详细的网络调优内容（限于篇幅，这里提供核心要点）：

```yaml
VMware网络调优要点:
  ✅ 使用VMXNET3网卡 (最佳性能)
  ✅ 启用Jumbo Frame (MTU 9000)
  ✅ 配置NIC绑定 (LACP/负载均衡)
  ✅ 启用SR-IOV (需硬件支持)
  ✅ 使用分布式交换机 (dVSwitch)
  ✅ 启用Network I/O Control (NIOC)
  ✅ 分离不同类型流量 (管理/vMotion/vSAN/VM)

KVM网络调优要点:
  ✅ 使用virtio网卡
  ✅ 启用vhost-net (内核加速)
  ✅ 启用多队列 (Multi-queue)
  ✅ 配置网桥绑定
  ✅ 启用Jumbo Frame
  ✅ 使用SR-IOV或PCI直通 (高性能场景)

网络监控关键指标:
  - 吞吐量: Mbps/Gbps
  - 丢包率: <0.01%
  - 延迟: <1ms (内网)
  - 错误率: 0
```

---

## vSAN性能调优

```yaml
vSAN性能优化要点:

1. 硬件配置:
  ✅ 缓存层使用NVMe SSD (不要用SATA SSD)
  ✅ 容量层使用SAS/SATA SSD或NVMe
  ✅ 每个磁盘组最多7块容量盘
  ✅ 网络至少10GbE (推荐25GbE)
  ✅ 使用HCL认证的硬件

2. 网络优化:
  ✅ 专用vSAN网络 (独立VLAN)
  ✅ 启用Jumbo Frame (MTU 9000)
  ✅ NIC绑定 (双10GbE or 双25GbE)
  ✅ LACP或负载均衡策略

3. 存储策略:
  - FTT=1 RAID1: 标准配置,50%空间开销
  - FTT=1 RAID5: 节省空间,需≥4节点
  - FTT=2: 高可用,但空间开销大

4. 性能特性:
  ✅ 启用去重压缩 (全闪存)
  ✅ 启用加密 (如需要,性能影响<5%)
  ✅ 使用对象空间预留 (关键VM)

5. 容量管理:
  ✅ 保持使用率<70% (最佳性能区间)
  ✅ 定期检查健康状态
  ✅ 及时重建故障磁盘
```

---

## 监控系统搭建

```yaml
推荐监控方案:

方案1: Prometheus + Grafana (开源,推荐)
  
  架构:
    VMware vSphere → vSphere Exporter → Prometheus → Grafana
    
  部署步骤:
    1. 部署Prometheus服务器
    2. 部署vSphere Exporter
    3. 配置Prometheus抓取指标
    4. 部署Grafana
    5. 导入vSphere仪表板
  
  监控指标:
    - 主机: CPU/内存/存储/网络
    - VM: 资源使用/性能指标
    - vSAN: 健康状态/性能
    - 告警: 自定义告警规则

方案2: VMware vRealize Operations (商业)
  
  特点:
    ✅ 深度集成vSphere
    ✅ 智能分析和预测
    ✅ 容量规划
    ✅ 故障根因分析
  
  缺点:
    ❌ 许可证成本高
    ❌ 资源占用较大

方案3: Zabbix (开源)
  
  特点:
    ✅ 功能全面
    ✅ 支持VMware监控
    ✅ 告警功能强大
  
  配置:
    - 使用VMware监控模板
    - 配置vCenter连接
    - 设置告警触发器

KVM监控:
  - Prometheus + libvirt-exporter
  - 或使用oVirt管理平台
  - 或Proxmox VE (Web界面)
```

---

## 性能基准测试

```bash
# 1. CPU性能测试

# sysbench
sysbench cpu --threads=8 --time=60 run

# 2. 内存性能测试

# sysbench内存测试
sysbench memory --threads=4 --memory-total-size=10G run

# 3. 存储性能测试

# fio - 最常用的存储测试工具

# 随机读IOPS测试
fio --name=randread --ioengine=libaio --direct=1 --bs=4k --iodepth=64 \
    --rw=randread --size=10G --numjobs=4 --runtime=60 --group_reporting

# 随机写IOPS测试
fio --name=randwrite --ioengine=libaio --direct=1 --bs=4k --iodepth=64 \
    --rw=randwrite --size=10G --numjobs=4 --runtime=60 --group_reporting

# 顺序读带宽测试
fio --name=seqread --ioengine=libaio --direct=1 --bs=1m --iodepth=32 \
    --rw=read --size=10G --runtime=60 --group_reporting

# 顺序写带宽测试
fio --name=seqwrite --ioengine=libaio --direct=1 --bs=1m --iodepth=32 \
    --rw=write --size=10G --runtime=60 --group_reporting

# 4. 网络性能测试

# iperf3带宽测试
# 服务器端:
iperf3 -s

# 客户端:
iperf3 -c server-ip -t 60 -P 10

# 5. 综合性能测试

# Geekbench (商业)
# SPECvirt (标准基准测试)
# VMmark (VMware官方)
```

---

**文档版本**: v1.0  
**创建时间**: 2025-10-19  
**适用范围**: VMware vSphere, KVM, 通用虚拟化平台  
**状态**: ✅ 生产就绪
