# 虚拟化平台故障排查与最佳实践

> **返回**: [虚拟化部署首页](README.md) | [部署指南首页](../00_索引导航/README.md)

---

## 📋 目录

- [虚拟化平台故障排查与最佳实践](#虚拟化平台故障排查与最佳实践)
  - [📋 目录](#-目录)
  - [故障排查方法论](#故障排查方法论)
  - [VMware vSphere常见问题](#vmware-vsphere常见问题)
    - [主机与连接问题](#主机与连接问题)
    - [虚拟机性能问题](#虚拟机性能问题)
    - [存储问题](#存储问题)
    - [网络问题](#网络问题)
    - [vSAN问题](#vsan问题)
  - [KVM常见问题](#kvm常见问题)
    - [虚拟化不可用](#虚拟化不可用)
    - [网络桥接问题](#网络桥接问题)
    - [性能问题](#性能问题)
  - [系统监控与日志分析](#系统监控与日志分析)
  - [灾难恢复场景](#灾难恢复场景)
  - [最佳实践汇总](#最佳实践汇总)

---

## 故障排查方法论

```yaml
标准排查流程 (5W1H方法):

1. What - 什么问题?
   - 准确描述问题现象
   - 问题影响范围
   - 业务影响程度
   示例: "web-server-01虚拟机无法访问，用户无法登录网站"

2. When - 何时发生?
   - 问题发生时间
   - 是否有规律 (间歇性/持续性)
   - 最近的变更时间点
   示例: "2025-10-19 14:30开始，问题持续至今"

3. Where - 在哪里?
   - 哪台主机/虚拟机
   - 哪个数据中心/集群
   - 哪个组件
   示例: "ESXi-02主机上的所有VM网络异常"

4. Who - 谁受影响?
   - 影响的用户/系统
   - 影响范围
   示例: "生产环境20台Web服务器，影响所有用户"

5. Why - 为什么发生?
   - 根本原因分析
   - 触发因素
   示例: "因为vSwitch配置错误导致网络中断"

6. How - 如何解决?
   - 临时解决方案 (快速恢复业务)
   - 永久解决方案 (根治问题)
   - 预防措施
   示例: "临时方案: 迁移VM到其他主机; 永久方案: 修复网络配置并添加监控"

故障排查工具箱:

VMware工具:
  - vCenter界面: 基本监控和告警
  - vSphere Client: 详细配置和日志
  - esxtop/resxtop: 实时性能监控
  - esxcli: 命令行管理
  - vSphere API: 自动化脚本
  - VMware Log Insight: 日志分析
  - vRealize Operations: 高级监控

Linux/KVM工具:
  - virsh: KVM管理命令
  - virt-top: 虚拟机资源监控
  - qemu-img: 磁盘镜像管理
  - journalctl: 系统日志查看
  - tcpdump: 网络抓包
  - strace: 系统调用跟踪
  - perf: 性能分析

网络工具:
  - ping: 连通性测试
  - traceroute: 路径追踪
  - mtr: 综合网络诊断
  - iperf3: 带宽测试
  - netstat/ss: 连接状态
  - tcpdump/wireshark: 抓包分析
  - nmap: 端口扫描

存储工具:
  - fio: 存储性能测试
  - iostat: I/O统计
  - iotop: I/O进程监控
  - smartctl: 磁盘健康检查
  - lsblk: 块设备列表
```

---

## VMware vSphere常见问题

### 主机与连接问题

**问题1: vCenter无法连接ESXi主机 (显示"未响应"或"已断开连接")**:

```yaml
现象:
  - vCenter中主机显示灰色/红色
  - 主机状态: "未响应" 或 "已断开连接"
  - 无法管理主机上的虚拟机

排查步骤:
  1. 检查网络连通性:
     # 从vCenter ping ESXi管理IP
     ping 192.168.10.11
     
     # 检查DNS解析
     nslookup esxi-01.company.local
     
     # 测试管理端口
     telnet 192.168.10.11 443
  
  2. 检查ESXi主机状态:
     # 直接访问ESXi Web界面
     https://192.168.10.11
     
     # 或SSH登录ESXi
     ssh root@192.168.10.11
     
     # 检查服务状态
     /etc/init.d/hostd status
     /etc/init.d/vpxa status
  
  3. 检查主机资源:
     # 查看CPU/内存使用
     esxtop
     # 按'c'查看CPU, 按'm'查看内存
     
     # 检查磁盘空间
     df -h
     # 特别注意根分区和日志分区
  
  4. 检查防火墙:
     # 查看防火墙规则
     esxcli network firewall ruleset list
     
     # 确保vSphereClient规则已启用
     esxcli network firewall ruleset set --enabled=true --ruleset-id=vSphereClient

常见原因及解决方案:
  原因1: 管理网络中断
    解决: 
      - 检查物理网线连接
      - 检查交换机端口状态
      - 检查VLAN配置
  
  原因2: hostd/vpxa服务异常
    解决:
      /etc/init.d/hostd restart
      /etc/init.d/vpxa restart
  
  原因3: 证书过期
    解决:
      # 重新生成证书
      /sbin/generate-certificates
      /etc/init.d/hostd restart
  
  原因4: 根分区满
    解决:
      # 清理旧日志
      find /var/log -name "*.gz" -mtime +30 -delete
      
      # 清理core dump
      rm -f /var/core/*
      
      # 重启syslog
      /etc/init.d/vmsyslogd restart
  
  原因5: ESXi hung (假死)
    解决:
      - 需要物理访问服务器
      - 检查iLO/iDRAC远程控制台
      - 可能需要硬重启 (最后手段)

预防措施:
  ✅ 配置管理网络冗余 (NIC绑定)
  ✅ 监控hostd/vpxa服务状态
  ✅ 定期清理日志 (配置日志轮转)
  ✅ 监控磁盘空间 (告警阈值80%)
  ✅ 配置NTP时间同步
```

**问题2: ESXi主机进入维护模式失败**:

```yaml
现象:
  - 执行"进入维护模式"卡住
  - 错误: "无法将虚拟机从主机迁移"
  - 某些VM迁移失败

排查步骤:
  1. 检查是否有VM阻止进入维护模式:
     # 查看主机上所有VM状态
     vim-cmd vmsvc/getallvms
     
     # 检查VM电源状态
     for vm in $(vim-cmd vmsvc/getallvms | awk 'NR>1 {print $1}'); do
       echo -n "VM ID $vm: "
       vim-cmd vmsvc/power.getstate $vm
     done
  
  2. 检查DRS是否启用:
     - vCenter → 集群 → 配置 → DRS
     - 确认DRS已启用且为"全自动"模式
  
  3. 检查vMotion网络:
     # 测试vMotion连通性
     vmkping -I vmk1 192.168.100.12
     vmkping -I vmk1 192.168.100.13
  
  4. 检查共享存储:
     # 确认所有主机可以访问同一数据存储
     esxcli storage filesystem list

解决方案:
  方案1: 手动迁移VM
    - 手动vMotion每个VM到其他主机
    - 或关闭非关键VM
  
  方案2: 强制进入维护模式
    # PowerCLI命令
    Get-VMHost esxi-01 | Set-VMHost -State Maintenance -Evacuate -Confirm:$false
  
  方案3: 如果有VM无法迁移
    原因: VM有本地ISO挂载
    解决: 断开CD/DVD连接
    
    原因: VM有USB设备直通
    解决: 移除USB设备
    
    原因: VM有PCIe直通设备
    解决: 关闭VM (无法热迁移)

预防措施:
  ✅ 避免使用本地ISO (使用共享存储)
  ✅ 限制PCIe直通的使用
  ✅ 确保DRS正常工作
  ✅ 维护模式前手动检查VM状态
```

### 虚拟机性能问题

**问题3: 虚拟机CPU性能差**:

```yaml
现象:
  - VM内应用响应慢
  - CPU使用率高
  - CPU Ready时间高

诊断步骤:
  1. 检查VM的CPU指标:
     # 在vCenter中查看VM的性能图表
     指标关注:
       - CPU使用率: 实际使用的CPU百分比
       - CPU Ready: 等待物理CPU的时间 (应<5%)
       - CPU Co-stop: 多vCPU等待调度 (应<3%)
  
  2. 使用esxtop分析:
     # SSH到ESXi主机
     esxtop
     # 按'c'进入CPU视图
     
     关键列:
       - %RDY: CPU Ready百分比 (高表示CPU争用)
       - %CSTP: Co-stop百分比 (高表示vCPU过多)
       - %MLMTD: 达到CPU限制 (检查是否设置了CPU限制)
  
  3. 检查VM配置:
     - vCPU数量是否过多? (一般不超过物理核心数)
     - 是否设置了CPU限制?
     - 是否设置了CPU预留?

原因分析及解决:
  原因1: CPU过量分配 (Over-commitment)
    现象: 主机CPU使用率>80%, 多个VM Ready高
    解决:
      - 迁移部分VM到其他主机
      - 启用DRS自动负载均衡
      - 增加物理主机
  
  原因2: VM分配vCPU过多
    现象: vCPU > 应用实际需求, Co-stop高
    解决:
      - 减少vCPU数量
      - 规则: 先分配少量vCPU, 监控后按需增加
      - 单线程应用分配1-2 vCPU即可
    
    示例:
      # PowerCLI减少vCPU
      Get-VM "web-server-01" | Set-VM -NumCpu 4 -Confirm:$false
  
  原因3: CPU限制设置不当
    现象: %MLMTD高
    解决:
      # 移除CPU限制
      Get-VM "web-server-01" | Get-VMResourceConfiguration | Set-VMResourceConfiguration -CpuLimitMhz $null
  
  原因4: NUMA配置不当
    现象: 大内存VM跨NUMA节点
    解决:
      - 启用vNUMA: VM设置 → VM选项 → 高级 → vNUMA
      - 确保VM适合单个NUMA节点
      - 或设置NUMA亲和性

CPU性能优化最佳实践:
  ✅ vCPU分配保守原则 (从少开始)
  ✅ 避免过量分配 (总vCPU:pCPU < 4:1)
  ✅ 监控CPU Ready (<5%为健康)
  ✅ 关键VM设置CPU预留
  ✅ 大VM (>8 vCPU) 启用CPU热添加
  ✅ 使用DRS自动平衡负载
```

**问题4: 虚拟机内存性能问题**:

```yaml
现象:
  - VM运行缓慢
  - 应用频繁swap
  - 内存气球(Balloon)膨胀

诊断步骤:
  1. 检查VM内存指标:
     vCenter指标:
       - 已使用内存: 实际消耗
       - 活动内存: 活跃工作集
       - 已消耗内存: ESXi视角的内存使用
       - Balloon: 内存气球大小
       - Swapped: 被swap到磁盘的内存
  
  2. 使用esxtop检查:
     esxtop
     # 按'm'进入内存视图
     
     关键列:
       - MCTLSZ: Balloon大小 (MB)
       - SWCUR: Swap使用量 (MB)
       - SWR/s, SWW/s: Swap读写速率
       - %ACTV: 活动内存百分比
  
  3. 检查主机内存压力:
     # 查看主机内存状态
     esxcli system stats vmmeminfo get
     
     # 检查内存状态
     #   High: 充足
     #   Soft: 轻度压力 (启用Balloon)
     #   Hard: 重度压力 (使用Swap)
     #   Low: 严重不足

原因分析及解决:
  原因1: 内存过量分配
    现象: 主机内存不足, 大量Balloon/Swap
    解决:
      - 迁移VM到其他主机
      - 增加物理内存
      - 减少VM数量
  
  原因2: VM内存分配不足
    现象: VM内部swap频繁, 但ESXi侧正常
    解决:
      # 增加VM内存
      Get-VM "db-server-01" | Set-VM -MemoryGB 64 -Confirm:$false
      
      # 或启用内存热添加
      Get-VM "db-server-01" | Get-AdvancedSetting -Name "Mem.HotAddEnabled" | Set-AdvancedSetting -Value "true"
  
  原因3: 内存预留过高
    现象: 无法启动新VM, 内存充足但预留满
    解决:
      # 减少或取消内存预留
      Get-VM "test-vm-*" | Get-VMResourceConfiguration | Set-VMResourceConfiguration -MemReservationMB 0
  
  原因4: TPS (透明页共享) 禁用
    现象: 多个相似VM内存使用高
    说明: ESXi 6.0+默认禁用TPS (安全原因)
    解决: 
      # 如果信任环境, 可启用TPS
      Get-VMHost | Get-AdvancedSetting -Name "Mem.ShareForceSalting" | Set-AdvancedSetting -Value 0

内存优化最佳实践:
  ✅ 内存分配略高于实际需求
  ✅ 避免过度承诺 (预留10-20%缓冲)
  ✅ 关键VM设置内存预留
  ✅ 监控Balloon/Swap (应为0或极小)
  ✅ 测试VM设置内存限制 (防止占用过多)
  ✅ 使用内存热添加 (大型VM)
```

### 存储问题

**问题5: 存储延迟高**:

```yaml
现象:
  - VM磁盘I/O慢
  - 应用数据库响应慢
  - 存储延迟告警

诊断步骤:
  1. 检查存储性能指标:
     vCenter → 数据存储 → 监控器 → 性能
     
     关键指标:
       - 读取延迟: 应<20ms (SSD <5ms)
       - 写入延迟: 应<20ms (SSD <5ms)
       - IOPS: 根据存储类型
       - 吞吐量: MB/s
  
  2. 使用esxtop分析:
     esxtop
     # 按'u'进入磁盘视图
     
     关键列:
       - DAVG/cmd: 设备平均延迟 (ms)
       - KAVG/cmd: VMkernel延迟 (ms)
       - GAVG/cmd: Guest OS感知的延迟 (ms)
       - CMDS/s: 每秒命令数 (IOPS)
  
  3. 检查存储路径:
     # 查看存储路径状态
     esxcli storage core path list
     
     # 检查多路径状态
     esxcli storage core path list | grep "State"
     # 所有路径应为"active"
  
  4. 检查存储队列:
     # 查看HBA队列深度
     esxcli storage core adapter list
     
     # 查看LUN队列深度
     esxcli storage core device list -d naa.xxx | grep "Queue Depth"

原因分析及解决:
  原因1: 存储设备性能瓶颈
    现象: DAVG高 (设备延迟高)
    解决:
      - 检查存储阵列性能 (联系存储管理员)
      - 优化RAID配置
      - 升级磁盘 (HDD → SSD → NVMe)
      - 增加缓存
  
  原因2: 多路径配置问题
    现象: 部分路径down, 单路径负载高
    解决:
      # 检查路径状态
      esxcli storage core path list | grep -E "(Path|State)"
      
      # 修复故障路径 (检查物理连接/HBA/交换机)
      
      # 配置多路径策略
      esxcli storage nmp device set --device naa.xxx --psp VMW_PSP_RR
  
  原因3: 队列深度不足
    现象: 高IOPS时延迟增加
    解决:
      # 增加HBA队列深度
      esxcli system module parameters set -m lpfc -p "lpfc_lun_queue_depth=128"
      
      # 增加磁盘队列深度
      esxcli storage core device set --device naa.xxx --queue-full-sample-size 64 --queue-full-threshold 32
  
  原因4: VM磁盘过度分配
    现象: 数据存储上VM过多
    解决:
      - 迁移部分VM到其他存储
      - Storage vMotion分散负载
      - 启用Storage DRS自动平衡
  
  原因5: 快照链过长
    现象: 有快照的VM性能明显下降
    解决:
      # 检查VM快照
      Get-VM | Get-Snapshot
      
      # 删除不需要的快照
      Get-VM "web-server-01" | Get-Snapshot | Remove-Snapshot -Confirm:$false
      
      # 合并快照 (自动)
      快照删除后会自动合并,可能需要几小时

存储性能优化最佳实践:
  ✅ 使用SSD/NVMe存储虚拟化环境
  ✅ 配置多路径 (至少双路径)
  ✅ 使用Round Robin多路径策略
  ✅ 启用Storage DRS自动平衡
  ✅ 避免快照长时间保留 (>24小时)
  ✅ 分离高I/O VM到独立LUN
  ✅ 使用VMFS6 (支持4K原生)
  ✅ 启用Jumbo Frame (存储网络)
```

### 网络问题

**问题6: 虚拟机网络不通**:

```yaml
现象:
  - VM无法ping通
  - 网络连接断开
  - 应用无法访问

诊断步骤:
  1. 检查VM网络配置:
     vCenter → VM → 编辑设置 → 网络适配器
     
     检查项:
       - 网络适配器已连接 ✅
       - 正确的端口组
       - MAC地址正确分配
  
  2. 检查端口组配置:
     vCenter → 网络 → 分布式端口组
     
     检查项:
       - VLAN ID正确
       - 端口组策略正常
       - 绑定正常
  
  3. 检查物理网卡状态:
     # ESXi命令
     esxcli network nic list
     
     # 检查网卡Link状态
     esxcli network nic get -n vmnic0
     # Link Status应为Up
  
  4. 在VM内部检查:
     # Linux VM
     ip addr show
     ip route show
     ping 网关
     
     # Windows VM
     ipconfig /all
     route print
     ping 网关

原因分析及解决:
  原因1: VLAN配置错误
    现象: VM可以ping同VLAN, 无法跨VLAN
    解决:
      # 检查VLAN ID
      Get-VDPortgroup | Select Name, VlanConfiguration
      
      # 修改VLAN
      Get-VDPortgroup "PG-Production" | Set-VDPortgroup -VlanId 20
  
  原因2: 上行链路故障
    现象: 多个VM同时网络中断
    解决:
      # 检查vSwitch上行链路
      esxcli network vswitch standard uplink list
      
      # 检查物理网络
      - 检查网线连接
      - 检查交换机端口状态
      - 检查LACP/绑定配置
  
  原因3: 防火墙规则阻止
    现象: 特定端口无法访问
    解决:
      # ESXi防火墙
      esxcli network firewall ruleset list
      esxcli network firewall ruleset set --ruleset-id httpClient --enabled true
      
      # VM内防火墙
      # Linux
      firewall-cmd --list-all
      firewall-cmd --add-service=http --permanent
      
      # Windows
      Get-NetFirewallRule
      New-NetFirewallRule -DisplayName "Allow HTTP" -Direction Inbound -LocalPort 80 -Protocol TCP -Action Allow
  
  原因4: NIC绑定故障转移
    现象: 网络间歇性中断
    解决:
      # 检查绑定策略
      Get-VDSwitch | Get-VDUplinkTeamingPolicy
      
      # 检查网卡故障转移顺序
      Get-VDPortgroup | Get-VDUplinkTeamingPolicy
      
      # 测试故障转移
      # 临时禁用一个uplink观察是否自动切换
  
  原因5: MTU不匹配
    现象: ping小包成功, 大包失败
    解决:
      # 检查MTU
      esxcli network vswitch standard list
      
      # 设置Jumbo Frame
      esxcli network vswitch standard set -v vSwitch0 -m 9000
      
      # VM内也要设置
      # Linux
      ip link set eth0 mtu 9000
      
      # 测试
      ping -M do -s 8972 目标IP

网络故障排查命令汇总:
  # 测试连通性
  vmkping -I vmk0 192.168.10.1
  
  # 查看MAC地址表
  esxcli network vm list
  
  # 查看端口统计
  esxcli network vswitch standard port list
  
  # 抓包分析
  pktcap-uw --uplink vmnic0 --capture UplinkSndKernel
  
  # 查看vSwitch配置
  esxcfg-vswitch -l

网络配置最佳实践:
  ✅ 使用分布式交换机 (dVSwitch)
  ✅ 配置NIC绑定 (至少双网卡)
  ✅ 分离流量 (管理/vMotion/vSAN/VM)
  ✅ 启用Jumbo Frame (存储/vMotion网络)
  ✅ 配置网络I/O控制 (NIOC)
  ✅ 定期检查物理连接
  ✅ 监控网络错误和丢包
```

### vSAN问题

**问题7: vSAN集群健康问题**:

```yaml
现象:
  - vSAN运行状况显示警告/错误
  - 性能下降
  - 数据无法访问

诊断步骤:
  1. 检查vSAN健康状态:
     vCenter → vSAN集群 → 监控器 → vSAN → 运行状况
     
     检查所有类别:
       - 网络
       - 物理磁盘
       - 集群
       - 数据
       - 限制
  
  2. 检查vSAN磁盘状态:
     # SSH到ESXi
     esxcli vsan storage list
     
     # 检查磁盘组
     esxcli vsan storage list | grep -E "(Group|Disk)"
  
  3. 检查vSAN网络:
     # 测试vSAN网络连通性
     esxcli vsan network list
     
     # Ping其他节点的vSAN接口
     vmkping -I vmk2 192.168.200.12
  
  4. 检查vSAN容量:
     vCenter → vSAN集群 → 监控器 → 容量
     
     检查:
       - 可用容量
       - 使用率
       - 去重/压缩比

常见问题及解决:
  问题1: vSAN网络分区
    现象: "vSAN网络分区"警告
    原因: vSAN主机间网络不通
    解决:
      # 检查所有主机vSAN网络连通性
      for host in esxi-{01..03}; do
        ssh $host "vmkping -I vmk2 192.168.200.11"
        ssh $host "vmkping -I vmk2 192.168.200.12"
        ssh $host "vmkping -I vmk2 192.168.200.13"
      done
      
      # 检查防火墙
      esxcli network firewall ruleset list | grep vSAN
      
      # 检查VLAN配置
      # 确保vSAN VMkernel端口在正确的VLAN
  
  问题2: 磁盘故障
    现象: "物理磁盘运行状况"错误
    解决:
      # 识别故障磁盘
      esxcli storage core device list | grep -B 5 "Degraded"
      
      # 查看SMART信息
      esxcli storage core device smart get -d naa.xxx
      
      # 从磁盘组中移除故障盘
      esxcli vsan storage remove -d naa.xxx
      
      # 物理更换磁盘后重新添加
      esxcli vsan storage add -d naa.xxx -s cache  # 或-s capacity
  
  问题3: 对象无法访问
    现象: VM无法启动/访问
    解决:
      # 检查对象健康状态
      cmmds-tool find -t DOM_OBJECT -f 1
      
      # 修复不合规对象
      # vCenter → vSAN → 虚拟对象 → 修复不合规对象
      
      # 或使用RVC (Ruby vSphere Console)
      rvc Administrator@vsphere.local@vcenter.company.local
      > vsan.resync_dashboard /dc1/computers/cluster1
  
  问题4: 容量不足
    现象: "vSAN容量"警告, 使用率>80%
    解决:
      # 短期方案:
      - 删除不需要的快照
      - 删除/迁移非关键VM
      - 启用去重压缩 (如未启用)
      
      # 长期方案:
      - 添加容量磁盘到现有磁盘组
      - 添加新的磁盘组
      - 添加新的主机到集群
      
      # 添加磁盘示例
      esxcli vsan storage add -d naa.xxx -s capacity
  
  问题5: 性能下降
    现象: 读写延迟增加
    排查:
      # 检查vSAN性能
      vCenter → vSAN → 性能 → VM
      
      # 检查磁盘性能
      vCenter → vSAN → 性能 → 物理磁盘
      
      # 使用vsan.check_limits检查配置限制
      # RVC命令
      vsan.check_limits /dc1/computers/cluster1
    
    解决:
      - 确保缓存层使用NVMe SSD
      - 确保10GbE及以上网络
      - 减少每个磁盘组的容量盘数量 (<7块)
      - 启用去重压缩 (减少I/O)
      - 调整存储策略 (降低FTT)

vSAN维护最佳实践:
  ✅ 使用HCL认证的硬件
  ✅ 缓存层使用NVMe SSD
  ✅ vSAN网络至少10GbE (推荐25GbE)
  ✅ 启用Jumbo Frame (MTU 9000)
  ✅ 定期检查健康状态
  ✅ 监控容量 (保持<70%)
  ✅ 定期更新固件和驱动
  ✅ 使用见证主机 (延伸集群)
```

---

## KVM常见问题

### 虚拟化不可用

**问题8: KVM模块加载失败**:

```bash
# 现象:
modprobe: ERROR: could not insert 'kvm_intel': Operation not supported

# 排查步骤:
# 1. 检查CPU是否支持虚拟化
grep -E '(vmx|svm)' /proc/cpuinfo
# Intel CPU应有'vmx', AMD应有'svm'

# 2. 检查BIOS是否启用虚拟化
# Intel: VT-x应该Enabled
# AMD: AMD-V应该Enabled
# 物理访问服务器进入BIOS检查

# 3. 检查KVM模块
lsmod | grep kvm
# 应该看到kvm和kvm_intel(或kvm_amd)

# 4. 手动加载模块
# Intel
modprobe kvm
modprobe kvm_intel

# AMD
modprobe kvm
modprobe kvm_amd

# 5. 检查dmesg错误
dmesg | grep kvm

# 解决方案:
# 如果BIOS禁用:
#   1. 重启服务器
#   2. 进入BIOS设置
#   3. 启用VT-x/AMD-V和VT-d/AMD-Vi
#   4. 保存并重启

# 如果是嵌套虚拟化:
# 在宿主机启用嵌套虚拟化
echo "options kvm_intel nested=1" > /etc/modprobe.d/kvm-nested.conf
modprobe -r kvm_intel
modprobe kvm_intel
# 验证
cat /sys/module/kvm_intel/parameters/nested
# 应显示'Y'
```

### 网络桥接问题

**问题9: VM无法通过网桥访问外网**:

```bash
# 现象:
# VM可以ping宿主机, 但无法ping网关或外网

# 排查步骤:
# 1. 检查网桥配置
ip link show br0
brctl show br0

# 2. 检查物理接口是否已加入网桥
brctl show
# 应该看到物理接口(如enp1s0)在br0下

# 3. 检查IP转发
sysctl net.ipv4.ip_forward
# 应该是1

# 4. 检查iptables规则
iptables -L -n -v
# 确保没有DROP规则阻止FORWARD链

# 5. 检查网桥STP
brctl showstp br0

# 常见问题及解决:
# 问题1: IP转发未启用
sysctl -w net.ipv4.ip_forward=1
echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.conf

# 问题2: iptables FORWARD链阻止
iptables -I FORWARD -m physdev --physdev-is-bridged -j ACCEPT
# 持久化
iptables-save > /etc/sysconfig/iptables

# 问题3: 网桥过滤
echo 0 > /proc/sys/net/bridge/bridge-nf-call-iptables
echo 0 > /proc/sys/net/bridge/bridge-nf-call-ip6tables

# 持久化
cat >> /etc/sysctl.conf << EOF
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-ip6tables = 0
EOF

# 问题4: NetworkManager干扰
# 禁止NetworkManager管理网桥
cat >> /etc/NetworkManager/NetworkManager.conf << EOF
[keyfile]
unmanaged-devices=interface-name:br0
EOF
systemctl restart NetworkManager

# 完整重建网桥示例:
# 1. 停止NetworkManager (可选)
systemctl stop NetworkManager

# 2. 删除旧网桥
ip link set br0 down
brctl delbr br0

# 3. 创建新网桥
brctl addbr br0
brctl addif br0 enp1s0

# 4. 配置IP (如果需要宿主机通信)
ip addr add 192.168.10.5/24 dev br0
ip link set br0 up
ip link set enp1s0 up

# 5. 设置默认路由
ip route add default via 192.168.10.1

# 6. 持久化配置 (Rocky Linux)
cat > /etc/sysconfig/network-scripts/ifcfg-br0 << EOF
DEVICE=br0
TYPE=Bridge
BOOTPROTO=static
IPADDR=192.168.10.5
NETMASK=255.255.255.0
GATEWAY=192.168.10.1
DNS1=192.168.10.53
ONBOOT=yes
DELAY=0
EOF

cat > /etc/sysconfig/network-scripts/ifcfg-enp1s0 << EOF
DEVICE=enp1s0
TYPE=Ethernet
BOOTPROTO=none
ONBOOT=yes
BRIDGE=br0
EOF

# 重启网络
systemctl restart NetworkManager
# 或
nmcli connection reload
nmcli connection up br0
```

### 性能问题

**问题10: KVM虚拟机I/O性能差**:

```bash
# 现象:
# VM内磁盘读写慢

# 诊断:
# 1. 检查磁盘驱动类型
virsh dumpxml vm-name | grep disk -A 10
# 应使用virtio

# 2. 检查磁盘缓存模式
# 查看当前配置
virsh dumpxml vm-name | grep cache

# 3. 在VM内测试性能
# VM内执行
fio --name=randwrite --ioengine=libaio --direct=1 --bs=4k --iodepth=64 --rw=randwrite --size=1G

# 优化方案:
# 方案1: 使用virtio驱动
virsh edit vm-name
# 修改disk部分:
<disk type='file' device='disk'>
  <driver name='qemu' type='qcow2' cache='none' io='native'/>
  <source file='/var/lib/libvirt/images/vm-name.qcow2'/>
  <target dev='vda' bus='virtio'/>
</disk>

# 方案2: 使用原始块设备 (Raw device)
# 性能最佳,但不支持快照
qemu-img convert -f qcow2 -O raw vm.qcow2 vm.raw

virsh edit vm-name
<disk type='file' device='disk'>
  <driver name='qemu' type='raw' cache='none' io='native'/>
  <source file='/var/lib/libvirt/images/vm.raw'/>
  <target dev='vda' bus='virtio'/>
</disk>

# 方案3: 使用LVM (最佳性能)
# 创建LV
lvcreate -L 50G -n vm-disk vg_storage

virsh edit vm-name
<disk type='block' device='disk'>
  <driver name='qemu' type='raw' cache='none' io='native'/>
  <source dev='/dev/vg_storage/vm-disk'/>
  <target dev='vda' bus='virtio'/>
</disk>

# 方案4: 启用io=threads
# 提升并发I/O性能
<driver name='qemu' type='qcow2' cache='none' io='threads' iothread='1'/>

# 并在domain级别定义iothread:
<domain type='kvm'>
  <iothreads>4</iothreads>
  ...
</domain>

# 方案5: 使用Ceph RBD (分布式存储)
<disk type='network' device='disk'>
  <driver name='qemu' type='raw' cache='writeback'/>
  <source protocol='rbd' name='pool/image'>
    <host name='ceph-mon-01' port='6789'/>
    <host name='ceph-mon-02' port='6789'/>
    <host name='ceph-mon-03' port='6789'/>
  </source>
  <auth username='libvirt'>
    <secret type='ceph' uuid='uuid-here'/>
  </auth>
  <target dev='vda' bus='virtio'/>
</disk>

# 缓存模式对比:
# cache='none': 直接I/O, 最安全但性能较低
# cache='writeback': 写回缓存, 性能最好但断电可能丢数据
# cache='writethrough': 写穿缓存, 平衡性能和安全
# 推荐: 数据库用none, 应用服务器用writeback+UPS

# 重启VM应用配置
virsh shutdown vm-name
virsh start vm-name
```

---

## 系统监控与日志分析

```yaml
VMware监控关键指标:

主机级别:
  CPU:
    - CPU使用率: <80%
    - CPU Ready: <5%
    - CPU Co-stop: <3%
  
  内存:
    - 内存使用率: <90%
    - Balloon: <10%
    - Swap: 0
    - Compressed: <10%
  
  存储:
    - 数据存储延迟: <20ms
    - IOPS: 根据存储类型
    - 队列深度: <32
  
  网络:
    - 丢包率: <0.01%
    - 错误率: <0.01%
    - 带宽使用率: <70%

VM级别:
  - 与主机级别类似
  - 额外关注Guest OS内部指标

日志位置:
  ESXi:
    - /var/log/vmkernel.log: 内核日志
    - /var/log/hostd.log: 主机守护进程
    - /var/log/vpxa.log: vCenter代理
    - /var/log/vmware.log: VM运行日志
    - /var/log/shell.log: Shell访问日志
  
  vCenter (VCSA):
    - /var/log/vmware/vpxd/: vCenter Server
    - /var/log/vmware/vsan-health/: vSAN健康
    - /var/log/vmware/vsphere-ui/: vSphere Client

常用日志分析命令:
  # 搜索错误
  grep -i error /var/log/vmkernel.log
  
  # 查看最近100行
  tail -100 /var/log/hostd.log
  
  # 实时监控
  tail -f /var/log/vmkernel.log
  
  # 查找特定VM日志
  grep "VM-Name" /var/log/hostd.log
  
  # 分析存储延迟
  grep "SCSI sense" /var/log/vmkernel.log

KVM监控:
  工具:
    - virt-top: 实时监控
    - virsh domstats: VM统计信息
    - qemu-img info: 磁盘镜像信息
    - prometheus node-exporter: 导出指标
  
  日志:
    - journalctl -u libvirtd: libvirt日志
    - /var/log/libvirt/qemu/: QEMU VM日志
    - dmesg: 内核消息
  
  监控示例:
    # 实时监控所有VM
    virt-top
    
    # 获取VM统计
    virsh domstats vm-name
    
    # 查看VM console日志
    virsh console vm-name
    
    # 检查VM定义
    virsh dumpxml vm-name
```

---

## 灾难恢复场景

**场景1: 主机完全故障**:

```yaml
情况: ESXi主机硬件故障, 无法启动

如果启用vSphere HA:
  1. HA自动检测主机故障 (30-60秒)
  2. 在其他主机重启VM (2-5分钟)
  3. 业务自动恢复
  
  无需人工干预 ✅

如果未启用HA:
  1. 确认主机确实故障 (非网络问题)
  2. 手动在其他主机注册VM:
     # 找到VM的vmx文件
     # 存储在共享存储上
     
     # 添加到清单
     vim-cmd solo/registervm /vmfs/volumes/datastore1/VM-Name/VM-Name.vmx
     
     # 启动VM
     vim-cmd vmsvc/power.on <vmid>
  
  3. 修复故障主机
  4. 重新加入集群

场景2: 数据存储故障**

情况: 共享存储LUN故障, 多个VM无法访问

立即响应:
  1. 确认故障范围
     - 哪些VM受影响?
     - 数据存储是否完全不可访问?
  
  2. 如果有备份:
     # 从最近备份恢复关键VM
     # 使用Veeam/Commvault等工具
     
     # Veeam示例:
     # 找到备份作业
     Get-VBRJob -Name "Production-Backup"
     
     # 恢复VM
     Start-VBRRestoreVM -RestorePoint $rp -Server $vCenter
  
  3. 如果无备份但存储可修复:
     - 联系存储厂商紧急修复
     - 恢复LUN访问
     - 重新扫描存储
       esxcli storage core adapter rescan --all
  
  4. 长期方案:
     - 实施3-2-1备份策略
     - 使用复制/快照
     - 考虑vSAN或分布式存储

场景3: vCenter故障

情况: vCenter服务器故障

影响:
  ❌ 无法通过vCenter管理
  ✅ 运行中的VM不受影响
  ✅ 可以直接连接ESXi主机管理

应对:
  1. 临时管理:
     # 直接连接ESXi Host Client
     https://esxi-ip/ui
     
     # 或使用PowerCLI直连
     Connect-VIServer -Server esxi-01.company.local
  
  2. 恢复vCenter:
     方案A: 从备份恢复VCSA
       - 使用vCenter备份文件
       - 部署新VCSA并恢复配置
     
     方案B: 部署新vCenter
       - 全新部署VCSA
       - 重新添加所有主机
       - 重新配置集群/网络/存储
       (耗时,不推荐)
  
  3. 预防:
     - 启用vCenter文件级别备份
     - 定期测试恢复流程
     - 考虑vCenter HA (需vCenter 6.5+)
```

---

## 最佳实践汇总

```yaml
架构设计:
  ✅ N+1冗余: 集群至少3台主机, 可容忍1台故障
  ✅ 资源池: 按业务划分资源池, 隔离资源
  ✅ DRS规则: 关键VM分散到不同主机
  ✅ 反亲和性: 同一应用的多个实例分散部署

容量规划:
  ✅ CPU超额 2-4:1 (保守环境2:1, 一般4:1)
  ✅ 内存超额 1.2-1.5:1 (保留20-30%缓冲)
  ✅ 存储使用率 <70% (vSAN <75%)
  ✅ 网络带宽 <70% 峰值

性能优化:
  ✅ 使用VMXNET3网卡 (VMware)
  ✅ 使用virtio驱动 (KVM)
  ✅ 使用PVSCSI控制器
  ✅ 使用SSD/NVMe存储
  ✅ 启用Jumbo Frame
  ✅ 配置多路径

高可用:
  ✅ 启用vSphere HA (准入控制)
  ✅ 启用VM监控
  ✅ 配置隔离响应
  ✅ 配置心跳数据存储

数据保护:
  ✅ 每日增量备份
  ✅ 每周全备份
  ✅ 异地备份副本
  ✅ 定期测试恢复
  ✅ 3-2-1规则: 3份数据, 2种介质, 1份异地

安全加固:
  ✅ 禁用不必要的服务
  ✅ 定期打安全补丁
  ✅ 使用AD/LDAP集中认证
  ✅ 启用SSH密钥认证
  ✅ 配置防火墙规则
  ✅ 启用审计日志
  ✅ 网络隔离 (VLAN)

监控告警:
  ✅ 监控所有关键指标
  ✅ 设置合理告警阈值
  ✅ 配置告警通知 (邮件/短信/微信)
  ✅ 定期review告警
  ✅ 建立告警处理SOP

运维管理:
  ✅ 标准化VM模板
  ✅ 统一命名规范
  ✅ 文档化所有配置
  ✅ 变更管理流程
  ✅ 定期巡检
  ✅ 演练容灾流程

自动化:
  ✅ 使用PowerCLI/Ansible自动化日常任务
  ✅ IaC (Terraform等)
  ✅ CI/CD集成
  ✅ 自动化备份
  ✅ 自动化补丁

合规性:
  ✅ 符合等保要求 (如适用)
  ✅ 数据加密 (静态+传输)
  ✅ 访问控制 (RBAC)
  ✅ 审计日志保留
  ✅ 定期安全审计
```

---

**文档版本**: v1.0  
**创建时间**: 2025-10-19  
**适用范围**: VMware vSphere, KVM, 通用虚拟化平台  
**状态**: ✅ 生产就绪
