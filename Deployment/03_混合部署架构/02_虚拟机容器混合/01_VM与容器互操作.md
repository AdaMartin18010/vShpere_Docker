# VM与容器互操作

> **返回**: [虚拟机容器混合首页](README.md) | [混合部署架构首页](../README.md) | [部署指南首页](../../00_索引导航/README.md)

---

## 📋 目录

- [VM与容器互操作](#vm与容器互操作)
  - [📋 目录](#-目录)
  - [1. 网络互通方案](#1-网络互通方案)
  - [2. 存储共享方案](#2-存储共享方案)
  - [3. 服务发现](#3-服务发现)
  - [4. 统一监控](#4-统一监控)

---

## 1. 网络互通方案

**方案1: Calico BGP模式**:

```yaml
# Calico与物理网络BGP对等
apiVersion: projectcalico.org/v3
kind: BGPConfiguration
metadata:
  name: default
spec:
  logSeverityScreen: Info
  nodeToNodeMeshEnabled: false
  asNumber: 64512
---
# BGP Peer配置 (与VM网络路由器对等)
apiVersion: projectcalico.org/v3
kind: BGPPeer
metadata:
  name: vm-network-peer
spec:
  peerIP: 172.16.0.1  # VM网络网关
  asNumber: 65000
```

**方案2: VXLAN Overlay**:

```yaml
# Calico VXLAN模式
kind: IPPool
apiVersion: projectcalico.org/v3
metadata:
  name: default-ipv4-ippool
spec:
  cidr: 172.16.16.0/20
  vxlanMode: Always
  natOutgoing: true
```

**VM访问容器服务**:

```yaml
# 通过NodePort暴露服务
apiVersion: v1
kind: Service
metadata:
  name: app
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30080
  selector:
    app: myapp
# VM通过 http://<any-k8s-node>:30080 访问
```

---

## 2. 存储共享方案

**方案1: NFS共享存储**:

```yaml
# Kubernetes使用NFS (VM提供)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
  - ReadWriteMany
  nfs:
    server: 172.16.1.10  # VM上的NFS服务器
    path: /exports/data
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
```

**方案2: S3兼容对象存储 (MinIO)**:

```yaml
# MinIO部署 (容器)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: minio
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: minio
        image: minio/minio:latest
        args:
        - server
        - /data
        - --console-address
        - ":9001"
        ports:
        - containerPort: 9000  # S3 API
        - containerPort: 9001  # Console
# VM通过S3 SDK访问MinIO
```

**方案3: Ceph统一存储**:

```text
Ceph集群 (物理机或VM)
  ├─ RBD → Kubernetes PV (块存储)
  ├─ CephFS → Kubernetes PV (文件存储)
  └─ RGW → VM应用 (对象存储，S3兼容)
```

---

## 3. 服务发现

**方案1: DNS集成**:

```yaml
# 容器服务DNS记录
# 自动创建: app.default.svc.cluster.local
# 配置VM DNS指向K8s CoreDNS

# VM服务注册到Kubernetes
apiVersion: v1
kind: Service
metadata:
  name: legacy-db
spec:
  type: ExternalName
  externalName: db.vm.local  # VM上的数据库DNS名
# 容器通过 legacy-db.default.svc.cluster.local 访问VM数据库
```

**方案2: Consul服务注册**:

```bash
# VM服务注册到Consul
curl -X PUT http://consul:8500/v1/agent/service/register \
  -d '{
    "ID": "legacy-app-1",
    "Name": "legacy-app",
    "Address": "172.16.1.100",
    "Port": 8080
  }'

# Kubernetes服务注册到Consul (consul-k8s)
kubectl apply -f consul-connect-inject.yaml
```

**方案3: Istio服务网格**:

```yaml
# VM服务注册到Istio
apiVersion: networking.istio.io/v1beta1
kind: WorkloadEntry
metadata:
  name: legacy-app-vm
spec:
  address: 172.16.1.100
  labels:
    app: legacy-app
    version: v1
  serviceAccount: legacy-sa
---
# Service定义 (VM+容器统一)
apiVersion: v1
kind: Service
metadata:
  name: legacy-app
spec:
  ports:
  - port: 8080
  selector:
    app: legacy-app
```

---

## 4. 统一监控

**Prometheus统一监控**:

```yaml
# VM监控 (Node Exporter)
# 在VM上安装
wget https://github.com/prometheus/node_exporter/releases/download/v1.6.1/node_exporter-1.6.1.linux-amd64.tar.gz
./node_exporter --web.listen-address=":9100"

# Prometheus配置抓取VM
scrape_configs:
- job_name: 'vm-nodes'
  static_configs:
  - targets: ['172.16.1.10:9100', '172.16.1.11:9100']
    labels:
      env: 'vm'
- job_name: 'kubernetes-pods'
  kubernetes_sd_configs:
  - role: pod
  relabel_configs:
  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
    action: keep
    regex: true
```

**统一日志收集 (ELK/Loki)**:

```yaml
# Filebeat部署在VM
filebeat.inputs:
- type: log
  paths:
    - /var/log/app/*.log
output.elasticsearch:
  hosts: ["elasticsearch.k8s.local:9200"]

# Promtail部署在Kubernetes
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: promtail
spec:
  template:
    spec:
      containers:
      - name: promtail
        image: grafana/promtail:2.9.0
        volumeMounts:
        - name: logs
          mountPath: /var/log
```

**统一告警 (Alertmanager)**:

```yaml
# Alertmanager规则
groups:
- name: hybrid_alerts
  rules:
  - alert: HighCPUUsage
    expr: node_cpu_seconds_total > 0.9
    labels:
      severity: warning
    annotations:
      summary: "High CPU on {{ $labels.instance }}"
      description: "CPU usage is above 90%"
```

---

**更新时间**: 2025-10-19  
**文档版本**: v1.0  
**状态**: ✅ 完成
